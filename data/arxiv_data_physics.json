[{"titles":"PhysHOI: Physics-Based Imitation of Dynamic Human-Object Interaction","summaries":"Humans interact with objects all the time. Enabling a humanoid to learn\nhuman-object interaction (HOI) is a key step for future smart animation and\nintelligent robotics systems. However, recent progress in physics-based HOI\nrequires carefully designed task-specific rewards, making the system unscalable\nand labor-intensive. This work focuses on dynamic HOI imitation: teaching\nhumanoid dynamic interaction skills through imitating kinematic HOI\ndemonstrations. It is quite challenging because of the complexity of the\ninteraction between body parts and objects and the lack of dynamic HOI data. To\nhandle the above issues, we present PhysHOI, the first physics-based whole-body\nHOI imitation approach without task-specific reward designs. Except for the\nkinematic HOI representations of humans and objects, we introduce the contact\ngraph to model the contact relations between body parts and objects explicitly.\nA contact graph reward is also designed, which proved to be critical for\nprecise HOI imitation. Based on the key designs, PhysHOI can imitate diverse\nHOI tasks simply yet effectively without prior knowledge. To make up for the\nlack of dynamic HOI scenarios in this area, we introduce the BallPlay dataset\nthat contains eight whole-body basketball skills. We validate PhysHOI on\ndiverse HOI tasks, including whole-body grasping and basketball skills.","terms":["cs.CV","cs.GR","cs.RO"]},{"titles":"Improved Efficient Two-Stage Denoising Diffusion Power System Measurement Recovery Against False Data Injection Attacks and Data Losses","summaries":"Measurement uncertainties, represented by cyber-attacks and data losses,\nseriously degrade the quality of power system measurements. Fortunately, the\npowerful generation ability of the denoising diffusion models can enable more\nprecise measurement generation for power system data recovery. However, the\ncontrollable data generation and efficient computing methods of denoising\ndiffusion models for deterministic trajectory still need further investigation.\nTo this end, this paper proposes an improved two-stage denoising diffusion\nmodel (TSDM) to identify and reconstruct the measurements with various\nmeasurement uncertainties. The first stage of the model comprises a\nclassifier-guided conditional anomaly detection component, while the second\nstage involves diffusion-based measurement imputation component. Moreover, the\nproposed TSDM adopts precise means and optimal variances to accelerate the\ndiffusion generation process with subsequence sampling. Extensive numerical\ncase studies demonstrate that the proposed TSDM can accurately recover power\nsystem measurements despite strong randomness under renewable energy\nintegration and highly nonlinear dynamics under complex cyber-physical\ncontingencies. Additionally, the proposed TSDM has stronger robustness compared\nto existing reconstruction networks and exhibits lower computational complexity\nthan general denoising diffusion models.","terms":["cs.LG","cs.CR"]},{"titles":"Surrogate Modelling for Sea Ice Concentration using Lightweight Neural Ensemble","summaries":"The modeling and forecasting of sea ice conditions in the Arctic region are\nimportant tasks for ship routing, offshore oil production, and environmental\nmonitoring. We propose the adaptive surrogate modeling approach named LANE-SI\n(Lightweight Automated Neural Ensembling for Sea Ice) that uses ensemble of\nrelatively simple deep learning models with different loss functions for\nforecasting of spatial distribution for sea ice concentration in the specified\nwater area. Experimental studies confirm the quality of a long-term forecast\nbased on a deep learning model fitted to the specific water area is comparable\nto resource-intensive physical modeling, and for some periods of the year, it\nis superior. We achieved a 20% improvement against the state-of-the-art\nphysics-based forecast system SEAS5 for the Kara Sea.","terms":["cs.LG","cs.AI","physics.ao-ph"]},{"titles":"Bayesian Methods for Media Mix Modelling with shape and funnel effects","summaries":"In recent years, significant progress in generative AI has highlighted the\nimportant role of physics-inspired models that utilize advanced mathematical\nconcepts based on fundamental physics principles to enhance artificial\nintelligence capabilities. Among these models, those based on diffusion\nequations have greatly improved image quality. This study aims to explore the\npotential uses of Maxwell-Boltzmann equation, which forms the basis of the\nkinetic theory of gases, and the Michaelis-Menten model in Marketing Mix\nModelling (MMM) applications. We propose incorporating these equations into\nHierarchical Bayesian models to analyse consumer behaviour in the context of\nadvertising. These equation sets excel in accurately describing the random\ndynamics in complex systems like social interactions and consumer-advertising\ninteractions.","terms":["cs.LG"]},{"titles":"SAMBA: A Trainable Segmentation Web-App with Smart Labelling","summaries":"Segmentation is the assigning of a semantic class to every pixel in an image\nand is a prerequisite for various statistical analysis tasks in materials\nscience, like phase quantification, physics simulations or morphological\ncharacterization. The wide range of length scales, imaging techniques and\nmaterials studied in materials science means any segmentation algorithm must\ngeneralise to unseen data and support abstract, user-defined semantic classes.\nTrainable segmentation is a popular interactive segmentation paradigm where a\nclassifier is trained to map from image features to user drawn labels. SAMBA is\na trainable segmentation tool that uses Meta's Segment Anything Model (SAM) for\nfast, high-quality label suggestions and a random forest classifier for robust,\ngeneralizable segmentations. It is accessible in the browser\n(https:\/\/www.sambasegment.com\/) without the need to download any external\ndependencies. The segmentation backend is run in the cloud, so does not require\nthe user to have powerful hardware.","terms":["cs.CV"]},{"titles":"Coherent energy and force uncertainty in deep learning force fields","summaries":"In machine learning energy potentials for atomic systems, forces are commonly\nobtained as the negative derivative of the energy function with respect to\natomic positions. To quantify aleatoric uncertainty in the predicted energies,\na widely used modeling approach involves predicting both a mean and variance\nfor each energy value. However, this model is not differentiable under the\nusual white noise assumption, so energy uncertainty does not naturally\ntranslate to force uncertainty. In this work we propose a machine learning\npotential energy model in which energy and force aleatoric uncertainty are\nlinked through a spatially correlated noise process. We demonstrate our\napproach on an equivariant messages passing neural network potential trained on\nenergies and forces on two out-of-equilibrium molecular datasets. Furthermore,\nwe also show how to obtain epistemic uncertainties in this setting based on a\nBayesian interpretation of deep ensemble models.","terms":["stat.ML","cs.LG","physics.comp-ph"]},{"titles":"Calibration in Machine Learning Uncertainty Quantification: beyond consistency to target adaptivity","summaries":"Reliable uncertainty quantification (UQ) in machine learning (ML) regression\ntasks is becoming the focus of many studies in materials and chemical science.\nIt is now well understood that average calibration is insufficient, and most\nstudies implement additional methods testing the conditional calibration with\nrespect to uncertainty, i.e. consistency. Consistency is assessed mostly by\nso-called reliability diagrams. There exists however another way beyond average\ncalibration, which is conditional calibration with respect to input features,\ni.e. adaptivity. In practice, adaptivity is the main concern of the final users\nof a ML-UQ method, seeking for the reliability of predictions and uncertainties\nfor any point in features space. This article aims to show that consistency and\nadaptivity are complementary validation targets, and that a good consistency\ndoes not imply a good adaptivity. Adapted validation methods are proposed and\nillustrated on a representative example.","terms":["stat.ML","cs.LG","physics.chem-ph","physics.data-an"]},{"titles":"Small Area Estimation of Case Growths for Timely COVID-19 Outbreak Detection","summaries":"The COVID-19 pandemic has exerted a profound impact on the global economy and\ncontinues to exact a significant toll on human lives. The COVID-19 case growth\nrate stands as a key epidemiological parameter to estimate and monitor for\neffective detection and containment of the resurgence of outbreaks. A\nfundamental challenge in growth rate estimation and hence outbreak detection is\nbalancing the accuracy-speed tradeoff, where accuracy typically degrades with\nshorter fitting windows. In this paper, we develop a machine learning (ML)\nalgorithm, which we call Transfer Learning Generalized Random Forest (TLGRF),\nthat balances this accuracy-speed tradeoff. Specifically, we estimate the\ninstantaneous COVID-19 exponential growth rate for each U.S. county by using\nTLGRF that chooses an adaptive fitting window size based on relevant day-level\nand county-level features affecting the disease spread. Through transfer\nlearning, TLGRF can accurately estimate case growth rates for counties with\nsmall sample sizes. Out-of-sample prediction analysis shows that TLGRF\noutperforms established growth rate estimation methods. Furthermore, we\nconducted a case study based on outbreak case data from the state of Colorado\nand showed that the timely detection of outbreaks could have been improved by\nup to 224% using TLGRF when compared to the decisions made by Colorado's\nDepartment of Health and Environment (CDPHE). To facilitate implementation, we\nhave developed a publicly available outbreak detection tool for timely\ndetection of COVID-19 outbreaks in each U.S. county, which received substantial\nattention from policymakers.","terms":["stat.ML","cs.LG","physics.soc-ph"]},{"titles":"Diffusion Illusions: Hiding Images in Plain Sight","summaries":"We explore the problem of computationally generating special `prime' images\nthat produce optical illusions when physically arranged and viewed in a certain\nway. First, we propose a formal definition for this problem. Next, we introduce\nDiffusion Illusions, the first comprehensive pipeline designed to automatically\ngenerate a wide range of these illusions. Specifically, we both adapt the\nexisting `score distillation loss' and propose a new `dream target loss' to\noptimize a group of differentially parametrized prime images, using a frozen\ntext-to-image diffusion model. We study three types of illusions, each where\nthe prime images are arranged in different ways and optimized using the\naforementioned losses such that images derived from them align with user-chosen\ntext prompts or images. We conduct comprehensive experiments on these illusions\nand verify the effectiveness of our proposed method qualitatively and\nquantitatively. Additionally, we showcase the successful physical fabrication\nof our illusions -- as they are all designed to work in the real world. Our\ncode and examples are publicly available at our interactive project website:\nhttps:\/\/diffusionillusions.com","terms":["cs.CV"]},{"titles":"Physical Symbolic Optimization","summaries":"We present a framework for constraining the automatic sequential generation\nof equations to obey the rules of dimensional analysis by construction.\nCombining this approach with reinforcement learning, we built $\\Phi$-SO, a\nPhysical Symbolic Optimization method for recovering analytical functions from\nphysical data leveraging units constraints. Our symbolic regression algorithm\nachieves state-of-the-art results in contexts in which variables and constants\nhave known physical units, outperforming all other methods on SRBench's Feynman\nbenchmark in the presence of noise (exceeding 0.1%) and showing resilience even\nin the presence of significant (10%) levels of noise.","terms":["cs.LG","astro-ph.IM","cs.SC","physics.comp-ph","physics.data-an"]},{"titles":"Autoencoders for discovering manifold dimension and coordinates in data from complex dynamical systems","summaries":"While many phenomena in physics and engineering are formally\nhigh-dimensional, their long-time dynamics often live on a lower-dimensional\nmanifold. The present work introduces an autoencoder framework that combines\nimplicit regularization with internal linear layers and $L_2$ regularization\n(weight decay) to automatically estimate the underlying dimensionality of a\ndata set, produce an orthogonal manifold coordinate system, and provide the\nmapping functions between the ambient space and manifold space, allowing for\nout-of-sample projections. We validate our framework's ability to estimate the\nmanifold dimension for a series of datasets from dynamical systems of varying\ncomplexities and compare to other state-of-the-art estimators. We analyze the\ntraining dynamics of the network to glean insight into the mechanism of\nlow-rank learning and find that collectively each of the implicit regularizing\nlayers compound the low-rank representation and even self-correct during\ntraining. Analysis of gradient descent dynamics for this architecture in the\nlinear case reveals the role of the internal linear layers in leading to faster\ndecay of a \"collective weight variable\" incorporating all layers, and the role\nof weight decay in breaking degeneracies and thus driving convergence along\ndirections in which no decay would occur in its absence. We show that this\nframework can be naturally extended for applications of state-space modeling\nand forecasting by generating a data-driven dynamic model of a spatiotemporally\nchaotic partial differential equation using only the manifold coordinates.\nFinally, we demonstrate that our framework is robust to hyperparameter choices.","terms":["cs.LG","nlin.CD"]},{"titles":"Towards Causal Representations of Climate Model Data","summaries":"Climate models, such as Earth system models (ESMs), are crucial for\nsimulating future climate change based on projected Shared Socioeconomic\nPathways (SSP) greenhouse gas emissions scenarios. While ESMs are sophisticated\nand invaluable, machine learning-based emulators trained on existing simulation\ndata can project additional climate scenarios much faster and are\ncomputationally efficient. However, they often lack generalizability and\ninterpretability. This work delves into the potential of causal representation\nlearning, specifically the \\emph{Causal Discovery with Single-parent Decoding}\n(CDSD) method, which could render climate model emulation efficient\n\\textit{and} interpretable. We evaluate CDSD on multiple climate datasets,\nfocusing on emissions, temperature, and precipitation. Our findings shed light\non the challenges, limitations, and promise of using CDSD as a stepping stone\ntowards more interpretable and robust climate model emulation.","terms":["cs.LG","cs.AI","physics.ao-ph","stat.ME"]},{"titles":"Physics Informed Neural Networks for Simulating Radiative Transfer","summaries":"We propose a novel machine learning algorithm for simulating radiative\ntransfer. Our algorithm is based on physics informed neural networks (PINNs),\nwhich are trained by minimizing the residual of the underlying radiative\ntranfer equations. We present extensive experiments and theoretical error\nestimates to demonstrate that PINNs provide a very easy to implement, fast,\nrobust and accurate method for simulating radiative transfer. We also present a\nPINN based algorithm for simulating inverse problems for radiative transfer\nefficiently.","terms":["cs.LG","stat.ML"]},{"titles":"On the variants of SVM methods applied to GPR data to classify tack coat characteristics in French pavements: two experimental case studies","summaries":"Among the commonly used non-destructive techniques, the Ground Penetrating\nRadar (GPR) is one of the most widely adopted today for assessing pavement\nconditions in France. However, conventional radar systems and their forward\nprocessing methods have shown their limitations for the physical and\ngeometrical characterization of very thin layers such as tack coats. However,\nthe use of Machine Learning methods applied to GPR with an inverse approach\nshowed that it was numerically possible to identify the tack coat\ncharacteristics despite masking effects due to low timefrequency resolution\nnoted in the raw B-scans. Thus, we propose in this paper to apply the inverse\napproach based on Machine Learning, already validated in previous works on\nnumerical data, on two experimental cases with different pavement structures.\nThe first case corresponds to a validation on known pavement structures on the\nGustave Eiffel University (Nantes, France) with its pavement fatigue carousel\nand the second case focuses on a new real road in Vend{\\'e}e department\n(France). In both case studies, the performances of SVM\/SVR methods showed the\nefficiency of supervised learning methods to classify and estimate the emulsion\nproportioning in the tack coats.","terms":["stat.ML","cs.LG"]},{"titles":"Neural parameter calibration and uncertainty quantification for epidemic forecasting","summaries":"The recent COVID-19 pandemic has thrown the importance of accurately\nforecasting contagion dynamics and learning infection parameters into sharp\nfocus. At the same time, effective policy-making requires knowledge of the\nuncertainty on such predictions, in order, for instance, to be able to ready\nhospitals and intensive care units for a worst-case scenario without needlessly\nwasting resources. In this work, we apply a novel and powerful computational\nmethod to the problem of learning probability densities on contagion parameters\nand providing uncertainty quantification for pandemic projections. Using a\nneural network, we calibrate an ODE model to data of the spread of COVID-19 in\nBerlin in 2020, achieving both a significantly more accurate calibration and\nprediction than Markov-Chain Monte Carlo (MCMC)-based sampling schemes. The\nuncertainties on our predictions provide meaningful confidence intervals e.g.\non infection figures and hospitalisation rates, while training and running the\nneural scheme takes minutes where MCMC takes hours. We show convergence of our\nmethod to the true posterior on a simplified SIR model of epidemics, and also\ndemonstrate our method's learning capabilities on a reduced dataset, where a\ncomplex model is learned from a small number of compartments for which data is\navailable.","terms":["cs.LG","math.OC","physics.soc-ph","49-02, 92-02, 68-02","J.3; G.1.6; I.2.1; G.3"]},{"titles":"Alchemist: Parametric Control of Material Properties with Diffusion Models","summaries":"We propose a method to control material attributes of objects like roughness,\nmetallic, albedo, and transparency in real images. Our method capitalizes on\nthe generative prior of text-to-image models known for photorealism, employing\na scalar value and instructions to alter low-level material properties.\nAddressing the lack of datasets with controlled material attributes, we\ngenerated an object-centric synthetic dataset with physically-based materials.\nFine-tuning a modified pre-trained text-to-image model on this synthetic\ndataset enables us to edit material properties in real-world images while\npreserving all other attributes. We show the potential application of our model\nto material edited NeRFs.","terms":["cs.CV","cs.AI","cs.GR"]},{"titles":"Uncertainty Quantification in Multivariable Regression for Material Property Prediction with Bayesian Neural Networks","summaries":"With the increased use of data-driven approaches and machine learning-based\nmethods in material science, the importance of reliable uncertainty\nquantification (UQ) of the predicted variables for informed decision-making\ncannot be overstated. UQ in material property prediction poses unique\nchallenges, including the multi-scale and multi-physics nature of advanced\nmaterials, intricate interactions between numerous factors, limited\navailability of large curated datasets for model training, etc. Recently,\nBayesian Neural Networks (BNNs) have emerged as a promising approach for UQ,\noffering a probabilistic framework for capturing uncertainties within neural\nnetworks. In this work, we introduce an approach for UQ within physics-informed\nBNNs, which integrates knowledge from governing laws in material modeling to\nguide the models toward physically consistent predictions. To evaluate the\neffectiveness of this approach, we present case studies for predicting the\ncreep rupture life of steel alloys. Experimental validation with three datasets\nof collected measurements from creep tests demonstrates the ability of BNNs to\nproduce accurate point and uncertainty estimates that are competitive or exceed\nthe performance of the conventional method of Gaussian Process Regression.\nSimilarly, we evaluated the suitability of BNNs for UQ in an active learning\napplication and reported competitive performance. The most promising framework\nfor creep life prediction is BNNs based on Markov Chain Monte Carlo\napproximation of the posterior distribution of network parameters, as it\nprovided more reliable results in comparison to BNNs based on variational\ninference approximation or related NNs with probabilistic outputs. The codes\nare available at:\nhttps:\/\/github.com\/avakanski\/Creep-uncertainty-quantification.","terms":["cs.LG","cond-mat.mtrl-sci","I.2.6"]},{"titles":"Realistic Scatterer Based Adversarial Attacks on SAR Image Classifiers","summaries":"Adversarial attacks have highlighted the vulnerability of classifiers based\non machine learning for Synthetic Aperture Radar (SAR) Automatic Target\nRecognition (ATR) tasks. An adversarial attack perturbs SAR images of on-ground\ntargets such that the classifiers are misled into making incorrect predictions.\nHowever, many existing attacking techniques rely on arbitrary manipulation of\nSAR images while overlooking the feasibility of executing the attacks on\nreal-world SAR imagery. Instead, adversarial attacks should be able to be\nimplemented by physical actions, for example, placing additional false objects\nas scatterers around the on-ground target to perturb the SAR image and fool the\nSAR ATR.\n  In this paper, we propose the On-Target Scatterer Attack (OTSA), a\nscatterer-based physical adversarial attack. To ensure the feasibility of its\nphysical execution, we enforce a constraint on the positioning of the\nscatterers. Specifically, we restrict the scatterers to be placed only on the\ntarget instead of in the shadow regions or the background. To achieve this, we\nintroduce a positioning score based on Gaussian kernels and formulate an\noptimization problem for our OTSA attack. Using a gradient ascent method to\nsolve the optimization problem, the OTSA can generate a vector of parameters\ndescribing the positions, shapes, sizes and amplitudes of the scatterers to\nguide the physical execution of the attack that will mislead SAR image\nclassifiers. The experimental results show that our attack obtains\nsignificantly higher success rates under the positioning constraint compared\nwith the existing method.","terms":["cs.CV"]},{"titles":"Attention-enhanced neural differential equations for physics-informed deep learning of ion transport","summaries":"Species transport models typically combine partial differential equations\n(PDEs) with relations from hindered transport theory to quantify\nelectromigrative, convective, and diffusive transport through complex\nnanoporous systems; however, these formulations are frequently substantial\nsimplifications of the governing dynamics, leading to the poor generalization\nperformance of PDE-based models. Given the growing interest in deep learning\nmethods for the physical sciences, we develop a machine learning-based approach\nto characterize ion transport across nanoporous membranes. Our proposed\nframework centers around attention-enhanced neural differential equations that\nincorporate electroneutrality-based inductive biases to improve generalization\nperformance relative to conventional PDE-based methods. In addition, we study\nthe role of the attention mechanism in illuminating physically-meaningful\nion-pairing relationships across diverse mixture compositions. Further, we\ninvestigate the importance of pre-training on simulated data from PDE-based\nmodels, as well as the performance benefits from hard vs. soft inductive\nbiases. Our results indicate that physics-informed deep learning solutions can\noutperform their classical PDE-based counterparts and provide promising avenues\nfor modelling complex transport phenomena across diverse applications.","terms":["cs.LG","math-ph","math.MP","physics.comp-ph"]},{"titles":"Expert-guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems","summaries":"Domain experts often possess valuable physical insights that are overlooked\nin fully automated decision-making processes such as Bayesian optimisation. In\nthis article we apply high-throughput (batch) Bayesian optimisation alongside\nanthropological decision theory to enable domain experts to influence the\nselection of optimal experiments. Our methodology exploits the hypothesis that\nhumans are better at making discrete choices than continuous ones and enables\nexperts to influence critical early decisions. At each iteration we solve an\naugmented multi-objective optimisation problem across a number of alternate\nsolutions, maximising both the sum of their utility function values and the\ndeterminant of their covariance matrix, equivalent to their total variability.\nBy taking the solution at the knee point of the Pareto front, we return a set\nof alternate solutions at each iteration that have both high utility values and\nare reasonably distinct, from which the expert selects one for evaluation. We\ndemonstrate that even in the case of an uninformed practitioner, our algorithm\nrecovers the regret of standard Bayesian optimisation.","terms":["cs.LG","cs.HC","math.OC"]},{"titles":"Learning \"Look-Ahead\" Nonlocal Traffic Dynamics in a Ring Road","summaries":"The macroscopic traffic flow model is widely used for traffic control and\nmanagement. To incorporate drivers' anticipative behaviors and to remove\nimpractical speed discontinuity inherent in the classic\nLighthill-Whitham-Richards (LWR) traffic model, nonlocal partial differential\nequation (PDE) models with ``look-ahead\" dynamics have been proposed, which\nassume that the speed is a function of weighted downstream traffic density.\nHowever, it lacks data validation on two important questions: whether there\nexist nonlocal dynamics, and how the length and weight of the ``look-ahead\"\nwindow affect the spatial temporal propagation of traffic densities. In this\npaper, we adopt traffic trajectory data from a ring-road experiment and design\na physics-informed neural network to learn the fundamental diagram and\nlook-ahead kernel that best fit the data, and reinvent a data-enhanced nonlocal\nLWR model via minimizing the loss function combining the data discrepancy and\nthe nonlocal model discrepancy. Results show that the learned nonlocal LWR\nyields a more accurate prediction of traffic wave propagation in three\ndifferent scenarios: stop-and-go oscillations, congested, and free traffic. We\nfirst demonstrate the existence of ``look-ahead\" effect with real traffic data.\nThe optimal nonlocal kernel is found out to take a length of around 35 to 50\nmeters, and the kernel weight within 5 meters accounts for the majority of the\nnonlocal effect. Our results also underscore the importance of choosing a\npriori physics in machine learning models.","terms":["cs.LG","cs.SY","eess.SY"]},{"titles":"Physics-informed neural networks with unknown measurement noise","summaries":"Physics-informed neural networks (PINNs) constitute a flexible approach to\nboth finding solutions and identifying parameters of partial differential\nequations. Most works on the topic assume noiseless data, or data contaminated\nwith weak Gaussian noise. We show that the standard PINN framework breaks down\nin case of non-Gaussian noise. We give a way of resolving this fundamental\nissue and we propose to jointly train an energy-based model (EBM) to learn the\ncorrect noise distribution. We illustrate the improved performance of our\napproach using multiple examples.","terms":["stat.ML","cs.LG"]},{"titles":"Solving Inverse Physics Problems with Score Matching","summaries":"We propose to solve inverse problems involving the temporal evolution of\nphysics systems by leveraging recent advances from diffusion models. Our method\nmoves the system's current state backward in time step by step by combining an\napproximate inverse physics simulator and a learned correction function. A\ncentral insight of our work is that training the learned correction with a\nsingle-step loss is equivalent to a score matching objective, while recursively\npredicting longer parts of the trajectory during training relates to maximum\nlikelihood training of a corresponding probability flow. We highlight the\nadvantages of our algorithm compared to standard denoising score matching and\nimplicit score matching, as well as fully learned baselines for a wide range of\ninverse physics problems. The resulting inverse solver has excellent accuracy\nand temporal stability and, in contrast to other learned inverse solvers,\nallows for sampling the posterior of the solutions.","terms":["cs.LG","physics.data-an"]},{"titles":"Revisit Human-Scene Interaction via Space Occupancy","summaries":"Human-scene Interaction (HSI) generation is a challenging task and crucial\nfor various downstream tasks. However, one of the major obstacles is the\nlimited data scale. High-quality data with simultaneously captured human and 3D\nenvironments is rare, resulting in limited data diversity and complexity. In\nthis work, we argue that interaction with a scene is essentially interacting\nwith the space occupancy of the scene from an abstract physical perspective,\nleading us to a unified novel view of Human-Occupancy Interaction. By treating\npure motion sequences as records of humans interacting with invisible scene\noccupancy, we can aggregate motion-only data into a large-scale paired\nhuman-occupancy interaction database: Motion Occupancy Base (MOB). Thus, the\nneed for costly paired motion-scene datasets with high-quality scene scans can\nbe substantially alleviated. With this new unified view of Human-Occupancy\ninteraction, a single motion controller is proposed to reach the target state\ngiven the surrounding occupancy. Once trained on MOB with complex occupancy\nlayout, the controller could handle cramped scenes and generalize well to\ngeneral scenes with limited complexity. With no GT 3D scenes for training, our\nmethod can generate realistic and stable HSI motions in diverse scenarios,\nincluding both static and dynamic scenes. Our code and data would be made\npublicly available at https:\/\/foruck.github.io\/occu-page\/.","terms":["cs.CV"]},{"titles":"H-GAP: Humanoid Control with a Generalist Planner","summaries":"Humanoid control is an important research challenge offering avenues for\nintegration into human-centric infrastructures and enabling physics-driven\nhumanoid animations. The daunting challenges in this field stem from the\ndifficulty of optimizing in high-dimensional action spaces and the instability\nintroduced by the bipedal morphology of humanoids. However, the extensive\ncollection of human motion-captured data and the derived datasets of humanoid\ntrajectories, such as MoCapAct, paves the way to tackle these challenges. In\nthis context, we present Humanoid Generalist Autoencoding Planner (H-GAP), a\nstate-action trajectory generative model trained on humanoid trajectories\nderived from human motion-captured data, capable of adeptly handling downstream\ncontrol tasks with Model Predictive Control (MPC). For 56 degrees of freedom\nhumanoid, we empirically demonstrate that H-GAP learns to represent and\ngenerate a wide range of motor behaviours. Further, without any learning from\nonline interactions, it can also flexibly transfer these behaviors to solve\nnovel downstream control tasks via planning. Notably, H-GAP excels established\nMPC baselines that have access to the ground truth dynamics model, and is\nsuperior or comparable to offline RL methods trained for individual tasks.\nFinally, we do a series of empirical studies on the scaling properties of\nH-GAP, showing the potential for performance gains via additional data but not\ncomputing. Code and videos are available at\nhttps:\/\/ycxuyingchen.github.io\/hgap\/.","terms":["cs.LG","cs.AI","cs.RO"]},{"titles":"Generating Visually Realistic Adversarial Patch","summaries":"Deep neural networks (DNNs) are vulnerable to various types of adversarial\nexamples, bringing huge threats to security-critical applications. Among these,\nadversarial patches have drawn increasing attention due to their good\napplicability to fool DNNs in the physical world. However, existing works often\ngenerate patches with meaningless noise or patterns, making it conspicuous to\nhumans. To address this issue, we explore how to generate visually realistic\nadversarial patches to fool DNNs. Firstly, we analyze that a high-quality\nadversarial patch should be realistic, position irrelevant, and printable to be\ndeployed in the physical world. Based on this analysis, we propose an effective\nattack called VRAP, to generate visually realistic adversarial patches.\nSpecifically, VRAP constrains the patch in the neighborhood of a real image to\nensure the visual reality, optimizes the patch at the poorest position for\nposition irrelevance, and adopts Total Variance loss as well as gamma\ntransformation to make the generated patch printable without losing\ninformation. Empirical evaluations on the ImageNet dataset demonstrate that the\nproposed VRAP exhibits outstanding attack performance in the digital world.\nMoreover, the generated adversarial patches can be disguised as the scrawl or\nlogo in the physical world to fool the deep models without being detected,\nbringing significant threats to DNNs-enabled applications.","terms":["cs.CV"]},{"titles":"Do AI models produce better weather forecasts than physics-based models? A quantitative evaluation case study of Storm Ciar\u00e1n","summaries":"There has been huge recent interest in the potential of making operational\nweather forecasts using machine learning techniques. As they become a part of\nthe weather forecasting toolbox, there is a pressing need to understand how\nwell current machine learning models can simulate high-impactweather events. We\ncompare forecasts of Storm Ciar\\'an, a European windstorm that caused sixteen\ndeaths and extensive damage in Northern Europe, made by machine learning and\nnumericalweather prediction models. The four machine learning models considered\n(FourCastNet, Pangu-Weather, GraphCast and FourCastNet-v2) produce forecasts\nthat accurately capture the synoptic-scale structure of the cyclone including\nthe position of the cloud head, shape of the warm sector and location of warm\nconveyor belt jet, and the large-scale dynamical drivers important for the\nrapid storm development such as the position of the storm relative to the\nupper-level jet exit. However, their ability to resolve the more detailed\nstructures important for issuing weather warnings is more mixed. All of the\nmachine learning models underestimate the peak amplitude of winds associated\nwith the storm, only some machine learning models resolve the warm core\nseclusion and none of the machine learning models capture the sharp bent-back\nwarm frontal gradient. Our study shows there is a great deal about the\nperformance and properties of machine learning weather forecasts that can be\nderived from case studies of high-impact weather events such as Storm Ciar\\'an.","terms":["cs.LG","physics.ao-ph"]},{"titles":"Discovering Interpretable Physical Models using Symbolic Regression and Discrete Exterior Calculus","summaries":"Computational modeling is a key resource to gather insight into physical\nsystems in modern scientific research and engineering. While access to large\namount of data has fueled the use of Machine Learning (ML) to recover physical\nmodels from experiments and increase the accuracy of physical simulations,\npurely data-driven models have limited generalization and interpretability. To\novercome these limitations, we propose a framework that combines Symbolic\nRegression (SR) and Discrete Exterior Calculus (DEC) for the automated\ndiscovery of physical models starting from experimental data. Since these\nmodels consist of mathematical expressions, they are interpretable and amenable\nto analysis, and the use of a natural, general-purpose discrete mathematical\nlanguage for physics favors generalization with limited input data.\nImportantly, DEC provides building blocks for the discrete analogue of field\ntheories, which are beyond the state-of-the-art applications of SR to physical\nproblems. Further, we show that DEC allows to implement a strongly-typed SR\nprocedure that guarantees the mathematical consistency of the recovered models\nand reduces the search space of symbolic expressions. Finally, we prove the\neffectiveness of our methodology by re-discovering three models of Continuum\nPhysics from synthetic experimental data: Poisson equation, the Euler's\nElastica and the equations of Linear Elasticity. Thanks to their\ngeneral-purpose nature, the methods developed in this paper may be applied to\ndiverse contexts of physical modeling.","terms":["cs.LG","cs.DM","cs.NE"]},{"titles":"Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey","summaries":"As artificial intelligence (AI) continues to rapidly evolve, the realm of\nEarth and atmospheric sciences is increasingly adopting data-driven models,\npowered by progressive developments in deep learning (DL). Specifically, DL\ntechniques are extensively utilized to decode the chaotic and nonlinear aspects\nof Earth systems, and to address climate challenges via understanding weather\nand climate data. Cutting-edge performance on specific tasks within narrower\nspatio-temporal scales has been achieved recently through DL. The rise of large\nmodels, specifically large language models (LLMs), has enabled fine-tuning\nprocesses that yield remarkable outcomes across various downstream tasks,\nthereby propelling the advancement of general AI. However, we are still\nnavigating the initial stages of crafting general AI for weather and climate.\nIn this survey, we offer an exhaustive, timely overview of state-of-the-art AI\nmethodologies specifically engineered for weather and climate data, with a\nspecial focus on time series and text data. Our primary coverage encompasses\nfour critical aspects: types of weather and climate data, principal model\narchitectures, model scopes and applications, and datasets for weather and\nclimate. Furthermore, in relation to the creation and application of foundation\nmodels for weather and climate data understanding, we delve into the field's\nprevailing challenges, offer crucial insights, and propose detailed avenues for\nfuture research. This comprehensive approach equips practitioners with the\nrequisite knowledge to make substantial progress in this domain. Our survey\nencapsulates the most recent breakthroughs in research on large, data-driven\nmodels for weather and climate data understanding, emphasizing robust\nfoundations, current advancements, practical applications, crucial resources,\nand prospective research opportunities.","terms":["cs.LG","cs.AI","cs.CV","physics.ao-ph"]},{"titles":"CityTFT: Temporal Fusion Transformer for Urban Building Energy Modeling","summaries":"Urban Building Energy Modeling (UBEM) is an emerging method to investigate\nurban design and energy systems against the increasing energy demand at urban\nand neighborhood levels. However, current UBEM methods are mostly physic-based\nand time-consuming in multiple climate change scenarios. This work proposes\nCityTFT, a data-driven UBEM framework, to accurately model the energy demands\nin urban environments. With the empowerment of the underlying TFT framework and\nan augmented loss function, CityTFT could predict heating and cooling triggers\nin unseen climate dynamics with an F1 score of 99.98 \\% while RMSE of loads of\n13.57 kWh.","terms":["stat.ML","cs.AI","cs.LG"]},{"titles":"Fedstellar: A Platform for Decentralized Federated Learning","summaries":"In 2016, Google proposed Federated Learning (FL) as a novel paradigm to train\nMachine Learning (ML) models across the participants of a federation while\npreserving data privacy. Since its birth, Centralized FL (CFL) has been the\nmost used approach, where a central entity aggregates participants' models to\ncreate a global one. However, CFL presents limitations such as communication\nbottlenecks, single point of failure, and reliance on a central server.\nDecentralized Federated Learning (DFL) addresses these issues by enabling\ndecentralized model aggregation and minimizing dependency on a central entity.\nDespite these advances, current platforms training DFL models struggle with key\nissues such as managing heterogeneous federation network topologies. To\novercome these challenges, this paper presents Fedstellar, a novel platform\ndesigned to train FL models in a decentralized, semi-decentralized, and\ncentralized fashion across diverse federations of physical or virtualized\ndevices. The Fedstellar implementation encompasses a web application with an\ninteractive graphical interface, a controller for deploying federations of\nnodes using physical or virtual devices, and a core deployed on each device\nwhich provides the logic needed to train, aggregate, and communicate in the\nnetwork. The effectiveness of the platform has been demonstrated in two\nscenarios: a physical deployment involving single-board devices such as\nRaspberry Pis for detecting cyberattacks, and a virtualized deployment\ncomparing various FL approaches in a controlled environment using MNIST and\nCIFAR-10 datasets. In both scenarios, Fedstellar demonstrated consistent\nperformance and adaptability, achieving F1 scores of 91%, 98%, and 91.2% using\nDFL for detecting cyberattacks and classifying MNIST and CIFAR-10,\nrespectively, reducing training time by 32% compared to centralized approaches.","terms":["cs.LG","cs.AI","cs.DC","cs.NI"]},{"titles":"AdsorbRL: Deep Multi-Objective Reinforcement Learning for Inverse Catalysts Design","summaries":"A central challenge of the clean energy transition is the development of\ncatalysts for low-emissions technologies. Recent advances in Machine Learning\nfor quantum chemistry drastically accelerate the computation of catalytic\nactivity descriptors such as adsorption energies. Here we introduce AdsorbRL, a\nDeep Reinforcement Learning agent aiming to identify potential catalysts given\na multi-objective binding energy target, trained using offline learning on the\nOpen Catalyst 2020 and Materials Project data sets. We experiment with Deep\nQ-Network agents to traverse the space of all ~160,000 possible unary, binary\nand ternary compounds of 55 chemical elements, with very sparse rewards based\non adsorption energy known for only between 2,000 and 3,000 catalysts per\nadsorbate. To constrain the actions space, we introduce Random Edge Traversal\nand train a single-objective DQN agent on the known states subgraph, which we\nfind strengthens target binding energy by an average of 4.1 eV. We extend this\napproach to multi-objective, goal-conditioned learning, and train a DQN agent\nto identify materials with the highest (respectively lowest) adsorption\nenergies for multiple simultaneous target adsorbates. We experiment with\nObjective Sub-Sampling, a novel training scheme aimed at encouraging\nexploration in the multi-objective setup, and demonstrate simultaneous\nadsorption energy improvement across all target adsorbates, by an average of\n0.8 eV. Overall, our results suggest strong potential for Deep Reinforcement\nLearning applied to the inverse catalysts design problem.","terms":["cs.LG","cs.AI","physics.chem-ph"]},{"titles":"Optimal Data Generation in Multi-Dimensional Parameter Spaces, using Bayesian Optimization","summaries":"Acquiring a substantial number of data points for training accurate machine\nlearning (ML) models is a big challenge in scientific fields where data\ncollection is resource-intensive. Here, we propose a novel approach for\nconstructing a minimal yet highly informative database for training ML models\nin complex multi-dimensional parameter spaces. To achieve this, we mimic the\nunderlying relation between the output and input parameters using Gaussian\nprocess regression (GPR). Using a set of known data, GPR provides predictive\nmeans and standard deviation for the unknown data. Given the predicted standard\ndeviation by GPR, we select data points using Bayesian optimization to obtain\nan efficient database for training ML models. We compare the performance of ML\nmodels trained on databases obtained through this method, with databases\nobtained using traditional approaches. Our results demonstrate that the ML\nmodels trained on the database obtained using Bayesian optimization approach\nconsistently outperform the other two databases, achieving high accuracy with a\nsignificantly smaller number of data points. Our work contributes to the\nresource-efficient collection of data in high-dimensional complex parameter\nspaces, to achieve high precision machine learning predictions.","terms":["cs.LG","physics.app-ph","physics.comp-ph"]},{"titles":"Class Symbolic Regression: Gotta Fit 'Em All","summaries":"We introduce \"Class Symbolic Regression\" a first framework for automatically\nfinding a single analytical functional form that accurately fits multiple\ndatasets - each governed by its own (possibly) unique set of fitting\nparameters. This hierarchical framework leverages the common constraint that\nall the members of a single class of physical phenomena follow a common\ngoverning law. Our approach extends the capabilities of our earlier Physical\nSymbolic Optimization ($\\Phi$-SO) framework for Symbolic Regression, which\nintegrates dimensional analysis constraints and deep reinforcement learning for\nsymbolic analytical function discovery from data. We demonstrate the efficacy\nof this novel approach by applying it to a panel of synthetic toy case datasets\nand showcase its practical utility for astrophysics by successfully extracting\nan analytic galaxy potential from a set of simulated orbits approximating\nstellar streams.","terms":["cs.LG","astro-ph.GA","astro-ph.IM","physics.comp-ph"]},{"titles":"GS-IR: 3D Gaussian Splatting for Inverse Rendering","summaries":"We propose GS-IR, a novel inverse rendering approach based on 3D Gaussian\nSplatting (GS) that leverages forward mapping volume rendering to achieve\nphotorealistic novel view synthesis and relighting results. Unlike previous\nworks that use implicit neural representations and volume rendering (e.g.\nNeRF), which suffer from low expressive power and high computational\ncomplexity, we extend GS, a top-performance representation for novel view\nsynthesis, to estimate scene geometry, surface material, and environment\nillumination from multi-view images captured under unknown lighting conditions.\nThere are two main problems when introducing GS to inverse rendering: 1) GS\ndoes not support producing plausible normal natively; 2) forward mapping (e.g.\nrasterization and splatting) cannot trace the occlusion like backward mapping\n(e.g. ray tracing). To address these challenges, our GS-IR proposes an\nefficient optimization scheme that incorporates a depth-derivation-based\nregularization for normal estimation and a baking-based occlusion to model\nindirect lighting. The flexible and expressive GS representation allows us to\nachieve fast and compact geometry reconstruction, photorealistic novel view\nsynthesis, and effective physically-based rendering. We demonstrate the\nsuperiority of our method over baseline methods through qualitative and\nquantitative evaluations on various challenging scenes.","terms":["cs.CV"]},{"titles":"Two-stage optimized unified adversarial patch for attacking visible-infrared cross-modal detectors in the physical world","summaries":"Currently, many studies have addressed security concerns related to visible\nand infrared detectors independently. In practical scenarios, utilizing\ncross-modal detectors for tasks proves more reliable than relying on\nsingle-modal detectors. Despite this, there is a lack of comprehensive security\nevaluations for cross-modal detectors. While existing research has explored the\nfeasibility of attacks against cross-modal detectors, the implementation of a\nrobust attack remains unaddressed. This work introduces the Two-stage Optimized\nUnified Adversarial Patch (TOUAP) designed for performing attacks against\nvisible-infrared cross-modal detectors in real-world, black-box settings. The\nTOUAP employs a two-stage optimization process: firstly, PSO optimizes an\nirregular polygonal infrared patch to attack the infrared detector; secondly,\nthe color QR code is optimized, and the shape information of the infrared patch\nfrom the first stage is used as a mask. The resulting irregular polygon visible\nmodal patch executes an attack on the visible detector. Through extensive\nexperiments conducted in both digital and physical environments, we validate\nthe effectiveness and robustness of the proposed method. As the TOUAP surpasses\nbaseline performance, we advocate for its widespread attention.","terms":["cs.CV"]},{"titles":"GenEM: Physics-Informed Generative Cryo-Electron Microscopy","summaries":"In the past decade, deep conditional generative models have revolutionized\nthe generation of realistic images, extending their application from\nentertainment to scientific domains. Single-particle cryo-electron microscopy\n(cryo-EM) is crucial in resolving near-atomic resolution 3D structures of\nproteins, such as the SARS-COV-2 spike protein. To achieve high-resolution\nreconstruction, AI models for particle picking and pose estimation have been\nadopted. However, their performance is still limited as they lack high-quality\nannotated datasets. To address this, we introduce physics-informed generative\ncryo-electron microscopy (GenEM), which for the first time integrates\nphysical-based cryo-EM simulation with a generative unpaired noise translation\nto generate physically correct synthetic cryo-EM datasets with realistic\nnoises. Initially, GenEM simulates the cryo-EM imaging process based on a\nvirtual specimen. To generate realistic noises, we leverage an unpaired noise\ntranslation via contrastive learning with a novel mask-guided sampling scheme.\nExtensive experiments show that GenEM is capable of generating realistic\ncryo-EM images. The generated dataset can further enhance particle picking and\npose estimation models, eventually improving the reconstruction resolution. We\nwill release our code and annotated synthetic datasets.","terms":["cs.CV"]},{"titles":"EDALearn: A Comprehensive RTL-to-Signoff EDA Benchmark for Democratized and Reproducible ML for EDA Research","summaries":"The application of Machine Learning (ML) in Electronic Design Automation\n(EDA) for Very Large-Scale Integration (VLSI) design has garnered significant\nresearch attention. Despite the requirement for extensive datasets to build\neffective ML models, most studies are limited to smaller, internally generated\ndatasets due to the lack of comprehensive public resources. In response, we\nintroduce EDALearn, the first holistic, open-source benchmark suite\nspecifically for ML tasks in EDA. This benchmark suite presents an end-to-end\nflow from synthesis to physical implementation, enriching data collection\nacross various stages. It fosters reproducibility and promotes research into ML\ntransferability across different technology nodes. Accommodating a wide range\nof VLSI design instances and sizes, our benchmark aptly represents the\ncomplexity of contemporary VLSI designs. Additionally, we provide an in-depth\ndata analysis, enabling users to fully comprehend the attributes and\ndistribution of our data, which is essential for creating efficient ML models.\nOur contributions aim to encourage further advances in the ML-EDA domain.","terms":["cs.LG"]},{"titles":"Toward Automated Quantum Variational Machine Learning","summaries":"In this work, we address the problem of automating quantum variational\nmachine learning. We develop a multi-locality parallelizable search algorithm,\ncalled MUSE, to find the initial points and the sets of parameters that achieve\nthe best performance for quantum variational circuit learning. Simulations with\nfive real-world classification datasets indicate that on average, MUSE improves\nthe detection accuracy of quantum variational classifiers 2.3 times with\nrespect to the observed lowest scores. Moreover, when applied to two real-world\nregression datasets, MUSE improves the quality of the predictions from negative\ncoefficients of determination to positive ones. Furthermore, the classification\nand regression scores of the quantum variational models trained with MUSE are\non par with the classical counterparts.","terms":["cs.LG","cs.ET","quant-ph"]},{"titles":"Neural Operators for Accelerating Scientific Simulations and Design","summaries":"Scientific discovery and engineering design are currently limited by the time\nand cost of physical experiments, selected mostly through trial-and-error and\nintuition that require deep domain expertise. Numerical simulations present an\nalternative to physical experiments but are usually infeasible for complex\nreal-world domains due to the computational requirements of existing numerical\nmethods. Artificial intelligence (AI) presents a potential paradigm shift by\ndeveloping fast data-driven surrogate models. In particular, an AI framework,\nknown as neural operators, presents a principled framework for learning\nmappings between functions defined on continuous domains, e.g., spatiotemporal\nprocesses and partial differential equations (PDE). They can extrapolate and\npredict solutions at new locations unseen during training, i.e., perform\nzero-shot super-resolution. Neural operators can augment or even replace\nexisting simulators in many applications, such as computational fluid dynamics,\nweather forecasting, and material modeling, while being 4-5 orders of magnitude\nfaster. Further, neural operators can be integrated with physics and other\ndomain constraints enforced at finer resolutions to obtain high-fidelity\nsolutions and good generalization. Since neural operators are differentiable,\nthey can directly optimize parameters for inverse design and other inverse\nproblems. We believe that neural operators present a transformative approach to\nsimulation and design, enabling rapid research and development.","terms":["cs.LG","physics.comp-ph"]},{"titles":"HandyPriors: Physically Consistent Perception of Hand-Object Interactions with Differentiable Priors","summaries":"Various heuristic objectives for modeling hand-object interaction have been\nproposed in past work. However, due to the lack of a cohesive framework, these\nobjectives often possess a narrow scope of applicability and are limited by\ntheir efficiency or accuracy. In this paper, we propose HandyPriors, a unified\nand general pipeline for pose estimation in human-object interaction scenes by\nleveraging recent advances in differentiable physics and rendering. Our\napproach employs rendering priors to align with input images and segmentation\nmasks along with physics priors to mitigate penetration and relative-sliding\nacross frames. Furthermore, we present two alternatives for hand and object\npose estimation. The optimization-based pose estimation achieves higher\naccuracy, while the filtering-based tracking, which utilizes the differentiable\npriors as dynamics and observation models, executes faster. We demonstrate that\nHandyPriors attains comparable or superior results in the pose estimation task,\nand that the differentiable physics module can predict contact information for\npose refinement. We also show that our approach generalizes to perception\ntasks, including robotic hand manipulation and human-object pose estimation in\nthe wild.","terms":["cs.CV","cs.RO"]},{"titles":"GAPS: Geometry-Aware, Physics-Based, Self-Supervised Neural Garment Draping","summaries":"Recent neural, physics-based modeling of garment deformations allows faster\nand visually aesthetic results as opposed to the existing methods.\nMaterial-specific parameters are used by the formulation to control the garment\ninextensibility. This delivers unrealistic results with physically implausible\nstretching. Oftentimes, the draped garment is pushed inside the body which is\neither corrected by an expensive post-processing, thus adding to further\ninconsistent stretching; or by deploying a separate training regime for each\nbody type, restricting its scalability. Additionally, the flawed skinning\nprocess deployed by existing methods produces incorrect results on loose\ngarments.\n  In this paper, we introduce a geometrical constraint to the existing\nformulation that is collision-aware and imposes garment inextensibility\nwherever possible. Thus, we obtain realistic results where draped clothes\nstretch only while covering bigger body regions. Furthermore, we propose a\ngeometry-aware garment skinning method by defining a body-garment closeness\nmeasure which works for all garment types, especially the loose ones.","terms":["cs.CV","cs.GR","cs.LG"]},{"titles":"Continuous Convolutional Neural Networks for Disruption Prediction in Nuclear Fusion Plasmas","summaries":"Grid decarbonization for climate change requires dispatchable carbon-free\nenergy like nuclear fusion. The tokamak concept offers a promising path for\nfusion, but one of the foremost challenges in implementation is the occurrence\nof energetic plasma disruptions. In this study, we delve into Machine Learning\napproaches to predict plasma state outcomes. Our contributions are twofold: (1)\nWe present a novel application of Continuous Convolutional Neural Networks for\ndisruption prediction and (2) We examine the advantages and disadvantages of\ncontinuous models over discrete models for disruption prediction by comparing\nour model with the previous, discrete state of the art, and show that\ncontinuous models offer significantly better performance (Area Under the\nReceiver Operating Characteristic Curve = 0.974 v.s. 0.799) with fewer\nparameters","terms":["cs.LG","physics.plasm-ph"]},{"titles":"EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations","summaries":"Equivariant Transformers such as Equiformer have demonstrated the efficacy of\napplying Transformers to the domain of 3D atomistic systems. However, they are\nlimited to small degrees of equivariant representations due to their\ncomputational complexity. In this paper, we investigate whether these\narchitectures can scale well to higher degrees. Starting from Equiformer, we\nfirst replace $SO(3)$ convolutions with eSCN convolutions to efficiently\nincorporate higher-degree tensors. Then, to better leverage the power of higher\ndegrees, we propose three architectural improvements -- attention\nre-normalization, separable $S^2$ activation and separable layer normalization.\nPutting this all together, we propose EquiformerV2, which outperforms previous\nstate-of-the-art methods on large-scale OC20 dataset by up to $9\\%$ on forces,\n$4\\%$ on energies, offers better speed-accuracy trade-offs, and $2\\times$\nreduction in DFT calculations needed for computing adsorption energies.\nAdditionally, EquiformerV2 trained on only OC22 dataset outperforms GemNet-OC\ntrained on both OC20 and OC22 datasets, achieving much better data efficiency.\nFinally, we compare EquiformerV2 with Equiformer on QM9 and OC20 S2EF-2M\ndatasets to better understand the performance gain brought by higher degrees.","terms":["cs.LG","cs.AI","physics.comp-ph"]},{"titles":"Physics-Informed Gaussian Process Regression Generalizes Linear PDE Solvers","summaries":"Linear partial differential equations (PDEs) are an important, widely applied\nclass of mechanistic models, describing physical processes such as heat\ntransfer, electromagnetism, and wave propagation. In practice, specialized\nnumerical methods based on discretization are used to solve PDEs. They\ngenerally use an estimate of the unknown model parameters and, if available,\nphysical measurements for initialization. Such solvers are often embedded into\nlarger scientific models with a downstream application and thus error\nquantification plays a key role. However, by ignoring parameter and measurement\nuncertainty, classical PDE solvers may fail to produce consistent estimates of\ntheir inherent approximation error. In this work, we approach this problem in a\nprincipled fashion by interpreting solving linear PDEs as physics-informed\nGaussian process (GP) regression. Our framework is based on a key\ngeneralization of the Gaussian process inference theorem to observations made\nvia an arbitrary bounded linear operator. Crucially, this probabilistic\nviewpoint allows to (1) quantify the inherent discretization error; (2)\npropagate uncertainty about the model parameters to the solution; and (3)\ncondition on noisy measurements. Demonstrating the strength of this\nformulation, we prove that it strictly generalizes methods of weighted\nresiduals, a central class of PDE solvers including collocation, finite volume,\npseudospectral, and (generalized) Galerkin methods such as finite element and\nspectral methods. This class can thus be directly equipped with a structured\nerror estimate. In summary, our results enable the seamless integration of\nmechanistic models as modular building blocks into probabilistic models by\nblurring the boundaries between numerical analysis and Bayesian inference.","terms":["cs.LG","cs.NA","math.NA","stat.ML"]},{"titles":"S2P3: Self-Supervised Polarimetric Pose Prediction","summaries":"This paper proposes the first self-supervised 6D object pose prediction from\nmultimodal RGB+polarimetric images. The novel training paradigm comprises 1) a\nphysical model to extract geometric information of polarized light, 2) a\nteacher-student knowledge distillation scheme and 3) a self-supervised loss\nformulation through differentiable rendering and an invertible physical\nconstraint. Both networks leverage the physical properties of polarized light\nto learn robust geometric representations by encoding shape priors and\npolarization characteristics derived from our physical model. Geometric\npseudo-labels from the teacher support the student network without the need for\nannotated real data. Dense appearance and geometric information of objects are\nobtained through a differentiable renderer with the predicted pose for\nself-supervised direct coupling. The student network additionally features our\nproposed invertible formulation of the physical shape priors that enables\nend-to-end self-supervised training through physical constraints of derived\npolarization characteristics compared against polarimetric input images. We\nspecifically focus on photometrically challenging objects with texture-less or\nreflective surfaces and transparent materials for which the most prominent\nperformance gain is reported.","terms":["cs.CV"]},{"titles":"QPoser: Quantized Explicit Pose Prior Modeling for Controllable Pose Generation","summaries":"Explicit pose prior models compress human poses into latent representations\nfor using in pose-related downstream tasks. A desirable explicit pose prior\nmodel should satisfy three desirable abilities: 1) correctness, i.e. ensuring\nto generate physically possible poses; 2) expressiveness, i.e. ensuring to\npreserve details in generation; 3) controllability, meaning that generation\nfrom reference poses and explicit instructions should be convenient. Existing\nexplicit pose prior models fail to achieve all of three properties, in special\ncontrollability. To break this situation, we propose QPoser, a highly\ncontrollable explicit pose prior model which guarantees correctness and\nexpressiveness. In QPoser, a multi-head vector quantized autoencoder (MS-VQVAE)\nis proposed for obtaining expressive and distributed pose representations.\nFurthermore, a global-local feature integration mechanism (GLIF-AE) is utilized\nto disentangle the latent representation and integrate full-body information\ninto local-joint features. Experimental results show that QPoser significantly\noutperforms state-of-the-art approaches in representing expressive and correct\nposes, meanwhile is easily to be used for detailed conditional generation from\nreference poses and prompting instructions.","terms":["cs.CV"]},{"titles":"Hybrid Quantum Neural Network in High-dimensional Data Classification","summaries":"The research explores the potential of quantum deep learning models to\naddress challenging machine learning problems that classical deep learning\nmodels find difficult to tackle. We introduce a novel model architecture that\ncombines classical convolutional layers with a quantum neural network, aiming\nto surpass state-of-the-art accuracy while maintaining a compact model size.\nThe experiment is to classify high-dimensional audio data from the Bird-CLEF\n2021 dataset. Our evaluation focuses on key metrics, including training\nduration, model accuracy, and total model size. This research demonstrates the\npromising potential of quantum machine learning in enhancing machine learning\ntasks and solving practical machine learning challenges available today.","terms":["cs.LG","cs.AI","quant-ph"]},{"titles":"Supervised Machine Learning and Physics based Machine Learning approach for prediction of peak temperature distribution in Additive Friction Stir Deposition of Aluminium Alloy","summaries":"Additive friction stir deposition (AFSD) is a novel solid-state additive\nmanufacturing technique that circumvents issues of porosity, cracking, and\nproperties anisotropy that plague traditional powder bed fusion and directed\nenergy deposition approaches. However, correlations between process parameters,\nthermal profiles, and resulting microstructure in AFSD remain poorly\nunderstood. This hinders process optimization for properties. This work employs\na framework combining supervised machine learning (SML) and physics-informed\nneural networks (PINNs) to predict peak temperature distribution in AFSD from\nprocess parameters. Eight regression algorithms were implemented for SML\nmodeling, while four PINNs leveraged governing equations for transport, wave\npropagation, heat transfer, and quantum mechanics. Across multiple statistical\nmeasures, ensemble techniques like gradient boosting proved superior for SML,\nwith lowest MSE of 165.78. The integrated ML approach was also applied to\nclassify deposition quality from process factors, with logistic regression\ndelivering robust accuracy. By fusing data-driven learning and fundamental\nphysics, this dual methodology provides comprehensive insights into tailoring\nmicrostructure through thermal management in AFSD. The work demonstrates the\npower of bridging statistical and physics-based modeling for elucidating AM\nprocess-property relationships.","terms":["cs.LG","math.OC","stat.ML"]},{"titles":"Extreme Event Prediction with Multi-agent Reinforcement Learning-based Parametrization of Atmospheric and Oceanic Turbulence","summaries":"Global climate models (GCMs) are the main tools for understanding and\npredicting climate change. However, due to limited numerical resolutions, these\nmodels suffer from major structural uncertainties; e.g., they cannot resolve\ncritical processes such as small-scale eddies in atmospheric and oceanic\nturbulence. Thus, such small-scale processes have to be represented as a\nfunction of the resolved scales via closures (parametrization). The accuracy of\nthese closures is particularly important for capturing climate extremes.\nTraditionally, such closures are based on heuristics and simplifying\nassumptions about the unresolved physics. Recently, supervised-learned\nclosures, trained offline on high-fidelity data, have been shown to outperform\nthe classical physics-based closures. However, this approach requires a\nsignificant amount of high-fidelity training data and can also lead to\ninstabilities. Reinforcement learning is emerging as a potent alternative for\ndeveloping such closures as it requires only low-order statistics and leads to\nstable closures. In Scientific Multi-Agent Reinforcement Learning (SMARL)\ncomputational elements serve a dual role of discretization points and learning\nagents. We leverage SMARL and fundamentals of turbulence physics to learn\nclosures for prototypes of atmospheric and oceanic turbulence. The policy is\ntrained using only the enstrophy spectrum, which is nearly invariant and can be\nestimated from a few high-fidelity samples (these few samples are far from\nenough for supervised\/offline learning). We show that these closures lead to\nstable low-resolution simulations that, at a fraction of the cost, can\nreproduce the high-fidelity simulations' statistics, including the tails of the\nprobability density functions. The results demonstrate the high potential of\nSMARL for closure modeling for GCMs, especially in the regime of scarce data\nand indirect observations.","terms":["cs.LG","cs.CE","physics.ao-ph","physics.comp-ph","physics.flu-dyn"]},{"titles":"Physics-based Indirect Illumination for Inverse Rendering","summaries":"We present a physics-based inverse rendering method that learns the\nillumination, geometry, and materials of a scene from posed multi-view RGB\nimages. To model the illumination of a scene, existing inverse rendering works\neither completely ignore the indirect illumination or model it by coarse\napproximations, leading to sub-optimal illumination, geometry, and material\nprediction of the scene. In this work, we propose a physics-based illumination\nmodel that first locates surface points through an efficient refined sphere\ntracing algorithm, then explicitly traces the incoming indirect lights at each\nsurface point based on reflection. Then, we estimate each identified indirect\nlight through an efficient neural network. Moreover, we utilize the Leibniz's\nintegral rule to resolve non-differentiability in the proposed illumination\nmodel caused by boundary lights inspired by differentiable irradiance in\ncomputer graphics. As a result, the proposed differentiable illumination model\ncan be learned end-to-end together with geometry and materials estimation. As a\nside product, our physics-based inverse rendering model also facilitates\nflexible and realistic material editing as well as relighting. Extensive\nexperiments on synthetic and real-world datasets demonstrate that the proposed\nmethod performs favorably against existing inverse rendering methods on novel\nview synthesis and inverse rendering.","terms":["cs.CV"]},{"titles":"Learning Robust Precipitation Forecaster by Temporal Frame Interpolation","summaries":"Recent advances in deep learning have significantly elevated weather\nprediction models. However, these models often falter in real-world scenarios\ndue to their sensitivity to spatial-temporal shifts. This issue is particularly\nacute in weather forecasting, where models are prone to overfit to local and\ntemporal variations, especially when tasked with fine-grained predictions. In\nthis paper, we address these challenges by developing a robust precipitation\nforecasting model that demonstrates resilience against such spatial-temporal\ndiscrepancies. We introduce Temporal Frame Interpolation (TFI), a novel\ntechnique that enhances the training dataset by generating synthetic samples\nthrough interpolating adjacent frames from satellite imagery and ground radar\ndata, thus improving the model's robustness against frame noise. Moreover, we\nincorporate a unique Multi-Level Dice (ML-Dice) loss function, leveraging the\nordinal nature of rainfall intensities to improve the model's performance. Our\napproach has led to significant improvements in forecasting precision,\nculminating in our model securing \\textit{1st place} in the transfer learning\nleaderboard of the \\textit{Weather4cast'23} competition. This achievement not\nonly underscores the effectiveness of our methodologies but also establishes a\nnew standard for deep learning applications in weather forecasting. Our code\nand weights have been public on \\url{https:\/\/github.com\/Secilia-Cxy\/UNetTFI}.","terms":["cs.LG","physics.ao-ph"]},{"titles":"Forecasting Trends in Food Security: a Reservoir Computing Approach","summaries":"Early warning systems are an essential tool for effective humanitarian\naction. Advance warnings on impending disasters facilitate timely and targeted\nresponse which help save lives, livelihoods, and scarce financial resources. In\nthis work we present a new quantitative methodology to forecast levels of food\nconsumption for 60 consecutive days, at the sub-national level, in four\ncountries: Mali, Nigeria, Syria, and Yemen. The methodology is built on\npublicly available data from the World Food Programme's integrated global\nhunger monitoring system which collects, processes, and displays daily updates\non key food security metrics, conflict, weather events, and other drivers of\nfood insecurity across 90 countries (https:\/\/hungermap.wfp.org\/). In this\nstudy, we assessed the performance of various models including ARIMA, XGBoost,\nLSTMs, CNNs, and Reservoir Computing (RC), by comparing their Root Mean Squared\nError (RMSE) metrics. This comprehensive analysis spanned classical\nstatistical, machine learning, and deep learning approaches. Our findings\nhighlight Reservoir Computing as a particularly well-suited model in the field\nof food security given both its notable resistance to over-fitting on limited\ndata samples and its efficient training capabilities. The methodology we\nintroduce establishes the groundwork for a global, data-driven early warning\nsystem designed to anticipate and detect food insecurity.","terms":["cs.LG","physics.soc-ph","stat.ML"]},{"titles":"Decentralized policy learning with partial observation and mechanical constraints for multiperson modeling","summaries":"Extracting the rules of real-world multi-agent behaviors is a current\nchallenge in various scientific and engineering fields. Biological agents\nindependently have limited observation and mechanical constraints; however,\nmost of the conventional data-driven models ignore such assumptions, resulting\nin lack of biological plausibility and model interpretability for behavioral\nanalyses. Here we propose sequential generative models with partial observation\nand mechanical constraints in a decentralized manner, which can model agents'\ncognition and body dynamics, and predict biologically plausible behaviors. We\nformulate this as a decentralized multi-agent imitation-learning problem,\nleveraging binary partial observation and decentralized policy models based on\nhierarchical variational recurrent neural networks with physical and\nbiomechanical penalties. Using real-world basketball and soccer datasets, we\nshow the effectiveness of our method in terms of the constraint violations,\nlong-term trajectory prediction, and partial observation. Our approach can be\nused as a multi-agent simulator to generate realistic trajectories using\nreal-world data.","terms":["cs.LG","cs.MA","stat.ML"]},{"titles":"Physics Inspired Criterion for Pruning-Quantization Joint Learning","summaries":"Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https:\/\/github.com\/fanxxxxyi\/PIC-PQ.","terms":["cs.LG","cs.CV"]},{"titles":"Transfer Learning Enhanced Full Waveform Inversion","summaries":"We propose a way to favorably employ neural networks in the field of\nnon-destructive testing using Full Waveform Inversion (FWI). The presented\nmethodology discretizes the unknown material distribution in the domain with a\nneural network within an adjoint optimization. To further increase efficiency\nof the FWI, pretrained neural networks are used to provide a good starting\npoint for the inversion. This reduces the number of iterations in the Full\nWaveform Inversion for specific, yet generalizable settings.","terms":["cs.LG","physics.comp-ph","physics.geo-ph"]},{"titles":"Timewarp: Transferable Acceleration of Molecular Dynamics by Learning Time-Coarsened Dynamics","summaries":"Molecular dynamics (MD) simulation is a widely used technique to simulate\nmolecular systems, most commonly at the all-atom resolution where equations of\nmotion are integrated with timesteps on the order of femtoseconds\n($1\\textrm{fs}=10^{-15}\\textrm{s}$). MD is often used to compute equilibrium\nproperties, which requires sampling from an equilibrium distribution such as\nthe Boltzmann distribution. However, many important processes, such as binding\nand folding, occur over timescales of milliseconds or beyond, and cannot be\nefficiently sampled with conventional MD. Furthermore, new MD simulations need\nto be performed for each molecular system studied. We present Timewarp, an\nenhanced sampling method which uses a normalising flow as a proposal\ndistribution in a Markov chain Monte Carlo method targeting the Boltzmann\ndistribution. The flow is trained offline on MD trajectories and learns to make\nlarge steps in time, simulating the molecular dynamics of $10^{5} -\n10^{6}\\:\\textrm{fs}$. Crucially, Timewarp is transferable between molecular\nsystems: once trained, we show that it generalises to unseen small peptides\n(2-4 amino acids) at all-atom resolution, exploring their metastable states and\nproviding wall-clock acceleration of sampling compared to standard MD. Our\nmethod constitutes an important step towards general, transferable algorithms\nfor accelerating MD.","terms":["stat.ML","cond-mat.stat-mech","cs.LG","physics.chem-ph"]},{"titles":"Symplectic Structure-Aware Hamiltonian (Graph) Embeddings","summaries":"In traditional Graph Neural Networks (GNNs), the assumption of a fixed\nembedding manifold often limits their adaptability to diverse graph geometries.\nRecently, Hamiltonian system-inspired GNNs have been proposed to address the\ndynamic nature of such embeddings by incorporating physical laws into node\nfeature updates. We present Symplectic Structure-Aware Hamiltonian GNN\n(SAH-GNN), a novel approach that generalizes Hamiltonian dynamics for more\nflexible node feature updates. Unlike existing Hamiltonian approaches, SAH-GNN\nemploys Riemannian optimization on the symplectic Stiefel manifold to\nadaptively learn the underlying symplectic structure, circumventing the\nlimitations of existing Hamiltonian GNNs that rely on a pre-defined form of\nstandard symplectic structure. This innovation allows SAH-GNN to automatically\nadapt to various graph datasets without extensive hyperparameter tuning.\nMoreover, it conserves energy during training meaning the implicit Hamiltonian\nsystem is physically meaningful. Finally, we empirically validate SAH-GNN's\nsuperiority and adaptability in node classification tasks across multiple types\nof graph datasets.","terms":["cs.LG","math.SG"]},{"titles":"Interpretable Meta-Learning of Physical Systems","summaries":"Machine learning methods can be a valuable aid in the scientific process, but\nthey need to face challenging settings where data come from inhomogeneous\nexperimental conditions. Recent meta-learning methods have made significant\nprogress in multi-task learning, but they rely on black-box neural networks,\nresulting in high computational costs and limited interpretability. Leveraging\nthe structure of the learning problem, we argue that multi-environment\ngeneralization can be achieved using a simpler learning model, with an affine\nstructure with respect to the learning task. Crucially, we prove that this\narchitecture can identify the physical parameters of the system, enabling\ninterpreable learning. We demonstrate the competitive generalization\nperformance and the low computational cost of our method by comparing it to\nstate-of-the-art algorithms on physical systems, ranging from toy models to\ncomplex, non-analytical systems. The interpretability of our method is\nillustrated with original applications to physical-parameter-induced adaptation\nand to adaptive control.","terms":["cs.LG","stat.ML"]},{"titles":"iTransformer: Inverted Transformers Are Effective for Time Series Forecasting","summaries":"The recent boom of linear forecasting models questions the ongoing passion\nfor architectural modifications of Transformer-based forecasters. These\nforecasters leverage Transformers to model the global dependencies over\ntemporal tokens of time series, with each token formed by multiple variates of\nthe same timestamp. However, Transformers are challenged in forecasting series\nwith larger lookback windows due to performance degradation and computation\nexplosion. Besides, the embedding for each temporal token fuses multiple\nvariates that represent potential delayed events and distinct physical\nmeasurements, which may fail in learning variate-centric representations and\nresult in meaningless attention maps. In this work, we reflect on the competent\nduties of Transformer components and repurpose the Transformer architecture\nwithout any modification to the basic components. We propose iTransformer that\nsimply applies the attention and feed-forward network on the inverted\ndimensions. Specifically, the time points of individual series are embedded\ninto variate tokens which are utilized by the attention mechanism to capture\nmultivariate correlations; meanwhile, the feed-forward network is applied for\neach variate token to learn nonlinear representations. The iTransformer model\nachieves state-of-the-art on challenging real-world datasets, which further\nempowers the Transformer family with promoted performance, generalization\nability across different variates, and better utilization of arbitrary lookback\nwindows, making it a nice alternative as the fundamental backbone of time\nseries forecasting.","terms":["cs.LG"]},{"titles":"Adaptability of Computer Vision at the Tactical Edge: Addressing Environmental Uncertainty","summaries":"Computer Vision (CV) systems are increasingly being adopted into Command and\nControl (C2) systems to improve intelligence analysis on the battlefield, the\ntactical edge. CV systems leverage Artificial Intelligence (AI) algorithms to\nhelp visualize and interpret the environment, enhancing situational awareness.\nHowever, the adaptability of CV systems at the tactical edge remains\nchallenging due to rapidly changing environments and objects which can confuse\nthe deployed models. A CV model leveraged in this environment can become\nuncertain in its predictions, as the environment and the objects existing in\nthe environment begin to change. Additionally, mission objectives can rapidly\nchange leading to adjustments in technology, camera angles, and image\nresolutions. All of which can negatively affect the performance of and\npotentially introduce uncertainty into the system. When the training\nenvironment and\/or technology differs from the deployment environment, CV\nmodels can perform unexpectedly. Unfortunately, most scenarios at the tactical\nedge do not incorporate Uncertainty Quantification (UQ) into their deployed C2\nand CV systems. This concept paper explores the idea of synchronizing robust\ndata operations and model fine-tuning driven by UQ all at the tactical edge.\nSpecifically, curating datasets and training child models based on the\nresiduals of predictions, using these child models to calculate prediction\nintervals (PI), and then using these PI to calibrate the deployed models. By\nincorporating UQ into the core operations surrounding C2 and CV systems at the\ntactical edge, we can help drive purposeful adaptability on the battlefield.","terms":["cs.CV"]},{"titles":"MD-Splatting: Learning Metric Deformation from 4D Gaussians in Highly Deformable Scenes","summaries":"Accurate 3D tracking in highly deformable scenes with occlusions and shadows\ncan facilitate new applications in robotics, augmented reality, and generative\nAI. However, tracking under these conditions is extremely challenging due to\nthe ambiguity that arises with large deformations, shadows, and occlusions. We\nintroduce MD-Splatting, an approach for simultaneous 3D tracking and novel view\nsynthesis, using video captures of a dynamic scene from various camera poses.\nMD-Splatting builds on recent advances in Gaussian splatting, a method that\nlearns the properties of a large number of Gaussians for state-of-the-art and\nfast novel view synthesis. MD-Splatting learns a deformation function to\nproject a set of Gaussians with non-metric, thus canonical, properties into\nmetric space. The deformation function uses a neural-voxel encoding and a\nmultilayer perceptron (MLP) to infer Gaussian position, rotation, and a shadow\nscalar. We enforce physics-inspired regularization terms based on local\nrigidity, conservation of momentum, and isometry, which leads to trajectories\nwith smaller trajectory errors. MD-Splatting achieves high-quality 3D tracking\non highly deformable scenes with shadows and occlusions. Compared to\nstate-of-the-art, we improve 3D tracking by an average of 23.9 %, while\nsimultaneously achieving high-quality novel view synthesis. With sufficient\ntexture such as in scene 6, MD-Splatting achieves a median tracking error of\n3.39 mm on a cloth of 1 x 1 meters in size. Project website:\nhttps:\/\/md-splatting.github.io\/.","terms":["cs.CV","cs.RO"]},{"titles":"Convergence of Nonconvex PnP-ADMM with MMSE Denoisers","summaries":"Plug-and-Play Alternating Direction Method of Multipliers (PnP-ADMM) is a\nwidely-used algorithm for solving inverse problems by integrating physical\nmeasurement models and convolutional neural network (CNN) priors. PnP-ADMM has\nbeen theoretically proven to converge for convex data-fidelity terms and\nnonexpansive CNNs. It has however been observed that PnP-ADMM often empirically\nconverges even for expansive CNNs. This paper presents a theoretical\nexplanation for the observed stability of PnP-ADMM based on the interpretation\nof the CNN prior as a minimum mean-squared error (MMSE) denoiser. Our\nexplanation parallels a similar argument recently made for the iterative\nshrinkage\/thresholding algorithm variant of PnP (PnP-ISTA) and relies on the\nconnection between MMSE denoisers and proximal operators. We also numerically\nevaluate the performance gap between PnP-ADMM using a nonexpansive DnCNN\ndenoiser and expansive DRUNet denoiser, thus motivating the use of expansive\nCNNs.","terms":["cs.CV"]},{"titles":"Efficient Baseline for Quantitative Precipitation Forecasting in Weather4cast 2023","summaries":"Accurate precipitation forecasting is indispensable for informed\ndecision-making across various industries. However, the computational demands\nof current models raise environmental concerns. We address the critical need\nfor accurate precipitation forecasting while considering the environmental\nimpact of computational resources and propose a minimalist U-Net architecture\nto be used as a baseline for future weather forecasting initiatives.","terms":["cs.LG","physics.ao-ph"]},{"titles":"Probing and Mitigating Intersectional Social Biases in Vision-Language Models with Counterfactual Examples","summaries":"While vision-language models (VLMs) have achieved remarkable performance\nimprovements recently, there is growing evidence that these models also posses\nharmful biases with respect to social attributes such as gender and race. Prior\nstudies have primarily focused on probing such bias attributes individually\nwhile ignoring biases associated with intersections between social attributes.\nThis could be due to the difficulty of collecting an exhaustive set of\nimage-text pairs for various combinations of social attributes. To address this\nchallenge, we employ text-to-image diffusion models to produce counterfactual\nexamples for probing intserctional social biases at scale. Our approach\nutilizes Stable Diffusion with cross attention control to produce sets of\ncounterfactual image-text pairs that are highly similar in their depiction of a\nsubject (e.g., a given occupation) while differing only in their depiction of\nintersectional social attributes (e.g., race & gender). Through our\nover-generate-then-filter methodology, we produce SocialCounterfactuals, a\nhigh-quality dataset containing over 171k image-text pairs for probing\nintersectional biases related to gender, race, and physical characteristics. We\nconduct extensive experiments to demonstrate the usefulness of our generated\ndataset for probing and mitigating intersectional social biases in\nstate-of-the-art VLMs.","terms":["cs.CV","cs.AI"]},{"titles":"Multi-task learning with cross-task consistency for improved depth estimation in colonoscopy","summaries":"Colonoscopy screening is the gold standard procedure for assessing\nabnormalities in the colon and rectum, such as ulcers and cancerous polyps.\nMeasuring the abnormal mucosal area and its 3D reconstruction can help quantify\nthe surveyed area and objectively evaluate disease burden. However, due to the\ncomplex topology of these organs and variable physical conditions, for example,\nlighting, large homogeneous texture, and image modality estimating distance\nfrom the camera aka depth) is highly challenging. Moreover, most colonoscopic\nvideo acquisition is monocular, making the depth estimation a non-trivial\nproblem. While methods in computer vision for depth estimation have been\nproposed and advanced on natural scene datasets, the efficacy of these\ntechniques has not been widely quantified on colonoscopy datasets. As the\ncolonic mucosa has several low-texture regions that are not well pronounced,\nlearning representations from an auxiliary task can improve salient feature\nextraction, allowing estimation of accurate camera depths. In this work, we\npropose to develop a novel multi-task learning (MTL) approach with a shared\nencoder and two decoders, namely a surface normal decoder and a depth estimator\ndecoder. Our depth estimator incorporates attention mechanisms to enhance\nglobal context awareness. We leverage the surface normal prediction to improve\ngeometric feature extraction. Also, we apply a cross-task consistency loss\namong the two geometrically related tasks, surface normal and camera depth. We\ndemonstrate an improvement of 14.17% on relative error and 10.4% improvement on\n$\\delta_{1}$ accuracy over the most accurate baseline state-of-the-art BTS\napproach. All experiments are conducted on a recently released C3VD dataset;\nthus, we provide a first benchmark of state-of-the-art methods.","terms":["cs.CV","cs.AI","cs.MM"]},{"titles":"Extending Explainable Boosting Machines to Scientific Image Data","summaries":"As the deployment of computer vision technology becomes increasingly common\nin science, the need for explanations of the system and its output has become a\nfocus of great concern. Driven by the pressing need for interpretable models in\nscience, we propose the use of Explainable Boosting Machines (EBMs) for\nscientific image data. Inspired by an important application underpinning the\ndevelopment of quantum technologies, we apply EBMs to cold-atom soliton image\ndata tabularized using Gabor Wavelet Transform-based techniques that preserve\nthe spatial structure of the data. In doing so, we demonstrate the use of EBMs\nfor image data for the first time and show that our approach provides\nexplanations that are consistent with human intuition about the data.","terms":["cs.CV","cond-mat.quant-gas","cs.LG"]},{"titles":"Combining deep generative models with extreme value theory for synthetic hazard simulation: a multivariate and spatially coherent approach","summaries":"Climate hazards can cause major disasters when they occur simultaneously as\ncompound hazards. To understand the distribution of climate risk and inform\nadaptation policies, scientists need to simulate a large number of physically\nrealistic and spatially coherent events. Current methods are limited by\ncomputational constraints and the probabilistic spatial distribution of\ncompound events is not given sufficient attention. The bottleneck in current\napproaches lies in modelling the dependence structure between variables, as\ninference on parametric models suffers from the curse of dimensionality.\nGenerative adversarial networks (GANs) are well-suited to such a problem due to\ntheir ability to implicitly learn the distribution of data in high-dimensional\nsettings. We employ a GAN to model the dependence structure for daily maximum\nwind speed, significant wave height, and total precipitation over the Bay of\nBengal, combining this with traditional extreme value theory for controlled\nextrapolation of the tails. Once trained, the model can be used to efficiently\ngenerate thousands of realistic compound hazard events, which can inform\nclimate risk assessments for climate adaptation and disaster preparedness. The\nmethod developed is flexible and transferable to other multivariate and spatial\nclimate datasets.","terms":["cs.LG"]},{"titles":"Q-Seg: Quantum Annealing-based Unsupervised Image Segmentation","summaries":"In this study, we present Q-Seg, a novel unsupervised image segmentation\nmethod based on quantum annealing, tailored for existing quantum hardware. We\nformulate the pixel-wise segmentation problem, which assimilates spectral and\nspatial information of the image, as a graph-cut optimization task. Our method\nefficiently leverages the interconnected qubit topology of the D-Wave Advantage\ndevice, offering superior scalability over existing quantum approaches and\noutperforming state-of-the-art classical methods. Our empirical evaluations on\nsynthetic datasets reveal that Q-Seg offers better runtime performance against\nthe classical optimizer Gurobi. Furthermore, we evaluate our method on\nsegmentation of Earth Observation images, an area of application where the\namount of labeled data is usually very limited. In this case, Q-Seg\ndemonstrates near-optimal results in flood mapping detection with respect to\nclassical supervised state-of-the-art machine learning methods. Also, Q-Seg\nprovides enhanced segmentation for forest coverage compared to existing\nannotated masks. Thus, Q-Seg emerges as a viable alternative for real-world\napplications using available quantum hardware, particularly in scenarios where\nthe lack of labeled data and computational runtime are critical.","terms":["cs.CV","quant-ph"]},{"titles":"Exploring the Temperature-Dependent Phase Transition in Modern Hopfield Networks","summaries":"The recent discovery of a connection between Transformers and Modern Hopfield\nNetworks (MHNs) has reignited the study of neural networks from a physical\nenergy-based perspective. This paper focuses on the pivotal effect of the\ninverse temperature hyperparameter $\\beta$ on the distribution of energy minima\nof the MHN. To achieve this, the distribution of energy minima is tracked in a\nsimplified MHN in which equidistant normalised patterns are stored. This\nnetwork demonstrates a phase transition at a critical temperature\n$\\beta_{\\text{c}}$, from a single global attractor towards highly pattern\nspecific minima as $\\beta$ is increased. Importantly, the dynamics are not\nsolely governed by the hyperparameter $\\beta$ but are instead determined by an\neffective inverse temperature $\\beta_{\\text{eff}}$ which also depends on the\ndistribution and size of the stored patterns. Recognizing the role of\nhyperparameters in the MHN could, in the future, aid researchers in the domain\nof Transformers to optimise their initial choices, potentially reducing the\nnecessity for time and energy expensive hyperparameter fine-tuning.","terms":["cs.LG","cond-mat.dis-nn"]},{"titles":"RainAI -- Precipitation Nowcasting from Satellite Data","summaries":"This paper presents a solution to the Weather4Cast 2023 competition, where\nthe goal is to forecast high-resolution precipitation with an 8-hour lead time\nusing lower-resolution satellite radiance images. We propose a simple, yet\neffective method for spatiotemporal feature learning using a 2D U-Net model,\nthat outperforms the official 3D U-Net baseline in both performance and\nefficiency. We place emphasis on refining the dataset, through importance\nsampling and dataset preparation, and show that such techniques have a\nsignificant impact on performance. We further study an alternative\ncross-entropy loss function that improves performance over the standard mean\nsquared error loss, while also enabling models to produce probabilistic\noutputs. Additional techniques are explored regarding the generation of\npredictions at different lead times, specifically through Conditioning Lead\nTime. Lastly, to generate high-resolution forecasts, we evaluate standard and\nlearned upsampling methods. The code and trained parameters are available at\nhttps:\/\/github.com\/rafapablos\/w4c23-rainai.","terms":["cs.CV","cs.LG","physics.ao-ph"]},{"titles":"Beyond the Field-of-View: Enhancing Scene Visibility and Perception with Clip-Recurrent Transformer","summaries":"Vision sensors are widely applied in vehicles, robots, and roadside\ninfrastructure. However, due to limitations in hardware cost and system size,\ncamera Field-of-View (FoV) is often restricted and may not provide sufficient\ncoverage. Nevertheless, from a spatiotemporal perspective, it is possible to\nobtain information beyond the camera's physical FoV from past video streams. In\nthis paper, we propose the concept of online video inpainting for autonomous\nvehicles to expand the field of view, thereby enhancing scene visibility,\nperception, and system safety. To achieve this, we introduce the FlowLens\narchitecture, which explicitly employs optical flow and implicitly incorporates\na novel clip-recurrent transformer for feature propagation. FlowLens offers two\nkey features: 1) FlowLens includes a newly designed Clip-Recurrent Hub with\n3D-Decoupled Cross Attention (DDCA) to progressively process global information\naccumulated over time. 2) It integrates a multi-branch Mix Fusion Feed Forward\nNetwork (MixF3N) to enhance the precise spatial flow of local features. To\nfacilitate training and evaluation, we derive the KITTI360 dataset with various\nFoV mask, which covers both outer- and inner FoV expansion scenarios. We also\nconduct quantitative assessments of beyond-FoV semantics across different\nmodels and perform qualitative comparisons of beyond-FoV object detection. We\nillustrate that employing FlowLens to reconstruct unseen scenes even enhances\nperception within the field of view by providing reliable semantic context.\nExtensive experiments and user studies involving offline and online video\ninpainting, as well as beyond-FoV perception tasks, demonstrate that FlowLens\nachieves state-of-the-art performance. The source code and dataset are made\npublicly available at https:\/\/github.com\/MasterHow\/FlowLens.","terms":["cs.CV","eess.IV"]},{"titles":"An Interventional Perspective on Identifiability in Gaussian LTI Systems with Independent Component Analysis","summaries":"We investigate the relationship between system identification and\nintervention design in dynamical systems. While previous research demonstrated\nhow identifiable representation learning methods, such as Independent Component\nAnalysis (ICA), can reveal cause-effect relationships, it relied on a passive\nperspective without considering how to collect data. Our work shows that in\nGaussian Linear Time-Invariant (LTI) systems, the system parameters can be\nidentified by introducing diverse intervention signals in a multi-environment\nsetting. By harnessing appropriate diversity assumptions motivated by the ICA\nliterature, our findings connect experiment design and representational\nidentifiability in dynamical systems. We corroborate our findings on synthetic\nand (simulated) physical data. Additionally, we show that Hidden Markov Models,\nin general, and (Gaussian) LTI systems, in particular, fulfil a generalization\nof the Causal de Finetti theorem with continuous parameters.","terms":["cs.LG","cs.CE","cs.SY","eess.SY","stat.ME"]},{"titles":"CG3D: Compositional Generation for Text-to-3D via Gaussian Splatting","summaries":"With the onset of diffusion-based generative models and their ability to\ngenerate text-conditioned images, content generation has received a massive\ninvigoration. Recently, these models have been shown to provide useful guidance\nfor the generation of 3D graphics assets. However, existing work in\ntext-conditioned 3D generation faces fundamental constraints: (i) inability to\ngenerate detailed, multi-object scenes, (ii) inability to textually control\nmulti-object configurations, and (iii) physically realistic scene composition.\nIn this work, we propose CG3D, a method for compositionally generating scalable\n3D assets that resolves these constraints. We find that explicit Gaussian\nradiance fields, parameterized to allow for compositions of objects, possess\nthe capability to enable semantically and physically consistent scenes. By\nutilizing a guidance framework built around this explicit representation, we\nshow state of the art results, capable of even exceeding the guiding diffusion\nmodel in terms of object combinations and physics accuracy.","terms":["cs.CV","cs.AI"]},{"titles":"Evaluating VLMs for Score-Based, Multi-Probe Annotation of 3D Objects","summaries":"Unlabeled 3D objects present an opportunity to leverage pretrained vision\nlanguage models (VLMs) on a range of annotation tasks -- from describing object\nsemantics to physical properties. An accurate response must take into account\nthe full appearance of the object in 3D, various ways of phrasing the\nquestion\/prompt, and changes in other factors that affect the response. We\npresent a method to marginalize over any factors varied across VLM queries,\nutilizing the VLM's scores for sampled responses. We first show that this\nprobabilistic aggregation can outperform a language model (e.g., GPT4) for\nsummarization, for instance avoiding hallucinations when there are contrasting\ndetails between responses. Secondly, we show that aggregated annotations are\nuseful for prompt-chaining; they help improve downstream VLM predictions (e.g.,\nof object material when the object's type is specified as an auxiliary input in\nthe prompt). Such auxiliary inputs allow ablating and measuring the\ncontribution of visual reasoning over language-only reasoning. Using these\nevaluations, we show how VLMs can approach, without additional training or\nin-context learning, the quality of human-verified type and material\nannotations on the large-scale Objaverse dataset.","terms":["cs.CV"]},{"titles":"HumanNorm: Learning Normal Diffusion Model for High-quality and Realistic 3D Human Generation","summaries":"Recent text-to-3D methods employing diffusion models have made significant\nadvancements in 3D human generation. However, these approaches face challenges\ndue to the limitations of text-to-image diffusion models, which lack an\nunderstanding of 3D structures. Consequently, these methods struggle to achieve\nhigh-quality human generation, resulting in smooth geometry and cartoon-like\nappearances. In this paper, we propose HumanNorm, a novel approach for\nhigh-quality and realistic 3D human generation. The main idea is to enhance the\nmodel's 2D perception of 3D geometry by learning a normal-adapted diffusion\nmodel and a normal-aligned diffusion model. The normal-adapted diffusion model\ncan generate high-fidelity normal maps corresponding to user prompts with\nview-dependent and body-aware text. The normal-aligned diffusion model learns\nto generate color images aligned with the normal maps, thereby transforming\nphysical geometry details into realistic appearance. Leveraging the proposed\nnormal diffusion model, we devise a progressive geometry generation strategy\nand a multi-step Score Distillation Sampling (SDS) loss to enhance the\nperformance of 3D human generation. Comprehensive experiments substantiate\nHumanNorm's ability to generate 3D humans with intricate geometry and realistic\nappearances. HumanNorm outperforms existing text-to-3D methods in both geometry\nand texture quality. The project page of HumanNorm is\nhttps:\/\/humannorm.github.io\/.","terms":["cs.CV"]},{"titles":"Physics-informed neural networks for transformed geometries and manifolds","summaries":"Physics-informed neural networks (PINNs) effectively embed physical\nprinciples into machine learning, but often struggle with complex or\nalternating geometries. We propose a novel method for integrating geometric\ntransformations within PINNs to robustly accommodate geometric variations. Our\nmethod incorporates a diffeomorphism as a mapping of a reference domain and\nadapts the derivative computation of the physics-informed loss function. This\ngeneralizes the applicability of PINNs not only to smoothly deformed domains,\nbut also to lower-dimensional manifolds and allows for direct shape\noptimization while training the network. We demonstrate the effectivity of our\napproach on several problems: (i) Eikonal equation on Archimedean spiral, (ii)\nPoisson problem on surface manifold, (iii) Incompressible Stokes flow in\ndeformed tube, and (iv) Shape optimization with Laplace operator. Through these\nexamples, we demonstrate the enhanced flexibility over traditional PINNs,\nespecially under geometric variations. The proposed framework presents an\noutlook for training deep neural operators over parametrized geometries, paving\nthe way for advanced modeling with PDEs on complex geometries in science and\nengineering.","terms":["cs.LG","cs.CE"]},{"titles":"Using Ornstein-Uhlenbeck Process to understand Denoising Diffusion Probabilistic Model and its Noise Schedules","summaries":"The aim of this short note is to show that Denoising Diffusion Probabilistic\nModel DDPM, a non-homogeneous discrete-time Markov process, can be represented\nby a time-homogeneous continuous-time Markov process observed at non-uniformly\nsampled discrete times. Surprisingly, this continuous-time Markov process is\nthe well-known and well-studied Ornstein-Ohlenbeck (OU) process, which was\ndeveloped in 1930's for studying Brownian particles in Harmonic potentials. We\nestablish the formal equivalence between DDPM and the OU process using its\nanalytical solution. We further demonstrate that the design problem of the\nnoise scheduler for non-homogeneous DDPM is equivalent to designing observation\ntimes for the OU process. We present several heuristic designs for observation\ntimes based on principled quantities such as auto-variance and Fisher\nInformation and connect them to ad hoc noise schedules for DDPM. Interestingly,\nwe show that the Fisher-Information-motivated schedule corresponds exactly the\ncosine schedule, which was developed without any theoretical foundation but is\nthe current state-of-the-art noise schedule.","terms":["stat.ML","cond-mat.stat-mech","cs.AI","cs.LG","math-ph","math.MP"]},{"titles":"Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion","summaries":"We present a novel method for precise 3D object localization in single images\nfrom a single calibrated camera using only 2D labels. No expensive 3D labels\nare needed. Thus, instead of using 3D labels, our model is trained with\neasy-to-annotate 2D labels along with the physical knowledge of the object's\nmotion. Given this information, the model can infer the latent third dimension,\neven though it has never seen this information during training. Our method is\nevaluated on both synthetic and real-world datasets, and we are able to achieve\na mean distance error of just 6 cm in our experiments on real data. The results\nindicate the method's potential as a step towards learning 3D object location\nestimation, where collecting 3D data for training is not feasible.","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"Natural & Adversarial Bokeh Rendering via Circle-of-Confusion Predictive Network","summaries":"Bokeh effect is a natural shallow depth-of-field phenomenon that blurs the\nout-of-focus part in photography. In recent years, a series of works have\nproposed automatic and realistic bokeh rendering methods for artistic and\naesthetic purposes. They usually employ cutting-edge data-driven deep\ngenerative networks with complex training strategies and network architectures.\nHowever, these works neglect that the bokeh effect, as a real phenomenon, can\ninevitably affect the subsequent visual intelligent tasks like recognition, and\ntheir data-driven nature prevents them from studying the influence of\nbokeh-related physical parameters (i.e., depth-of-the-field) on the intelligent\ntasks. To fill this gap, we study a totally new problem, i.e., natural &\nadversarial bokeh rendering, which consists of two objectives: rendering\nrealistic and natural bokeh and fooling the visual perception models (i.e.,\nbokeh-based adversarial attack). To this end, beyond the pure data-driven\nsolution, we propose a hybrid alternative by taking the respective advantages\nof data-driven and physical-aware methods. Specifically, we propose the\ncircle-of-confusion predictive network (CoCNet) by taking the all-in-focus\nimage and depth image as inputs to estimate circle-of-confusion parameters for\neach pixel, which are employed to render the final image through a well-known\nphysical model of bokeh. With the hybrid solution, our method could achieve\nmore realistic rendering results with the naive training strategy and a much\nlighter network.","terms":["cs.CV"]},{"titles":"Uncertainty Quantification in Neural-Network Based Pain Intensity Estimation","summaries":"Improper pain management can lead to severe physical or mental consequences,\nincluding suffering, and an increased risk of opioid dependency. Assessing the\npresence and severity of pain is imperative to prevent such outcomes and\ndetermine the appropriate intervention. However, the evaluation of pain\nintensity is challenging because different individuals experience pain\ndifferently. To overcome this, researchers have employed machine learning\nmodels to evaluate pain intensity objectively. However, these efforts have\nprimarily focused on point estimation of pain, disregarding the inherent\nuncertainty and variability present in the data and model. Consequently, the\npoint estimates provide only partial information for clinical decision-making.\nThis study presents a neural network-based method for objective pain interval\nestimation, incorporating uncertainty quantification. This work explores three\nalgorithms: the bootstrap method, lower and upper bound estimation (LossL)\noptimized by genetic algorithm, and modified lower and upper bound estimation\n(LossS) optimized by gradient descent algorithm. Our empirical results reveal\nthat LossS outperforms the other two by providing a narrower prediction\ninterval. As LossS outperforms, we assessed its performance in three different\nscenarios for pain assessment: (1) a generalized approach (single model for the\nentire population), (2) a personalized approach (separate model for each\nindividual), and (3) a hybrid approach (separate model for each cluster of\nindividuals). Our findings demonstrate the hybrid approach's superior\nperformance, with notable practicality in clinical contexts. It has the\npotential to be a valuable tool for clinicians, enabling objective pain\nintensity assessment while taking uncertainty into account. This capability is\ncrucial in facilitating effective pain management and reducing the risks\nassociated with improper treatment.","terms":["cs.LG"]},{"titles":"Discovering Galaxy Features via Dataset Distillation","summaries":"In many applications, Neural Nets (NNs) have classification performance on\npar or even exceeding human capacity. Moreover, it is likely that NNs leverage\nunderlying features that might differ from those humans perceive to classify.\nCan we \"reverse-engineer\" pertinent features to enhance our scientific\nunderstanding? Here, we apply this idea to the notoriously difficult task of\ngalaxy classification: NNs have reached high performance for this task, but\nwhat does a neural net (NN) \"see\" when it classifies galaxies? Are there\nmorphological features that the human eye might overlook that could help with\nthe task and provide new insights? Can we visualize tracers of early evolution,\nor additionally incorporated spectral data? We present a novel way to summarize\nand visualize galaxy morphology through the lens of neural networks, leveraging\nDataset Distillation, a recent deep-learning methodology with the primary\nobjective to distill knowledge from a large dataset and condense it into a\ncompact synthetic dataset, such that a model trained on this synthetic dataset\nachieves performance comparable to a model trained on the full dataset. We\ncurate a class-balanced, medium-size high-confidence version of the Galaxy Zoo\n2 dataset, and proceed with dataset distillation from our accurate\nNN-classifier to create synthesized prototypical images of galaxy morphological\nfeatures, demonstrating its effectiveness. Of independent interest, we\nintroduce a self-adaptive version of the state-of-the-art Matching Trajectory\nalgorithm to automate the distillation process, and show enhanced performance\non computer vision benchmarks.","terms":["cs.CV","astro-ph.IM","cs.LG"]},{"titles":"Model Performance Prediction for Hyperparameter Optimization of Deep Learning Models Using High Performance Computing and Quantum Annealing","summaries":"Hyperparameter Optimization (HPO) of Deep Learning-based models tends to be a\ncompute resource intensive process as it usually requires to train the target\nmodel with many different hyperparameter configurations. We show that\nintegrating model performance prediction with early stopping methods holds\ngreat potential to speed up the HPO process of deep learning models. Moreover,\nwe propose a novel algorithm called Swift-Hyperband that can use either\nclassical or quantum support vector regression for performance prediction and\nbenefit from distributed High Performance Computing environments. This\nalgorithm is tested not only for the Machine-Learned Particle Flow model used\nin High Energy Physics, but also for a wider range of target models from\ndomains such as computer vision and natural language processing.\nSwift-Hyperband is shown to find comparable (or better) hyperparameters as well\nas using less computational resources in all test cases.","terms":["cs.LG","physics.data-an"]},{"titles":"HandRefiner: Refining Malformed Hands in Generated Images by Diffusion-based Conditional Inpainting","summaries":"Diffusion models have achieved remarkable success in generating realistic\nimages but suffer from generating accurate human hands, such as incorrect\nfinger counts or irregular shapes. This difficulty arises from the complex task\nof learning the physical structure and pose of hands from training images,\nwhich involves extensive deformations and occlusions. For correct hand\ngeneration, our paper introduces a lightweight post-processing solution called\n$\\textbf{HandRefiner}$. HandRefiner employs a conditional inpainting approach\nto rectify malformed hands while leaving other parts of the image untouched. We\nleverage the hand mesh reconstruction model that consistently adheres to the\ncorrect number of fingers and hand shape, while also being capable of fitting\nthe desired hand pose in the generated image. Given a generated failed image\ndue to malformed hands, we utilize ControlNet modules to re-inject such correct\nhand information. Additionally, we uncover a phase transition phenomenon within\nControlNet as we vary the control strength. It enables us to take advantage of\nmore readily available synthetic data without suffering from the domain gap\nbetween realistic and synthetic hands. Experiments demonstrate that HandRefiner\ncan significantly improve the generation quality quantitatively and\nqualitatively. The code is available at\nhttps:\/\/github.com\/wenquanlu\/HandRefiner .","terms":["cs.CV"]},{"titles":"NeISF: Neural Incident Stokes Field for Geometry and Material Estimation","summaries":"Multi-view inverse rendering is the problem of estimating the scene\nparameters such as shapes, materials, or illuminations from a sequence of\nimages captured under different viewpoints. Many approaches, however, assume\nsingle light bounce and thus fail to recover challenging scenarios like\ninter-reflections. On the other hand, simply extending those methods to\nconsider multi-bounced light requires more assumptions to alleviate the\nambiguity. To address this problem, we propose Neural Incident Stokes Fields\n(NeISF), a multi-view inverse rendering framework that reduces ambiguities\nusing polarization cues. The primary motivation for using polarization cues is\nthat it is the accumulation of multi-bounced light, providing rich information\nabout geometry and material. Based on this knowledge, the proposed incident\nStokes field efficiently models the accumulated polarization effect with the\naid of an original physically-based differentiable polarimetric renderer.\nLastly, experimental results show that our method outperforms the existing\nworks in synthetic and real scenarios.","terms":["cs.CV"]},{"titles":"Image Clustering Conditioned on Text Criteria","summaries":"Classical clustering methods do not provide users with direct control of the\nclustering results, and the clustering results may not be consistent with the\nrelevant criterion that a user has in mind. In this work, we present a new\nmethodology for performing image clustering based on user-specified text\ncriteria by leveraging modern vision-language models and large language models.\nWe call our method Image Clustering Conditioned on Text Criteria (IC|TC), and\nit represents a different paradigm of image clustering. IC|TC requires a\nminimal and practical degree of human intervention and grants the user\nsignificant control over the clustering results in return. Our experiments show\nthat IC|TC can effectively cluster images with various criteria, such as human\naction, physical location, or the person's mood, while significantly\noutperforming baselines.","terms":["cs.CV","cs.AI"]},{"titles":"RADAP: A Robust and Adaptive Defense Against Diverse Adversarial Patches on Face Recognition","summaries":"Face recognition (FR) systems powered by deep learning have become widely\nused in various applications. However, they are vulnerable to adversarial\nattacks, especially those based on local adversarial patches that can be\nphysically applied to real-world objects. In this paper, we propose RADAP, a\nrobust and adaptive defense mechanism against diverse adversarial patches in\nboth closed-set and open-set FR systems. RADAP employs innovative techniques,\nsuch as FCutout and F-patch, which use Fourier space sampling masks to improve\nthe occlusion robustness of the FR model and the performance of the patch\nsegmenter. Moreover, we introduce an edge-aware binary cross-entropy (EBCE)\nloss function to enhance the accuracy of patch detection. We also present the\nsplit and fill (SAF) strategy, which is designed to counter the vulnerability\nof the patch segmenter to complete white-box adaptive attacks. We conduct\ncomprehensive experiments to validate the effectiveness of RADAP, which shows\nsignificant improvements in defense performance against various adversarial\npatches, while maintaining clean accuracy higher than that of the undefended\nVanilla model.","terms":["cs.CV","cs.CR"]},{"titles":"BOIS: Bayesian Optimization of Interconnected Systems","summaries":"Bayesian optimization (BO) has proven to be an effective paradigm for the\nglobal optimization of expensive-to-sample systems. One of the main advantages\nof BO is its use of Gaussian processes (GPs) to characterize model uncertainty\nwhich can be leveraged to guide the learning and search process. However, BO\ntypically treats systems as black-boxes and this limits the ability to exploit\nstructural knowledge (e.g., physics and sparse interconnections). Composite\nfunctions of the form $f(x, y(x))$, wherein GP modeling is shifted from the\nperformance function $f$ to an intermediate function $y$, offer an avenue for\nexploiting structural knowledge. However, the use of composite functions in a\nBO framework is complicated by the need to generate a probability density for\n$f$ from the Gaussian density of $y$ calculated by the GP (e.g., when $f$ is\nnonlinear it is not possible to obtain a closed-form expression). Previous work\nhas handled this issue using sampling techniques; these are easy to implement\nand flexible but are computationally intensive. In this work, we introduce a\nnew paradigm which allows for the efficient use of composite functions in BO;\nthis uses adaptive linearizations of $f$ to obtain closed-form expressions for\nthe statistical moments of the composite function. We show that this simple\napproach (which we call BOIS) enables the exploitation of structural knowledge,\nsuch as that arising in interconnected systems as well as systems that embed\nmultiple GP models and combinations of physics and GP models. Using a chemical\nprocess optimization case study, we benchmark the effectiveness of BOIS against\nstandard BO and sampling approaches. Our results indicate that BOIS achieves\nperformance gains and accurately captures the statistics of composite\nfunctions.","terms":["stat.ML","cs.LG"]},{"titles":"Fourier Neural Differential Equations for learning Quantum Field Theories","summaries":"A Quantum Field Theory is defined by its interaction Hamiltonian, and linked\nto experimental data by the scattering matrix. The scattering matrix is\ncalculated as a perturbative series, and represented succinctly as a first\norder differential equation in time. Neural Differential Equations (NDEs) learn\nthe time derivative of a residual network's hidden state, and have proven\nefficacy in learning differential equations with physical constraints. Hence\nusing an NDE to learn particle scattering matrices presents a possible\nexperiment-theory phenomenological connection. In this paper, NDE models are\nused to learn $\\phi^4$ theory, Scalar-Yukawa theory and Scalar Quantum\nElectrodynamics. A new NDE architecture is also introduced, the Fourier Neural\nDifferential Equation (FNDE), which combines NDE integration and Fourier\nnetwork convolution. The FNDE model demonstrates better generalisability than\nthe non-integrated equivalent FNO model. It is also shown that by training on\nscattering data, the interaction Hamiltonian of a theory can be extracted from\nnetwork parameters.","terms":["cs.LG","hep-ph","quant-ph"]},{"titles":"Minimax Exploiter: A Data Efficient Approach for Competitive Self-Play","summaries":"Recent advances in Competitive Self-Play (CSP) have achieved, or even\nsurpassed, human level performance in complex game environments such as Dota 2\nand StarCraft II using Distributed Multi-Agent Reinforcement Learning (MARL).\nOne core component of these methods relies on creating a pool of learning\nagents -- consisting of the Main Agent, past versions of this agent, and\nExploiter Agents -- where Exploiter Agents learn counter-strategies to the Main\nAgents. A key drawback of these approaches is the large computational cost and\nphysical time that is required to train the system, making them impractical to\ndeploy in highly iterative real-life settings such as video game productions.\nIn this paper, we propose the Minimax Exploiter, a game theoretic approach to\nexploiting Main Agents that leverages knowledge of its opponents, leading to\nsignificant increases in data efficiency. We validate our approach in a\ndiversity of settings, including simple turn based games, the arcade learning\nenvironment, and For Honor, a modern video game. The Minimax Exploiter\nconsistently outperforms strong baselines, demonstrating improved stability and\ndata efficiency, leading to a robust CSP-MARL method that is both flexible and\neasy to deploy.","terms":["cs.LG","cs.AI","cs.MA"]},{"titles":"Material Palette: Extraction of Materials from a Single Image","summaries":"In this paper, we propose a method to extract physically-based rendering\n(PBR) materials from a single real-world image. We do so in two steps: first,\nwe map regions of the image to material concepts using a diffusion model, which\nallows the sampling of texture images resembling each material in the scene.\nSecond, we benefit from a separate network to decompose the generated textures\ninto Spatially Varying BRDFs (SVBRDFs), providing us with materials ready to be\nused in rendering applications. Our approach builds on existing synthetic\nmaterial libraries with SVBRDF ground truth, but also exploits a\ndiffusion-generated RGB texture dataset to allow generalization to new samples\nusing unsupervised domain adaptation (UDA). Our contributions are thoroughly\nevaluated on synthetic and real-world datasets. We further demonstrate the\napplicability of our method for editing 3D scenes with materials estimated from\nreal photographs. The code and models will be made open-source. Project page:\nhttps:\/\/astra-vision.github.io\/MaterialPalette\/","terms":["cs.CV","cs.GR"]},{"titles":"An Investigation of Time Reversal Symmetry in Reinforcement Learning","summaries":"One of the fundamental challenges associated with reinforcement learning (RL)\nis that collecting sufficient data can be both time-consuming and expensive. In\nthis paper, we formalize a concept of time reversal symmetry in a Markov\ndecision process (MDP), which builds upon the established structure of\ndynamically reversible Markov chains (DRMCs) and time-reversibility in\nclassical physics. Specifically, we investigate the utility of this concept in\nreducing the sample complexity of reinforcement learning. We observe that\nutilizing the structure of time reversal in an MDP allows every environment\ntransition experienced by an agent to be transformed into a feasible\nreverse-time transition, effectively doubling the number of experiences in the\nenvironment. To test the usefulness of this newly synthesized data, we develop\na novel approach called time symmetric data augmentation (TSDA) and investigate\nits application in both proprioceptive and pixel-based state within the realm\nof off-policy, model-free RL. Empirical evaluations showcase how these\nsynthetic transitions can enhance the sample efficiency of RL agents in time\nreversible scenarios without friction or contact. We also test this method in\nmore realistic environments where these assumptions are not globally satisfied.\nWe find that TSDA can significantly degrade sample efficiency and policy\nperformance, but can also improve sample efficiency under the right conditions.\nUltimately we conclude that time symmetry shows promise in enhancing the sample\nefficiency of reinforcement learning and provide guidance when the environment\nand reward structures are of an appropriate form for TSDA to be employed\neffectively.","terms":["cs.LG"]},{"titles":"On the Impact of Sampling on Deep Sequential State Estimation","summaries":"State inference and parameter learning in sequential models can be\nsuccessfully performed with approximation techniques that maximize the evidence\nlower bound to the marginal log-likelihood of the data distribution. These\nmethods may be referred to as Dynamical Variational Autoencoders, and our\nspecific focus lies on the deep Kalman filter. It has been shown that the ELBO\nobjective can oversimplify data representations, potentially compromising\nestimation quality. Tighter Monte Carlo objectives have been proposed in the\nliterature to enhance generative modeling performance. For instance, the IWAE\nobjective uses importance weights to reduce the variance of marginal\nlog-likelihood estimates. In this paper, importance sampling is applied to the\nDKF framework for learning deep Markov models, resulting in the IW-DKF, which\nshows an improvement in terms of log-likelihood estimates and KL divergence\nbetween the variational distribution and the transition model. The framework\nusing the sampled DKF update rule is also accommodated to address sequential\nstate and parameter estimation when working with highly non-linear\nphysics-based models. An experiment with the 3-space Lorenz attractor shows an\nenhanced generative modeling performance and also a decrease in RMSE when\nestimating the model parameters and latent states, indicating that tighter MCOs\nlead to improved state inference performance.","terms":["cs.LG","eess.SP"]},{"titles":"SE(3) Equivariant Augmented Coupling Flows","summaries":"Coupling normalizing flows allow for fast sampling and density evaluation,\nmaking them the tool of choice for probabilistic modeling of physical systems.\nHowever, the standard coupling architecture precludes endowing flows that\noperate on the Cartesian coordinates of atoms with the SE(3) and permutation\ninvariances of physical systems. This work proposes a coupling flow that\npreserves SE(3) and permutation equivariance by performing coordinate splits\nalong additional augmented dimensions. At each layer, the flow maps atoms'\npositions into learned SE(3) invariant bases, where we apply standard flow\ntransformations, such as monotonic rational-quadratic splines, before returning\nto the original basis. Crucially, our flow preserves fast sampling and density\nevaluation, and may be used to produce unbiased estimates of expectations with\nrespect to the target distribution via importance sampling. When trained on the\nDW4, LJ13, and QM9-positional datasets, our flow is competitive with\nequivariant continuous normalizing flows, while allowing sampling more than an\norder of magnitude faster. Moreover, to the best of our knowledge, we are the\nfirst to learn the full Boltzmann distribution of alanine dipeptide by only\nmodeling the Cartesian positions of its atoms. Lastly, we demonstrate that our\nflow can be trained to approximately sample from the Boltzmann distribution of\nthe DW4 and LJ13 particle systems using only their energy functions.","terms":["cs.LG","physics.comp-ph"]},{"titles":"Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation","summaries":"Current AI-based methods do not provide comprehensible physical\ninterpretations of the utilized data, extracted features, and\npredictions\/inference operations. As a result, deep learning models trained\nusing high-resolution satellite imagery lack transparency and explainability\nand can be merely seen as a black box, which limits their wide-level adoption.\nExperts need help understanding the complex behavior of AI models and the\nunderlying decision-making process. The explainable artificial intelligence\n(XAI) field is an emerging field providing means for robust, practical, and\ntrustworthy deployment of AI models. Several XAI techniques have been proposed\nfor image classification tasks, whereas the interpretation of image\nsegmentation remains largely unexplored. This paper offers to bridge this gap\nby adapting the recent XAI classification algorithms and making them usable for\nmuti-class image segmentation, where we mainly focus on buildings' segmentation\nfrom high-resolution satellite images. To benchmark and compare the performance\nof the proposed approaches, we introduce a new XAI evaluation methodology and\nmetric based on \"Entropy\" to measure the model uncertainty. Conventional XAI\nevaluation methods rely mainly on feeding area-of-interest regions from the\nimage back to the pre-trained (utility) model and then calculating the average\nchange in the probability of the target class. Those evaluation metrics lack\nthe needed robustness, and we show that using Entropy to monitor the model\nuncertainty in segmenting the pixels within the target class is more suitable.\nWe hope this work will pave the way for additional XAI research for image\nsegmentation and applications in the remote sensing discipline.","terms":["cs.CV","cs.AI","cs.CL","cs.LG"]},{"titles":"CompenHR: Efficient Full Compensation for High-resolution Projector","summaries":"Full projector compensation is a practical task of projector-camera systems.\nIt aims to find a projector input image, named compensation image, such that\nwhen projected it cancels the geometric and photometric distortions due to the\nphysical environment and hardware. State-of-the-art methods use deep learning\nto address this problem and show promising performance for low-resolution\nsetups. However, directly applying deep learning to high-resolution setups is\nimpractical due to the long training time and high memory cost. To address this\nissue, this paper proposes a practical full compensation solution. Firstly, we\ndesign an attention-based grid refinement network to improve geometric\ncorrection quality. Secondly, we integrate a novel sampling scheme into an\nend-to-end compensation network to alleviate computation and introduce\nattention blocks to preserve key features. Finally, we construct a benchmark\ndataset for high-resolution projector full compensation. In experiments, our\nmethod demonstrates clear advantages in both efficiency and quality.","terms":["cs.CV","cs.MM"]},{"titles":"SplitNeRF: Split Sum Approximation Neural Field for Joint Geometry, Illumination, and Material Estimation","summaries":"We present a novel approach for digitizing real-world objects by estimating\ntheir geometry, material properties, and environmental lighting from a set of\nposed images with fixed lighting. Our method incorporates into Neural Radiance\nField (NeRF) pipelines the split sum approximation used with image-based\nlighting for real-time physical-based rendering. We propose modeling the\nscene's lighting with a single scene-specific MLP representing pre-integrated\nimage-based lighting at arbitrary resolutions. We achieve accurate modeling of\npre-integrated lighting by exploiting a novel regularizer based on efficient\nMonte Carlo sampling. Additionally, we propose a new method of supervising\nself-occlusion predictions by exploiting a similar regularizer based on Monte\nCarlo sampling. Experimental results demonstrate the efficiency and\neffectiveness of our approach in estimating scene geometry, material\nproperties, and lighting. Our method is capable of attaining state-of-the-art\nrelighting quality after only ${\\sim}1$ hour of training in a single NVIDIA\nA100 GPU.","terms":["cs.CV","cs.AI","cs.GR"]},{"titles":"MultiModal-Learning for Predicting Molecular Properties: A Framework Based on Image and Graph Structures","summaries":"The quest for accurate prediction of drug molecule properties poses a\nfundamental challenge in the realm of Artificial Intelligence Drug Discovery\n(AIDD). An effective representation of drug molecules emerges as a pivotal\ncomponent in this pursuit. Contemporary leading-edge research predominantly\nresorts to self-supervised learning (SSL) techniques to extract meaningful\nstructural representations from large-scale, unlabeled molecular data,\nsubsequently fine-tuning these representations for an array of downstream\ntasks. However, an inherent shortcoming of these studies lies in their singular\nreliance on one modality of molecular information, such as molecule image or\nSMILES representations, thus neglecting the potential complementarity of\nvarious molecular modalities. In response to this limitation, we propose MolIG,\na novel MultiModaL molecular pre-training framework for predicting molecular\nproperties based on Image and Graph structures. MolIG model innovatively\nleverages the coherence and correlation between molecule graph and molecule\nimage to execute self-supervised tasks, effectively amalgamating the strengths\nof both molecular representation forms. This holistic approach allows for the\ncapture of pivotal molecular structural characteristics and high-level semantic\ninformation. Upon completion of pre-training, Graph Neural Network (GNN)\nEncoder is used for the prediction of downstream tasks. In comparison to\nadvanced baseline models, MolIG exhibits enhanced performance in downstream\ntasks pertaining to molecular property prediction within benchmark groups such\nas MoleculeNet Benchmark Group and ADMET Benchmark Group.","terms":["cs.LG","cs.AI","physics.chem-ph","q-bio.BM"]},{"titles":"Pseudo-Likelihood Inference","summaries":"Simulation-Based Inference (SBI) is a common name for an emerging family of\napproaches that infer the model parameters when the likelihood is intractable.\nExisting SBI methods either approximate the likelihood, such as Approximate\nBayesian Computation (ABC) or directly model the posterior, such as Sequential\nNeural Posterior Estimation (SNPE). While ABC is efficient on low-dimensional\nproblems, on higher-dimensional tasks, it is generally outperformed by SNPE,\nwhich leverages function approximation. In this paper, we propose\nPseudo-Likelihood Inference (PLI), a new method that brings neural\napproximation into ABC, making it competitive on challenging Bayesian system\nidentification tasks. By utilizing integral probability metrics, we introduce a\nsmooth likelihood kernel with an adaptive bandwidth that is updated based on\ninformation-theoretic trust regions. Thanks to this formulation, our method (i)\nallows for optimizing neural posteriors via gradient descent, (ii) does not\nrely on summary statistics, and (iii) enables multiple observations as input.\nIn comparison to SNPE, it leads to improved performance when more data is\navailable. The effectiveness of PLI is evaluated on four classical SBI\nbenchmark tasks and on a highly dynamic physical system, showing particular\nadvantages on stochastic simulations and multi-modal posterior landscapes.","terms":["cs.LG","stat.ML"]},{"titles":"Augmenting x-ray single particle imaging reconstruction with self-supervised machine learning","summaries":"The development of X-ray Free Electron Lasers (XFELs) has opened numerous\nopportunities to probe atomic structure and ultrafast dynamics of various\nmaterials. Single Particle Imaging (SPI) with XFELs enables the investigation\nof biological particles in their natural physiological states with unparalleled\ntemporal resolution, while circumventing the need for cryogenic conditions or\ncrystallization. However, reconstructing real-space structures from\nreciprocal-space x-ray diffraction data is highly challenging due to the\nabsence of phase and orientation information, which is further complicated by\nweak scattering signals and considerable fluctuations in the number of photons\nper pulse. In this work, we present an end-to-end, self-supervised machine\nlearning approach to recover particle orientations and estimate reciprocal\nspace intensities from diffraction images only. Our method demonstrates great\nrobustness under demanding experimental conditions with significantly enhanced\nreconstruction capabilities compared with conventional algorithms, and\nsignifies a paradigm shift in SPI as currently practiced at XFELs.","terms":["cs.CV","eess.IV","physics.app-ph","physics.comp-ph"]},{"titles":"Opening the Black Box: Towards inherently interpretable energy data imputation models using building physics insight","summaries":"Missing data are frequently observed by practitioners and researchers in the\nbuilding energy modeling community. In this regard, advanced data-driven\nsolutions, such as Deep Learning methods, are typically required to reflect the\nnon-linear behavior of these anomalies. As an ongoing research question related\nto Deep Learning, a model's applicability to limited data settings can be\nexplored by introducing prior knowledge in the network. This same strategy can\nalso lead to more interpretable predictions, hence facilitating the field\napplication of the approach. For that purpose, the aim of this paper is to\npropose the use of Physics-informed Denoising Autoencoders (PI-DAE) for missing\ndata imputation in commercial buildings. In particular, the presented method\nenforces physics-inspired soft constraints to the loss function of a Denoising\nAutoencoder (DAE). In order to quantify the benefits of the physical component,\nan ablation study between different DAE configurations is conducted. First,\nthree univariate DAEs are optimized separately on indoor air temperature,\nheating, and cooling data. Then, two multivariate DAEs are derived from the\nprevious configurations. Eventually, a building thermal balance equation is\ncoupled to the last multivariate configuration to obtain PI-DAE. Additionally,\ntwo commonly used benchmarks are employed to support the findings. It is shown\nhow introducing physical knowledge in a multivariate Denoising Autoencoder can\nenhance the inherent model interpretability through the optimized physics-based\ncoefficients. While no significant improvement is observed in terms of\nreconstruction error with the proposed PI-DAE, its enhanced robustness to\nvarying rates of missing data and the valuable insights derived from the\nphysics-based coefficients create opportunities for wider applications within\nbuilding systems and the built environment.","terms":["stat.ML","cs.LG"]},{"titles":"Symmetry-regularized neural ordinary differential equations","summaries":"Neural Ordinary Differential Equations (Neural ODEs) is a class of deep\nneural network models that interpret the hidden state dynamics of neural\nnetworks as an ordinary differential equation, thereby capable of capturing\nsystem dynamics in a continuous time framework. In this work, I integrate\nsymmetry regularization into Neural ODEs. In particular, I use continuous Lie\nsymmetry of ODEs and PDEs associated with the model to derive conservation laws\nand add them to the loss function, making it physics-informed. This\nincorporation of inherent structural properties into the loss function could\nsignificantly improve robustness and stability of the model during training. To\nillustrate this method, I employ a toy model that utilizes a cosine rate of\nchange in the hidden state, showcasing the process of identifying Lie\nsymmetries, deriving conservation laws, and constructing a new loss function.","terms":["stat.ML","cs.LG"]},{"titles":"Robust Ocean Subgrid-Scale Parameterizations Using Fourier Neural Operators","summaries":"In climate simulations, small-scale processes shape ocean dynamics but remain\ncomputationally expensive to resolve directly. For this reason, their\ncontributions are commonly approximated using empirical parameterizations,\nwhich lead to significant errors in long-term projections. In this work, we\ndevelop parameterizations based on Fourier Neural Operators, showcasing their\naccuracy and generalizability in comparison to other approaches. Finally, we\ndiscuss the potential and limitations of neural networks operating in the\nfrequency domain, paving the way for future investigation.","terms":["cs.LG","physics.ao-ph"]},{"titles":"Multi-Irreducible Spectral Synchronization for Robust Rotation Averaging","summaries":"Rotation averaging (RA) is a fundamental problem in robotics and computer\nvision. In RA, the goal is to estimate a set of $N$ unknown orientations\n$R_{1}, ..., R_{N} \\in SO(3)$, given noisy measurements $R_{ij} \\sim R^{-1}_{i}\nR_{j}$ of a subset of their pairwise relative rotations. This problem is both\nnonconvex and NP-hard, and thus difficult to solve in the general case. We\napply harmonic analysis on compact groups to derive a (convex) spectral\nrelaxation constructed from truncated Fourier decompositions of the individual\nsummands appearing in the RA objective; we then recover an estimate of the RA\nsolution by computing a few extremal eigenpairs of this relaxation, and\n(approximately) solving a consensus problem. Our approach affords several\nnotable advantages versus prior RA methods: it can be used in conjunction with\n\\emph{any} smooth loss function (including, but not limited to, robust\nM-estimators), does not require any initialization, and is implemented using\nonly simple (and highly scalable) linear-algebraic computations and\nparallelizable optimizations over band-limited functions of individual\nrotational states. Moreover, under the (physically well-motivated) assumption\nof multiplicative Langevin measurement noise, we derive explicit performance\nguarantees for our spectral estimator (in the form of probabilistic tail bounds\non the estimation error) that are parameterized in terms of graph-theoretic\nquantities of the underlying measurement network. By concretely linking\nestimator performance with properties of the underlying measurement graph, our\nresults also indicate how to devise measurement networks that are\n\\emph{guaranteed} to achieve accurate estimation, enabling such downstream\ntasks as sensor placement, network compression, and active sensing.","terms":["cs.CV","math.GR","math.OC"]},{"titles":"Personalized Predictions of Glioblastoma Infiltration: Mathematical Models, Physics-Informed Neural Networks and Multimodal Scans","summaries":"Predicting the infiltration of Glioblastoma (GBM) from medical MRI scans is\ncrucial for understanding tumor growth dynamics and designing personalized\nradiotherapy treatment plans.Mathematical models of GBM growth can complement\nthe data in the prediction of spatial distributions of tumor cells. However,\nthis requires estimating patient-specific parameters of the model from clinical\ndata, which is a challenging inverse problem due to limited temporal data and\nthe limited time between imaging and diagnosis. This work proposes a method\nthat uses Physics-Informed Neural Networks (PINNs) to estimate patient-specific\nparameters of a reaction-diffusion PDE model of GBM growth from a single 3D\nstructural MRI snapshot. PINNs embed both the data and the PDE into a loss\nfunction, thus integrating theory and data. Key innovations include the\nidentification and estimation of characteristic non-dimensional parameters, a\npre-training step that utilizes the non-dimensional parameters and a\nfine-tuning step to determine the patient specific parameters. Additionally,\nthe diffuse domain method is employed to handle the complex brain geometry\nwithin the PINN framework. Our method is validated both on synthetic and\npatient datasets, and shows promise for real-time parametric inference in the\nclinical setting for personalized GBM treatment.","terms":["cs.LG","eess.IV","q-bio.QM","92-08, 92C50, 35Q92","J.3; J.2; I.2.6"]},{"titles":"Almost Equivariance via Lie Algebra Convolutions","summaries":"Recently, the equivariance of models with respect to a group action has\nbecome an important topic of research in machine learning. However, imbuing an\narchitecture with a specific group equivariance imposes a strong prior on the\ntypes of data transformations that the model expects to see. While\nstrictly-equivariant models enforce symmetries, real-world data does not always\nconform to such strict equivariances, be it due to noise in the data or\nunderlying physical laws that encode only approximate or partial symmetries. In\nsuch cases, the prior of strict equivariance can actually prove too strong and\ncause models to underperform on real-world data. Therefore, in this work we\nstudy a closely related topic, that of almost equivariance. We provide a\ndefinition of almost equivariance that differs from those extant in the current\nliterature and give a practical method for encoding almost equivariance in\nmodels by appealing to the Lie algebra of a Lie group. Specifically, we define\nLie algebra convolutions and demonstrate that they offer several benefits over\nLie group convolutions, including being well-defined for non-compact groups.\nFrom there, we pivot to the realm of theory and demonstrate connections between\nthe notions of equivariance and isometry and those of almost equivariance and\nalmost isometry, respectively. We prove two existence theorems, one showing the\nexistence of almost isometries within bounded distance of isometries of a\ngeneral manifold, and another showing the converse for Hilbert spaces. We then\nextend these theorems to prove the existence of almost equivariant manifold\nembeddings within bounded distance of fully equivariant embedding functions,\nsubject to certain constraints on the group action and the function class.\nFinally, we demonstrate the validity of our approach by benchmarking against\ndatasets in fully equivariant and almost equivariant settings.","terms":["cs.LG","stat.ML","I.2.6"]},{"titles":"Physics-Informed Neural Network for Discovering Systems with Unmeasurable States with Application to Lithium-Ion Batteries","summaries":"Combining machine learning with physics is a trending approach for\ndiscovering unknown dynamics, and one of the most intensively studied\nframeworks is the physics-informed neural network (PINN). However, PINN often\nfails to optimize the network due to its difficulty in concurrently minimizing\nmultiple losses originating from the system's governing equations. This problem\ncan be more serious when the system's states are unmeasurable, like lithium-ion\nbatteries (LiBs). In this work, we introduce a robust method for training PINN\nthat uses fewer loss terms and thus constructs a less complex landscape for\noptimization. In particular, instead of having loss terms from each\ndifferential equation, this method embeds the dynamics into a loss function\nthat quantifies the error between observed and predicted system outputs. This\nis accomplished by numerically integrating the predicted states from the neural\nnetwork(NN) using known dynamics and transforming them to obtain a sequence of\npredicted outputs. Minimizing such a loss optimizes the NN to predict states\nconsistent with observations given the physics. Further, the system's\nparameters can be added to the optimization targets. To demonstrate the ability\nof this method to perform various modeling and control tasks, we apply it to a\nbattery model to concurrently estimate its states and parameters.","terms":["cs.LG","cs.SY","eess.SY"]},{"titles":"CG-HOI: Contact-Guided 3D Human-Object Interaction Generation","summaries":"We propose CG-HOI, the first method to address the task of generating dynamic\n3D human-object interactions (HOIs) from text. We model the motion of both\nhuman and object in an interdependent fashion, as semantically rich human\nmotion rarely happens in isolation without any interactions. Our key insight is\nthat explicitly modeling contact between the human body surface and object\ngeometry can be used as strong proxy guidance, both during training and\ninference. Using this guidance to bridge human and object motion enables\ngenerating more realistic and physically plausible interaction sequences, where\nthe human body and corresponding object move in a coherent manner. Our method\nfirst learns to model human motion, object motion, and contact in a joint\ndiffusion process, inter-correlated through cross-attention. We then leverage\nthis learned contact for guidance during inference synthesis of realistic,\ncoherent HOIs. Extensive evaluation shows that our joint contact-based\nhuman-object interaction approach generates realistic and physically plausible\nsequences, and we show two applications highlighting the capabilities of our\nmethod. Conditioned on a given object trajectory, we can generate the\ncorresponding human motion without re-training, demonstrating strong\nhuman-object interdependency learning. Our approach is also flexible, and can\nbe applied to static real-world 3D scene scans.","terms":["cs.CV","I.2.10; I.4.8; I.5.1; I.5.4"]},{"titles":"Have we built machines that think like people?","summaries":"A chief goal of artificial intelligence is to build machines that think like\npeople. Yet it has been argued that deep neural network architectures fail to\naccomplish this. Researchers have asserted these models' limitations in the\ndomains of causal reasoning, intuitive physics, and intuitive psychology. Yet\nrecent advancements, namely the rise of large language models, particularly\nthose designed for visual processing, have rekindled interest in the potential\nto emulate human-like cognitive abilities. This paper evaluates the current\nstate of vision-based large language models in the domains of intuitive\nphysics, causal reasoning, and intuitive psychology. Through a series of\ncontrolled experiments, we investigate the extent to which these modern models\ngrasp complex physical interactions, causal relationships, and intuitive\nunderstanding of others' preferences. Our findings reveal that, while these\nmodels demonstrate a notable proficiency in processing and interpreting visual\ndata, they still fall short of human capabilities in these areas. The models\nexhibit a rudimentary understanding of physical laws and causal relationships,\nbut their performance is hindered by a lack of deeper insights-a key aspect of\nhuman cognition. Furthermore, in tasks requiring an intuitive theory of mind,\nthe models fail altogether. Our results emphasize the need for integrating more\nrobust mechanisms for understanding causality, physical dynamics, and social\ncognition into modern-day, vision-based language models, and point out the\nimportance of cognitively-inspired benchmarks.","terms":["cs.LG"]},{"titles":"Relightable 3D Gaussian: Real-time Point Cloud Relighting with BRDF Decomposition and Ray Tracing","summaries":"We present a novel differentiable point-based rendering framework for\nmaterial and lighting decomposition from multi-view images, enabling editing,\nray-tracing, and real-time relighting of the 3D point cloud. Specifically, a 3D\nscene is represented as a set of relightable 3D Gaussian points, where each\npoint is additionally associated with a normal direction, BRDF parameters, and\nincident lights from different directions. To achieve robust lighting\nestimation, we further divide incident lights of each point into global and\nlocal components, as well as view-dependent visibilities. The 3D scene is\noptimized through the 3D Gaussian Splatting technique while BRDF and lighting\nare decomposed by physically-based differentiable rendering. Moreover, we\nintroduce an innovative point-based ray-tracing approach based on the bounding\nvolume hierarchy for efficient visibility baking, enabling real-time rendering\nand relighting of 3D Gaussian points with accurate shadow effects. Extensive\nexperiments demonstrate improved BRDF estimation and novel view rendering\nresults compared to state-of-the-art material estimation approaches. Our\nframework showcases the potential to revolutionize the mesh-based graphics\npipeline with a relightable, traceable, and editable rendering pipeline solely\nbased on point cloud. Project\npage:https:\/\/nju-3dv.github.io\/projects\/Relightable3DGaussian\/.","terms":["cs.CV","cs.GR"]},{"titles":"From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding","summaries":"As a vital step toward the intelligent agent, Action understanding matters\nfor intelligent agents and has attracted long-term attention. It can be formed\nas the mapping from the action physical space to the semantic space. Typically,\nresearchers built action datasets according to idiosyncratic choices to define\nclasses and push the envelope of benchmarks respectively. Thus, datasets are\nincompatible with each other like \"Isolated Islands\" due to semantic gaps and\nvarious class granularities, e.g., do housework in dataset A and wash plate in\ndataset B. We argue that a more principled semantic space is an urgent need to\nconcentrate the community efforts and enable us to use all datasets together to\npursue generalizable action learning. To this end, we design a structured\naction semantic space in view of verb taxonomy hierarchy and covering massive\nactions. By aligning the classes of previous datasets to our semantic space, we\ngather (image\/video\/skeleton\/MoCap) datasets into a unified database in a\nunified label system, i.e., bridging ``isolated islands'' into a \"Pangea\".\nAccordingly, we propose a novel model mapping from the physical space to\nsemantic space to fully use Pangea. In extensive experiments, our new system\nshows significant superiority, especially in transfer learning. Code and data\nwill be made publicly available.","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"Diagnosis driven Anomaly Detection for CPS","summaries":"In Cyber-Physical Systems (CPS) research, anomaly detection (detecting\nabnormal behavior) and diagnosis (identifying the underlying root cause) are\noften treated as distinct, isolated tasks. However, diagnosis algorithms\nrequire symptoms, i.e. temporally and spatially isolated anomalies, as input.\nThus, anomaly detection and diagnosis must be developed together to provide a\nholistic solution for diagnosis in CPS. We therefore propose a method for\nutilizing deep learning-based anomaly detection to generate inputs for\nConsistency-Based Diagnosis (CBD). We evaluate our approach on a simulated and\na real-world CPS dataset, where our model demonstrates strong performance\nrelative to other state-of-the-art models.","terms":["cs.LG","cs.AI"]},{"titles":"Long-Range Neural Atom Learning for Molecular Graphs","summaries":"Graph Neural Networks (GNNs) have been widely adopted for drug discovery with\nmolecular graphs. Nevertheless, current GNNs are mainly good at leveraging\nshort-range interactions (SRI) but struggle to capture long-range interactions\n(LRI), both of which are crucial for determining molecular properties. To\ntackle this issue, we propose a method that implicitly projects all original\natoms into a few Neural Atoms, which abstracts the collective information of\natomic groups within a molecule. Specifically, we explicitly exchange the\ninformation among neural atoms and project them back to the atoms'\nrepresentations as an enhancement. With this mechanism, neural atoms establish\nthe communication channels among distant nodes, effectively reducing the\ninteraction scope of arbitrary node pairs into a single hop. To provide an\ninspection of our method from a physical perspective, we reveal its connection\nwith the traditional LRI calculation method, Ewald Summation. We conduct\nextensive experiments on three long-range graph benchmarks, covering both\ngraph-level and link-level tasks on molecular graphs. We empirically justify\nthat our method can be equipped with an arbitrary GNN and help to capture LRI.","terms":["cs.LG","q-bio.QM"]},{"titles":"Optimization of Image Processing Algorithms for Character Recognition in Cultural Typewritten Documents","summaries":"Linked Data is used in various fields as a new way of structuring and\nconnecting data. Cultural heritage institutions have been using linked data to\nimprove archival descriptions and facilitate the discovery of information. Most\narchival records have digital representations of physical artifacts in the form\nof scanned images that are non-machine-readable. Optical Character Recognition\n(OCR) recognizes text in images and translates it into machine-encoded text.\nThis paper evaluates the impact of image processing methods and parameter\ntuning in OCR applied to typewritten cultural heritage documents. The approach\nuses a multi-objective problem formulation to minimize Levenshtein edit\ndistance and maximize the number of words correctly identified with a\nnon-dominated sorting genetic algorithm (NSGA-II) to tune the methods'\nparameters. Evaluation results show that parameterization by digital\nrepresentation typology benefits the performance of image pre-processing\nalgorithms in OCR. Furthermore, our findings suggest that employing image\npre-processing algorithms in OCR might be more suitable for typologies where\nthe text recognition task without pre-processing does not produce good results.\nIn particular, Adaptive Thresholding, Bilateral Filter, and Opening are the\nbest-performing algorithms for the theatre plays' covers, letters, and overall\ndataset, respectively, and should be applied before OCR to improve its\nperformance.","terms":["cs.CV","cs.DL"]},{"titles":"Universal Event Detection in Time Series","summaries":"In our previously published work, we introduced a supervised deep learning\nmethod for event detection in multivariate time series data, employing\nregression instead of binary classification. This simplification avoids the\nneed for point-wise labels throughout the entire dataset, relying solely on\nground truth events defined as time points or intervals. In this paper, we\nestablish mathematically that our method is universal, and capable of detecting\nany type of event with arbitrary precision under mild continuity assumptions on\nthe time series. These events may encompass change points, frauds, anomalies,\nphysical occurrences, and more. We substantiate our theoretical results using\nthe universal approximation theorem for feed-forward neural networks (FFN).\nAdditionally, we provide empirical validations that confirm our claims,\ndemonstrating that our method, with a limited number of parameters, outperforms\nother deep learning approaches, particularly for rare events and imbalanced\ndatasets from different domains.","terms":["stat.ML","cs.LG"]},{"titles":"Auto-PINN: Understanding and Optimizing Physics-Informed Neural Architecture","summaries":"Physics-informed neural networks (PINNs) are revolutionizing science and\nengineering practice by bringing together the power of deep learning to bear on\nscientific computation. In forward modeling problems, PINNs are meshless\npartial differential equation (PDE) solvers that can handle irregular,\nhigh-dimensional physical domains. Naturally, the neural architecture\nhyperparameters have a large impact on the efficiency and accuracy of the PINN\nsolver. However, this remains an open and challenging problem because of the\nlarge search space and the difficulty of identifying a proper search objective\nfor PDEs. Here, we propose Auto-PINN, the first systematic, automated\nhyperparameter optimization approach for PINNs, which employs Neural\nArchitecture Search (NAS) techniques to PINN design. Auto-PINN avoids manually\nor exhaustively searching the hyperparameter space associated with PINNs. A\ncomprehensive set of pre-experiments using standard PDE benchmarks allows us to\nprobe the structure-performance relationship in PINNs. We find that the\ndifferent hyperparameters can be decoupled, and that the training loss function\nof PINNs is a good search objective. Comparison experiments with baseline\nmethods demonstrate that Auto-PINN produces neural architectures with superior\nstability and accuracy over alternative baselines.","terms":["cs.LG"]},{"titles":"SSIN: Self-Supervised Learning for Rainfall Spatial Interpolation","summaries":"The acquisition of accurate rainfall distribution in space is an important\ntask in hydrological analysis and natural disaster pre-warning. However, it is\nimpossible to install rain gauges on every corner. Spatial interpolation is a\ncommon way to infer rainfall distribution based on available raingauge data.\nHowever, the existing works rely on some unrealistic pre-settings to capture\nspatial correlations, which limits their performance in real scenarios. To\ntackle this issue, we propose the SSIN, which is a novel data-driven\nself-supervised learning framework for rainfall spatial interpolation by mining\nlatent spatial patterns from historical observation data. Inspired by the Cloze\ntask and BERT, we fully consider the characteristics of spatial interpolation\nand design the SpaFormer model based on the Transformer architecture as the\ncore of SSIN. Our main idea is: by constructing rich self-supervision signals\nvia random masking, SpaFormer can learn informative embeddings for raw data and\nthen adaptively model spatial correlations based on rainfall spatial context.\nExtensive experiments on two real-world raingauge datasets show that our method\noutperforms the state-of-the-art solutions. In addition, we take traffic\nspatial interpolation as another use case to further explore the performance of\nour method, and SpaFormer achieves the best performance on one large real-world\ntraffic dataset, which further confirms the effectiveness and generality of our\nmethod.","terms":["cs.LG","cs.AI","physics.ao-ph"]},{"titles":"DeepSimHO: Stable Pose Estimation for Hand-Object Interaction via Physics Simulation","summaries":"This paper addresses the task of 3D pose estimation for a hand interacting\nwith an object from a single image observation. When modeling hand-object\ninteraction, previous works mainly exploit proximity cues, while overlooking\nthe dynamical nature that the hand must stably grasp the object to counteract\ngravity and thus preventing the object from slipping or falling. These works\nfail to leverage dynamical constraints in the estimation and consequently often\nproduce unstable results. Meanwhile, refining unstable configurations with\nphysics-based reasoning remains challenging, both by the complexity of contact\ndynamics and by the lack of effective and efficient physics inference in the\ndata-driven learning framework. To address both issues, we present DeepSimHO: a\nnovel deep-learning pipeline that combines forward physics simulation and\nbackward gradient approximation with a neural network. Specifically, for an\ninitial hand-object pose estimated by a base network, we forward it to a\nphysics simulator to evaluate its stability. However, due to non-smooth contact\ngeometry and penetration, existing differentiable simulators can not provide\nreliable state gradient. To remedy this, we further introduce a deep network to\nlearn the stability evaluation process from the simulator, while smoothly\napproximating its gradient and thus enabling effective back-propagation.\nExtensive experiments show that our method noticeably improves the stability of\nthe estimation and achieves superior efficiency over test-time optimization.\nThe code is available at https:\/\/github.com\/rongakowang\/DeepSimHO.","terms":["cs.CV"]},{"titles":"Global $\\mathcal{L}^2$ minimization with certainty via geometrically adapted gradient descent in Deep Learning","summaries":"We consider the gradient descent flow widely used for the minimization of the\n$\\mathcal{L}^2$ cost function in Deep Learning networks, and introduce two\nmodified versions; one adapted for the overparametrized setting, and the other\nfor the underparametrized setting. Both have a clear and natural invariant\ngeometric meaning, taking into account the pullback vector bundle structure in\nthe overparametrized, and the pushforward vector bundle structure in the\nunderparametrized setting. In the overparametrized case, we prove that,\nprovided that a rank condition holds, all orbits of the modified gradient\ndescent drive the $\\mathcal{L}^2$ cost to its global minimum at a uniform\nexponential convergence rate. We point out relations of the latter to\nsub-Riemannian geometry.","terms":["cs.LG","cs.AI","math-ph","math.MP","math.OC","stat.ML","57R70, 62M45"]},{"titles":"Domain knowledge-informed Synthetic fault sample generation with Health Data Map for cross-domain Planetary Gearbox Fault Diagnosis","summaries":"Extensive research has been conducted on fault diagnosis of planetary\ngearboxes using vibration signals and deep learning (DL) approaches. However,\nDL-based methods are susceptible to the domain shift problem caused by varying\noperating conditions of the gearbox. Although domain adaptation and data\nsynthesis methods have been proposed to overcome such domain shifts, they are\noften not directly applicable in real-world situations where only healthy data\nis available in the target domain. To tackle the challenge of extreme domain\nshift scenarios where only healthy data is available in the target domain, this\npaper proposes two novel domain knowledge-informed data synthesis methods\nutilizing the health data map (HDMap). The two proposed approaches are referred\nto as scaled CutPaste and FaultPaste. The HDMap is used to physically represent\nthe vibration signal of the planetary gearbox as an image-like matrix, allowing\nfor visualization of fault-related features. CutPaste and FaultPaste are then\napplied to generate faulty samples based on the healthy data in the target\ndomain, using domain knowledge and fault signatures extracted from the source\ndomain, respectively. In addition to generating realistic faults, the proposed\nmethods introduce scaling of fault signatures for controlled synthesis of\nfaults with various severity levels. A case study is conducted on a planetary\ngearbox testbed to evaluate the proposed approaches. The results show that the\nproposed methods are capable of accurately diagnosing faults, even in cases of\nextreme domain shift, and can estimate the severity of faults that have not\nbeen previously observed in the target domain.","terms":["cs.LG","cs.AI","cs.CY","eess.SP"]},{"titles":"GAN-Based LiDAR Intensity Simulation","summaries":"Realistic vehicle sensor simulation is an important element in developing\nautonomous driving. As physics-based implementations of visual sensors like\nLiDAR are complex in practice, data-based approaches promise solutions. Using\npairs of camera images and LiDAR scans from real test drives, GANs can be\ntrained to translate between them. For this process, we contribute two\nadditions. First, we exploit the camera images, acquiring segmentation data and\ndense depth maps as additional input for training. Second, we test the\nperformance of the LiDAR simulation by testing how well an object detection\nnetwork generalizes between real and synthetic point clouds to enable\nevaluation without ground truth point clouds. Combining both, we simulate LiDAR\npoint clouds and demonstrate their realism.","terms":["cs.CV","eess.IV"]},{"titles":"Understanding and Mitigating Extrapolation Failures in Physics-Informed Neural Networks","summaries":"Physics-informed Neural Networks (PINNs) have recently gained popularity due\nto their effective approximation of partial differential equations (PDEs) using\ndeep neural networks (DNNs). However, their out of domain behavior is not well\nunderstood, with previous work speculating that the presence of high frequency\ncomponents in the solution function might be to blame for poor extrapolation\nperformance. In this paper, we study the extrapolation behavior of PINNs on a\nrepresentative set of PDEs of different types, including high-dimensional PDEs.\nWe find that failure to extrapolate is not caused by high frequencies in the\nsolution function, but rather by shifts in the support of the Fourier spectrum\nover time. We term these spectral shifts and quantify them by introducing a\nWeighted Wasserstein-Fourier distance (WWF). We show that the WWF can be used\nto predict PINN extrapolation performance, and that in the absence of\nsignificant spectral shifts, PINN predictions stay close to the true solution\neven in extrapolation. Finally, we propose a transfer learning-based strategy\nto mitigate the effects of larger spectral shifts, which decreases\nextrapolation errors by up to 82%.","terms":["cs.LG"]},{"titles":"Spatial and Temporal Characteristics of Freight Tours: A Data-Driven Exploratory Analysis","summaries":"This paper presents a modeling approach to infer scheduling and routing\npatterns from digital freight transport activity data for different freight\nmarkets. We provide a complete modeling framework including a new\ndiscrete-continuous decision tree approach for extracting rules from the\nfreight transport data. We apply these models to collected tour data for the\nNetherlands to understand departure time patterns and tour strategies, also\nallowing us to evaluate the effectiveness of the proposed algorithm. We find\nthat spatial and temporal characteristics are important to capture the types of\ntours and time-of-day patterns of freight activities. Also, the empirical\nevidence indicates that carriers in most of the transport markets are sensitive\nto the level of congestion. Many of them adjust the type of tour, departure\ntime, and the number of stops per tour when facing a congested zone. The\nresults can be used by practitioners to get more grip on transport markets and\ndevelop freight and traffic management measures.","terms":["cs.LG","cs.AI","physics.soc-ph"]},{"titles":"Bias-Variance Trade-off in Physics-Informed Neural Networks with Randomized Smoothing for High-Dimensional PDEs","summaries":"While physics-informed neural networks (PINNs) have been proven effective for\nlow-dimensional partial differential equations (PDEs), the computational cost\nremains a hurdle in high-dimensional scenarios. This is particularly pronounced\nwhen computing high-order and high-dimensional derivatives in the\nphysics-informed loss. Randomized Smoothing PINN (RS-PINN) introduces Gaussian\nnoise for stochastic smoothing of the original neural net model, enabling Monte\nCarlo methods for derivative approximation, eliminating the need for costly\nauto-differentiation. Despite its computational efficiency in high dimensions,\nRS-PINN introduces biases in both loss and gradients, negatively impacting\nconvergence, especially when coupled with stochastic gradient descent (SGD). We\npresent a comprehensive analysis of biases in RS-PINN, attributing them to the\nnonlinearity of the Mean Squared Error (MSE) loss and the PDE nonlinearity. We\npropose tailored bias correction techniques based on the order of PDE\nnonlinearity. The unbiased RS-PINN allows for a detailed examination of its\npros and cons compared to the biased version. Specifically, the biased version\nhas a lower variance and runs faster than the unbiased version, but it is less\naccurate due to the bias. To optimize the bias-variance trade-off, we combine\nthe two approaches in a hybrid method that balances the rapid convergence of\nthe biased version with the high accuracy of the unbiased version. In addition,\nwe present an enhanced implementation of RS-PINN. Extensive experiments on\ndiverse high-dimensional PDEs, including Fokker-Planck, HJB, viscous Burgers',\nAllen-Cahn, and Sine-Gordon equations, illustrate the bias-variance trade-off\nand highlight the effectiveness of the hybrid RS-PINN. Empirical guidelines are\nprovided for selecting biased, unbiased, or hybrid versions, depending on the\ndimensionality and nonlinearity of the specific PDE problem.","terms":["cs.LG","cs.AI","cs.NA","math.DS","math.NA","stat.ML","14J60"]},{"titles":"HumanRecon: Neural Reconstruction of Dynamic Human Using Geometric Cues and Physical Priors","summaries":"Recent methods for dynamic human reconstruction have attained promising\nreconstruction results. Most of these methods rely only on RGB color\nsupervision without considering explicit geometric constraints. This leads to\nexisting human reconstruction techniques being more prone to overfitting to\ncolor and causes geometrically inherent ambiguities, especially in the sparse\nmulti-view setup.\n  Motivated by recent advances in the field of monocular geometry prediction,\nwe consider the geometric constraints of estimated depth and normals in the\nlearning of neural implicit representation for dynamic human reconstruction. As\na geometric regularization, this provides reliable yet explicit supervision\ninformation, and improves reconstruction quality. We also exploit several\nbeneficial physical priors, such as adding noise into view direction and\nmaximizing the density on the human surface. These priors ensure the color\nrendered along rays to be robust to view direction and reduce the inherent\nambiguities of density estimated along rays. Experimental results demonstrate\nthat depth and normal cues, predicted by human-specific monocular estimators,\ncan provide effective supervision signals and render more accurate images.\nFinally, we also show that the proposed physical priors significantly reduce\noverfitting and improve the overall quality of novel view synthesis. Our code\nis available\nat:~\\href{https:\/\/github.com\/PRIS-CV\/HumanRecon}{https:\/\/github.com\/PRIS-CV\/HumanRecon}.","terms":["cs.CV"]},{"titles":"On-Device Soft Sensors: Real-Time Fluid Flow Estimation from Level Sensor Data","summaries":"Soft sensors are crucial in bridging autonomous systems' physical and digital\nrealms, enhancing sensor fusion and perception. Instead of deploying soft\nsensors on the Cloud, this study shift towards employing on-device soft\nsensors, promising heightened efficiency and bolstering data security. Our\napproach substantially improves energy efficiency by deploying Artificial\nIntelligence (AI) directly on devices within a wireless sensor network.\nFurthermore, the synergistic integration of the Microcontroller Unit and\nField-Programmable Gate Array (FPGA) leverages the rapid AI inference\ncapabilities of the latter. Empirical evidence from our real-world use case\ndemonstrates that FPGA-based soft sensors achieve inference times ranging\nremarkably from 1.04 to 12.04 microseconds. These compelling results highlight\nthe considerable potential of our innovative approach for executing real-time\ninference tasks efficiently, thereby presenting a feasible alternative that\neffectively addresses the latency challenges intrinsic to Cloud-based\ndeployments.","terms":["cs.LG","cs.AI"]},{"titles":"Enhancing Object Coherence in Layout-to-Image Synthesis","summaries":"Layout-to-image synthesis is an emerging technique in conditional image\ngeneration. It aims to generate complex scenes, where users require fine\ncontrol over the layout of the objects in a scene. However, it remains\nchallenging to control the object coherence, including semantic coherence\n(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the\nhand and the racket should not be misaligned). In this paper, we propose a\nnovel diffusion model with effective global semantic fusion (GSF) and\nself-similarity feature enhancement modules to guide the object coherence for\nthis task. For semantic coherence, we argue that the image caption contains\nrich information for defining the semantic relationship within the objects in\nthe images. Instead of simply employing cross-attention between captions and\ngenerated images, which addresses the highly relevant layout restriction and\nsemantic coherence separately and thus leads to unsatisfying results shown in\nour experiments, we develop GSF to fuse the supervision from the layout\nrestriction and semantic coherence requirement and exploit it to guide the\nimage synthesis process. Moreover, to improve the physical coherence, we\ndevelop a Self-similarity Coherence Attention (SCA) module to explicitly\nintegrate local contextual physical coherence into each pixel's generation\nprocess. Specifically, we adopt a self-similarity map to encode the coherence\nrestrictions and employ it to extract coherent features from text embedding.\nThrough visualization of our self-similarity map, we explore the essence of\nSCA, revealing that its effectiveness is not only in capturing reliable\nphysical coherence patterns but also in enhancing complex texture generation.\nExtensive experiments demonstrate the superiority of our proposed method in\nboth image generation quality and controllability.","terms":["cs.CV","cs.AI"]},{"titles":"One-Shot Transfer Learning for Nonlinear ODEs","summaries":"We introduce a generalizable approach that combines perturbation method and\none-shot transfer learning to solve nonlinear ODEs with a single polynomial\nterm, using Physics-Informed Neural Networks (PINNs). Our method transforms\nnon-linear ODEs into linear ODE systems, trains a PINN across varied\nconditions, and offers a closed-form solution for new instances within the same\nnon-linear ODE class. We demonstrate the effectiveness of this approach on the\nDuffing equation and suggest its applicability to similarly structured PDEs and\nODE systems.","terms":["cs.LG","68T07","I.2.1"]},{"titles":"Physics-Informed Graph Convolutional Networks: Towards a generalized framework for complex geometries","summaries":"Since the seminal work of [9] and their Physics-Informed neural networks\n(PINNs), many efforts have been conducted towards solving partial differential\nequations (PDEs) with Deep Learning models. However, some challenges remain,\nfor instance the extension of such models to complex three-dimensional\ngeometries, and a study on how such approaches could be combined to classical\nnumerical solvers. In this work, we justify the use of graph neural networks\nfor these problems, based on the similarity between these architectures and the\nmeshes used in traditional numerical techniques for solving partial\ndifferential equations. After proving an issue with the Physics-Informed\nframework for complex geometries, during the computation of PDE residuals, an\nalternative procedure is proposed, by combining classical numerical solvers and\nthe Physics-Informed framework. Finally, we propose an implementation of this\napproach, that we test on a three-dimensional problem on an irregular geometry.","terms":["cs.LG","math-ph","math.AP","math.MP"]},{"titles":"LLamol: A Dynamic Multi-Conditional Generative Transformer for De Novo Molecular Design","summaries":"Generative models have demonstrated substantial promise in Natural Language\nProcessing (NLP) and have found application in designing molecules, as seen in\nGeneral Pretrained Transformer (GPT) models. In our efforts to develop such a\ntool for exploring the organic chemical space in search of potentially\nelectro-active compounds, we present \"LLamol\", a single novel generative\ntransformer model based on the LLama 2 architecture, which was trained on a 13M\nsuperset of organic compounds drawn from diverse public sources. To allow for a\nmaximum flexibility in usage and robustness in view of potentially incomplete\ndata, we introduce \"Stochastic Context Learning\" as a new training procedure.\nWe demonstrate that the resulting model adeptly handles single- and\nmulti-conditional organic molecule generation with up to four conditions, yet\nmore are possible. The model generates valid molecular structures in SMILES\nnotation while flexibly incorporating three numerical and\/or one token sequence\ninto the generative process, just as requested. The generated compounds are\nvery satisfactory in all scenarios tested. In detail, we showcase the model's\ncapability to utilize token sequences for conditioning, either individually or\nin combination with numerical properties, making LLamol a potent tool for de\nnovo molecule design, easily expandable with new properties.","terms":["cs.LG","cs.AI","physics.chem-ph"]},{"titles":"Collective Relational Inference for learning heterogeneous interactions","summaries":"Interacting systems are ubiquitous in nature and engineering, ranging from\nparticle dynamics in physics to functionally connected brain regions. These\ninteracting systems can be modeled by graphs where edges correspond to the\ninteractions between interactive entities. Revealing interaction laws is of\nfundamental importance but also particularly challenging due to underlying\nconfigurational complexities. The associated challenges become exacerbated for\nheterogeneous systems that are prevalent in reality, where multiple interaction\ntypes coexist simultaneously and relational inference is required. Here, we\npropose a novel probabilistic method for relational inference, which possesses\ntwo distinctive characteristics compared to existing methods. First, it infers\nthe interaction types of different edges collectively, and second, it allows\nhandling systems with variable topological structure over time. We evaluate the\nproposed methodology across several benchmark datasets and demonstrate that it\noutperforms existing methods in accurately inferring interaction types. We\nfurther show that when combined with known constraints, it allows us, for\nexample, to discover physics-consistent interaction laws of particle systems.\nOverall the proposed model is data-efficient and generalizable to large systems\nwhen trained on smaller ones. The developed methodology constitutes a key\nelement for understanding interacting systems and may find application in graph\nstructure learning.","terms":["cs.LG"]},{"titles":"Deciphering and integrating invariants for neural operator learning with various physical mechanisms","summaries":"Neural operators have been explored as surrogate models for simulating\nphysical systems to overcome the limitations of traditional partial\ndifferential equation (PDE) solvers. However, most existing operator learning\nmethods assume that the data originate from a single physical mechanism,\nlimiting their applicability and performance in more realistic scenarios. To\nthis end, we propose Physical Invariant Attention Neural Operator (PIANO) to\ndecipher and integrate the physical invariants (PI) for operator learning from\nthe PDE series with various physical mechanisms. PIANO employs self-supervised\nlearning to extract physical knowledge and attention mechanisms to integrate\nthem into dynamic convolutional layers. Compared to existing techniques, PIANO\ncan reduce the relative error by 13.6\\%-82.2\\% on PDE forecasting tasks across\nvarying coefficients, forces, or boundary conditions. Additionally, varied\ndownstream tasks reveal that the PI embeddings deciphered by PIANO align well\nwith the underlying invariants in the PDE systems, verifying the physical\nsignificance of PIANO. The source code will be publicly available at:\nhttps:\/\/github.com\/optray\/PIANO.","terms":["cs.LG","cs.NA","math.NA","physics.comp-ph"]},{"titles":"Equivariant flow matching","summaries":"Normalizing flows are a class of deep generative models that are especially\ninteresting for modeling probability distributions in physics, where the exact\nlikelihood of flows allows reweighting to known target energy functions and\ncomputing unbiased observables. For instance, Boltzmann generators tackle the\nlong-standing sampling problem in statistical physics by training flows to\nproduce equilibrium samples of many-body systems such as small molecules and\nproteins. To build effective models for such systems, it is crucial to\nincorporate the symmetries of the target energy into the model, which can be\nachieved by equivariant continuous normalizing flows (CNFs). However, CNFs can\nbe computationally expensive to train and generate samples from, which has\nhampered their scalability and practical application. In this paper, we\nintroduce equivariant flow matching, a new training objective for equivariant\nCNFs that is based on the recently proposed optimal transport flow matching.\nEquivariant flow matching exploits the physical symmetries of the target energy\nfor efficient, simulation-free training of equivariant CNFs. We demonstrate the\neffectiveness of flow matching on rotation and permutation invariant\nmany-particle systems and a small molecule, alanine dipeptide, where for the\nfirst time we obtain a Boltzmann generator with significant sampling efficiency\nwithout relying on tailored internal coordinate featurization. Our results show\nthat the equivariant flow matching objective yields flows with shorter\nintegration paths, improved sampling efficiency, and higher scalability\ncompared to existing methods.","terms":["stat.ML","cs.LG","physics.chem-ph","physics.comp-ph"]},{"titles":"Designing and evaluating an online reinforcement learning agent for physical exercise recommendations in N-of-1 trials","summaries":"Personalized adaptive interventions offer the opportunity to increase patient\nbenefits, however, there are challenges in their planning and implementation.\nOnce implemented, it is an important question whether personalized adaptive\ninterventions are indeed clinically more effective compared to a fixed gold\nstandard intervention. In this paper, we present an innovative N-of-1 trial\nstudy design testing whether implementing a personalized intervention by an\nonline reinforcement learning agent is feasible and effective. Throughout, we\nuse a new study on physical exercise recommendations to reduce pain in\nendometriosis for illustration. We describe the design of a contextual bandit\nrecommendation agent and evaluate the agent in simulation studies. The results\nshow that, first, implementing a personalized intervention by an online\nreinforcement learning agent is feasible. Second, such adaptive interventions\nhave the potential to improve patients' benefits even if only few observations\nare available. As one challenge, they add complexity to the design and\nimplementation process. In order to quantify the expected benefit, data from\nprevious interventional studies is required. We expect our approach to be\ntransferable to other interventions and clinical interventions.","terms":["cs.LG","stat.AP","stat.ME"]},{"titles":"Zero Coordinate Shift: Whetted Automatic Differentiation for Physics-informed Operator Learning","summaries":"Automatic differentiation (AD) is a critical step in physics-informed machine\nlearning, required for computing the high-order derivatives of network output\nw.r.t. coordinates of collocation points. In this paper, we present a novel and\nlightweight algorithm to conduct AD for physics-informed operator learning,\nwhich we call the trick of Zero Coordinate Shift (ZCS). Instead of making all\nsampled coordinates as leaf variables, ZCS introduces only one scalar-valued\nleaf variable for each spatial or temporal dimension, simplifying the wanted\nderivatives from \"many-roots-many-leaves\" to \"one-root-many-leaves\" whereby\nreverse-mode AD becomes directly utilisable. It has led to an outstanding\nperformance leap by avoiding the duplication of the computational graph along\nthe dimension of functions (physical parameters). ZCS is easy to implement with\ncurrent deep learning libraries; our own implementation is achieved by\nextending the DeepXDE package. We carry out a comprehensive benchmark analysis\nand several case studies, training physics-informed DeepONets to solve partial\ndifferential equations (PDEs) without data. The results show that ZCS has\npersistently reduced GPU memory consumption and wall time for training by an\norder of magnitude, and such reduction factor scales with the number of\nfunctions. As a low-level optimisation technique, ZCS imposes no restrictions\non data, physics (PDE) or network architecture and does not compromise training\nresults from any aspect.","terms":["cs.LG","cs.AI","cs.NA","math.NA","physics.comp-ph"]},{"titles":"Exactly conservative physics-informed neural networks and deep operator networks for dynamical systems","summaries":"We introduce a method for training exactly conservative physics-informed\nneural networks and physics-informed deep operator networks for dynamical\nsystems. The method employs a projection-based technique that maps a candidate\nsolution learned by the neural network solver for any given dynamical system\npossessing at least one first integral onto an invariant manifold. We\nillustrate that exactly conservative physics-informed neural network solvers\nand physics-informed deep operator networks for dynamical systems vastly\noutperform their non-conservative counterparts for several real-world problems\nfrom the mathematical sciences.","terms":["cs.LG","cs.NA","math.NA"]},{"titles":"MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations","summaries":"Learning unsupervised world models for autonomous driving has the potential\nto improve the reasoning capabilities of today's systems dramatically. However,\nmost work neglects the physical attributes of the world and focuses on sensor\ndata alone. We propose MUVO, a MUltimodal World Model with Geometric VOxel\nRepresentations to address this challenge. We utilize raw camera and lidar data\nto learn a sensor-agnostic geometric representation of the world, which can\ndirectly be used by downstream tasks, such as planning. We demonstrate\nmultimodal future predictions and show that our geometric representation\nimproves the prediction quality of both camera images and lidar point clouds.","terms":["cs.LG","cs.RO"]},{"titles":"Optimal Power Flow in Highly Renewable Power System Based on Attention Neural Networks","summaries":"The Optimal Power Flow (OPF) problem is pivotal for power system operations,\nguiding generator output and power distribution to meet demand at minimized\ncosts, while adhering to physical and engineering constraints. The integration\nof renewable energy sources, like wind and solar, however, poses challenges due\nto their inherent variability. This variability, driven largely by changing\nweather conditions, demands frequent recalibrations of power settings, thus\nnecessitating recurrent OPF resolutions. This task is daunting using\ntraditional numerical methods, particularly for extensive power systems. In\nthis work, we present a cutting-edge, physics-informed machine learning\nmethodology, trained using imitation learning and historical European weather\ndatasets. Our approach directly correlates electricity demand and weather\npatterns with power dispatch and generation, circumventing the iterative\nrequirements of traditional OPF solvers. This offers a more expedient solution\napt for real-time applications. Rigorous evaluations on aggregated European\npower systems validate our method's superiority over existing data-driven\ntechniques in OPF solving. By presenting a quick, robust, and efficient\nsolution, this research sets a new standard in real-time OPF resolution, paving\nthe way for more resilient power systems in the era of renewable energy.","terms":["cs.LG","cs.SY","eess.SY"]},{"titles":"Can Physics Informed Neural Operators Self Improve?","summaries":"Self-training techniques have shown remarkable value across many deep\nlearning models and tasks. However, such techniques remain largely unexplored\nwhen considered in the context of learning fast solvers for systems of partial\ndifferential equations (Eg: Neural Operators). In this work, we explore the use\nof self-training for Fourier Neural Operators (FNO). Neural Operators emerged\nas a data driven technique, however, data from experiments or traditional\nsolvers is not always readily available. Physics Informed Neural Operators\n(PINO) overcome this constraint by utilizing a physics loss for the training,\nhowever the accuracy of PINO trained without data does not match the\nperformance obtained by training with data. In this work we show that\nself-training can be used to close this gap in performance. We examine\ncanonical examples, namely the 1D-Burgers and 2D-Darcy PDEs, to showcase the\nefficacy of self-training. Specifically, FNOs, when trained exclusively with\nphysics loss through self-training, approach 1.07x for Burgers and 1.02x for\nDarcy, compared to FNOs trained with both data and physics loss. Furthermore,\nwe discover that pseudo-labels can be used for self-training without\nnecessarily training to convergence in each iteration. A consequence of this is\nthat we are able to discover self-training schedules that improve upon the\nbaseline performance of PINO in terms of accuracy as well as time.","terms":["cs.LG","cs.AI","math.AP"]},{"titles":"Molecular Identification and Peak Assignment: Leveraging Multi-Level Multimodal Alignment on NMR","summaries":"Nuclear magnetic resonance (NMR) spectroscopy plays an essential role across\nvarious scientific disciplines, providing valuable insights into molecular\ndynamics and interactions. Despite the promise of AI-enhanced NMR prediction\nmodels, challenges persist in the interpretation of spectra for tasks such as\nmolecular retrieval, isomer recognition, and peak assignment. In response, this\npaper introduces Multi-Level Multimodal Alignment with Knowledge-Guided\nInstance-Wise Discrimination (K-M3AID) to establish meaningful correspondences\nbetween two heterogeneous modalities: molecular graphs (structures) and NMR\nspectra. In particular, K-M3AID employs a dual-coordinated contrastive learning\narchitecture, and incorporates a graph-level alignment module, a node-level\nalignment module, and a communication channel. Notably, the framework\nintroduces knowledge-guided instance-wise discrimination into contrastive\nlearning within the node-level alignment module, significantly enhancing\naccuracy in cross-modal alignment. Additionally, K-M3AID showcases its\ncapability of meta-learning by demonstrating that skills acquired during\nnode-level alignment positively impact graph-level alignment. Empirical\nvalidation underscores K-M3AID's effectiveness in addressing multiple zero-shot\ntasks, offering a promising solution to bridge the gap between structural\ninformation and spectral data in complex NMR scenarios.","terms":["cs.LG","physics.chem-ph","q-bio.QM"]},{"titles":"Deep Learning based Spatially Dependent Acoustical Properties Recovery","summaries":"The physics-informed neural network (PINN) is capable of recovering partial\ndifferential equation (PDE) coefficients that remain constant throughout the\nspatial domain directly from physical measurements. In this work, we propose a\nspatially dependent physics-informed neural network (SD-PINN), which enables\nthe recovery of coefficients in spatially-dependent PDEs using a single neural\nnetwork, eliminating the requirement for domain-specific physical expertise. We\napply the SD-PINN to spatially-dependent wave equation coefficients recovery to\nreveal the spatial distribution of acoustical properties in the inhomogeneous\nmedium. The proposed method exhibits robustness to noise owing to the\nincorporation of a loss function for the physical constraint that the assumed\nPDE must be satisfied. For the coefficients recovery of spatially\ntwo-dimensional PDEs, we store the PDE coefficients at all locations in the 2D\nregion of interest into a matrix and incorporate the low-rank assumption for\nsuch a matrix to recover the coefficients at locations without available\nmeasurements.","terms":["cs.LG","eess.SP"]},{"titles":"Model scale versus domain knowledge in statistical forecasting of chaotic systems","summaries":"Chaos and unpredictability are traditionally synonymous, yet large-scale\nmachine learning methods recently have demonstrated a surprising ability to\nforecast chaotic systems well beyond typical predictability horizons. However,\nrecent works disagree on whether specialized methods grounded in dynamical\nsystems theory, such as reservoir computers or neural ordinary differential\nequations, outperform general-purpose large-scale learning methods such as\ntransformers or recurrent neural networks. These prior studies perform\ncomparisons on few individually-chosen chaotic systems, thereby precluding\nrobust quantification of how statistical modeling choices and dynamical\ninvariants of different chaotic systems jointly determine empirical\npredictability. Here, we perform the largest to-date comparative study of\nforecasting methods on the classical problem of forecasting chaos: we benchmark\n24 state-of-the-art forecasting methods on a crowdsourced database of 135\nlow-dimensional systems with 17 forecast metrics. We find that large-scale,\ndomain-agnostic forecasting methods consistently produce predictions that\nremain accurate up to two dozen Lyapunov times, thereby accessing a new\nlong-horizon forecasting regime well beyond classical methods. We find that, in\nthis regime, accuracy decorrelates with classical invariant measures of\npredictability like the Lyapunov exponent. However, in data-limited settings\noutside the long-horizon regime, we find that physics-based hybrid methods\nretain a comparative advantage due to their strong inductive biases.","terms":["cs.LG","physics.comp-ph"]},{"titles":"Prediction of Effective Elastic Moduli of Rocks using Graph Neural Networks","summaries":"This study presents a Graph Neural Networks (GNNs)-based approach for\npredicting the effective elastic moduli of rocks from their digital CT-scan\nimages. We use the Mapper algorithm to transform 3D digital rock images into\ngraph datasets, encapsulating essential geometrical information. These graphs,\nafter training, prove effective in predicting elastic moduli. Our GNN model\nshows robust predictive capabilities across various graph sizes derived from\nvarious subcube dimensions. Not only does it perform well on the test dataset,\nbut it also maintains high prediction accuracy for unseen rocks and unexplored\nsubcube sizes. Comparative analysis with Convolutional Neural Networks (CNNs)\nreveals the superior performance of GNNs in predicting unseen rock properties.\nMoreover, the graph representation of microstructures significantly reduces GPU\nmemory requirements (compared to the grid representation for CNNs), enabling\ngreater flexibility in the batch size selection. This work demonstrates the\npotential of GNN models in enhancing the prediction accuracy of rock properties\nand boosting the efficiency of digital rock analysis.","terms":["cs.LG","physics.comp-ph","physics.geo-ph"]},{"titles":"MergeSFL: Split Federated Learning with Feature Merging and Batch Size Regulation","summaries":"Recently, federated learning (FL) has emerged as a popular technique for edge\nAI to mine valuable knowledge in edge computing (EC) systems. To mitigate the\ncomputing\/communication burden on resource-constrained workers and protect\nmodel privacy, split federated learning (SFL) has been released by integrating\nboth data and model parallelism. Despite resource limitations, SFL still faces\ntwo other critical challenges in EC, i.e., statistical heterogeneity and system\nheterogeneity. To address these challenges, we propose a novel SFL framework,\ntermed MergeSFL, by incorporating feature merging and batch size regulation in\nSFL. Concretely, feature merging aims to merge the features from workers into a\nmixed feature sequence, which is approximately equivalent to the features\nderived from IID data and is employed to promote model accuracy. While batch\nsize regulation aims to assign diverse and suitable batch sizes for\nheterogeneous workers to improve training efficiency. Moreover, MergeSFL\nexplores to jointly optimize these two strategies upon their coupled\nrelationship to better enhance the performance of SFL. Extensive experiments\nare conducted on a physical platform with 80 NVIDIA Jetson edge devices, and\nthe experimental results show that MergeSFL can improve the final model\naccuracy by 5.82% to 26.22%, with a speedup by about 1.74x to 4.14x, compared\nto the baselines.","terms":["cs.LG","cs.DC","cs.NI"]},{"titles":"Improving performance of heart rate time series classification by grouping subjects","summaries":"Unlike the more commonly analyzed ECG or PPG data for activity\nclassification, heart rate time series data is less detailed, often noisier and\ncan contain missing data points. Using the BigIdeasLab_STEP dataset, which\nincludes heart rate time series annotated with specific tasks performed by\nindividuals, we sought to determine if general classification was achievable.\nOur analyses showed that the accuracy is sensitive to the choice of\nwindow\/stride size. Moreover, we found variable classification performances\nbetween subjects due to differences in the physical structure of their hearts.\nVarious techniques were used to minimize this variability. First of all,\nnormalization proved to be a crucial step and significantly improved the\nperformance. Secondly, grouping subjects and performing classification inside a\ngroup helped to improve performance and decrease inter-subject variability.\nFinally, we show that including handcrafted features as input to a deep\nlearning (DL) network improves the classification performance further.\nTogether, these findings indicate that heart rate time series can be utilized\nfor classification tasks like predicting activity. However, normalization or\ngrouping techniques need to be chosen carefully to minimize the issue of\nsubject variability.","terms":["cs.LG","eess.SP"]},{"titles":"Differentiable Radio Frequency Ray Tracing for Millimeter-Wave Sensing","summaries":"Millimeter wave (mmWave) sensing is an emerging technology with applications\nin 3D object characterization and environment mapping. However, realizing\nprecise 3D reconstruction from sparse mmWave signals remains challenging.\nExisting methods rely on data-driven learning, constrained by dataset\navailability and difficulty in generalization. We propose DiffSBR, a\ndifferentiable framework for mmWave-based 3D reconstruction. DiffSBR\nincorporates a differentiable ray tracing engine to simulate radar point clouds\nfrom virtual 3D models. A gradient-based optimizer refines the model parameters\nto minimize the discrepancy between simulated and real point clouds.\nExperiments using various radar hardware validate DiffSBR's capability for\nfine-grained 3D reconstruction, even for novel objects unseen by the radar\npreviously. By integrating physics-based simulation with gradient optimization,\nDiffSBR transcends the limitations of data-driven approaches and pioneers a new\nparadigm for mmWave sensing.","terms":["cs.CV"]},{"titles":"Learning continuous models for continuous physics","summaries":"Dynamical systems that evolve continuously over time are ubiquitous\nthroughout science and engineering. Machine learning (ML) provides data-driven\napproaches to model and predict the dynamics of such systems. A core issue with\nthis approach is that ML models are typically trained on discrete data, using\nML methodologies that are not aware of underlying continuity properties. This\nresults in models that often do not capture any underlying continuous dynamics\n-- either of the system of interest, or indeed of any related system. To\naddress this challenge, we develop a convergence test based on numerical\nanalysis theory. Our test verifies whether a model has learned a function that\naccurately approximates an underlying continuous dynamics. Models that fail\nthis test fail to capture relevant dynamics, rendering them of limited utility\nfor many scientific prediction tasks; while models that pass this test enable\nboth better interpolation and better extrapolation in multiple ways. Our\nresults illustrate how principled numerical analysis methods can be coupled\nwith existing ML training\/testing methodologies to validate models for science\nand engineering applications.","terms":["cs.LG"]},{"titles":"PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF","summaries":"We show that physics-based simulations can be seamlessly integrated with NeRF\nto generate high-quality elastodynamics of real-world objects. Unlike existing\nmethods, we discretize nonlinear hyperelasticity in a meshless way, obviating\nthe necessity for intermediate auxiliary shape proxies like a tetrahedral mesh\nor voxel grid. A quadratic generalized moving least square (Q-GMLS) is employed\nto capture nonlinear dynamics and large deformation on the implicit model. Such\nmeshless integration enables versatile simulations of complex and codimensional\nshapes. We adaptively place the least-square kernels according to the NeRF\ndensity field to significantly reduce the complexity of the nonlinear\nsimulation. As a result, physically realistic animations can be conveniently\nsynthesized using our method for a wide range of hyperelastic materials at an\ninteractive rate. For more information, please visit our project page at\nhttps:\/\/fytalon.github.io\/pienerf\/.","terms":["cs.CV","cs.AI","cs.GR","cs.LG"]},{"titles":"Camera-Independent Single Image Depth Estimation from Defocus Blur","summaries":"Monocular depth estimation is an important step in many downstream tasks in\nmachine vision. We address the topic of estimating monocular depth from defocus\nblur which can yield more accurate results than the semantic based depth\nestimation methods. The existing monocular depth from defocus techniques are\nsensitive to the particular camera that the images are taken from. We show how\nseveral camera-related parameters affect the defocus blur using optical physics\nequations and how they make the defocus blur depend on these parameters. The\nsimple correction procedure we propose can alleviate this problem which does\nnot require any retraining of the original model. We created a synthetic\ndataset which can be used to test the camera independent performance of depth\nfrom defocus blur models. We evaluate our model on both synthetic and real\ndatasets (DDFF12 and NYU depth V2) obtained with different cameras and show\nthat our methods are significantly more robust to the changes of cameras. Code:\nhttps:\/\/github.com\/sleekEagle\/defocus_camind.git","terms":["cs.CV"]},{"titles":"Physics-guided Shape-from-Template: Monocular Video Perception through Neural Surrogate Models","summaries":"3D reconstruction of dynamic scenes is a long-standing problem in computer\ngraphics and increasingly difficult the less information is available.\nShape-from-Template (SfT) methods aim to reconstruct a template-based geometry\nfrom RGB images or video sequences, often leveraging just a single monocular\ncamera without depth information, such as regular smartphone recordings.\nUnfortunately, existing reconstruction methods are either unphysical and noisy\nor slow in optimization. To solve this problem, we propose a novel SfT\nreconstruction algorithm for cloth using a pre-trained neural surrogate model\nthat is fast to evaluate, stable, and produces smooth reconstructions due to a\nregularizing physics simulation. Differentiable rendering of the simulated mesh\nenables pixel-wise comparisons between the reconstruction and a target video\nsequence that can be used for a gradient-based optimization procedure to\nextract not only shape information but also physical parameters such as\nstretching, shearing, or bending stiffness of the cloth. This allows to retain\na precise, stable, and smooth reconstructed geometry while reducing the runtime\nby a factor of 400-500 compared to $\\phi$-SfT, a state-of-the-art physics-based\nSfT approach.","terms":["cs.CV","cs.LG"]},{"titles":"Neural-Integrated Meshfree (NIM) Method: A differentiable programming-based hybrid solver for computational mechanics","summaries":"We present the neural-integrated meshfree (NIM) method, a differentiable\nprogramming-based hybrid meshfree approach within the field of computational\nmechanics. NIM seamlessly integrates traditional physics-based meshfree\ndiscretization techniques with deep learning architectures. It employs a hybrid\napproximation scheme, NeuroPU, to effectively represent the solution by\ncombining continuous DNN representations with partition of unity (PU) basis\nfunctions associated with the underlying spatial discretization. This\nneural-numerical hybridization not only enhances the solution representation\nthrough functional space decomposition but also reduces both the size of DNN\nmodel and the need for spatial gradient computations based on automatic\ndifferentiation, leading to a significant improvement in training efficiency.\nUnder the NIM framework, we propose two truly meshfree solvers: the strong\nform-based NIM (S-NIM) and the local variational form-based NIM (V-NIM). In the\nS-NIM solver, the strong-form governing equation is directly considered in the\nloss function, while the V-NIM solver employs a local Petrov-Galerkin approach\nthat allows the construction of variational residuals based on arbitrary\noverlapping subdomains. This ensures both the satisfaction of underlying\nphysics and the preservation of meshfree property. We perform extensive\nnumerical experiments on both stationary and transient benchmark problems to\nassess the effectiveness of the proposed NIM methods in terms of accuracy,\nscalability, generalizability, and convergence properties. Moreover,\ncomparative analysis with other physics-informed machine learning methods\ndemonstrates that NIM, especially V-NIM, significantly enhances both accuracy\nand efficiency in end-to-end predictive capabilities.","terms":["cs.LG","cs.CE"]},{"titles":"Carbohydrate NMR chemical shift predictions using E(3) equivariant graph neural networks","summaries":"Carbohydrates, vital components of biological systems, are well-known for\ntheir structural diversity. Nuclear Magnetic Resonance (NMR) spectroscopy plays\na crucial role in understanding their intricate molecular arrangements and is\nessential in assessing and verifying the molecular structure of organic\nmolecules. An important part of this process is to predict the NMR chemical\nshift from the molecular structure. This work introduces a novel approach that\nleverages E(3) equivariant graph neural networks to predict carbohydrate NMR\nspectra. Notably, our model achieves a substantial reduction in mean absolute\nerror, up to threefold, compared to traditional models that rely solely on\ntwo-dimensional molecular structure. Even with limited data, the model excels,\nhighlighting its robustness and generalization capabilities. The implications\nare far-reaching and go beyond an advanced understanding of carbohydrate\nstructures and spectral interpretation. For example, it could accelerate\nresearch in pharmaceutical applications, biochemistry, and structural biology,\noffering a faster and more reliable analysis of molecular structures.\nFurthermore, our approach is a key step towards a new data-driven era in\nspectroscopy, potentially influencing spectroscopic techniques beyond NMR.","terms":["cs.LG","physics.chem-ph"]},{"titles":"GPT4Motion: Scripting Physical Motions in Text-to-Video Generation via Blender-Oriented GPT Planning","summaries":"Recent advances in text-to-video generation have harnessed the power of\ndiffusion models to create visually compelling content conditioned on text\nprompts. However, they usually encounter high computational costs and often\nstruggle to produce videos with coherent physical motions. To tackle these\nissues, we propose GPT4Motion, a training-free framework that leverages the\nplanning capability of large language models such as GPT, the physical\nsimulation strength of Blender, and the excellent image generation ability of\ntext-to-image diffusion models to enhance the quality of video synthesis.\nSpecifically, GPT4Motion employs GPT-4 to generate a Blender script based on a\nuser textual prompt, which commands Blender's built-in physics engine to craft\nfundamental scene components that encapsulate coherent physical motions across\nframes. Then these components are inputted into Stable Diffusion to generate a\nvideo aligned with the textual prompt. Experimental results on three basic\nphysical motion scenarios, including rigid object drop and collision, cloth\ndraping and swinging, and liquid flow, demonstrate that GPT4Motion can generate\nhigh-quality videos efficiently in maintaining motion coherency and entity\nconsistency. GPT4Motion offers new insights in text-to-video research,\nenhancing its quality and broadening its horizon for future explorations.","terms":["cs.CV"]},{"titles":"TouchSDF: A DeepSDF Approach for 3D Shape Reconstruction using Vision-Based Tactile Sensing","summaries":"Humans rely on their visual and tactile senses to develop a comprehensive 3D\nunderstanding of their physical environment. Recently, there has been a growing\ninterest in exploring and manipulating objects using data-driven approaches\nthat utilise high-resolution vision-based tactile sensors. However, 3D shape\nreconstruction using tactile sensing has lagged behind visual shape\nreconstruction because of limitations in existing techniques, including the\ninability to generalise over unseen shapes, the absence of real-world testing,\nand limited expressive capacity imposed by discrete representations. To address\nthese challenges, we propose TouchSDF, a Deep Learning approach for tactile 3D\nshape reconstruction that leverages the rich information provided by a\nvision-based tactile sensor and the expressivity of the implicit neural\nrepresentation DeepSDF. Our technique consists of two components: (1) a\nConvolutional Neural Network that maps tactile images into local meshes\nrepresenting the surface at the touch location, and (2) an implicit neural\nfunction that predicts a signed distance function to extract the desired 3D\nshape. This combination allows TouchSDF to reconstruct smooth and continuous 3D\nshapes from tactile inputs in simulation and real-world settings, opening up\nresearch avenues for robust 3D-aware representations and improved multimodal\nperception in robotics. Code and supplementary material are available at:\nhttps:\/\/touchsdf.github.io\/","terms":["cs.CV","cs.LG"]},{"titles":"Local Convolution Enhanced Global Fourier Neural Operator For Multiscale Dynamic Spaces Prediction","summaries":"Neural operators extend the capabilities of traditional neural networks by\nallowing them to handle mappings between function spaces for the purpose of\nsolving partial differential equations (PDEs). One of the most notable methods\nis the Fourier Neural Operator (FNO), which is inspired by Green's function\nmethod and approximate operator kernel directly in the frequency domain. In\nthis work, we focus on predicting multiscale dynamic spaces, which is\nequivalent to solving multiscale PDEs. Multiscale PDEs are characterized by\nrapid coefficient changes and solution space oscillations, which are crucial\nfor modeling atmospheric convection and ocean circulation. To solve this\nproblem, models should have the ability to capture rapid changes and process\nthem at various scales. However, the FNO only approximates kernels in the\nlow-frequency domain, which is insufficient when solving multiscale PDEs. To\naddress this challenge, we propose a novel hierarchical neural operator that\nintegrates improved Fourier layers with attention mechanisms, aiming to capture\nall details and handle them at various scales. These mechanisms complement each\nother in the frequency domain and encourage the model to solve multiscale\nproblems. We perform experiments on dynamic spaces governed by forward and\nreverse problems of multiscale elliptic equations, Navier-Stokes equations and\nsome other physical scenarios, and reach superior performance in existing PDE\nbenchmarks, especially equations characterized by rapid coefficient variations.","terms":["cs.LG","cs.NA","math.DS","math.NA"]},{"titles":"Board-to-Board: Evaluating Moonboard Grade Prediction Generalization","summaries":"Bouldering is a sport where athletes aim to climb up an obstacle using a set\nof defined holds called a route. Typically routes are assigned a grade to\ninform climbers of its difficulty and allow them to more easily track their\nprogression. However, the variation in individual climbers technical and\nphysical attributes and many nuances of an individual route make grading a\ndifficult and often biased task. In this work, we apply classical and\ndeep-learning modelling techniques to the 2016, 2017 and 2019 Moonboard\ndatasets, achieving state of the art grade prediction performance with 0.87 MAE\nand 1.12 RMSE. We achieve this performance on a feature-set that does not\nrequire decomposing routes into individual moves, which is a method common in\nliterature and introduces bias. We also demonstrate the generalization\ncapability of this model between editions and introduce a novel vision-based\nmethod of grade prediction. While the generalization performance of these\ntechniques is below human level performance currently, we propose these methods\nas a basis for future work. Such a tool could be implemented in pre-existing\nmobile applications and would allow climbers to better track their progress and\nassess new routes with reduced bias.","terms":["cs.LG","cs.CV"]},{"titles":"RFTrans: Leveraging Refractive Flow of Transparent Objects for Surface Normal Estimation and Manipulation","summaries":"Transparent objects are widely used in our daily lives, making it important\nto teach robots to interact with them. However, it's not easy because the\nreflective and refractive effects can make RGB-D cameras fail to give accurate\ngeometry measurements. To solve this problem, this paper introduces RFTrans, an\nRGB-D-based method for surface normal estimation and manipulation of\ntransparent objects. By leveraging refractive flow as an intermediate\nrepresentation, RFTrans circumvents the drawbacks of directly predicting the\ngeometry (e.g. surface normal) from RGB images and helps bridge the sim-to-real\ngap. RFTrans integrates the RFNet, which predicts refractive flow, object mask,\nand boundaries, followed by the F2Net, which estimates surface normal from the\nrefractive flow. To make manipulation possible, a global optimization module\nwill take in the predictions, refine the raw depth, and construct the point\ncloud with normal. An analytical grasp planning algorithm, ISF, is followed to\ngenerate the grasp poses. We build a synthetic dataset with physically\nplausible ray-tracing rendering techniques to train the networks. Results show\nthat the RFTrans trained on the synthetic dataset can consistently outperform\nthe baseline ClearGrasp in both synthetic and real-world benchmarks by a large\nmargin. Finally, a real-world robot grasping task witnesses an 83% success\nrate, proving that refractive flow can help enable direct sim-to-real transfer.\nThe code, data, and supplementary materials are available at\nhttps:\/\/rftrans.robotflow.ai.","terms":["cs.CV","cs.RO"]},{"titles":"Stacked networks improve physics-informed training: applications to neural networks and deep operator networks","summaries":"Physics-informed neural networks and operator networks have shown promise for\neffectively solving equations modeling physical systems. However, these\nnetworks can be difficult or impossible to train accurately for some systems of\nequations. We present a novel multifidelity framework for stacking\nphysics-informed neural networks and operator networks that facilitates\ntraining. We successively build a chain of networks, where the output at one\nstep can act as a low-fidelity input for training the next step, gradually\nincreasing the expressivity of the learned model. The equations imposed at each\nstep of the iterative process can be the same or different (akin to simulated\nannealing). The iterative (stacking) nature of the proposed method allows us to\nprogressively learn features of a solution that are hard to learn directly.\nThrough benchmark problems including a nonlinear pendulum, the wave equation,\nand the viscous Burgers equation, we show how stacking can be used to improve\nthe accuracy and reduce the required size of physics-informed neural networks\nand operator networks.","terms":["cs.LG","cs.NA","math.NA"]},{"titles":"Modality Mixer Exploiting Complementary Information for Multi-modal Action Recognition","summaries":"Due to the distinctive characteristics of sensors, each modality exhibits\nunique physical properties. For this reason, in the context of multi-modal\naction recognition, it is important to consider not only the overall action\ncontent but also the complementary nature of different modalities. In this\npaper, we propose a novel network, named Modality Mixer (M-Mixer) network,\nwhich effectively leverages and incorporates the complementary information\nacross modalities with the temporal context of actions for action recognition.\nA key component of our proposed M-Mixer is the Multi-modal Contextualization\nUnit (MCU), a simple yet effective recurrent unit. Our MCU is responsible for\ntemporally encoding a sequence of one modality (e.g., RGB) with action content\nfeatures of other modalities (e.g., depth and infrared modalities). This\nprocess encourages M-Mixer network to exploit global action content and also to\nsupplement complementary information of other modalities. Furthermore, to\nextract appropriate complementary information regarding to the given modality\nsettings, we introduce a new module, named Complementary Feature Extraction\nModule (CFEM). CFEM incorporates sepearte learnable query embeddings for each\nmodality, which guide CFEM to extract complementary information and global\naction content from the other modalities. As a result, our proposed method\noutperforms state-of-the-art methods on NTU RGB+D 60, NTU RGB+D 120, and\nNW-UCLA datasets. Moreover, through comprehensive ablation studies, we further\nvalidate the effectiveness of our proposed method.","terms":["cs.CV"]},{"titles":"DiffAvatar: Simulation-Ready Garment Optimization with Differentiable Simulation","summaries":"The realism of digital avatars is crucial in enabling telepresence\napplications with self-expression and customization. A key aspect of this\nrealism originates from the physical accuracy of both a true-to-life body shape\nand clothing. While physical simulations can produce high-quality, realistic\nmotions for clothed humans, they require precise estimation of body shape and\nhigh-quality garment assets with associated physical parameters for cloth\nsimulations. However, manually creating these assets and calibrating their\nparameters is labor-intensive and requires specialized expertise. To address\nthis gap, we propose DiffAvatar, a novel approach that performs body and\ngarment co-optimization using differentiable simulation. By integrating\nphysical simulation into the optimization loop and accounting for the complex\nnonlinear behavior of cloth and its intricate interaction with the body, our\nframework recovers body and garment geometry and extracts important material\nparameters in a physically plausible way. Our experiments demonstrate that our\napproach generates realistic clothing and body shape that can be easily used in\ndownstream applications.","terms":["cs.CV"]},{"titles":"Categorizing the Visual Environment and Analyzing the Visual Attention of Dogs","summaries":"Dogs have a unique evolutionary relationship with humans and serve many\nimportant roles e.g. search and rescue, blind assistance, emotional support.\nHowever, few datasets exist to categorize visual features and objects available\nto dogs, as well as how dogs direct their visual attention within their\nenvironment. We collect and study a dataset with over 11,698 gazes to\ncategorize the objects available to be gazed at by 11 dogs in everyday outdoor\nenvironments i.e. a walk around a college campus and urban area. We explore the\navailability of these object categories and the visual attention of dogs over\nthese categories using a head mounted eye tracking apparatus. A small portion\n(approx. 600 images or < 20% of total dataset) of the collected data is used to\nfine tune a MaskRCNN for the novel image domain to segment objects present in\nthe scene, enabling further statistical analysis on the visual gaze tendencies\nof dogs. The MaskRCNN, with eye tracking apparatus, serves as an end to end\nmodel for automatically classifying the visual fixations of dogs. The fine\ntuned MaskRCNN performs far better than chance. There are few individual\ndifferences between the 11 dogs and we observe greater visual fixations on\nbuses, plants, pavement, and construction equipment. This work takes a step\ntowards understanding visual behavior of dogs and their interaction with the\nphysical world.","terms":["cs.CV","cs.AI"]},{"titles":"Sustainable Concrete via Bayesian Optimization","summaries":"Eight percent of global carbon dioxide emissions can be attributed to the\nproduction of cement, the main component of concrete, which is also the\ndominant source of CO2 emissions in the construction of data centers. The\ndiscovery of lower-carbon concrete formulae is therefore of high significance\nfor sustainability. However, experimenting with new concrete formulae is time\nconsuming and labor intensive, as one usually has to wait to record the\nconcrete's 28-day compressive strength, a quantity whose measurement can by its\ndefinition not be accelerated. This provides an opportunity for experimental\ndesign methodology like Bayesian Optimization (BO) to accelerate the search for\nstrong and sustainable concrete formulae. Herein, we 1) propose modeling steps\nthat make concrete strength amenable to be predicted accurately by a Gaussian\nprocess model with relatively few measurements, 2) formulate the search for\nsustainable concrete as a multi-objective optimization problem, and 3) leverage\nthe proposed model to carry out multi-objective BO with real-world strength\nmeasurements of the algorithmically proposed mixes. Our experimental results\nshow improved trade-offs between the mixtures' global warming potential (GWP)\nand their associated compressive strengths, compared to mixes based on current\nindustry practices. Our methods are open-sourced at\ngithub.com\/facebookresearch\/SustainableConcrete.","terms":["cs.LG","physics.soc-ph"]},{"titles":"Establishing Central Sensitization Inventory Cut-off Values in patients with Chronic Low Back Pain by Unsupervised Machine Learning","summaries":"Human Assumed Central Sensitization is involved in the development and\nmaintenance of chronic low back pain (CLBP). The Central Sensitization\nInventory (CSI) was developed to evaluate the presence of HACS, with a cut-off\nvalue of 40\/100 based on patients with chronic pain. However, various factors\nincluding pain conditions (e.g., CLBP), and gender may influence this cut-off\nvalue. For chronic pain condition such as CLBP, unsupervised clustering\napproaches can take these factors into consideration and automatically learn\nthe HACS-related patterns. Therefore, this study aimed to determine the cut-off\nvalues for a Dutch-speaking population with CLBP, considering the total group\nand stratified by gender based on unsupervised machine learning. In this study,\nquestionnaire data covering pain, physical, and psychological aspects were\ncollected from patients with CLBP and aged-matched pain-free adults (referred\nto as healthy controls, HC). Four clustering approaches were applied to\nidentify HACS-related clusters based on the questionnaire data and gender. The\nclustering performance was assessed using internal and external indicators.\nSubsequently, receiver operating characteristic analysis was conducted on the\nbest clustering results to determine the optimal cut-off values. The study\nincluded 151 subjects, consisting of 63 HCs and 88 patients with CLBP.\nHierarchical clustering yielded the best results, identifying three clusters:\nhealthy group, CLBP with low HACS level, and CLBP with high HACS level groups.\nBased on the low HACS levels group (including HC and CLBP with low HACS level)\nand high HACS level group, the cut-off value for the overall groups were 35, 34\nfor females, and 35 for. The findings suggest that the optimal cut-off values\nfor CLBP is 35. The gender-related cut-off values should be interpreted with\ncaution due to the unbalanced gender distribution in the sample.","terms":["cs.LG","cs.AI","q-bio.NC"]},{"titles":"SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics","summaries":"Simulating fluid dynamics is crucial for the design and development process,\nranging from simple valves to complex turbomachinery. Accurately solving the\nunderlying physical equations is computationally expensive. Therefore,\nlearning-based solvers that model interactions on meshes have gained interest\ndue to their promising speed-ups. However, it is unknown to what extent these\nmodels truly understand the underlying physical principles and can generalize\nrather than interpolate. Generalization is a key requirement for a\ngeneral-purpose fluid simulator, which should adapt to different topologies,\nresolutions, or thermodynamic ranges. We propose SURF, a benchmark designed to\ntest the $\\textit{generalization}$ of learned graph-based fluid simulators.\nSURF comprises individual datasets and provides specific performance and\ngeneralization metrics for evaluating and comparing different models. We\nempirically demonstrate the applicability of SURF by thoroughly investigating\nthe two state-of-the-art graph-based models, yielding new insights into their\ngeneralization.","terms":["cs.LG","cs.AI","physics.flu-dyn"]},{"titles":"Holistic Inverse Rendering of Complex Facade via Aerial 3D Scanning","summaries":"In this work, we use multi-view aerial images to reconstruct the geometry,\nlighting, and material of facades using neural signed distance fields (SDFs).\nWithout the requirement of complex equipment, our method only takes simple RGB\nimages captured by a drone as inputs to enable physically based and\nphotorealistic novel-view rendering, relighting, and editing. However, a\nreal-world facade usually has complex appearances ranging from diffuse rocks\nwith subtle details to large-area glass windows with specular reflections,\nmaking it hard to attend to everything. As a result, previous methods can\npreserve the geometry details but fail to reconstruct smooth glass windows or\nverse vise. In order to address this challenge, we introduce three spatial- and\nsemantic-adaptive optimization strategies, including a semantic regularization\napproach based on zero-shot segmentation techniques to improve material\nconsistency, a frequency-aware geometry regularization to balance surface\nsmoothness and details in different surfaces, and a visibility probe-based\nscheme to enable efficient modeling of the local lighting in large-scale\noutdoor environments. In addition, we capture a real-world facade aerial 3D\nscanning image set and corresponding point clouds for training and\nbenchmarking. The experiment demonstrates the superior quality of our method on\nfacade holistic inverse rendering, novel view synthesis, and scene editing\ncompared to state-of-the-art baselines.","terms":["cs.CV","cs.GR"]},{"titles":"AdvGen: Physical Adversarial Attack on Face Presentation Attack Detection Systems","summaries":"Evaluating the risk level of adversarial images is essential for safely\ndeploying face authentication models in the real world. Popular approaches for\nphysical-world attacks, such as print or replay attacks, suffer from some\nlimitations, like including physical and geometrical artifacts. Recently,\nadversarial attacks have gained attraction, which try to digitally deceive the\nlearning strategy of a recognition system using slight modifications to the\ncaptured image. While most previous research assumes that the adversarial image\ncould be digitally fed into the authentication systems, this is not always the\ncase for systems deployed in the real world. This paper demonstrates the\nvulnerability of face authentication systems to adversarial images in physical\nworld scenarios. We propose AdvGen, an automated Generative Adversarial\nNetwork, to simulate print and replay attacks and generate adversarial images\nthat can fool state-of-the-art PADs in a physical domain attack setting. Using\nthis attack strategy, the attack success rate goes up to 82.01%. We test AdvGen\nextensively on four datasets and ten state-of-the-art PADs. We also demonstrate\nthe effectiveness of our attack by conducting experiments in a realistic,\nphysical environment.","terms":["cs.CV"]},{"titles":"A novel transformer-based approach for soil temperature prediction","summaries":"Soil temperature is one of the most significant parameters that plays a\ncrucial role in glacier energy, dynamics of mass balance, processes of surface\nhydrological, coaction of glacier-atmosphere, nutrient cycling, ecological\nstability, the management of soil, water, and field crop. In this work, we\nintroduce a novel approach using transformer models for the purpose of\nforecasting soil temperature prediction. To the best of our knowledge, the\nusage of transformer models in this work is the very first attempt to predict\nsoil temperature. Experiments are carried out using six different FLUXNET\nstations by modeling them with five different transformer models, namely,\nVanilla Transformer, Informer, Autoformer, Reformer, and ETSformer. To\ndemonstrate the effectiveness of the proposed model, experiment results are\ncompared with both deep learning approaches and literature studies. Experiment\nresults show that the utilization of transformer models ensures a significant\ncontribution to the literature, thence determining the new state-of-the-art.","terms":["cs.LG","cs.AI","cs.CE","physics.ao-ph"]},{"titles":"Geometric Algebra Transformer","summaries":"Problems involving geometric data arise in physics, chemistry, robotics,\ncomputer vision, and many other fields. Such data can take numerous forms, for\ninstance points, direction vectors, translations, or rotations, but to date\nthere is no single architecture that can be applied to such a wide variety of\ngeometric types while respecting their symmetries. In this paper we introduce\nthe Geometric Algebra Transformer (GATr), a general-purpose architecture for\ngeometric data. GATr represents inputs, outputs, and hidden states in the\nprojective geometric (or Clifford) algebra, which offers an efficient\n16-dimensional vector-space representation of common geometric objects as well\nas operators acting on them. GATr is equivariant with respect to E(3), the\nsymmetry group of 3D Euclidean space. As a Transformer, GATr is versatile,\nefficient, and scalable. We demonstrate GATr in problems from n-body modeling\nto wall-shear-stress estimation on large arterial meshes to robotic motion\nplanning. GATr consistently outperforms both non-geometric and equivariant\nbaselines in terms of error, data efficiency, and scalability.","terms":["cs.LG","cs.RO","stat.ML"]},{"titles":"Predicting urban tree cover from incomplete point labels and limited background information","summaries":"Trees inside cities are important for the urban microclimate, contributing\npositively to the physical and mental health of the urban dwellers. Despite\ntheir importance, often only limited information about city trees is available.\nTherefore in this paper, we propose a method for mapping urban trees in\nhigh-resolution aerial imagery using limited datasets and deep learning. Deep\nlearning has become best-practice for this task, however, existing approaches\nrely on large and accurately labelled training datasets, which can be difficult\nand expensive to obtain. However, often noisy and incomplete data may be\navailable that can be combined and utilized to solve more difficult tasks than\nthose datasets were intended for. This paper studies how to combine accurate\npoint labels of urban trees along streets with crowd-sourced annotations from\nan open geographic database to delineate city trees in remote sensing images, a\ntask which is challenging even for humans. To that end, we perform semantic\nsegmentation of very high resolution aerial imagery using a fully convolutional\nneural network. The main challenge is that our segmentation maps are sparsely\nannotated and incomplete. Small areas around the point labels of the street\ntrees coming from official and crowd-sourced data are marked as foreground\nclass. Crowd-sourced annotations of streets, buildings, etc. define the\nbackground class. Since the tree data is incomplete, we introduce a masking to\navoid class confusion. Our experiments in Hamburg, Germany, showed that the\nsystem is able to produce tree cover maps, not limited to trees along streets,\nwithout providing tree delineations. We evaluated the method on manually\nlabelled trees and show that performance drastically deteriorates if the open\ngeographic database is not used.","terms":["cs.CV"]},{"titles":"NePF: Neural Photon Field for Single-Stage Inverse Rendering","summaries":"We present a novel single-stage framework, Neural Photon Field (NePF), to\naddress the ill-posed inverse rendering from multi-view images. Contrary to\nprevious methods that recover the geometry, material, and illumination in\nmultiple stages and extract the properties from various multi-layer perceptrons\nacross different neural fields, we question such complexities and introduce our\nmethod - a single-stage framework that uniformly recovers all properties. NePF\nachieves this unification by fully utilizing the physical implication behind\nthe weight function of neural implicit surfaces and the view-dependent\nradiance. Moreover, we introduce an innovative coordinate-based illumination\nmodel for rapid volume physically-based rendering. To regularize this\nillumination, we implement the subsurface scattering model for diffuse\nestimation. We evaluate our method on both real and synthetic datasets. The\nresults demonstrate the superiority of our approach in recovering high-fidelity\ngeometry and visual-plausible material attributes.","terms":["cs.CV","cs.GR"]},{"titles":"Physics-Enhanced TinyML for Real-Time Detection of Ground Magnetic Anomalies","summaries":"Space weather phenomena like geomagnetic disturbances (GMDs) and\ngeomagnetically induced currents (GICs) pose significant risks to critical\ntechnological infrastructure. While traditional predictive models, grounded in\nsimulation, hold theoretical robustness, they grapple with challenges, notably\nthe assimilation of imprecise data and extensive computational complexities. In\nrecent years, Tiny Machine Learning (TinyML) has been adopted to develop\nMachine Learning (ML)-enabled magnetometer systems for predicting real-time\nterrestrial magnetic perturbations as a proxy measure for GIC. While TinyML\noffers efficient, real-time data processing, its intrinsic limitations prevent\nthe utilization of robust methods with high computational needs. This paper\ndeveloped a physics-guided TinyML framework to address the above challenges.\nThis framework integrates physics-based regularization at the stages of model\ntraining and compression, thereby augmenting the reliability of predictions.\nThe developed pruning scheme within the framework harnesses the inherent\nphysical characteristics of the domain, striking a balance between model size\nand robustness. The study presents empirical results, drawing a comprehensive\ncomparison between the accuracy and reliability of the developed framework and\nits traditional counterpart. Such a comparative analysis underscores the\nprospective applicability of the developed framework in conceptualizing robust,\nML-enabled magnetometer systems for real-time space weather forecasting.","terms":["cs.LG","eess.SP"]},{"titles":"Reflection-Equivariant Diffusion for 3D Structure Determination from Isotopologue Rotational Spectra in Natural Abundance","summaries":"Structure determination is necessary to identify unknown organic molecules,\nsuch as those in natural products, forensic samples, the interstellar medium,\nand laboratory syntheses. Rotational spectroscopy enables structure\ndetermination by providing accurate 3D information about small organic\nmolecules via their moments of inertia. Using these moments, Kraitchman\nanalysis determines isotopic substitution coordinates, which are the unsigned\n$|x|,|y|,|z|$ coordinates of all atoms with natural isotopic abundance,\nincluding carbon, nitrogen, and oxygen. While unsigned substitution coordinates\ncan verify guesses of structures, the missing $+\/-$ signs make it challenging\nto determine the actual structure from the substitution coordinates alone. To\ntackle this inverse problem, we develop KREED (Kraitchman\nREflection-Equivariant Diffusion), a generative diffusion model that infers a\nmolecule's complete 3D structure from its molecular formula, moments of\ninertia, and unsigned substitution coordinates of heavy atoms. KREED's top-1\npredictions identify the correct 3D structure with >98% accuracy on the QM9 and\nGEOM datasets when provided with substitution coordinates of all heavy atoms\nwith natural isotopic abundance. When substitution coordinates are restricted\nto only a subset of carbons, accuracy is retained at 91% on QM9 and 32% on\nGEOM. On a test set of experimentally measured substitution coordinates\ngathered from the literature, KREED predicts the correct all-atom 3D structure\nin 25 of 33 cases, demonstrating experimental applicability for context-free 3D\nstructure determination with rotational spectroscopy.","terms":["cs.LG","astro-ph.GA","physics.chem-ph"]},{"titles":"Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts","summaries":"Multi-Task Reinforcement Learning (MTRL) tackles the long-standing problem of\nendowing agents with skills that generalize across a variety of problems. To\nthis end, sharing representations plays a fundamental role in capturing both\nunique and common characteristics of the tasks. Tasks may exhibit similarities\nin terms of skills, objects, or physical properties while leveraging their\nrepresentations eases the achievement of a universal policy. Nevertheless, the\npursuit of learning a shared set of diverse representations is still an open\nchallenge. In this paper, we introduce a novel approach for representation\nlearning in MTRL that encapsulates common structures among the tasks using\northogonal representations to promote diversity. Our method, named Mixture Of\nOrthogonal Experts (MOORE), leverages a Gram-Schmidt process to shape a shared\nsubspace of representations generated by a mixture of experts. When\ntask-specific information is provided, MOORE generates relevant representations\nfrom this shared subspace. We assess the effectiveness of our approach on two\nMTRL benchmarks, namely MiniGrid and MetaWorld, showing that MOORE surpasses\nrelated baselines and establishes a new state-of-the-art result on MetaWorld.","terms":["cs.LG"]},{"titles":"Uncertainty quantification for noisy inputs-outputs in physics-informed neural networks and neural operators","summaries":"Uncertainty quantification (UQ) in scientific machine learning (SciML)\nbecomes increasingly critical as neural networks (NNs) are being widely adopted\nin addressing complex problems across various scientific disciplines.\nRepresentative SciML models are physics-informed neural networks (PINNs) and\nneural operators (NOs). While UQ in SciML has been increasingly investigated in\nrecent years, very few works have focused on addressing the uncertainty caused\nby the noisy inputs, such as spatial-temporal coordinates in PINNs and input\nfunctions in NOs. The presence of noise in the inputs of the models can pose\nsignificantly more challenges compared to noise in the outputs of the models,\nprimarily due to the inherent nonlinearity of most SciML algorithms. As a\nresult, UQ for noisy inputs becomes a crucial factor for reliable and\ntrustworthy deployment of these models in applications involving physical\nknowledge. To this end, we introduce a Bayesian approach to quantify\nuncertainty arising from noisy inputs-outputs in PINNs and NOs. We show that\nthis approach can be seamlessly integrated into PINNs and NOs, when they are\nemployed to encode the physical information. PINNs incorporate physics by\nincluding physics-informed terms via automatic differentiation, either in the\nloss function or the likelihood, and often take as input the spatial-temporal\ncoordinate. Therefore, the present method equips PINNs with the capability to\naddress problems where the observed coordinate is subject to noise. On the\nother hand, pretrained NOs are also commonly employed as equation-free\nsurrogates in solving differential equations and Bayesian inverse problems, in\nwhich they take functions as inputs. The proposed approach enables them to\nhandle noisy measurements for both input and output functions with UQ.","terms":["cs.LG","physics.comp-ph"]},{"titles":"A Universal Framework for Accurate and Efficient Geometric Deep Learning of Molecular Systems","summaries":"Molecular sciences address a wide range of problems involving molecules of\ndifferent types and sizes and their complexes. Recently, geometric deep\nlearning, especially Graph Neural Networks, has shown promising performance in\nmolecular science applications. However, most existing works often impose\ntargeted inductive biases to a specific molecular system, and are inefficient\nwhen applied to macromolecules or large-scale tasks, thereby limiting their\napplications to many real-world problems. To address these challenges, we\npresent PAMNet, a universal framework for accurately and efficiently learning\nthe representations of three-dimensional (3D) molecules of varying sizes and\ntypes in any molecular system. Inspired by molecular mechanics, PAMNet induces\na physics-informed bias to explicitly model local and non-local interactions\nand their combined effects. As a result, PAMNet can reduce expensive\noperations, making it time and memory efficient. In extensive benchmark\nstudies, PAMNet outperforms state-of-the-art baselines regarding both accuracy\nand efficiency in three diverse learning tasks: small molecule properties, RNA\n3D structures, and protein-ligand binding affinities. Our results highlight the\npotential for PAMNet in a broad range of molecular science applications.","terms":["cs.LG","q-bio.BM"]},{"titles":"Attention-Based Real-Time Defenses for Physical Adversarial Attacks in Vision Applications","summaries":"Deep neural networks exhibit excellent performance in computer vision tasks,\nbut their vulnerability to real-world adversarial attacks, achieved through\nphysical objects that can corrupt their predictions, raises serious security\nconcerns for their application in safety-critical domains. Existing defense\nmethods focus on single-frame analysis and are characterized by high\ncomputational costs that limit their applicability in multi-frame scenarios,\nwhere real-time decisions are crucial.\n  To address this problem, this paper proposes an efficient attention-based\ndefense mechanism that exploits adversarial channel-attention to quickly\nidentify and track malicious objects in shallow network layers and mask their\nadversarial effects in a multi-frame setting. This work advances the state of\nthe art by enhancing existing over-activation techniques for real-world\nadversarial attacks to make them usable in real-time applications. It also\nintroduces an efficient multi-frame defense framework, validating its efficacy\nthrough extensive experiments aimed at evaluating both defense performance and\ncomputational cost.","terms":["cs.CV","cs.AI"]},{"titles":"Deep Tensor Network","summaries":"In this paper, we delve into the foundational principles of tensor\ncategories, harnessing the universal property of the tensor product to pioneer\nnovel methodologies in deep network architectures. Our primary contribution is\nthe introduction of the Tensor Attention and Tensor Interaction Mechanism, a\ngroundbreaking approach that leverages the tensor category to enhance the\ncomputational efficiency and the expressiveness of deep networks, and can even\nbe generalized into the quantum realm.","terms":["cs.LG","cs.AI","cs.CV","quant-ph"]},{"titles":"Single-shot Phase Retrieval from a Fractional Fourier Transform Perspective","summaries":"The realm of classical phase retrieval concerns itself with the arduous task\nof recovering a signal from its Fourier magnitude measurements, which are\nfraught with inherent ambiguities. A single-exposure intensity measurement is\ncommonly deemed insufficient for the reconstruction of the primal signal, given\nthat the absent phase component is imperative for the inverse transformation.\nIn this work, we present a novel single-shot phase retrieval paradigm from a\nfractional Fourier transform (FrFT) perspective, which involves integrating the\nFrFT-based physical measurement model within a self-supervised reconstruction\nscheme. Specifically, the proposed FrFT-based measurement model addresses the\naliasing artifacts problem in the numerical calculation of Fresnel diffraction,\nfeaturing adaptability to both short-distance and long-distance propagation\nscenarios. Moreover, the intensity measurement in the FrFT domain proves highly\neffective in alleviating the ambiguities of phase retrieval and relaxing the\nprevious conditions on oversampled or multiple measurements in the Fourier\ndomain. Furthermore, the proposed self-supervised reconstruction approach\nharnesses the fast discrete algorithm of FrFT alongside untrained neural\nnetwork priors, thereby attaining preeminent results. Through numerical\nsimulations, we demonstrate that both amplitude and phase objects can be\neffectively retrieved from a single-shot intensity measurement using the\nproposed approach and provide a promising technique for support-free coherent\ndiffraction imaging.","terms":["cs.CV","physics.optics"]},{"titles":"Compact and Intuitive Airfoil Parameterization Method through Physics-aware Variational Autoencoder","summaries":"Airfoil shape optimization plays a critical role in the design of\nhigh-performance aircraft. However, the high-dimensional nature of airfoil\nrepresentation causes the challenging problem known as the \"curse of\ndimensionality\". To overcome this problem, numerous airfoil parameterization\nmethods have been developed, which can be broadly classified as\npolynomial-based and data-driven approaches. Each of these methods has\ndesirable characteristics such as flexibility, parsimony, feasibility, and\nintuitiveness, but a single approach that encompasses all of these attributes\nhas yet to be found. For example, polynomial-based methods struggle to balance\nparsimony and flexibility, while data-driven methods lack in feasibility and\nintuitiveness. In recent years, generative models, such as generative\nadversarial networks and variational autoencoders, have shown promising\npotential in airfoil parameterization. However, these models still face\nchallenges related to intuitiveness due to their black-box nature. To address\nthis issue, we developed a novel airfoil parameterization method using\nphysics-aware variational autoencoder. The proposed method not only explicitly\nseparates the generation of thickness and camber distributions to produce\nsmooth and non-intersecting airfoils, thereby improving feasibility, but it\nalso directly aligns its latent dimensions with geometric features of the\nairfoil, significantly enhancing intuitiveness. Finally, extensive comparative\nstudies were performed to demonstrate the effectiveness of our approach.","terms":["cs.LG","cs.AI"]},{"titles":"Zero-Shot Digital Rock Image Segmentation with a Fine-Tuned Segment Anything Model","summaries":"Accurate image segmentation is crucial in reservoir modelling and material\ncharacterization, enhancing oil and gas extraction efficiency through detailed\nreservoir models. This precision offers insights into rock properties,\nadvancing digital rock physics understanding. However, creating pixel-level\nannotations for complex CT and SEM rock images is challenging due to their size\nand low contrast, lengthening analysis time. This has spurred interest in\nadvanced semi-supervised and unsupervised segmentation techniques in digital\nrock image analysis, promising more efficient, accurate, and less\nlabour-intensive methods. Meta AI's Segment Anything Model (SAM) revolutionized\nimage segmentation in 2023, offering interactive and automated segmentation\nwith zero-shot capabilities, essential for digital rock physics with limited\ntraining data and complex image features. Despite its advanced features, SAM\nstruggles with rock CT\/SEM images due to their absence in its training set and\nthe low-contrast nature of grayscale images. Our research fine-tunes SAM for\nrock CT\/SEM image segmentation, optimizing parameters and handling large-scale\nimages to improve accuracy. Experiments on rock CT and SEM images show that\nfine-tuning significantly enhances SAM's performance, enabling high-quality\nmask generation in digital rock image analysis. Our results demonstrate the\nfeasibility and effectiveness of the fine-tuned SAM model (RockSAM) for rock\nimages, offering segmentation without extensive training or complex labelling.","terms":["cs.CV"]},{"titles":"Unsupervised embedding of trajectories captures the latent structure of scientific migration","summaries":"Human migration and mobility drives major societal phenomena including\nepidemics, economies, innovation, and the diffusion of ideas. Although human\nmobility and migration have been heavily constrained by geographic distance\nthroughout the history, advances and globalization are making other factors\nsuch as language and culture increasingly more important. Advances in neural\nembedding models, originally designed for natural language, provide an\nopportunity to tame this complexity and open new avenues for the study of\nmigration. Here, we demonstrate the ability of the model word2vec to encode\nnuanced relationships between discrete locations from migration trajectories,\nproducing an accurate, dense, continuous, and meaningful vector-space\nrepresentation. The resulting representation provides a functional distance\nbetween locations, as well as a digital double that can be distributed,\nre-used, and itself interrogated to understand the many dimensions of\nmigration. We show that the unique power of word2vec to encode migration\npatterns stems from its mathematical equivalence with the gravity model of\nmobility. Focusing on the case of scientific migration, we apply word2vec to a\ndatabase of three million migration trajectories of scientists derived from the\naffiliations listed on their publication records. Using techniques that\nleverage its semantic structure, we demonstrate that embeddings can learn the\nrich structure that underpins scientific migration, such as cultural,\nlinguistic, and prestige relationships at multiple levels of granularity. Our\nresults provide a theoretical foundation and methodological framework for using\nneural embeddings to represent and understand migration both within and beyond\nscience.","terms":["cs.LG","physics.soc-ph"]},{"titles":"Domain Generalization of 3D Object Detection by Density-Resampling","summaries":"Point-cloud-based 3D object detection suffers from performance degradation\nwhen encountering data with novel domain gaps. To tackle it, the single-domain\ngeneralization (SDG) aims to generalize the detection model trained in a\nlimited single source domain to perform robustly on unexplored domains. In this\npaper, we propose an SDG method to improve the generalizability of 3D object\ndetection to unseen target domains. Unlike prior SDG works for 3D object\ndetection solely focusing on data augmentation, our work introduces a novel\ndata augmentation method and contributes a new multi-task learning strategy in\nthe methodology. Specifically, from the perspective of data augmentation, we\ndesign a universal physical-aware density-based data augmentation (PDDA) method\nto mitigate the performance loss stemming from diverse point densities. From\nthe learning methodology viewpoint, we develop a multi-task learning for 3D\nobject detection: during source training, besides the main standard detection\ntask, we leverage an auxiliary self-supervised 3D scene restoration task to\nenhance the comprehension of the encoder on background and foreground details\nfor better recognition and detection of objects. Furthermore, based on the\nauxiliary self-supervised task, we propose the first test-time adaptation\nmethod for domain generalization of 3D object detection, which efficiently\nadjusts the encoder's parameters to adapt to unseen target domains during\ntesting time, to further bridge domain gaps. Extensive cross-dataset\nexperiments covering \"Car\", \"Pedestrian\", and \"Cyclist\" detections, demonstrate\nour method outperforms state-of-the-art SDG methods and even overpass\nunsupervised domain adaptation methods under some circumstances. The code will\nbe made publicly available.","terms":["cs.CV"]},{"titles":"Online Calibration of Deep Learning Sub-Models for Hybrid Numerical Modeling Systems","summaries":"Artificial intelligence and deep learning are currently reshaping numerical\nsimulation frameworks by introducing new modeling capabilities. These\nframeworks are extensively investigated in the context of model correction and\nparameterization where they demonstrate great potential and often outperform\ntraditional physical models. Most of these efforts in defining hybrid dynamical\nsystems follow {offline} learning strategies in which the neural\nparameterization (called here sub-model) is trained to output an ideal\ncorrection. Yet, these hybrid models can face hard limitations when defining\nwhat should be a relevant sub-model response that would translate into a good\nforecasting performance. End-to-end learning schemes, also referred to as\nonline learning, could address such a shortcoming by allowing the deep learning\nsub-models to train on historical data. However, defining end-to-end training\nschemes for the calibration of neural sub-models in hybrid systems requires\nworking with an optimization problem that involves the solver of the physical\nequations. Online learning methodologies thus require the numerical model to be\ndifferentiable, which is not the case for most modeling systems. To overcome\nthis difficulty and bypass the differentiability challenge of physical models,\nwe present an efficient and practical online learning approach for hybrid\nsystems. The method, called EGA for Euler Gradient Approximation, assumes an\nadditive neural correction to the physical model, and an explicit Euler\napproximation of the gradients. We demonstrate that the EGA converges to the\nexact gradients in the limit of infinitely small time steps. Numerical\nexperiments are performed on various case studies, including prototypical\nocean-atmosphere dynamics. Results show significant improvements over offline\nlearning, highlighting the potential of end-to-end online learning for hybrid\nmodeling.","terms":["cs.LG","cs.NA","math.NA","physics.comp-ph"]},{"titles":"Predicting the Probability of Collision of a Satellite with Space Debris: A Bayesian Machine Learning Approach","summaries":"Space is becoming more crowded in Low Earth Orbit due to increased space\nactivity. Such a dense space environment increases the risk of collisions\nbetween space objects endangering the whole space population. Therefore, the\nneed to consider collision avoidance as part of routine operations is evident\nto satellite operators. Current procedures rely on the analysis of multiple\ncollision warnings by human analysts. However, with the continuous growth of\nthe space population, this manual approach may become unfeasible, highlighting\nthe importance of automation in risk assessment. In 2019, ESA launched a\ncompetition to study the feasibility of applying machine learning in collision\nrisk estimation and released a dataset that contained sequences of Conjunction\nData Messages (CDMs) in support of real close encounters. The competition\nresults showed that the naive forecast and its variants are strong predictors\nfor this problem, which suggests that the CDMs may follow the Markov property.\nThe proposed work investigates this theory by benchmarking Hidden Markov Models\n(HMM) in predicting the risk of collision between two resident space objects by\nusing one feature of the entire dataset: the sequence of the probability in the\nCDMs. In addition, Bayesian statistics are used to infer a joint distribution\nfor the parameters of the models, which allows the development of robust and\nreliable probabilistic predictive models that can incorporate physical or prior\nknowledge about the problem within a rigorous theoretical framework and\nprovides prediction uncertainties that nicely reflect the accuracy of the\npredicted risk. This work shows that the implemented HMM outperforms the naive\nsolution in some metrics, which further adds to the idea that the collision\nwarnings may be Markovian and suggests that this is a powerful method to be\nfurther explored.","terms":["cs.LG"]},{"titles":"Graph Neural Networks for Pressure Estimation in Water Distribution Systems","summaries":"Pressure and flow estimation in Water Distribution Networks (WDN) allows\nwater management companies to optimize their control operations. For many\nyears, mathematical simulation tools have been the most common approach to\nreconstructing an estimate of the WDN hydraulics. However, pure physics-based\nsimulations involve several challenges, e.g. partially observable data, high\nuncertainty, and extensive manual configuration. Thus, data-driven approaches\nhave gained traction to overcome such limitations. In this work, we combine\nphysics-based modeling and Graph Neural Networks (GNN), a data-driven approach,\nto address the pressure estimation problem. First, we propose a new data\ngeneration method using a mathematical simulation but not considering temporal\npatterns and including some control parameters that remain untouched in\nprevious works; this contributes to a more diverse training data. Second, our\ntraining strategy relies on random sensor placement making our GNN-based\nestimation model robust to unexpected sensor location changes. Third, a\nrealistic evaluation protocol considers real temporal patterns and additionally\ninjects the uncertainties intrinsic to real-world scenarios. Finally, a\nmulti-graph pre-training strategy allows the model to be reused for pressure\nestimation in unseen target WDNs. Our GNN-based model estimates the pressure of\na large-scale WDN in The Netherlands with a MAE of 1.94mH$_2$O and a MAPE of\n7%, surpassing the performance of previous studies. Likewise, it outperformed\nprevious approaches on other WDN benchmarks, showing a reduction of absolute\nerror up to approximately 52% in the best cases.","terms":["cs.LG"]},{"titles":"Hybrid quantum physics-informed neural networks for simulating computational fluid dynamics in complex shapes","summaries":"Finding the distribution of the velocities and pressures of a fluid (by\nsolving the Navier-Stokes equations) is a principal task in the chemical,\nenergy, and pharmaceutical industries, as well as in mechanical engineering and\nthe design of pipeline systems. With existing solvers, such as OpenFOAM and\nAnsys, simulations of fluid dynamics in intricate geometries are\ncomputationally expensive and require re-simulation whenever the geometric\nparameters or the initial and boundary conditions are altered. Physics-informed\nneural networks are a promising tool for simulating fluid flows in complex\ngeometries, as they can adapt to changes in the geometry and mesh definitions,\nallowing for generalization across different shapes. We present a hybrid\nquantum physics-informed neural network that simulates laminar fluid flows in\n3D Y-shaped mixers. Our approach combines the expressive power of a quantum\nmodel with the flexibility of a physics-informed neural network, resulting in a\n21% higher accuracy compared to a purely classical neural network. Our findings\nhighlight the potential of machine learning approaches, and in particular\nhybrid quantum physics-informed neural network, for complex shape optimization\ntasks in computational fluid dynamics. By improving the accuracy of fluid\nsimulations in complex geometries, our research using hybrid quantum models\ncontributes to the development of more efficient and reliable fluid dynamics\nsolvers.","terms":["cs.LG","physics.flu-dyn","quant-ph"]},{"titles":"Accurate and Fast Fischer-Tropsch Reaction Microkinetics using PINNs","summaries":"Microkinetics allows detailed modelling of chemical transformations occurring\nin many industrially relevant reactions. Traditional way of solving the\nmicrokinetics model for Fischer-Tropsch synthesis (FTS) becomes inefficient\nwhen it comes to more advanced real-time applications. In this work, we address\nthese challenges by using physics-informed neural networks(PINNs) for modelling\nFTS microkinetics. We propose a computationally efficient and accurate method,\nenabling the ultra-fast solution of the existing microkinetics models in\nrealistic process conditions. The proposed PINN model computes the fraction of\nvacant catalytic sites, a key quantity in FTS microkinetics, with median\nrelative error (MRE) of 0.03%, and the FTS product formation rates with MRE of\n0.1%. Compared to conventional equation solvers, the model achieves up to 1E+06\ntimes speed-up when running on GPUs, thus being fast enough for multi-scale and\nmulti-physics reactor modelling and enabling its applications in real-time\nprocess control and optimization.","terms":["cs.LG","cs.AI","physics.chem-ph","physics.comp-ph"]},{"titles":"Enhancing Student Engagement in Online Learning through Facial Expression Analysis and Complex Emotion Recognition using Deep Learning","summaries":"In response to the COVID-19 pandemic, traditional physical classrooms have\ntransitioned to online environments, necessitating effective strategies to\nensure sustained student engagement. A significant challenge in online teaching\nis the absence of real-time feedback from teachers on students learning\nprogress. This paper introduces a novel approach employing deep learning\ntechniques based on facial expressions to assess students engagement levels\nduring online learning sessions. Human emotions cannot be adequately conveyed\nby a student using only the basic emotions, including anger, disgust, fear,\njoy, sadness, surprise, and neutrality. To address this challenge, proposed a\ngeneration of four complex emotions such as confusion, satisfaction,\ndisappointment, and frustration by combining the basic emotions. These complex\nemotions are often experienced simultaneously by students during the learning\nsession. To depict these emotions dynamically,utilized a continuous stream of\nimage frames instead of discrete images. The proposed work utilized a\nConvolutional Neural Network (CNN) model to categorize the fundamental\nemotional states of learners accurately. The proposed CNN model demonstrates\nstrong performance, achieving a 95% accuracy in precise categorization of\nlearner emotions.","terms":["cs.CV"]},{"titles":"How False Data Affects Machine Learning Models in Electrochemistry?","summaries":"Recently, the selection of machine learning model based on only the data\ndistribution without concerning the noise of the data. This study aims to\ndistinguish, which models perform well under noisy data, and establish whether\nstacking machine learning models actually provide robustness to otherwise\nweak-to-noise models. The electrochemical data were tested with 12 standalone\nmodels and stacking model. This includes XGB, LGBM, RF, GB, ADA, NN, ELAS,\nLASS, RIDGE, SVM, KNN, DT, and the stacking model. It is found that linear\nmodels handle noise well with the average error of (slope) to 1.75 F g-1 up to\nerror per 100% percent noise added; but it suffers from prediction accuracy due\nto having an average of 60.19 F g-1 estimated at minimal error at 0% noise\nadded. Tree-based models fail in terms of noise handling (average slope is\n55.24 F g-1 at 100% percent noise), but it can provide higher prediction\naccuracy (lowest error of 23.9 F g-1) than that of linear. To address the\ncontroversial between prediction accuracy and error handling, the stacking\nmodel was constructed, which is not only show high accuracy (intercept of 25.03\nF g-1), but it also exhibits good noise handling (slope of 43.58 F g-1), making\nstacking models a relatively low risk and viable choice for beginner and\nexperienced machine learning research in electrochemistry. Even though neural\nnetworks (NN) are gaining popularity in the electrochemistry field. However,\nthis study presents that NN is not suitable for electrochemical data, and\nimproper tuning resulting in a model that is susceptible to noise. Thus, STACK\nmodels should provide better benefits in that even with untuned base models,\nthey can achieve an accurate and noise-tolerant model. Overall, this work\nprovides insight into machine learning model selection for electrochemical\ndata, which should aid the understanding of data science in chemistry context.","terms":["cs.LG","physics.chem-ph"]},{"titles":"Physics-Enhanced Multi-fidelity Learning for Optical Surface Imprint","summaries":"Human fingerprints serve as one unique and powerful characteristic for each\nperson, from which policemen can recognize the identity. Similar to humans,\nmany natural bodies and intrinsic mechanical qualities can also be uniquely\nidentified from surface characteristics. To measure the elasto-plastic\nproperties of one material, one formally sharp indenter is pushed into the\nmeasured body under constant force and retracted, leaving a unique residual\nimprint of the minute size from several micrometers to nanometers. However, one\ngreat challenge is how to map the optical image of this residual imprint into\nthe real wanted mechanical properties, i.e., the tensile force curve. In this\npaper, we propose a novel method to use multi-fidelity neural networks (MFNN)\nto solve this inverse problem. We first actively train the NN model via pure\nsimulation data, and then bridge the sim-to-real gap via transfer learning. The\nmost innovative part is that we use NN to dig out the unknown physics and also\nimplant the known physics into the transfer learning framework, thus highly\nimproving the model stability and decreasing the data requirement. This work\nserves as one great example of applying machine learning into the real\nexperimental research, especially under the constraints of data limitation and\nfidelity variance.","terms":["cs.LG","cs.AI","cs.CV","physics.app-ph"]},{"titles":"FREE: The Foundational Semantic Recognition for Modeling Environmental Ecosystems","summaries":"Modeling environmental ecosystems is critical for the sustainability of our\nplanet, but is extremely challenging due to the complex underlying processes\ndriven by interactions amongst a large number of physical variables. As many\nvariables are difficult to measure at large scales, existing works often\nutilize a combination of observable features and locally available measurements\nor modeled values as input to build models for a specific study region and time\nperiod. This raises a fundamental question in advancing the modeling of\nenvironmental ecosystems: how to build a general framework for modeling the\ncomplex relationships amongst various environmental data over space and time?\nIn this paper, we introduce a new framework, FREE, which maps available\nenvironmental data into a text space and then converts the traditional\npredictive modeling task in environmental science to the semantic recognition\nproblem. The proposed FREE framework leverages recent advances in Large\nLanguage Models (LLMs) to supplement the original input features with natural\nlanguage descriptions. This facilitates capturing the data semantics and also\nallows harnessing the irregularities of input features. When used for long-term\nprediction, FREE has the flexibility to incorporate newly collected\nobservations to enhance future prediction. The efficacy of FREE is evaluated in\nthe context of two societally important real-world applications, predicting\nstream water temperature in the Delaware River Basin and predicting annual corn\nyield in Illinois and Iowa. Beyond the superior predictive performance over\nmultiple baseline methods, FREE is shown to be more data- and\ncomputation-efficient as it can be pre-trained on simulated data generated by\nphysics-based models.","terms":["cs.LG","q-bio.PE"]},{"titles":"Finding Real-World Orbital Motion Laws from Data","summaries":"A novel approach is presented for discovering PDEs that govern the motion of\nsatellites in space. The method is based on SINDy, a data-driven technique\ncapable of identifying the underlying dynamics of complex physical systems from\ntime series data. SINDy is utilized to uncover PDEs that describe the laws of\nphysics in space, which are non-deterministic and influenced by various factors\nsuch as drag or the reference area (related to the attitude of the satellite).\nIn contrast to prior works, the physically interpretable coordinate system is\nmaintained, and no dimensionality reduction technique is applied to the data.\nBy training the model with multiple representative trajectories of LEO -\nencompassing various inclinations, eccentricities, and altitudes - and testing\nit with unseen orbital motion patterns, a mean error of around 140 km for the\npositions and 0.12 km\/s for the velocities is achieved. The method offers the\nadvantage of delivering interpretable, accurate, and complex models of orbital\nmotion that can be employed for propagation or as inputs to predictive models\nfor other variables of interest, such as atmospheric drag or the probability of\ncollision in an encounter with a spacecraft or space objects. In conclusion,\nthe work demonstrates the promising potential of using SINDy to discover the\nequations governing the behaviour of satellites in space. The technique has\nbeen successfully applied to uncover PDEs describing the motion of satellites\nin LEO with high accuracy. The method possesses several advantages over\ntraditional models, including the ability to provide physically interpretable,\naccurate, and complex models of orbital motion derived from high-entropy\ndatasets. These models can be utilised for propagation or as inputs to\npredictive models for other variables of interest.","terms":["cs.LG","astro-ph.EP","astro-ph.IM"]},{"titles":"Runtime Verification of Learning Properties for Reinforcement Learning Algorithms","summaries":"Reinforcement learning (RL) algorithms interact with their environment in a\ntrial-and-error fashion. Such interactions can be expensive, inefficient, and\ntimely when learning on a physical system rather than in a simulation. This\nwork develops new runtime verification techniques to predict when the learning\nphase has not met or will not meet qualitative and timely expectations. This\npaper presents three verification properties concerning the quality and\ntimeliness of learning in RL algorithms. With each property, we propose design\nsteps for monitoring and assessing the properties during the system's\noperation.","terms":["cs.LG"]},{"titles":"FedFusion: Manifold Driven Federated Learning for Multi-satellite and Multi-modality Fusion","summaries":"Multi-satellite, multi-modality in-orbit fusion is a challenging task as it\nexplores the fusion representation of complex high-dimensional data under\nlimited computational resources. Deep neural networks can reveal the underlying\ndistribution of multi-modal remote sensing data, but the in-orbit fusion of\nmultimodal data is more difficult because of the limitations of different\nsensor imaging characteristics, especially when the multimodal data follows\nnon-independent identically distribution (Non-IID) distributions. To address\nthis problem while maintaining classification performance, this paper proposes\na manifold-driven multi-modality fusion framework, FedFusion, which randomly\nsamples local data on each client to jointly estimate the prominent manifold\nstructure of shallow features of each client and explicitly compresses the\nfeature matrices into a low-rank subspace through cascading and additive\napproaches, which is used as the feature input of the subsequent classifier.\nConsidering the physical space limitations of the satellite constellation, we\ndeveloped a multimodal federated learning module designed specifically for\nmanifold data in a deep latent space. This module achieves iterative updating\nof the sub-network parameters of each client through global weighted averaging,\nconstructing a framework that can represent compact representations of each\nclient. The proposed framework surpasses existing methods in terms of\nperformance on three multimodal datasets, achieving a classification average\naccuracy of 94.35$\\%$ while compressing communication costs by a factor of 4.\nFurthermore, extensive numerical evaluations of real-world satellite images\nwere conducted on the orbiting edge computing architecture based on Jetson TX2\nindustrial modules, which demonstrated that FedFusion significantly reduced\ntraining time by 48.4 minutes (15.18%) while optimizing accuracy.}","terms":["cs.CV"]},{"titles":"Adaptive Interventions with User-Defined Goals for Health Behavior Change","summaries":"Physical inactivity remains a major public health concern, having\nassociations with adverse health outcomes such as cardiovascular disease and\ntype-2 diabetes. Mobile health applications present a promising avenue for\nlow-cost, scalable physical activity promotion, yet often suffer from small\neffect sizes and low adherence rates, particularly in comparison to human\ncoaching. Goal-setting is a critical component of health coaching that has been\nunderutilized in adaptive algorithms for mobile health interventions. This\npaper introduces a modification to the Thompson sampling algorithm that places\nemphasis on individualized goal-setting by optimizing personalized reward\nfunctions. As a step towards supporting goal-setting, this paper offers a\nbalanced approach that can leverage shared structure while optimizing\nindividual preferences and goals. We prove that our modification incurs only a\nconstant penalty on the cumulative regret while preserving the sample\ncomplexity benefits of data sharing. In a physical activity simulator, we\ndemonstrate that our algorithm achieves substantial improvements in cumulative\nregret compared to baselines that do not share data or do not optimize for\nindividualized rewards.","terms":["cs.LG","cs.AI"]},{"titles":"Biomembrane-based Memcapacitive Reservoir Computing System for Energy Efficient Temporal Data Processing","summaries":"Reservoir computing is a highly efficient machine learning framework for\nprocessing temporal data by extracting features from the input signal and\nmapping them into higher dimensional spaces. Physical reservoir layers have\nbeen realized using spintronic oscillators, atomic switch networks, silicon\nphotonic modules, ferroelectric transistors, and volatile memristors. However,\nthese devices are intrinsically energy-dissipative due to their resistive\nnature, which leads to increased power consumption. Therefore, capacitive\nmemory devices can provide a more energy-efficient approach. Here, we leverage\nvolatile biomembrane-based memcapacitors that closely mimic certain short-term\nsynaptic plasticity functions as reservoirs to solve classification tasks and\nanalyze time-series data in simulation and experimentally. Our system achieves\na 99.6% accuracy rate for spoken digit classification and a normalized mean\nsquare error of 7.81*10^{-4} in a second-order non-linear regression task.\nFurthermore, to showcase the device's real-time temporal data processing\ncapability, we achieve 100% accuracy for a real-time epilepsy detection problem\nfrom an inputted electroencephalography (EEG) signal. Most importantly, we\ndemonstrate that each memcapacitor consumes an average of 41.5 fJ of energy per\nspike, regardless of the selected input voltage pulse width, while maintaining\nan average power of 415 fW for a pulse width of 100 ms. These values are orders\nof magnitude lower than those achieved by state-of-the-art memristors used as\nreservoirs. Lastly, we believe the biocompatible, soft nature of our\nmemcapacitor makes it highly suitable for computing and signal-processing\napplications in biological environments.","terms":["cs.LG","cs.AI","cs.ET","cs.NE"]},{"titles":"Inexpensive High Fidelity Melt Pool Models in Additive Manufacturing Using Generative Deep Diffusion","summaries":"Defects in laser powder bed fusion (L-PBF) parts often result from the\nmeso-scale dynamics of the molten alloy near the laser, known as the melt pool.\nFor instance, the melt pool can directly contribute to the formation of\nundesirable porosity, residual stress, and surface roughness in the final part.\nExperimental in-situ monitoring of the three-dimensional melt pool physical\nfields is challenging, due to the short length and time scales involved in the\nprocess. Multi-physics simulation methods can describe the three-dimensional\ndynamics of the melt pool, but are computationally expensive at the mesh\nrefinement required for accurate predictions of complex effects, such as the\nformation of keyhole porosity. Therefore, in this work, we develop a generative\ndeep learning model based on the probabilistic diffusion framework to map\nlow-fidelity, coarse-grained simulation information to the high-fidelity\ncounterpart. By doing so, we bypass the computational expense of conducting\nmultiple high-fidelity simulations for analysis by instead upscaling\nlightweight coarse mesh simulations. Specifically, we implement a 2-D diffusion\nmodel to spatially upscale cross-sections of the coarsely simulated melt pool\nto their high-fidelity equivalent. We demonstrate the preservation of key\nmetrics of the melting process between the ground truth simulation data and the\ndiffusion model output, such as the temperature field, the melt pool dimensions\nand the variability of the keyhole vapor cavity. Specifically, we predict the\nmelt pool depth within 3 $\\mu m$ based on low-fidelity input data 4$\\times$\ncoarser than the high-fidelity simulations, reducing analysis time by two\norders of magnitude.","terms":["cs.LG","cond-mat.mtrl-sci"]},{"titles":"Artificial Intelligence for Science in Quantum, Atomistic, and Continuum Systems","summaries":"Advances in artificial intelligence (AI) are fueling a new paradigm of\ndiscoveries in natural sciences. Today, AI has started to advance natural\nsciences by improving, accelerating, and enabling our understanding of natural\nphenomena at a wide range of spatial and temporal scales, giving rise to a new\narea of research known as AI for science (AI4Science). Being an emerging\nresearch paradigm, AI4Science is unique in that it is an enormous and highly\ninterdisciplinary area. Thus, a unified and technical treatment of this field\nis needed yet challenging. This work aims to provide a technically thorough\naccount of a subarea of AI4Science; namely, AI for quantum, atomistic, and\ncontinuum systems. These areas aim at understanding the physical world from the\nsubatomic (wavefunctions and electron density), atomic (molecules, proteins,\nmaterials, and interactions), to macro (fluids, climate, and subsurface) scales\nand form an important subarea of AI4Science. A unique advantage of focusing on\nthese areas is that they largely share a common set of challenges, thereby\nallowing a unified and foundational treatment. A key common challenge is how to\ncapture physics first principles, especially symmetries, in natural systems by\ndeep learning methods. We provide an in-depth yet intuitive account of\ntechniques to achieve equivariance to symmetry transformations. We also discuss\nother common technical challenges, including explainability,\nout-of-distribution generalization, knowledge transfer with foundation and\nlarge language models, and uncertainty quantification. To facilitate learning\nand education, we provide categorized lists of resources that we found to be\nuseful. We strive to be thorough and unified and hope this initial effort may\ntrigger more community interests and efforts to further advance AI4Science.","terms":["cs.LG","physics.comp-ph"]},{"titles":"Improved Sparse Ising Optimization","summaries":"Sparse Ising problems can be found in application areas such as logistics,\ncondensed matter physics and training of deep Boltzmann networks, but can be\nvery difficult to tackle with high efficiency and accuracy. This report\npresents new data demonstrating significantly higher performance on some\nlongstanding benchmark problems with up to 20,000 variables. The data come from\na new heuristic algorithm tested on the large sparse instances from the Gset\nbenchmark suite. Relative to leading reported combinations of speed and\naccuracy (e.g., from Toshiba's Simulated Bifurcation Machine and Breakout Local\nSearch), a proof-of-concept implementation reached targets 2-4 orders of\nmagnitude faster. For two instances (G72 and G77) the new algorithm discovered\na better solution than all previously reported values. Solution bitstrings\nconfirming these two best solutions are provided. The data suggest exciting\npossibilities for pushing the sparse Ising performance frontier to potentially\nstrengthen algorithm portfolios, AI toolkits and decision-making systems.","terms":["cs.LG","cond-mat.dis-nn"]},{"titles":"Machine-learning parameter tracking with partial state observation","summaries":"Complex and nonlinear dynamical systems often involve parameters that change\nwith time, accurate tracking of which is essential to tasks such as state\nestimation, prediction, and control. Existing machine-learning methods require\nfull state observation of the underlying system and tacitly assume adiabatic\nchanges in the parameter. Formulating an inverse problem and exploiting\nreservoir computing, we develop a model-free and fully data-driven framework to\naccurately track time-varying parameters from partial state observation in real\ntime. In particular, with training data from a subset of the dynamical\nvariables of the system for a small number of known parameter values, the\nframework is able to accurately predict the parameter variations in time. Low-\nand high-dimensional, Markovian and non-Markovian nonlinear dynamical systems\nare used to demonstrate the power of the machine-learning based\nparameter-tracking framework. Pertinent issues affecting the tracking\nperformance are addressed.","terms":["cs.LG","math.DS","nlin.CD","physics.comp-ph"]},{"titles":"Fast Detection of Phase Transitions with Multi-Task Learning-by-Confusion","summaries":"Machine learning has been successfully used to study phase transitions. One\nof the most popular approaches to identifying critical points from data without\nprior knowledge of the underlying phases is the learning-by-confusion scheme.\nAs input, it requires system samples drawn from a grid of the parameter whose\nchange is associated with potential phase transitions. Up to now, the scheme\nrequired training a distinct binary classifier for each possible splitting of\nthe grid into two sides, resulting in a computational cost that scales linearly\nwith the number of grid points. In this work, we propose and showcase an\nalternative implementation that only requires the training of a single\nmulti-class classifier. Ideally, such multi-task learning eliminates the\nscaling with respect to the number of grid points. In applications to the Ising\nmodel and an image dataset generated with Stable Diffusion, we find significant\nspeedups that closely correspond to the ideal case, with only minor deviations.","terms":["cs.LG","cond-mat.dis-nn","cond-mat.stat-mech"]},{"titles":"Pre-Training on Large-Scale Generated Docking Conformations with HelixDock to Unlock the Potential of Protein-ligand Structure Prediction Models","summaries":"Protein-ligand structure prediction is an essential task in drug discovery,\npredicting the binding interactions between small molecules (ligands) and\ntarget proteins (receptors). Although conventional physics-based docking tools\nare widely utilized, their accuracy is compromised by limited conformational\nsampling and imprecise scoring functions. Recent advances have incorporated\ndeep learning techniques to improve the accuracy of structure prediction.\nNevertheless, the experimental validation of docking conformations remains\ncostly, it raises concerns regarding the generalizability of these deep\nlearning-based methods due to the limited training data. In this work, we show\nthat by pre-training a geometry-aware SE(3)-Equivariant neural network on a\nlarge-scale docking conformation generated by traditional physics-based docking\ntools and then fine-tuning with a limited set of experimentally validated\nreceptor-ligand complexes, we can achieve outstanding performance. This process\ninvolved the generation of 100 million docking conformations, consuming roughly\n1 million CPU core days. The proposed model, HelixDock, aims to acquire the\nphysical knowledge encapsulated by the physics-based docking tools during the\npre-training phase. HelixDock has been benchmarked against both physics-based\nand deep learning-based baselines, showing that it outperforms its closest\ncompetitor by over 40% for RMSD. HelixDock also exhibits enhanced performance\non a dataset that poses a greater challenge, thereby highlighting its\nrobustness. Moreover, our investigation reveals the scaling laws governing\npre-trained structure prediction models, indicating a consistent enhancement in\nperformance with increases in model parameters and pre-training data. This\nstudy illuminates the strategic advantage of leveraging a vast and varied\nrepository of generated data to advance the frontiers of AI-driven drug\ndiscovery.","terms":["cs.LG","cs.CE","q-bio.BM"]},{"titles":"Physical Adversarial Examples for Multi-Camera Systems","summaries":"Neural networks build the foundation of several intelligent systems, which,\nhowever, are known to be easily fooled by adversarial examples. Recent advances\nmade these attacks possible even in air-gapped scenarios, where the autonomous\nsystem observes its surroundings by, e.g., a camera. We extend these ideas in\nour research and evaluate the robustness of multi-camera setups against such\nphysical adversarial examples. This scenario becomes ever more important with\nthe rise in popularity of autonomous vehicles, which fuse the information of\nseveral cameras for their driving decision. While we find that multi-camera\nsetups provide some robustness towards past attack methods, we see that this\nadvantage reduces when optimizing on multiple perspectives at once. We propose\na novel attack method that we call Transcender-MC, where we incorporate online\n3D renderings and perspective projections in the training process. Moreover, we\nmotivate that certain data augmentation techniques can facilitate the\ngeneration of successful adversarial examples even further. Transcender-MC is\n11% more effective in successfully attacking multi-camera setups than\nstate-of-the-art methods. Our findings offer valuable insights regarding the\nresilience of object detection in a setup with multiple cameras and motivate\nthe need of developing adequate defense mechanisms against them.","terms":["cs.CV","cs.CR","cs.LG"]},{"titles":"Towards Open-Ended Visual Recognition with Large Language Model","summaries":"Localizing and recognizing objects in the open-ended physical world poses a\nlong-standing challenge within the domain of machine perception. Recent methods\nhave endeavored to address the issue by employing a class-agnostic mask (or\nbox) proposal model, complemented by an open-vocabulary classifier (e.g., CLIP)\nusing pre-extracted text embeddings. However, it is worth noting that these\nopen-vocabulary recognition models still exhibit limitations in practical\napplications. On one hand, they rely on the provision of class names during\ntesting, where the recognition performance heavily depends on this predefined\nset of semantic classes by users. On the other hand, when training with\nmultiple datasets, human intervention is required to alleviate the label\ndefinition conflict between them. In this paper, we introduce the OmniScient\nModel (OSM), a novel Large Language Model (LLM) based mask classifier, as a\nstraightforward and effective solution to the aforementioned challenges.\nSpecifically, OSM predicts class labels in a generative manner, thus removing\nthe supply of class names during both training and testing. It also enables\ncross-dataset training without any human interference, exhibiting robust\ngeneralization capabilities due to the world knowledge acquired from the LLM.\nBy combining OSM with an off-the-shelf mask proposal model, we present\npromising results on various benchmarks, and demonstrate its effectiveness in\nhandling novel concepts. Code\/model are available at\nhttps:\/\/github.com\/bytedance\/OmniScient-Model.","terms":["cs.CV"]},{"titles":"Generalized partitioned local depth","summaries":"In this paper we provide a generalization of the concept of cohesion as\nintroduced recently by Berenhaut, Moore and Melvin [Proceedings of the National\nAcademy of Sciences, 119 (4) (2022)]. The formulation presented builds on the\ntechnique of partitioned local depth by distilling two key probabilistic\nconcepts: local relevance and support division. Earlier results are extended\nwithin the new context, and examples of applications to revealing communities\nin data with uncertainty are included. The work sheds light on the foundations\nof partitioned local depth, and extends the original ideas to enable\nprobabilistic consideration of uncertain, variable and potentially conflicting\ninformation.","terms":["stat.ML","cs.LG","cs.SI","physics.soc-ph"]},{"titles":"Efficient Surrogate Models for Materials Science Simulations: Machine Learning-based Prediction of Microstructure Properties","summaries":"Determining, understanding, and predicting the so-called structure-property\nrelation is an important task in many scientific disciplines, such as\nchemistry, biology, meteorology, physics, engineering, and materials science.\nStructure refers to the spatial distribution of, e.g., substances, material, or\nmatter in general, while property is a resulting characteristic that usually\ndepends in a non-trivial way on spatial details of the structure.\nTraditionally, forward simulations models have been used for such tasks.\nRecently, several machine learning algorithms have been applied in these\nscientific fields to enhance and accelerate simulation models or as surrogate\nmodels. In this work, we develop and investigate the applications of six\nmachine learning techniques based on two different datasets from the domain of\nmaterials science: data from a two-dimensional Ising model for predicting the\nformation of magnetic domains and data representing the evolution of dual-phase\nmicrostructures from the Cahn-Hilliard model. We analyze the accuracy and\nrobustness of all models and elucidate the reasons for the differences in their\nperformances. The impact of including domain knowledge through tailored\nfeatures is studied, and general recommendations based on the availability and\nquality of training data are derived from this.","terms":["cs.LG","cond-mat.mtrl-sci","cs.CV"]},{"titles":"Data-driven building energy efficiency prediction based on envelope heat losses using physics-informed neural networks","summaries":"The analytical prediction of building energy performance in residential\nbuildings based on the heat losses of its individual envelope components is a\nchallenging task. It is worth noting that this field is still in its infancy,\nwith relatively limited research conducted in this specific area to date,\nespecially when it comes for data-driven approaches. In this paper we introduce\na novel physics-informed neural network model for addressing this problem.\nThrough the employment of unexposed datasets that encompass general building\ninformation, audited characteristics, and heating energy consumption, we feed\nthe deep learning model with general building information, while the model's\noutput consists of the structural components and several thermal properties\nthat are in fact the basic elements of an energy performance certificate (EPC).\nOn top of this neural network, a function, based on physics equations,\ncalculates the energy consumption of the building based on heat losses and\nenhances the loss function of the deep learning model. This methodology is\ntested on a real case study for 256 buildings located in Riga, Latvia. Our\ninvestigation comes up with promising results in terms of prediction accuracy,\npaving the way for automated, and data-driven energy efficiency performance\nprediction based on basic properties of the building, contrary to exhaustive\nenergy efficiency audits led by humans, which are the current status quo.","terms":["cs.LG","cs.AI","cs.CE"]},{"titles":"Benchmarking Individual Tree Mapping with Sub-meter Imagery","summaries":"There is a rising interest in mapping trees using satellite or aerial\nimagery, but there is no standardized evaluation protocol for comparing and\nenhancing methods. In dense canopy areas, the high variability of tree sizes\nand their spatial proximity makes it arduous to define the quality of the\npredictions. Concurrently, object-centric approaches such as bounding box\ndetection usuallyperform poorly on small and dense objects. It thus remains\nunclear what is the ideal framework for individual tree mapping, in regards to\ndetection and segmentation approaches, convolutional neural networks and\ntransformers. In this paper, we introduce an evaluation framework suited for\nindividual tree mapping in any physical environment, with annotation costs and\napplicative goals in mind. We review and compare different approaches and deep\narchitectures, and introduce a new method that we experimentally prove to be a\ngood compromise between segmentation and detection.","terms":["cs.CV"]},{"titles":"Analyzing Transformer Dynamics as Movement through Embedding Space","summaries":"Transformer based language models exhibit intelligent behaviors such as\nunderstanding natural language, recognizing patterns, acquiring knowledge,\nreasoning, planning, reflecting and using tools. This paper explores how their\nunderlying mechanics give rise to intelligent behaviors. Towards that end, we\npropose framing Transformer dynamics as movement through embedding space.\nExamining Transformers through this perspective reveals key insights,\nestablishing a Theory of Transformers: 1) Intelligent behaviours map to paths\nin Embedding Space which, the Transformer random-walks through during\ninferencing. 2) LM training learns a probability distribution over all possible\npaths. `Intelligence' is learnt by assigning higher probabilities to paths\nrepresenting intelligent behaviors. No learning can take place in-context;\ncontext only narrows the subset of paths sampled during decoding. 5) The\nTransformer is a self-mapping composition function, folding a context sequence\ninto a context-vector such that it's proximity to a token-vector reflects its\nco-occurrence and conditioned probability. Thus, the physical arrangement of\nvectors in Embedding Space determines path probabilities. 6) Context vectors\nare composed by aggregating features of the sequence's tokens via a process we\ncall the encoding walk. Attention contributes a - potentially redundant -\nassociation-bias to this process. 7) This process is comprised of two principal\noperation types: filtering (data independent) and aggregation (data dependent).\nThis generalization unifies Transformers with other sequence models. Building\nupon this foundation, we formalize a popular semantic interpretation of\nembeddings into a ``concept-space theory'' and find some evidence of it's\nvalidity.","terms":["cs.LG","cs.AI","cs.CL","cs.NE"]},{"titles":"Statistical Parameterized Physics-Based Machine Learning Digital Twin Models for Laser Powder Bed Fusion Process","summaries":"A digital twin (DT) is a virtual representation of physical process, products\nand\/or systems that requires a high-fidelity computational model for continuous\nupdate through the integration of sensor data and user input. In the context of\nlaser powder bed fusion (LPBF) additive manufacturing, a digital twin of the\nmanufacturing process can offer predictions for the produced parts, diagnostics\nfor manufacturing defects, as well as control capabilities. This paper\nintroduces a parameterized physics-based digital twin (PPB-DT) for the\nstatistical predictions of LPBF metal additive manufacturing process. We\naccomplish this by creating a high-fidelity computational model that accurately\nrepresents the melt pool phenomena and subsequently calibrating and validating\nit through controlled experiments. In PPB-DT, a mechanistic reduced-order\nmethod-driven stochastic calibration process is introduced, which enables the\nstatistical predictions of the melt pool geometries and the identification of\ndefects such as lack-of-fusion porosity and surface roughness, specifically for\ndiagnostic applications. Leveraging data derived from this physics-based model\nand experiments, we have trained a machine learning-based digital twin\n(PPB-ML-DT) model for predicting, monitoring, and controlling melt pool\ngeometries. These proposed digital twin models can be employed for predictions,\ncontrol, optimization, and quality assurance within the LPBF process,\nultimately expediting product development and certification in LPBF-based metal\nadditive manufacturing.","terms":["cs.LG","cs.CE","cs.NA","math.NA","physics.data-an"]},{"titles":"Probabilistic Physics-integrated Neural Differentiable Modeling for Isothermal Chemical Vapor Infiltration Process","summaries":"Chemical vapor infiltration (CVI) is a widely adopted manufacturing technique\nused in producing carbon-carbon and carbon-silicon carbide composites. These\nmaterials are especially valued in the aerospace and automotive industries for\ntheir robust strength and lightweight characteristics. The densification\nprocess during CVI critically influences the final performance, quality, and\nconsistency of these composite materials. Experimentally optimizing the CVI\nprocesses is challenging due to long experimental time and large optimization\nspace. To address these challenges, this work takes a modeling-centric\napproach. Due to the complexities and limited experimental data of the\nisothermal CVI densification process, we have developed a data-driven\npredictive model using the physics-integrated neural differentiable (PiNDiff)\nmodeling framework. An uncertainty quantification feature has been embedded\nwithin the PiNDiff method, bolstering the model's reliability and robustness.\nThrough comprehensive numerical experiments involving both synthetic and\nreal-world manufacturing data, the proposed method showcases its capability in\nmodeling densification during the CVI process. This research highlights the\npotential of the PiNDiff framework as an instrumental tool for advancing our\nunderstanding, simulation, and optimization of the CVI manufacturing process,\nparticularly when faced with sparse data and an incomplete description of the\nunderlying physics.","terms":["cs.LG"]},{"titles":"Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of Urban Areas","summaries":"Urban flood risk emerges from complex and nonlinear interactions among\nmultiple features related to flood hazard, flood exposure, and social and\nphysical vulnerabilities, along with the complex spatial flood dependence\nrelationships. Existing approaches for characterizing urban flood risk,\nhowever, are primarily based on flood plain maps, focusing on a limited number\nof features, primarily hazard and exposure features, without consideration of\nfeature interactions or the dependence relationships among spatial areas. To\naddress this gap, this study presents an integrated urban flood-risk rating\nmodel based on a novel unsupervised graph deep learning model (called\nFloodRisk-Net). FloodRisk-Net is capable of capturing spatial dependence among\nareas and complex and nonlinear interactions among flood hazards and urban\nfeatures for specifying emergent flood risk. Using data from multiple\nmetropolitan statistical areas (MSAs) in the United States, the model\ncharacterizes their flood risk into six distinct city-specific levels. The\nmodel is interpretable and enables feature analysis of areas within each\nflood-risk level, allowing for the identification of the three archetypes\nshaping the highest flood risk within each MSA. Flood risk is found to be\nspatially distributed in a hierarchical structure within each MSA, where the\ncore city disproportionately bears the highest flood risk. Multiple cities are\nfound to have high overall flood-risk levels and low spatial inequality,\nindicating limited options for balancing urban development and flood-risk\nreduction. Relevant flood-risk reduction strategies are discussed considering\nways that the highest flood risk and uneven spatial distribution of flood risk\nare formed.","terms":["cs.LG","cs.AI","cs.CY","stat.AP","I.2.1 Applications and Expert Systems"]},{"titles":"Interpretable Fine-Tuning for Graph Neural Network Surrogate Models","summaries":"Data-based surrogate modeling has surged in capability in recent years with\nthe emergence of graph neural networks (GNNs), which can operate directly on\nmesh-based representations of data. The goal of this work is to introduce an\ninterpretable fine-tuning strategy for GNNs, with application to unstructured\nmesh-based fluid dynamics modeling. The end result is a fine-tuned GNN that\nadds interpretability to a pre-trained baseline GNN through an adaptive\nsub-graph sampling strategy that isolates regions in physical space\nintrinsically linked to the forecasting task, while retaining the predictive\ncapability of the baseline. The structures identified by the fine-tuned GNNs,\nwhich are adaptively produced in the forward pass as explicit functions of the\ninput, serve as an accessible link between the baseline model architecture, the\noptimization goal, and known problem-specific physics. Additionally, through a\nregularization procedure, the fine-tuned GNNs can also be used to identify,\nduring inference, graph nodes that correspond to a majority of the anticipated\nforecasting error, adding a novel interpretable error-tagging capability to\nbaseline models. Demonstrations are performed using unstructured flow data\nsourced from flow over a backward-facing step at high Reynolds numbers.","terms":["cs.LG","physics.comp-ph","physics.flu-dyn"]},{"titles":"Machine learning for uncertainty estimation in fusing precipitation observations from satellites and ground-based gauges","summaries":"To form precipitation datasets that are accurate and, at the same time, have\nhigh spatial densities, data from satellites and gauges are often merged in the\nliterature. However, uncertainty estimates for the data acquired in this manner\nare scarcely provided, although the importance of uncertainty quantification in\npredictive modelling is widely recognized. Furthermore, the benefits that\nmachine learning can bring to the task of providing such estimates have not\nbeen broadly realized and properly explored through benchmark experiments. The\npresent study aims at filling in this specific gap by conducting the first\nbenchmark tests on the topic. On a large dataset that comprises 15-year-long\nmonthly data spanning across the contiguous United States, we extensively\ncompared six learners that are, by their construction, appropriate for\npredictive uncertainty quantification. These are the quantile regression (QR),\nquantile regression forests (QRF), generalized random forests (GRF), gradient\nboosting machines (GBM), light gradient boosting machines (LightGBM) and\nquantile regression neural networks (QRNN). The comparison referred to the\ncompetence of the learners in issuing predictive quantiles at nine levels that\nfacilitate a good approximation of the entire predictive probability\ndistribution, and was primarily based on the quantile and continuous ranked\nprobability skill scores. Three types of predictor variables (i.e., satellite\nprecipitation variables, distances between a point of interest and satellite\ngrid points, and elevation at a point of interest) were used in the comparison\nand were additionally compared with each other. This additional comparison was\nbased on the explainable machine learning concept of feature importance. The\nresults suggest that the order from the best to the worst of the learners for\nthe task investigated is the following: LightGBM, QRF, GRF, GBM, QRNN and QR...","terms":["stat.ML","cs.LG","physics.ao-ph","stat.AP","stat.ME"]},{"titles":"Simulator-Based Inference with Waldo: Confidence Regions by Leveraging Prediction Algorithms and Posterior Estimators for Inverse Problems","summaries":"Prediction algorithms, such as deep neural networks (DNNs), are used in many\ndomain sciences to directly estimate internal parameters of interest in\nsimulator-based models, especially in settings where the observations include\nimages or complex high-dimensional data. In parallel, modern neural density\nestimators, such as normalizing flows, are becoming increasingly popular for\nuncertainty quantification, especially when both parameters and observations\nare high-dimensional. However, parameter inference is an inverse problem and\nnot a prediction task; thus, an open challenge is to construct conditionally\nvalid and precise confidence regions, with a guaranteed probability of covering\nthe true parameters of the data-generating process, no matter what the\n(unknown) parameter values are, and without relying on large-sample theory.\nMany simulator-based inference (SBI) methods are indeed known to produce biased\nor overly confident parameter regions, yielding misleading uncertainty\nestimates. This paper presents WALDO, a novel method to construct confidence\nregions with finite-sample conditional validity by leveraging prediction\nalgorithms or posterior estimators that are currently widely adopted in SBI.\nWALDO reframes the well-known Wald test statistic, and uses a computationally\nefficient regression-based machinery for classical Neyman inversion of\nhypothesis tests. We apply our method to a recent high-energy physics problem,\nwhere prediction with DNNs has previously led to estimates with prediction\nbias. We also illustrate how our approach can correct overly confident\nposterior regions computed with normalizing flows.","terms":["stat.ML","cs.LG"]},{"titles":"CycleCL: Self-supervised Learning for Periodic Videos","summaries":"Analyzing periodic video sequences is a key topic in applications such as\nautomatic production systems, remote sensing, medical applications, or physical\ntraining. An example is counting repetitions of a physical exercise. Due to the\ndistinct characteristics of periodic data, self-supervised methods designed for\nstandard image datasets do not capture changes relevant to the progression of\nthe cycle and fail to ignore unrelated noise. They thus do not work well on\nperiodic data. In this paper, we propose CycleCL, a self-supervised learning\nmethod specifically designed to work with periodic data. We start from the\ninsight that a good visual representation for periodic data should be sensitive\nto the phase of a cycle, but be invariant to the exact repetition, i.e. it\nshould generate identical representations for a specific phase throughout all\nrepetitions. We exploit the repetitions in videos to design a novel contrastive\nlearning method based on a triplet loss that optimizes for these desired\nproperties. Our method uses pre-trained features to sample pairs of frames from\napproximately the same phase and negative pairs of frames from different\nphases. Then, we iterate between optimizing a feature encoder and resampling\ntriplets, until convergence. By optimizing a model this way, we are able to\nlearn features that have the mentioned desired properties. We evaluate CycleCL\non an industrial and multiple human actions datasets, where it significantly\noutperforms previous video-based self-supervised learning methods on all tasks.","terms":["cs.CV"]},{"titles":"NDDepth: Normal-Distance Assisted Monocular Depth Estimation and Completion","summaries":"Over the past few years, monocular depth estimation and completion have been\npaid more and more attention from the computer vision community because of\ntheir widespread applications. In this paper, we introduce novel physics\n(geometry)-driven deep learning frameworks for these two tasks by assuming that\n3D scenes are constituted with piece-wise planes. Instead of directly\nestimating the depth map or completing the sparse depth map, we propose to\nestimate the surface normal and plane-to-origin distance maps or complete the\nsparse surface normal and distance maps as intermediate outputs. To this end,\nwe develop a normal-distance head that outputs pixel-level surface normal and\ndistance. Meanwhile, the surface normal and distance maps are regularized by a\ndeveloped plane-aware consistency constraint, which are then transformed into\ndepth maps. Furthermore, we integrate an additional depth head to strengthen\nthe robustness of the proposed frameworks. Extensive experiments on the\nNYU-Depth-v2, KITTI and SUN RGB-D datasets demonstrate that our method exceeds\nin performance prior state-of-the-art monocular depth estimation and completion\ncompetitors. The source code will be available at\nhttps:\/\/github.com\/ShuweiShao\/NDDepth.","terms":["cs.CV"]},{"titles":"Bias-inducing geometries: an exactly solvable data model with fairness implications","summaries":"Machine learning (ML) may be oblivious to human bias but it is not immune to\nits perpetuation. Marginalisation and iniquitous group representation are often\ntraceable in the very data used for training, and may be reflected or even\nenhanced by the learning models. In the present work, we aim at clarifying the\nrole played by data geometry in the emergence of ML bias. We introduce an\nexactly solvable high-dimensional model of data imbalance, where parametric\ncontrol over the many bias-inducing factors allows for an extensive exploration\nof the bias inheritance mechanism. Through the tools of statistical physics, we\nanalytically characterise the typical properties of learning models trained in\nthis synthetic framework and obtain exact predictions for the observables that\nare commonly employed for fairness assessment. Despite the simplicity of the\ndata model, we retrace and unpack typical unfairness behaviour observed on\nreal-world datasets. We also obtain a detailed analytical characterisation of a\nclass of bias mitigation strategies. We first consider a basic loss-reweighing\nscheme, which allows for an implicit minimisation of different unfairness\nmetrics, and quantify the incompatibilities between some existing fairness\ncriteria. Then, we consider a novel mitigation strategy based on a matched\ninference approach, consisting in the introduction of coupled learning models.\nOur theoretical analysis of this approach shows that the coupled strategy can\nstrike superior fairness-accuracy trade-offs.","terms":["cs.LG","cond-mat.dis-nn","stat.ML"]},{"titles":"Reversible and irreversible bracket-based dynamics for deep graph neural networks","summaries":"Recent works have shown that physics-inspired architectures allow the\ntraining of deep graph neural networks (GNNs) without oversmoothing. The role\nof these physics is unclear, however, with successful examples of both\nreversible (e.g., Hamiltonian) and irreversible (e.g., diffusion) phenomena\nproducing comparable results despite diametrically opposed mechanisms, and\nfurther complications arising due to empirical departures from mathematical\ntheory. This work presents a series of novel GNN architectures based upon\nstructure-preserving bracket-based dynamical systems, which are provably\nguaranteed to either conserve energy or generate positive dissipation with\nincreasing depth. It is shown that the theoretically principled framework\nemployed here allows for inherently explainable constructions, which\ncontextualize departures from theory in current architectures and better\nelucidate the roles of reversibility and irreversibility in network\nperformance.","terms":["cs.LG"]},{"titles":"Non-approximability of constructive global $\\mathcal{L}^2$ minimizers by gradient descent in Deep Learning","summaries":"We analyze geometric aspects of the gradient descent algorithm in Deep\nLearning (DL) networks. In particular, we prove that the globally minimizing\nweights and biases for the $\\mathcal{L}^2$ cost obtained constructively in\n[Chen-Munoz Ewald 2023] for underparametrized ReLU DL networks can generically\nnot be approximated via the gradient descent flow. We therefore conclude that\nthe method introduced in [Chen-Munoz Ewald 2023] is disjoint from the gradient\ndescent method.","terms":["cs.LG","cs.AI","math-ph","math.MP","math.OC","stat.ML","57R70, 62M45"]},{"titles":"PICS in Pics: Physics Informed Contour Selection for Rapid Image Segmentation","summaries":"Effective training of deep image segmentation models is challenging due to\nthe need for abundant, high-quality annotations. Generating annotations is\nlaborious and time-consuming for human experts, especially in medical image\nsegmentation. To facilitate image annotation, we introduce Physics Informed\nContour Selection (PICS) - an interpretable, physics-informed algorithm for\nrapid image segmentation without relying on labeled data. PICS draws\ninspiration from physics-informed neural networks (PINNs) and an active contour\nmodel called snake. It is fast and computationally lightweight because it\nemploys cubic splines instead of a deep neural network as a basis function. Its\ntraining parameters are physically interpretable because they directly\nrepresent control knots of the segmentation curve. Traditional snakes involve\nminimization of the edge-based loss functionals by deriving the Euler-Lagrange\nequation followed by its numerical solution. However, PICS directly minimizes\nthe loss functional, bypassing the Euler Lagrange equations. It is the first\nsnake variant to minimize a region-based loss function instead of traditional\nedge-based loss functions. PICS uniquely models the three-dimensional (3D)\nsegmentation process with an unsteady partial differential equation (PDE),\nwhich allows accelerated segmentation via transfer learning. To demonstrate its\neffectiveness, we apply PICS for 3D segmentation of the left ventricle on a\npublicly available cardiac dataset. While doing so, we also introduce a new\nconvexity-preserving loss term that encodes the shape information of the left\nventricle to enhance PICS's segmentation quality. Overall, PICS presents\nseveral novelties in network architecture, transfer learning, and\nphysics-inspired losses for image segmentation, thereby showing promising\noutcomes and potential for further refinement.","terms":["cs.CV","cs.LG"]},{"titles":"Physics-Informed Data Denoising for Real-Life Sensing Systems","summaries":"Sensors measuring real-life physical processes are ubiquitous in today's\ninterconnected world. These sensors inherently bear noise that often adversely\naffects performance and reliability of the systems they support. Classic\nfiltering-based approaches introduce strong assumptions on the time or\nfrequency characteristics of sensory measurements, while learning-based\ndenoising approaches typically rely on using ground truth clean data to train a\ndenoising model, which is often challenging or prohibitive to obtain for many\nreal-world applications. We observe that in many scenarios, the relationships\nbetween different sensor measurements (e.g., location and acceleration) are\nanalytically described by laws of physics (e.g., second-order differential\nequation). By incorporating such physics constraints, we can guide the\ndenoising process to improve even in the absence of ground truth data. In light\nof this, we design a physics-informed denoising model that leverages the\ninherent algebraic relationships between different measurements governed by the\nunderlying physics. By obviating the need for ground truth clean data, our\nmethod offers a practical denoising solution for real-world applications. We\nconducted experiments in various domains, including inertial navigation, CO2\nmonitoring, and HVAC control, and achieved state-of-the-art performance\ncompared with existing denoising methods. Our method can denoise data in real\ntime (4ms for a sequence of 1s) for low-cost noisy sensors and produces results\nthat closely align with those from high-precision, high-cost alternatives,\nleading to an efficient, cost-effective approach for more accurate sensor-based\nsystems.","terms":["cs.LG","cs.AI","eess.SP","stat.ML"]},{"titles":"Tackling the Curse of Dimensionality with Physics-Informed Neural Networks","summaries":"The curse-of-dimensionality taxes computational resources heavily with\nexponentially increasing computational cost as the dimension increases. This\nposes great challenges in solving high-dimensional PDEs, as Richard E. Bellman\nfirst pointed out over 60 years ago. While there has been some recent success\nin solving numerically partial differential equations (PDEs) in high\ndimensions, such computations are prohibitively expensive, and true scaling of\ngeneral nonlinear PDEs to high dimensions has never been achieved. We develop a\nnew method of scaling up physics-informed neural networks (PINNs) to solve\narbitrary high-dimensional PDEs. The new method, called Stochastic Dimension\nGradient Descent (SDGD), decomposes a gradient of PDEs into pieces\ncorresponding to different dimensions and randomly samples a subset of these\ndimensional pieces in each iteration of training PINNs. We prove theoretically\nthe convergence and other desired properties of the proposed method. We\ndemonstrate in various diverse tests that the proposed method can solve many\nnotoriously hard high-dimensional PDEs, including the Hamilton-Jacobi-Bellman\n(HJB) and the Schr\\\"{o}dinger equations in tens of thousands of dimensions very\nfast on a single GPU using the PINNs mesh-free approach. Notably, we solve\nnonlinear PDEs with nontrivial, anisotropic, and inseparable solutions in\n100,000 effective dimensions in 12 hours on a single GPU using SDGD with PINNs.\nSince SDGD is a general training methodology of PINNs, it can be applied to any\ncurrent and future variants of PINNs to scale them up for arbitrary\nhigh-dimensional PDEs.","terms":["cs.LG","cs.AI","cs.NA","math.DS","math.NA","stat.ML","14J60","F.2.2; I.2.7"]},{"titles":"Dream to Adapt: Meta Reinforcement Learning by Latent Context Imagination and MDP Imagination","summaries":"Meta reinforcement learning (Meta RL) has been amply explored to quickly\nlearn an unseen task by transferring previously learned knowledge from similar\ntasks. However, most state-of-the-art algorithms require the meta-training\ntasks to have a dense coverage on the task distribution and a great amount of\ndata for each of them. In this paper, we propose MetaDreamer, a context-based\nMeta RL algorithm that requires less real training tasks and data by doing\nmeta-imagination and MDP-imagination. We perform meta-imagination by\ninterpolating on the learned latent context space with disentangled properties,\nas well as MDP-imagination through the generative world model where physical\nknowledge is added to plain VAE networks. Our experiments with various\nbenchmarks show that MetaDreamer outperforms existing approaches in data\nefficiency and interpolated generalization.","terms":["cs.LG","cs.AI","cs.RO"]},{"titles":"VT-Former: A Transformer-based Vehicle Trajectory Prediction Approach For Intelligent Highway Transportation Systems","summaries":"Enhancing roadway safety and traffic management has become an essential focus\narea for a broad range of modern cyber-physical systems and intelligent\ntransportation systems. Vehicle Trajectory Prediction is a pivotal element\nwithin numerous applications for highway and road safety. These applications\nencompass a wide range of use cases, spanning from traffic management and\naccident prevention to enhancing work-zone safety and optimizing energy\nconservation. The ability to implement intelligent management in this context\nhas been greatly advanced by the developments in the field of Artificial\nIntelligence (AI), alongside the increasing deployment of surveillance cameras\nacross road networks. In this paper, we introduce a novel transformer-based\napproach for vehicle trajectory prediction for highway safety and surveillance,\ndenoted as VT-Former. In addition to utilizing transformers to capture\nlong-range temporal patterns, a new Graph Attentive Tokenization (GAT) module\nhas been proposed to capture intricate social interactions among vehicles.\nCombining these two core components culminates in a precise approach for\nvehicle trajectory prediction. Our study on three benchmark datasets with three\ndifferent viewpoints demonstrates the State-of-The-Art (SoTA) performance of\nVT-Former in vehicle trajectory prediction and its generalizability and\nrobustness. We also evaluate VT-Former's efficiency on embedded boards and\nexplore its potential for vehicle anomaly detection as a sample application,\nshowcasing its broad applicability.","terms":["cs.CV","cs.AI"]},{"titles":"Computer Vision for Particle Size Analysis of Coarse-Grained Soils","summaries":"Particle size analysis (PSA) is a fundamental technique for evaluating the\nphysical characteristics of soils. However, traditional methods like sieving\ncan be time-consuming and labor-intensive. In this study, we present a novel\napproach that utilizes computer vision (CV) and the Python programming language\nfor PSA of coarse-grained soils, employing a standard mobile phone camera. By\neliminating the need for a high-performance camera, our method offers\nconvenience and cost savings. Our methodology involves using the OPENCV library\nto detect and measure soil particles in digital photographs taken under\nordinary lighting conditions. For accurate particle size determination, a\ncalibration target with known dimensions is placed on a plain paper alongside\n20 different sand samples. The proposed method is compared with traditional\nsieve analysis and exhibits satisfactory performance for soil particles larger\nthan 2 mm, with a mean absolute percent error (MAPE) of approximately 6%.\nHowever, particles smaller than 2 mm result in higher MAPE, reaching up to 60%.\nTo address this limitation, we recommend using a higher-resolution camera to\ncapture images of the smaller soil particles. Furthermore, we discuss the\nadvantages, limitations, and potential future improvements of our method.\nRemarkably, the program can be executed on a mobile phone, providing immediate\nresults without the need to send soil samples to a laboratory. This\nfield-friendly feature makes our approach highly convenient for on-site usage,\noutside of a traditional laboratory setting. Ultimately, this novel method\nrepresents an initial disruption to the industry, enabling efficient particle\nsize analysis of soil without the reliance on laboratory-based sieve analysis.\nKEYWORDS: Computer vision, Grain size, ARUCO","terms":["cs.CV","cs.LG"]},{"titles":"Monitoring and Adapting the Physical State of a Camera for Autonomous Vehicles","summaries":"Autonomous vehicles and robots require increasingly more robustness and\nreliability to meet the demands of modern tasks. These requirements specially\napply to cameras onboard such vehicles because they are the predominant sensors\nto acquire information about the environment and support actions. Cameras must\nmaintain proper functionality and take automatic countermeasures if necessary.\nExisting solutions are typically tailored to specific problems or detached from\nthe downstream computer vision tasks of the machines, which, however, determine\nthe requirements on the quality of the produced camera images. We propose a\ngeneric and task-oriented self-health-maintenance framework for cameras based\non data- and physically-grounded models. To this end, we determine two\nreliable, real-time capable estimators for typical image effects of a camera in\npoor condition (blur, noise phenomena and most common combinations) by\nevaluating traditional and customized machine learning-based approaches in\nextensive experiments. Furthermore, we implement the framework on a real-world\nground vehicle and demonstrate how a camera can adjust its parameters to\ncounter an identified poor condition to achieve optimal application capability\nbased on experimental (non-linear and non-monotonic) input-output performance\ncurves. Object detection is chosen as target application, and the image effects\nmotion blur and sensor noise as conditioning examples. Our framework not only\nprovides a practical ready-to-use solution to monitor and maintain the health\nof cameras, but can also serve as a basis for extensions to tackle more\nsophisticated problems that combine additional data sources (e.g., sensor or\nenvironment parameters) empirically in order to attain fully reliable and\nrobust machines. Code:\nhttps:\/\/github.com\/MaikWischow\/Camera-Condition-Monitoring","terms":["cs.CV","eess.IV"]},{"titles":"Graph ODE with Factorized Prototypes for Modeling Complicated Interacting Dynamics","summaries":"This paper studies the problem of modeling interacting dynamical systems,\nwhich is critical for understanding physical dynamics and biological processes.\nRecent research predominantly uses geometric graphs to represent these\ninteractions, which are then captured by powerful graph neural networks (GNNs).\nHowever, predicting interacting dynamics in challenging scenarios such as\nout-of-distribution shift and complicated underlying rules remains unsolved. In\nthis paper, we propose a new approach named Graph ODE with factorized\nprototypes (GOAT) to address the problem. The core of GOAT is to incorporate\nfactorized prototypes from contextual knowledge into a continuous graph ODE\nframework. Specifically, GOAT employs representation disentanglement and system\nparameters to extract both object-level and system-level contexts from\nhistorical trajectories, which allows us to explicitly model their independent\ninfluence and thus enhances the generalization capability under system changes.\nThen, we integrate these disentangled latent representations into a graph ODE\nmodel, which determines a combination of various interacting prototypes for\nenhanced model expressivity. The entire model is optimized using an end-to-end\nvariational inference framework to maximize the likelihood. Extensive\nexperiments in both in-distribution and out-of-distribution settings validate\nthe superiority of GOAT.","terms":["cs.LG"]},{"titles":"TURBO: The Swiss Knife of Auto-Encoders","summaries":"We present a novel information-theoretic framework, termed as TURBO, designed\nto systematically analyse and generalise auto-encoding methods. We start by\nexamining the principles of information bottleneck and bottleneck-based\nnetworks in the auto-encoding setting and identifying their inherent\nlimitations, which become more prominent for data with multiple relevant,\nphysics-related representations. The TURBO framework is then introduced,\nproviding a comprehensive derivation of its core concept consisting of the\nmaximisation of mutual information between various data representations\nexpressed in two directions reflecting the information flows. We illustrate\nthat numerous prevalent neural network models are encompassed within this\nframework. The paper underscores the insufficiency of the information\nbottleneck concept in elucidating all such models, thereby establishing TURBO\nas a preferable theoretical reference. The introduction of TURBO contributes to\na richer understanding of data representation and the structure of neural\nnetwork models, enabling more efficient and versatile applications.","terms":["cs.LG","cs.CR","cs.IT","hep-ph","math.IT"]},{"titles":"Polarimetric PatchMatch Multi-View Stereo","summaries":"PatchMatch Multi-View Stereo (PatchMatch MVS) is one of the popular MVS\napproaches, owing to its balanced accuracy and efficiency. In this paper, we\npropose Polarimetric PatchMatch multi-view Stereo (PolarPMS), which is the\nfirst method exploiting polarization cues to PatchMatch MVS. The key of\nPatchMatch MVS is to generate depth and normal hypotheses, which form local 3D\nplanes and slanted stereo matching windows, and efficiently search for the best\nhypothesis based on the consistency among multi-view images. In addition to\nstandard photometric consistency, our PolarPMS evaluates polarimetric\nconsistency to assess the validness of a depth and normal hypothesis, motivated\nby the physical property that the polarimetric information is related to the\nobject's surface normal. Experimental results demonstrate that our PolarPMS can\nimprove the accuracy and the completeness of reconstructed 3D models,\nespecially for texture-less surfaces, compared with state-of-the-art PatchMatch\nMVS methods.","terms":["cs.CV"]},{"titles":"Tipping Points of Evolving Epidemiological Networks: Machine Learning-Assisted, Data-Driven Effective Modeling","summaries":"We study the tipping point collective dynamics of an adaptive\nsusceptible-infected-susceptible (SIS) epidemiological network in a\ndata-driven, machine learning-assisted manner. We identify a\nparameter-dependent effective stochastic differential equation (eSDE) in terms\nof physically meaningful coarse mean-field variables through a deep-learning\nResNet architecture inspired by numerical stochastic integrators. We construct\nan approximate effective bifurcation diagram based on the identified drift term\nof the eSDE and contrast it with the mean-field SIS model bifurcation diagram.\nWe observe a subcritical Hopf bifurcation in the evolving network's effective\nSIS dynamics, that causes the tipping point behavior; this takes the form of\nlarge amplitude collective oscillations that spontaneously -- yet rarely --\narise from the neighborhood of a (noisy) stationary state. We study the\nstatistics of these rare events both through repeated brute force simulations\nand by using established mathematical\/computational tools exploiting the\nright-hand-side of the identified SDE. We demonstrate that such a collective\nSDE can also be identified (and the rare events computations also performed) in\nterms of data-driven coarse observables, obtained here via manifold learning\ntechniques, in particular Diffusion Maps. The workflow of our study is\nstraightforwardly applicable to other complex dynamics problems exhibiting\ntipping point dynamics.","terms":["cs.LG","cs.AI","math.DS","q-bio.PE"]},{"titles":"MultiIoT: Towards Large-scale Multisensory Learning for the Internet of Things","summaries":"The Internet of Things (IoT), the network integrating billions of smart\nphysical devices embedded with sensors, software, and communication\ntechnologies for the purpose of connecting and exchanging data with other\ndevices and systems, is a critical and rapidly expanding component of our\nmodern world. The IoT ecosystem provides a rich source of real-world modalities\nsuch as motion, thermal, geolocation, imaging, depth, sensors, video, and audio\nfor prediction tasks involving the pose, gaze, activities, and gestures of\nhumans as well as the touch, contact, pose, 3D of physical objects. Machine\nlearning presents a rich opportunity to automatically process IoT data at\nscale, enabling efficient inference for impact in understanding human\nwellbeing, controlling physical devices, and interconnecting smart cities. To\ndevelop machine learning technologies for IoT, this paper proposes MultiIoT,\nthe most expansive IoT benchmark to date, encompassing over 1.15 million\nsamples from 12 modalities and 8 tasks. MultiIoT introduces unique challenges\ninvolving (1) learning from many sensory modalities, (2) fine-grained\ninteractions across long temporal ranges, and (3) extreme heterogeneity due to\nunique structure and noise topologies in real-world sensors. We also release a\nset of strong modeling baselines, spanning modality and task-specific methods\nto multisensory and multitask models to encourage future research in\nmultisensory representation learning for IoT.","terms":["cs.LG","cs.AI","cs.CL","cs.CV","cs.MM"]},{"titles":"Enhancing Rock Image Segmentation in Digital Rock Physics: A Fusion of Generative AI and State-of-the-Art Neural Networks","summaries":"In digital rock physics, analysing microstructures from CT and SEM scans is\ncrucial for estimating properties like porosity and pore connectivity.\nTraditional segmentation methods like thresholding and CNNs often fall short in\naccurately detailing rock microstructures and are prone to noise. U-Net\nimproved segmentation accuracy but required many expert-annotated samples, a\nlaborious and error-prone process due to complex pore shapes. Our study\nemployed an advanced generative AI model, the diffusion model, to overcome\nthese limitations. This model generated a vast dataset of CT\/SEM and binary\nsegmentation pairs from a small initial dataset. We assessed the efficacy of\nthree neural networks: U-Net, Attention-U-net, and TransUNet, for segmenting\nthese enhanced images. The diffusion model proved to be an effective data\naugmentation technique, improving the generalization and robustness of deep\nlearning models. TransU-Net, incorporating Transformer structures, demonstrated\nsuperior segmentation accuracy and IoU metrics, outperforming both U-Net and\nAttention-U-net. Our research advances rock image segmentation by combining the\ndiffusion model with cutting-edge neural networks, reducing dependency on\nextensive expert data and boosting segmentation accuracy and robustness.\nTransU-Net sets a new standard in digital rock physics, paving the way for\nfuture geoscience and engineering breakthroughs.","terms":["cs.CV","eess.IV"]},{"titles":"Symbolic Regression as Feature Engineering Method for Machine and Deep Learning Regression Tasks","summaries":"In the realm of machine and deep learning regression tasks, the role of\neffective feature engineering (FE) is pivotal in enhancing model performance.\nTraditional approaches of FE often rely on domain expertise to manually design\nfeatures for machine learning models. In the context of deep learning models,\nthe FE is embedded in the neural network's architecture, making it hard for\ninterpretation. In this study, we propose to integrate symbolic regression (SR)\nas an FE process before a machine learning model to improve its performance. We\nshow, through extensive experimentation on synthetic and real-world\nphysics-related datasets, that the incorporation of SR-derived features\nsignificantly enhances the predictive capabilities of both machine and deep\nlearning regression models with 34-86% root mean square error (RMSE)\nimprovement in synthetic datasets and 4-11.5% improvement in real-world\ndatasets. In addition, as a realistic use-case, we show the proposed method\nimproves the machine learning performance in predicting superconducting\ncritical temperatures based on Eliashberg theory by more than 20% in terms of\nRMSE. These results outline the potential of SR as an FE component in\ndata-driven models.","terms":["cs.LG"]},{"titles":"Diffusion Shape Prior for Wrinkle-Accurate Cloth Registration","summaries":"Registering clothes from 4D scans with vertex-accurate correspondence is\nchallenging, yet important for dynamic appearance modeling and physics\nparameter estimation from real-world data. However, previous methods either\nrely on texture information, which is not always reliable, or achieve only\ncoarse-level alignment. In this work, we present a novel approach to enabling\naccurate surface registration of texture-less clothes with large deformation.\nOur key idea is to effectively leverage a shape prior learned from pre-captured\nclothing using diffusion models. We also propose a multi-stage guidance scheme\nbased on learned functional maps, which stabilizes registration for large-scale\ndeformation even when they vary significantly from training data. Using\nhigh-fidelity real captured clothes, our experiments show that the proposed\napproach based on diffusion models generalizes better than surface registration\nwith VAE or PCA-based priors, outperforming both optimization-based and\nlearning-based non-rigid registration methods for both interpolation and\nextrapolation tests.","terms":["cs.CV"]},{"titles":"EgoEnv: Human-centric environment representations from egocentric video","summaries":"First-person video highlights a camera-wearer's activities in the context of\ntheir persistent environment. However, current video understanding approaches\nreason over visual features from short video clips that are detached from the\nunderlying physical space and capture only what is immediately visible. To\nfacilitate human-centric environment understanding, we present an approach that\nlinks egocentric video and the environment by learning representations that are\npredictive of the camera-wearer's (potentially unseen) local surroundings. We\ntrain such models using videos from agents in simulated 3D environments where\nthe environment is fully observable, and test them on human-captured real-world\nvideos from unseen environments. On two human-centric video tasks, we show that\nmodels equipped with our environment-aware features consistently outperform\ntheir counterparts with traditional clip features. Moreover, despite being\ntrained exclusively on simulated videos, our approach successfully handles\nreal-world videos from HouseTours and Ego4D, and achieves state-of-the-art\nresults on the Ego4D NLQ challenge. Project page:\nhttps:\/\/vision.cs.utexas.edu\/projects\/ego-env\/","terms":["cs.CV"]},{"titles":"Diffusion-Generative Multi-Fidelity Learning for Physical Simulation","summaries":"Multi-fidelity surrogate learning is important for physical simulation\nrelated applications in that it avoids running numerical solvers from scratch,\nwhich is known to be costly, and it uses multi-fidelity examples for training\nand greatly reduces the cost of data collection. Despite the variety of\nexisting methods, they all build a model to map the input parameters outright\nto the solution output. Inspired by the recent breakthrough in generative\nmodels, we take an alternative view and consider the solution output as\ngenerated from random noises. We develop a diffusion-generative multi-fidelity\n(DGMF) learning method based on stochastic differential equations (SDE), where\nthe generation is a continuous denoising process. We propose a conditional\nscore model to control the solution generation by the input parameters and the\nfidelity. By conditioning on additional inputs (temporal or spacial variables),\nour model can efficiently learn and predict multi-dimensional solution arrays.\nOur method naturally unifies discrete and continuous fidelity modeling. The\nadvantage of our method in several typical applications shows a promising new\ndirection for multi-fidelity learning.","terms":["cs.LG"]},{"titles":"Reconstructing Objects in-the-wild for Realistic Sensor Simulation","summaries":"Reconstructing objects from real world data and rendering them at novel views\nis critical to bringing realism, diversity and scale to simulation for robotics\ntraining and testing. In this work, we present NeuSim, a novel approach that\nestimates accurate geometry and realistic appearance from sparse in-the-wild\ndata captured at distance and at limited viewpoints. Towards this goal, we\nrepresent the object surface as a neural signed distance function and leverage\nboth LiDAR and camera sensor data to reconstruct smooth and accurate geometry\nand normals. We model the object appearance with a robust physics-inspired\nreflectance representation effective for in-the-wild data. Our experiments show\nthat NeuSim has strong view synthesis performance on challenging scenarios with\nsparse training views. Furthermore, we showcase composing NeuSim assets into a\nvirtual world and generating realistic multi-sensor data for evaluating\nself-driving perception models.","terms":["cs.CV","cs.RO"]},{"titles":"Sorting Out Quantum Monte Carlo","summaries":"Molecular modeling at the quantum level requires choosing a parameterization\nof the wavefunction that both respects the required particle symmetries, and is\nscalable to systems of many particles. For the simulation of fermions, valid\nparameterizations must be antisymmetric with respect to the exchange of\nparticles. Typically, antisymmetry is enforced by leveraging the anti-symmetry\nof determinants with respect to the exchange of matrix rows, but this involves\ncomputing a full determinant each time the wavefunction is evaluated. Instead,\nwe introduce a new antisymmetrization layer derived from sorting, the\n$\\textit{sortlet}$, which scales as $O(N \\log N)$ with regards to the number of\nparticles -- in contrast to $O(N^3)$ for the determinant. We show numerically\nthat applying this anti-symmeterization layer on top of an attention based\nneural-network backbone yields a flexible wavefunction parameterization capable\nof reaching chemical accuracy when approximating the ground state of first-row\natoms and small molecules.","terms":["cs.LG","physics.chem-ph","physics.comp-ph"]},{"titles":"Training neural operators to preserve invariant measures of chaotic attractors","summaries":"Chaotic systems make long-horizon forecasts difficult because small\nperturbations in initial conditions cause trajectories to diverge at an\nexponential rate. In this setting, neural operators trained to minimize squared\nerror losses, while capable of accurate short-term forecasts, often fail to\nreproduce statistical or structural properties of the dynamics over longer time\nhorizons and can yield degenerate results. In this paper, we propose an\nalternative framework designed to preserve invariant measures of chaotic\nattractors that characterize the time-invariant statistical properties of the\ndynamics. Specifically, in the multi-environment setting (where each sample\ntrajectory is governed by slightly different dynamics), we consider two novel\napproaches to training with noisy data. First, we propose a loss based on the\noptimal transport distance between the observed dynamics and the neural\noperator outputs. This approach requires expert knowledge of the underlying\nphysics to determine what statistical features should be included in the\noptimal transport loss. Second, we show that a contrastive learning framework,\nwhich does not require any specialized prior knowledge, can preserve\nstatistical properties of the dynamics nearly as well as the optimal transport\napproach. On a variety of chaotic systems, our method is shown empirically to\npreserve invariant measures of chaotic attractors.","terms":["cs.LG","math.DS"]},{"titles":"GAN-generated Faces Detection: A Survey and New Perspectives","summaries":"Generative Adversarial Networks (GAN) have led to the generation of very\nrealistic face images, which have been used in fake social media accounts and\nother disinformation matters that can generate profound impacts. Therefore, the\ncorresponding GAN-face detection techniques are under active development that\ncan examine and expose such fake faces. In this work, we aim to provide a\ncomprehensive review of recent progress in GAN-face detection. We focus on\nmethods that can detect face images that are generated or synthesized from GAN\nmodels. We classify the existing detection works into four categories: (1) deep\nlearning-based, (2) physical-based, (3) physiological-based methods, and (4)\nevaluation and comparison against human visual performance. For each category,\nwe summarize the key ideas and connect them with method implementations. We\nalso discuss open problems and suggest future research directions.","terms":["cs.CV"]},{"titles":"Latent Task-Specific Graph Network Simulators","summaries":"Simulating dynamic physical interactions is a critical challenge across\nmultiple scientific domains, with applications ranging from robotics to\nmaterial science. For mesh-based simulations, Graph Network Simulators (GNSs)\npose an efficient alternative to traditional physics-based simulators. Their\ninherent differentiability and speed make them particularly well-suited for\ninverse design problems. Yet, adapting to new tasks from limited available data\nis an important aspect for real-world applications that current methods\nstruggle with. We frame mesh-based simulation as a meta-learning problem and\nuse a recent Bayesian meta-learning method to improve GNSs adaptability to new\nscenarios by leveraging context data and handling uncertainties. Our approach,\nlatent task-specific graph network simulator, uses non-amortized task posterior\napproximations to sample latent descriptions of unknown system properties.\nAdditionally, we leverage movement primitives for efficient full trajectory\nprediction, effectively addressing the issue of accumulating errors encountered\nby previous auto-regressive methods. We validate the effectiveness of our\napproach through various experiments, performing on par with or better than\nestablished baseline methods. Movement primitives further allow us to\naccommodate various types of context data, as demonstrated through the\nutilization of point clouds during inference. By combining GNSs with\nmeta-learning, we bring them closer to real-world applicability, particularly\nin scenarios with smaller datasets.","terms":["cs.LG"]},{"titles":"Self-similarity Prior Distillation for Unsupervised Remote Physiological Measurement","summaries":"Remote photoplethysmography (rPPG) is a noninvasive technique that aims to\ncapture subtle variations in facial pixels caused by changes in blood volume\nresulting from cardiac activities. Most existing unsupervised methods for rPPG\ntasks focus on the contrastive learning between samples while neglecting the\ninherent self-similar prior in physiological signals. In this paper, we propose\na Self-Similarity Prior Distillation (SSPD) framework for unsupervised rPPG\nestimation, which capitalizes on the intrinsic self-similarity of cardiac\nactivities. Specifically, we first introduce a physical-prior embedded\naugmentation technique to mitigate the effect of various types of noise. Then,\nwe tailor a self-similarity-aware network to extract more reliable self-similar\nphysiological features. Finally, we develop a hierarchical self-distillation\nparadigm to assist the network in disentangling self-similar physiological\npatterns from facial videos. Comprehensive experiments demonstrate that the\nunsupervised SSPD framework achieves comparable or even superior performance\ncompared to the state-of-the-art supervised methods. Meanwhile, SSPD maintains\nthe lowest inference time and computation cost among end-to-end models. The\nsource codes are available at https:\/\/github.com\/LinXi1C\/SSPD.","terms":["cs.CV","cs.MM"]},{"titles":"Geometry-Calibrated DRO: Combating Over-Pessimism with Free Energy Implications","summaries":"Machine learning algorithms minimizing average risk are susceptible to\ndistributional shifts. Distributionally Robust Optimization (DRO) addresses\nthis issue by optimizing the worst-case risk within an uncertainty set.\nHowever, DRO suffers from over-pessimism, leading to low-confidence\npredictions, poor parameter estimations as well as poor generalization. In this\nwork, we conduct a theoretical analysis of a probable root cause of\nover-pessimism: excessive focus on noisy samples. To alleviate the impact of\nnoise, we incorporate data geometry into calibration terms in DRO, resulting in\nour novel Geometry-Calibrated DRO (GCDRO) for regression. We establish the\nconnection between our risk objective and the Helmholtz free energy in\nstatistical physics, and this free-energy-based risk can extend to standard DRO\nmethods. Leveraging gradient flow in Wasserstein space, we develop an\napproximate minimax optimization algorithm with a bounded error ratio and\nelucidate how our approach mitigates noisy sample effects. Comprehensive\nexperiments confirm GCDRO's superiority over conventional DRO methods.","terms":["cs.LG","cs.AI"]},{"titles":"Quantum Generative Modeling of Sequential Data with Trainable Token Embedding","summaries":"Generative models are a class of machine learning models that aim to learn\nthe underlying probability distribution of data. Unlike discriminative models,\ngenerative models focus on capturing the data's inherent structure, allowing\nthem to generate new samples that resemble the original data. To fully exploit\nthe potential of modeling probability distributions using quantum physics, a\nquantum-inspired generative model known as the Born machines have shown great\nadvancements in learning classical and quantum data over matrix product\nstate(MPS) framework. The Born machines support tractable log-likelihood,\nautoregressive and mask sampling, and have shown outstanding performance in\nvarious unsupervised learning tasks. However, much of the current research has\nbeen centered on improving the expressive power of MPS, predominantly embedding\neach token directly by a corresponding tensor index. In this study, we\ngeneralize the embedding method into trainable quantum measurement operators\nthat can be simultaneously honed with MPS. Our study indicated that combined\nwith trainable embedding, Born machines can exhibit better performance and\nlearn deeper correlations from the dataset.","terms":["cs.LG","quant-ph"]},{"titles":"Causal disentanglement of multimodal data","summaries":"Causal representation learning algorithms discover lower-dimensional\nrepresentations of data that admit a decipherable interpretation of cause and\neffect; as achieving such interpretable representations is challenging, many\ncausal learning algorithms utilize elements indicating prior information, such\nas (linear) structural causal models, interventional data, or weak supervision.\nUnfortunately, in exploratory causal representation learning, such elements and\nprior information may not be available or warranted. Alternatively, scientific\ndatasets often have multiple modalities or physics-based constraints, and the\nuse of such scientific, multimodal data has been shown to improve\ndisentanglement in fully unsupervised settings. Consequently, we introduce a\ncausal representation learning algorithm (causalPIMA) that can use multimodal\ndata and known physics to discover important features with causal\nrelationships. Our innovative algorithm utilizes a new differentiable\nparametrization to learn a directed acyclic graph (DAG) together with a latent\nspace of a variational autoencoder in an end-to-end differentiable framework\nvia a single, tractable evidence lower bound loss function. We place a Gaussian\nmixture prior on the latent space and identify each of the mixtures with an\noutcome of the DAG nodes; this novel identification enables feature discovery\nwith causal relationships. Tested against a synthetic and a scientific dataset,\nour results demonstrate the capability of learning an interpretable causal\nstructure while simultaneously discovering key features in a fully unsupervised\nsetting.","terms":["cs.LG","cs.AI","stat.ML","68T07"]},{"titles":"Efficient and Equivariant Graph Networks for Predicting Quantum Hamiltonian","summaries":"We consider the prediction of the Hamiltonian matrix, which finds use in\nquantum chemistry and condensed matter physics. Efficiency and equivariance are\ntwo important, but conflicting factors. In this work, we propose a\nSE(3)-equivariant network, named QHNet, that achieves efficiency and\nequivariance. Our key advance lies at the innovative design of QHNet\narchitecture, which not only obeys the underlying symmetries, but also enables\nthe reduction of number of tensor products by 92\\%. In addition, QHNet prevents\nthe exponential growth of channel dimension when more atom types are involved.\nWe perform experiments on MD17 datasets, including four molecular systems.\nExperimental results show that our QHNet can achieve comparable performance to\nthe state of the art methods at a significantly faster speed. Besides, our\nQHNet consumes 50\\% less memory due to its streamlined architecture. Our code\nis publicly available as part of the AIRS library\n(\\url{https:\/\/github.com\/divelab\/AIRS}).","terms":["cs.LG","physics.comp-ph"]},{"titles":"Bridging Dimensions: Confident Reachability for High-Dimensional Controllers","summaries":"Autonomous systems are increasingly implemented using end-end-end trained\ncontrollers. Such controllers make decisions that are executed on the real\nsystem with images as one of the primary sensing modalities. Deep neural\nnetworks form a fundamental building block of such controllers. Unfortunately,\nthe existing neural-network verification tools do not scale to inputs with\nthousands of dimensions. Especially when the individual inputs (such as pixels)\nare devoid of clear physical meaning. This paper takes a step towards\nconnecting exhaustive closed-loop verification with high-dimensional\ncontrollers. Our key insight is that the behavior of a high-dimensional\ncontroller can be approximated with several low-dimensional controllers in\ndifferent regions of the state space. To balance approximation and\nverifiability, we leverage the latest verification-aware knowledge\ndistillation. Then, if low-dimensional reachability results are inflated with\nstatistical approximation errors, they yield a high-confidence reachability\nguarantee for the high-dimensional controller. We investigate two inflation\ntechniques -- based on trajectories and actions -- both of which show\nconvincing performance in two OpenAI gym benchmarks.","terms":["cs.LG"]},{"titles":"Versatile Energy-Based Probabilistic Models for High Energy Physics","summaries":"As a classical generative modeling approach, energy-based models have the\nnatural advantage of flexibility in the form of the energy function. Recently,\nenergy-based models have achieved great success in modeling high-dimensional\ndata in computer vision and natural language processing. In line with these\nadvancements, we build a multi-purpose energy-based probabilistic model for\nHigh Energy Physics events at the Large Hadron Collider. This framework builds\non a powerful generative model and describes higher-order inter-particle\ninteractions. It suits different encoding architectures and builds on implicit\ngeneration. As for applicational aspects, it can serve as a powerful\nparameterized event generator for physics simulation, a generic anomalous\nsignal detector free from spurious correlations, and an augmented event\nclassifier for particle identification.","terms":["cs.LG","hep-ex","hep-ph","stat.ML"]},{"titles":"Solution of FPK Equation for Stochastic Dynamics Subjected to Additive Gaussian Noise via Deep Learning Approach","summaries":"The Fokker-Plank-Kolmogorov (FPK) equation is an idealized model representing\nmany stochastic systems commonly encountered in the analysis of stochastic\nstructures as well as many other applications. Its solution thus provides an\ninvaluable insight into the performance of many engineering systems. Despite\nits great importance, the solution of the FPK equation is still extremely\nchallenging. For systems of practical significance, the FPK equation is usually\nhigh dimensional, rendering most of the numerical methods ineffective. In this\nrespect, the present work introduces the FPK-DP Net as a physics-informed\nnetwork that encodes the physical insights, i.e. the governing constrained\ndifferential equations emanated out of physical laws, into a deep neural\nnetwork. FPK-DP Net is a mesh-free learning method that can solve the density\nevolution of stochastic dynamics subjected to additive white Gaussian noise\nwithout any prior simulation data and can be used as an efficient surrogate\nmodel afterward. FPK-DP Net uses the dimension-reduced FPK equation. Therefore,\nit can be used to address high-dimensional practical problems as well. To\ndemonstrate the potential applicability of the proposed framework, and to study\nits accuracy and efficacy, numerical implementations on five different\nbenchmark problems are investigated.","terms":["cs.LG","math.DS","math.PR","stat.AP","60G65","G.3; I.2.6"]},{"titles":"Solving High Frequency and Multi-Scale PDEs with Gaussian Processes","summaries":"Machine learning based solvers have garnered much attention in physical\nsimulation and scientific computing, with a prominent example, physics-informed\nneural networks (PINNs). However, PINNs often struggle to solve high-frequency\nand multi-scale PDEs, which can be due to spectral bias during neural network\ntraining. To address this problem, we resort to the Gaussian process (GP)\nframework. To flexibly capture the dominant frequencies, we model the power\nspectrum of the PDE solution with a student t mixture or Gaussian mixture. We\nthen apply the inverse Fourier transform to obtain the covariance function\n(according to the Wiener-Khinchin theorem). The covariance derived from the\nGaussian mixture spectrum corresponds to the known spectral mixture kernel. We\nare the first to discover its rationale and effectiveness for PDE solving.\nNext,we estimate the mixture weights in the log domain, which we show is\nequivalent to placing a Jeffreys prior. It automatically induces sparsity,\nprunes excessive frequencies, and adjusts the remaining toward the ground\ntruth. Third, to enable efficient and scalable computation on massive\ncollocation points, which are critical to capture high frequencies, we place\nthe collocation points on a grid, and multiply our covariance function at each\ninput dimension. We use the GP conditional mean to predict the solution and its\nderivatives so as to fit the boundary condition and the equation itself. As a\nresult, we can derive a Kronecker product structure in the covariance matrix.\nWe use Kronecker product properties and multilinear algebra to greatly promote\ncomputational efficiency and scalability, without any low-rank approximations.\nWe show the advantage of our method in systematic experiments.","terms":["cs.LG","cs.CE"]},{"titles":"Evaluating Uncertainty Quantification approaches for Neural PDEs in scientific applications","summaries":"The accessibility of spatially distributed data, enabled by affordable\nsensors, field, and numerical experiments, has facilitated the development of\ndata-driven solutions for scientific problems, including climate change,\nweather prediction, and urban planning. Neural Partial Differential Equations\n(Neural PDEs), which combine deep learning (DL) techniques with domain\nexpertise (e.g., governing equations) for parameterization, have proven to be\neffective in capturing valuable correlations within spatiotemporal datasets.\nHowever, sparse and noisy measurements coupled with modeling approximation\nintroduce aleatoric and epistemic uncertainties. Therefore, quantifying\nuncertainties propagated from model inputs to outputs remains a challenge and\nan essential goal for establishing the trustworthiness of Neural PDEs. This\nwork evaluates various Uncertainty Quantification (UQ) approaches for both\nForward and Inverse Problems in scientific applications. Specifically, we\ninvestigate the effectiveness of Bayesian methods, such as Hamiltonian Monte\nCarlo (HMC) and Monte-Carlo Dropout (MCD), and a more conventional approach,\nDeep Ensembles (DE). To illustrate their performance, we take two canonical\nPDEs: Burger's equation and the Navier-Stokes equation. Our results indicate\nthat Neural PDEs can effectively reconstruct flow systems and predict the\nassociated unknown parameters. However, it is noteworthy that the results\nderived from Bayesian methods, based on our observations, tend to display a\nhigher degree of certainty in their predictions as compared to those obtained\nusing the DE. This elevated certainty in predictions suggests that Bayesian\ntechniques might underestimate the true underlying uncertainty, thereby\nappearing more confident in their predictions than the DE approach.","terms":["cs.LG","cs.AI","physics.comp-ph","physics.flu-dyn"]},{"titles":"Lie Point Symmetry and Physics Informed Networks","summaries":"Symmetries have been leveraged to improve the generalization of neural\nnetworks through different mechanisms from data augmentation to equivariant\narchitectures. However, despite their potential, their integration into neural\nsolvers for partial differential equations (PDEs) remains largely unexplored.\nWe explore the integration of PDE symmetries, known as Lie point symmetries, in\na major family of neural solvers known as physics-informed neural networks\n(PINNs). We propose a loss function that informs the network about Lie point\nsymmetries in the same way that PINN models try to enforce the underlying PDE\nthrough a loss function. Intuitively, our symmetry loss ensures that the\ninfinitesimal generators of the Lie group conserve the PDE solutions.\nEffectively, this means that once the network learns a solution, it also learns\nthe neighbouring solutions generated by Lie point symmetries. Empirical\nevaluations indicate that the inductive bias introduced by the Lie point\nsymmetries of the PDEs greatly boosts the sample efficiency of PINNs.","terms":["cs.LG"]},{"titles":"Wearable data from subjects playing Super Mario, sitting university exams, or performing physical exercise help detect acute mood episodes via self-supervised learning","summaries":"Personal sensing, leveraging data passively and near-continuously collected\nwith wearables from patients in their ecological environment, is a promising\nparadigm to monitor mood disorders (MDs), a major determinant of worldwide\ndisease burden. However, collecting and annotating wearable data is very\nresource-intensive. Studies of this kind can thus typically afford to recruit\nonly a couple dozens of patients. This constitutes one of the major obstacles\nto applying modern supervised machine learning techniques to MDs detection. In\nthis paper, we overcome this data bottleneck and advance the detection of MDs\nacute episode vs stable state from wearables data on the back of recent\nadvances in self-supervised learning (SSL). This leverages unlabelled data to\nlearn representations during pre-training, subsequently exploited for a\nsupervised task. First, we collected open-access datasets recording with an\nEmpatica E4 spanning different, unrelated to MD monitoring, personal sensing\ntasks -- from emotion recognition in Super Mario players to stress detection in\nundergraduates -- and devised a pre-processing pipeline performing on-\/off-body\ndetection, sleep-wake detection, segmentation, and (optionally) feature\nextraction. With 161 E4-recorded subjects, we introduce E4SelfLearning, the\nlargest to date open access collection, and its pre-processing pipeline.\nSecond, we show that SSL confidently outperforms fully-supervised pipelines\nusing either our novel E4-tailored Transformer architecture (E4mer) or\nclassical baseline XGBoost: 81.23% against 75.35% (E4mer) and 72.02% (XGBoost)\ncorrectly classified recording segments from 64 (half acute, half stable)\npatients. Lastly, we illustrate that SSL performance is strongly associated\nwith the specific surrogate task employed for pre-training as well as with\nunlabelled data availability.","terms":["cs.LG","cs.AI","eess.SP"]},{"titles":"Spatio-Temporal Anomaly Detection with Graph Networks for Data Quality Monitoring of the Hadron Calorimeter","summaries":"The compact muon solenoid (CMS) experiment is a general-purpose detector for\nhigh-energy collision at the large hadron collider (LHC) at CERN. It employs an\nonline data quality monitoring (DQM) system to promptly spot and diagnose\nparticle data acquisition problems to avoid data quality loss. In this study,\nwe present semi-supervised spatio-temporal anomaly detection (AD) monitoring\nfor the physics particle reading channels of the hadronic calorimeter (HCAL) of\nthe CMS using three-dimensional digi-occupancy map data of the DQM. We propose\nthe GraphSTAD system, which employs convolutional and graph neural networks to\nlearn local spatial characteristics induced by particles traversing the\ndetector, and global behavior owing to shared backend circuit connections and\nhousing boxes of the channels, respectively. Recurrent neural networks capture\nthe temporal evolution of the extracted spatial features. We have validated the\naccuracy of the proposed AD system in capturing diverse channel fault types\nusing the LHC Run-2 collision data sets. The GraphSTAD system has achieved\nproduction-level accuracy and is being integrated into the CMS core production\nsystem--for real-time monitoring of the HCAL. We have also provided a\nquantitative performance comparison with alternative benchmark models to\ndemonstrate the promising leverage of the presented system.","terms":["cs.LG","cs.AI"]},{"titles":"On Leakage in Machine Learning Pipelines","summaries":"Machine learning (ML) provides powerful tools for predictive modeling. ML's\npopularity stems from the promise of sample-level prediction with applications\nacross a variety of fields from physics and marketing to healthcare. However,\nif not properly implemented and evaluated, ML pipelines may contain leakage\ntypically resulting in overoptimistic performance estimates and failure to\ngeneralize to new data. This can have severe negative financial and societal\nimplications. Our aim is to expand understanding associated with causes leading\nto leakage when designing, implementing, and evaluating ML pipelines.\nIllustrated by concrete examples, we provide a comprehensive overview and\ndiscussion of various types of leakage that may arise in ML pipelines.","terms":["cs.LG","cs.AI"]},{"titles":"Generative learning for nonlinear dynamics","summaries":"Modern generative machine learning models demonstrate surprising ability to\ncreate realistic outputs far beyond their training data, such as photorealistic\nartwork, accurate protein structures, or conversational text. These successes\nsuggest that generative models learn to effectively parametrize and sample\narbitrarily complex distributions. Beginning half a century ago, foundational\nworks in nonlinear dynamics used tools from information theory to infer\nproperties of chaotic attractors from time series, motivating the development\nof algorithms for parametrizing chaos in real datasets. In this perspective, we\naim to connect these classical works to emerging themes in large-scale\ngenerative statistical learning. We first consider classical attractor\nreconstruction, which mirrors constraints on latent representations learned by\nstate space models of time series. We next revisit early efforts to use\nsymbolic approximations to compare minimal discrete generators underlying\ncomplex processes, a problem relevant to modern efforts to distill and\ninterpret black-box statistical models. Emerging interdisciplinary works bridge\nnonlinear dynamics and learning theory, such as operator-theoretic methods for\ncomplex fluid flows, or detection of broken detailed balance in biological\ndatasets. We anticipate that future machine learning techniques may revisit\nother classical concepts from nonlinear dynamics, such as transinformation\ndecay and complexity-entropy tradeoffs.","terms":["cs.LG","nlin.CD","physics.comp-ph"]},{"titles":"Generative Structural Design Integrating BIM and Diffusion Model","summaries":"Intelligent structural design using AI can effectively reduce time overhead\nand increase efficiency. It has potential to become the new design paradigm in\nthe future to assist and even replace engineers, and so it has become a\nresearch hotspot in the academic community. However, current methods have some\nlimitations to be addressed, whether in terms of application scope, visual\nquality of generated results, or evaluation metrics of results. This study\nproposes a comprehensive solution. Firstly, we introduce building information\nmodeling (BIM) into intelligent structural design and establishes a structural\ndesign pipeline integrating BIM and generative AI, which is a powerful\nsupplement to the previous frameworks that only considered CAD drawings. In\norder to improve the perceptual quality and details of generations, this study\nmakes 3 contributions. Firstly, in terms of generation framework, inspired by\nthe process of human drawing, a novel 2-stage generation framework is proposed\nto replace the traditional end-to-end framework to reduce the generation\ndifficulty for AI models. Secondly, in terms of generative AI tools adopted,\ndiffusion models (DMs) are introduced to replace widely used generative\nadversarial network (GAN)-based models, and a novel physics-based conditional\ndiffusion model (PCDM) is proposed to consider different design prerequisites.\nThirdly, in terms of neural networks, an attention block (AB) consisting of a\nself-attention block (SAB) and a parallel cross-attention block (PCAB) is\ndesigned to facilitate cross-domain data fusion. The quantitative and\nqualitative results demonstrate the powerful generation and representation\ncapabilities of PCDM. Necessary ablation studies are conducted to examine the\nvalidity of the methods. This study also shows that DMs have the potential to\nreplace GANs and become the new benchmark for generative problems in civil\nengineering.","terms":["cs.LG","cs.CV"]},{"titles":"Curating Naturally Adversarial Datasets for Learning-Enabled Medical Cyber-Physical Systems","summaries":"Deep learning models have shown promising predictive accuracy for time-series\nhealthcare applications. However, ensuring the robustness of these models is\nvital for building trustworthy AI systems. Existing research predominantly\nfocuses on robustness to synthetic adversarial examples, crafted by adding\nimperceptible perturbations to clean input data. However, these synthetic\nadversarial examples do not accurately reflect the most challenging real-world\nscenarios, especially in the context of healthcare data. Consequently,\nrobustness to synthetic adversarial examples may not necessarily translate to\nrobustness against naturally occurring adversarial examples, which is highly\ndesirable for trustworthy AI. We propose a method to curate datasets comprised\nof natural adversarial examples to evaluate model robustness. The method relies\non probabilistic labels obtained from automated weakly-supervised labeling that\ncombines noisy and cheap-to-obtain labeling heuristics. Based on these labels,\nour method adversarially orders the input data and uses this ordering to\nconstruct a sequence of increasingly adversarial datasets. Our evaluation on\nsix medical case studies and three non-medical case studies demonstrates the\nefficacy and statistical validity of our approach to generating naturally\nadversarial datasets","terms":["cs.LG","cs.AI"]},{"titles":"Size Matters: Large Graph Generation with HiGGs","summaries":"Large graphs are present in a variety of domains, including social networks,\ncivil infrastructure, and the physical sciences to name a few. Graph generation\nis similarly widespread, with applications in drug discovery, network analysis\nand synthetic datasets among others. While GNN (Graph Neural Network) models\nhave been applied in these domains their high in-memory costs restrict them to\nsmall graphs. Conversely less costly rule-based methods struggle to reproduce\ncomplex structures. We propose HIGGS (Hierarchical Generation of Graphs) as a\nmodel-agnostic framework of producing large graphs with realistic local\nstructures. HIGGS uses GNN models with conditional generation capabilities to\nsample graphs in hierarchies of resolution. As a result HIGGS has the capacity\nto extend the scale of generated graphs from a given GNN model by quadratic\norder. As a demonstration we implement HIGGS using DiGress, a recent\ngraph-diffusion model, including a novel edge-predictive-diffusion variant\nedge-DiGress. We use this implementation to generate categorically attributed\ngraphs with tens of thousands of nodes. These HIGGS generated graphs are far\nlarger than any previously produced using GNNs. Despite this jump in scale we\ndemonstrate that the graphs produced by HIGGS are, on the local scale, more\nrealistic than those from the rule-based model BTER.","terms":["cs.LG","cs.AI","cs.SI"]},{"titles":"Loss Dynamics of Temporal Difference Reinforcement Learning","summaries":"Reinforcement learning has been successful across several applications in\nwhich agents have to learn to act in environments with sparse feedback.\nHowever, despite this empirical success there is still a lack of theoretical\nunderstanding of how the parameters of reinforcement learning models and the\nfeatures used to represent states interact to control the dynamics of learning.\nIn this work, we use concepts from statistical physics, to study the typical\ncase learning curves for temporal difference learning of a value function with\nlinear function approximators. Our theory is derived under a Gaussian\nequivalence hypothesis where averages over the random trajectories are replaced\nwith temporally correlated Gaussian feature averages and we validate our\nassumptions on small scale Markov Decision Processes. We find that the\nstochastic semi-gradient noise due to subsampling the space of possible\nepisodes leads to significant plateaus in the value error, unlike in\ntraditional gradient descent dynamics. We study how learning dynamics and\nplateaus depend on feature structure, learning rate, discount factor, and\nreward function. We then analyze how strategies like learning rate annealing\nand reward shaping can favorably alter learning dynamics and plateaus. To\nconclude, our work introduces new tools to open a new direction towards\ndeveloping a theory of learning dynamics in reinforcement learning.","terms":["stat.ML","cond-mat.dis-nn","cs.AI","cs.LG"]},{"titles":"Transport meets Variational Inference: Controlled Monte Carlo Diffusions","summaries":"Connecting optimal transport and variational inference, we present a\nprincipled and systematic framework for sampling and generative modelling\ncentred around divergences on path space. Our work culminates in the\ndevelopment of the \\emph{Controlled Monte Carlo Diffusion} sampler (CMCD) for\nBayesian computation, a score-based annealing technique that crucially adapts\nboth forward and backward dynamics in a diffusion model. On the way, we clarify\nthe relationship between the EM-algorithm and iterative proportional fitting\n(IPF) for Schr{\\\"o}dinger bridges, deriving as well a regularised objective\nthat bypasses the iterative bottleneck of standard IPF-updates. Finally, we\nshow that CMCD has a strong foundation in the Jarzinsky and Crooks identities\nfrom statistical physics, and that it convincingly outperforms competing\napproaches across a wide array of experiments.","terms":["stat.ML","cs.LG"]},{"titles":"Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery","summaries":"This paper revisits datasets and evaluation criteria for Symbolic Regression\n(SR), specifically focused on its potential for scientific discovery. Focused\non a set of formulas used in the existing datasets based on Feynman Lectures on\nPhysics, we recreate 120 datasets to discuss the performance of symbolic\nregression for scientific discovery (SRSD). For each of the 120 SRSD datasets,\nwe carefully review the properties of the formula and its variables to design\nreasonably realistic sampling ranges of values so that our new SRSD datasets\ncan be used for evaluating the potential of SRSD such as whether or not an SR\nmethod can (re)discover physical laws from such datasets. We also create\nanother 120 datasets that contain dummy variables to examine whether SR methods\ncan choose necessary variables only. Besides, we propose to use normalized edit\ndistances (NED) between a predicted equation and the true equation trees for\naddressing a critical issue that existing SR metrics are either binary or\nerrors between the target values and an SR model's predicted values for a given\ninput. We conduct benchmark experiments on our new SRSD datasets using various\nrepresentative SR methods. The experimental results show that we provide a more\nrealistic performance evaluation, and our user study shows that the NED\ncorrelates with human judges significantly more than an existing SR metric.","terms":["cs.LG","cs.AI","cs.NE","cs.SC"]},{"titles":"ClimateSet: A Large-Scale Climate Model Dataset for Machine Learning","summaries":"Climate models have been key for assessing the impact of climate change and\nsimulating future climate scenarios. The machine learning (ML) community has\ntaken an increased interest in supporting climate scientists' efforts on\nvarious tasks such as climate model emulation, downscaling, and prediction\ntasks. Many of those tasks have been addressed on datasets created with single\nclimate models. However, both the climate science and ML communities have\nsuggested that to address those tasks at scale, we need large, consistent, and\nML-ready climate model datasets. Here, we introduce ClimateSet, a dataset\ncontaining the inputs and outputs of 36 climate models from the Input4MIPs and\nCMIP6 archives. In addition, we provide a modular dataset pipeline for\nretrieving and preprocessing additional climate models and scenarios. We\nshowcase the potential of our dataset by using it as a benchmark for ML-based\nclimate model emulation. We gain new insights about the performance and\ngeneralization capabilities of the different ML models by analyzing their\nperformance across different climate models. Furthermore, the dataset can be\nused to train an ML emulator on several climate models instead of just one.\nSuch a \"super emulator\" can quickly project new climate change scenarios,\ncomplementing existing scenarios already provided to policymakers. We believe\nClimateSet will create the basis needed for the ML community to tackle\nclimate-related tasks at scale.","terms":["cs.LG","cs.AI","cs.CE","physics.ao-ph"]},{"titles":"A Physics-Guided Bi-Fidelity Fourier-Featured Operator Learning Framework for Predicting Time Evolution of Drag and Lift Coefficients","summaries":"In the pursuit of accurate experimental and computational data while\nminimizing effort, there is a constant need for high-fidelity results. However,\nachieving such results often requires significant computational resources. To\naddress this challenge, this paper proposes a deep operator learning-based\nframework that requires a limited high-fidelity dataset for training. We\nintroduce a novel physics-guided, bi-fidelity, Fourier-featured Deep Operator\nNetwork (DeepONet) framework that effectively combines low and high-fidelity\ndatasets, leveraging the strengths of each. In our methodology, we began by\ndesigning a physics-guided Fourier-featured DeepONet, drawing inspiration from\nthe intrinsic physical behavior of the target solution. Subsequently, we train\nthis network to primarily learn the low-fidelity solution, utilizing an\nextensive dataset. This process ensures a comprehensive grasp of the\nfoundational solution patterns. Following this foundational learning, the\nlow-fidelity deep operator network's output is enhanced using a physics-guided\nFourier-featured residual deep operator network. This network refines the\ninitial low-fidelity output, achieving the high-fidelity solution by employing\na small high-fidelity dataset for training. Notably, in our framework, we\nemploy the Fourier feature network as the Trunk network for the DeepONets,\ngiven its proficiency in capturing and learning the oscillatory nature of the\ntarget solution with high precision. We validate our approach using a\nwell-known 2D benchmark cylinder problem, which aims to predict the time\ntrajectories of lift and drag coefficients. The results highlight that the\nphysics-guided Fourier-featured deep operator network, serving as a\nfoundational building block of our framework, possesses superior predictive\ncapability for the lift and drag coefficients compared to its data-driven\ncounterparts.","terms":["cs.LG","cs.NA","math.NA","physics.flu-dyn"]},{"titles":"Climate-Invariant Machine Learning","summaries":"Projecting climate change is a generalization problem: we extrapolate the\nrecent past using physical models across past, present, and future climates.\nCurrent climate models require representations of processes that occur at\nscales smaller than model grid size, which have been the main source of model\nprojection uncertainty. Recent machine learning (ML) algorithms hold promise to\nimprove such process representations, but tend to extrapolate poorly to climate\nregimes they were not trained on. To get the best of the physical and\nstatistical worlds, we propose a new framework -- termed \"climate-invariant\" ML\n-- incorporating knowledge of climate processes into ML algorithms, and show\nthat it can maintain high offline accuracy across a wide range of climate\nconditions and configurations in three distinct atmospheric models. Our results\nsuggest that explicitly incorporating physical knowledge into data-driven\nmodels of Earth system processes can improve their consistency, data\nefficiency, and generalizability across climate regimes.","terms":["cs.LG","physics.ao-ph","physics.comp-ph"]},{"titles":"Deep-learning-based decomposition of overlapping-sparse images: application at the vertex of neutrino interactions","summaries":"Image decomposition plays a crucial role in various computer vision tasks,\nenabling the analysis and manipulation of visual content at a fundamental\nlevel. Overlapping images, which occur when multiple objects or scenes\npartially occlude each other, pose unique challenges for decomposition\nalgorithms. The task intensifies when working with sparse images, where the\nscarcity of meaningful information complicates the precise extraction of\ncomponents. This paper presents a solution that leverages the power of deep\nlearning to accurately extract individual objects within multi-dimensional\noverlapping-sparse images, with a direct application in high-energy physics\nwith decomposition of overlaid elementary particles obtained from imaging\ndetectors. In particular, the proposed approach tackles a highly complex yet\nunsolved problem: identifying and measuring independent particles at the vertex\nof neutrino interactions, where one expects to observe detector images with\nmultiple indiscernible overlapping charged particles. By decomposing the image\nof the detector activity at the vertex through deep learning, it is possible to\ninfer the kinematic parameters of the identified low-momentum particles - which\notherwise would remain neglected - and enhance the reconstructed energy\nresolution of the neutrino event. We also present an additional step - that can\nbe tuned directly on detector data - combining the above method with a\nfully-differentiable generative model to improve the image decomposition\nfurther and, consequently, the resolution of the measured parameters, achieving\nunprecedented results. This improvement is crucial for precisely measuring the\nparameters that govern neutrino flavour oscillations and searching for\nasymmetries between matter and antimatter.","terms":["cs.CV","cs.LG","hep-ex"]},{"titles":"An enrichment approach for enhancing the expressivity of neural operators with applications to seismology","summaries":"The Eikonal equation plays a central role in seismic wave propagation and\nhypocenter localization, a crucial aspect of efficient earthquake early warning\nsystems. Despite recent progress, real-time earthquake localization remains\nchallenging due to the need to learn a generalizable Eikonal operator. We\nintroduce a novel deep learning architecture, Enriched-DeepONet (En-DeepONet),\naddressing the limitations of current operator learning models in dealing with\nmoving-solution operators. Leveraging addition and subtraction operations and a\nnovel `root' network, En-DeepONet is particularly suitable for learning such\noperators and achieves up to four orders of magnitude improved accuracy without\nincreased training cost. We demonstrate the effectiveness of En-DeepONet in\nearthquake localization under variable velocity and arrival time conditions.\nOur results indicate that En-DeepONet paves the way for real-time hypocenter\nlocalization for velocity models of practical interest. The proposed method\nrepresents a significant advancement in operator learning that is applicable to\na gamut of scientific problems, including those in seismology, fracture\nmechanics, and phase-field problems.","terms":["cs.LG","cs.CE","physics.comp-ph"]},{"titles":"Enabling Efficient, Reliable Real-World Reinforcement Learning with Approximate Physics-Based Models","summaries":"We focus on developing efficient and reliable policy optimization strategies\nfor robot learning with real-world data. In recent years, policy gradient\nmethods have emerged as a promising paradigm for training control policies in\nsimulation. However, these approaches often remain too data inefficient or\nunreliable to train on real robotic hardware. In this paper we introduce a\nnovel policy gradient-based policy optimization framework which systematically\nleverages a (possibly highly simplified) first-principles model and enables\nlearning precise control policies with limited amounts of real-world data. Our\napproach $1)$ uses the derivatives of the model to produce sample-efficient\nestimates of the policy gradient and $2)$ uses the model to design a low-level\ntracking controller, which is embedded in the policy class. Theoretical\nanalysis provides insight into how the presence of this feedback controller\novercomes key limitations of stand-alone policy gradient methods, while\nhardware experiments with a small car and quadruped demonstrate that our\napproach can learn precise control strategies reliably and with only minutes of\nreal-world data.","terms":["cs.LG","cs.RO"]},{"titles":"Weight-Sharing Regularization","summaries":"Weight-sharing is ubiquitous in deep learning. Motivated by this, we\nintroduce ''weight-sharing regularization'' for neural networks, defined as\n$R(w) = \\frac{1}{d - 1}\\sum_{i > j}^d |w_i - w_j|$. We study the proximal\nmapping of $R$ and provide an intuitive interpretation of it in terms of a\nphysical system of interacting particles. Using this interpretation, we design\na novel parallel algorithm for $\\operatorname{prox}_R$ which provides an\nexponential speedup over previous algorithms, with a depth of $O(\\log^3 d)$.\nOur algorithm makes it feasible to train weight-sharing regularized deep neural\nnetworks with proximal gradient descent. Experiments reveal that weight-sharing\nregularization enables fully-connected networks to learn convolution-like\nfilters.","terms":["cs.LG","stat.ML"]},{"titles":"Equivariance Is Not All You Need: Characterizing the Utility of Equivariant Graph Neural Networks for Particle Physics Tasks","summaries":"Incorporating inductive biases into ML models is an active area of ML\nresearch, especially when ML models are applied to data about the physical\nworld. Equivariant Graph Neural Networks (GNNs) have recently become a popular\nmethod for learning from physics data because they directly incorporate the\nsymmetries of the underlying physical system. Drawing from the relevant\nliterature around group equivariant networks, this paper presents a\ncomprehensive evaluation of the proposed benefits of equivariant GNNs by using\nreal-world particle physics reconstruction tasks as an evaluation test-bed. We\ndemonstrate that many of the theoretical benefits generally associated with\nequivariant networks may not hold for realistic systems and introduce\ncompelling directions for future research that will benefit both the scientific\ntheory of ML and physics applications.","terms":["cs.LG","hep-ex"]},{"titles":"V2X Cooperative Perception for Autonomous Driving: Recent Advances and Challenges","summaries":"Accurate perception is essential for advancing autonomous driving and\naddressing safety challenges in modern transportation systems. Despite\nsignificant advancements in computer vision for object recognition, current\nperception methods still face difficulties in complex real-world traffic\nenvironments. Challenges such as physical occlusion and limited sensor field of\nview persist for individual vehicle systems. Cooperative Perception (CP) with\nVehicle-to-Everything (V2X) technologies has emerged as a solution to overcome\nthese obstacles and enhance driving automation systems. While some research has\nexplored CP's fundamental architecture and critical components, there remains a\nlack of comprehensive summaries of the latest innovations, particularly in the\ncontext of V2X communication technologies. To address this gap, this paper\nprovides a comprehensive overview of the evolution of CP technologies, spanning\nfrom early explorations to recent developments, including advancements in V2X\ncommunication technologies. Additionally, a contemporary generic framework is\nalso proposed to illustrate the V2X-based CP workflow, aiding in the structured\nunderstanding of CP system components. Furthermore, this paper categorizes\nprevailing V2X-based CP methodologies based on the critical issues they\naddress. An extensive literature review is conducted within this taxonomy,\nevaluating existing datasets and simulators. Finally, open challenges and\nfuture directions in CP for autonomous driving are discussed by considering\nboth perception and V2X communication advancements.","terms":["cs.CV"]},{"titles":"A Generative Neural Network Approach for 3D Multi-Criteria Design Generation and Optimization of an Engine Mount for an Unmanned Air Vehicle","summaries":"One of the most promising developments in computer vision in recent years is\nthe use of generative neural networks for functionality condition-based 3D\ndesign reconstruction and generation. Here, neural networks learn dependencies\nbetween functionalities and a geometry in a very effective way. For a neural\nnetwork the functionalities are translated in conditions to a certain geometry.\nBut the more conditions the design generation needs to reflect, the more\ndifficult it is to learn clear dependencies. This leads to a multi criteria\ndesign problem due various conditions, which are not considered in the neural\nnetwork structure so far.\n  In this paper, we address this multi-criteria challenge for a 3D design use\ncase related to an unmanned aerial vehicle (UAV) motor mount. We generate\n10,000 abstract 3D designs and subject them all to simulations for three\nphysical disciplines: mechanics, thermodynamics, and aerodynamics. Then, we\ntrain a Conditional Variational Autoencoder (CVAE) using the geometry and\ncorresponding multicriteria functional constraints as input. We use our trained\nCVAE as well as the Marching cubes algorithm to generate meshes for simulation\nbased evaluation. The results are then evaluated with the generated UAV\ndesigns. Subsequently, we demonstrate the ability to generate optimized designs\nunder self-defined functionality conditions using the trained neural network.","terms":["cs.LG","cs.AI"]},{"titles":"A physics-informed and attention-based graph learning approach for regional electric vehicle charging demand prediction","summaries":"Along with the proliferation of electric vehicles (EVs), optimizing the use\nof EV charging space can significantly alleviate the growing load on\nintelligent transportation systems. As the foundation to achieve such an\noptimization, a spatiotemporal method for EV charging demand prediction in\nurban areas is required. Although several solutions have been proposed by using\ndata-driven deep learning methods, it can be found that these\nperformance-oriented methods may suffer from misinterpretations to correctly\nhandle the reverse relationship between charging demands and prices. To tackle\nthe emerging challenges of training an accurate and interpretable prediction\nmodel, this paper proposes a novel approach that enables the integration of\ngraph and temporal attention mechanisms for feature extraction and the usage of\nphysic-informed meta-learning in the model pre-training step for knowledge\ntransfer. Evaluation results on a dataset of 18,013 EV charging piles in\nShenzhen, China, show that the proposed approach, named PAG, can achieve\nstate-of-the-art forecasting performance and the ability in understanding the\nadaptive changes in charging demands caused by price fluctuations.","terms":["cs.LG"]},{"titles":"Training Multi-layer Neural Networks on Ising Machine","summaries":"As a dedicated quantum device, Ising machines could solve large-scale binary\noptimization problems in milliseconds. There is emerging interest in utilizing\nIsing machines to train feedforward neural networks due to the prosperity of\ngenerative artificial intelligence. However, existing methods can only train\nsingle-layer feedforward networks because of the complex nonlinear network\ntopology. This paper proposes an Ising learning algorithm to train quantized\nneural network (QNN), by incorporating two essential techinques, namely binary\nrepresentation of topological network and order reduction of loss function. As\nfar as we know, this is the first algorithm to train multi-layer feedforward\nnetworks on Ising machines, providing an alternative to gradient-based\nbackpropagation. Firstly, training QNN is formulated as a quadratic constrained\nbinary optimization (QCBO) problem by representing neuron connection and\nactivation function as equality constraints. All quantized variables are\nencoded by binary bits based on binary encoding protocol. Secondly, QCBO is\nconverted to a quadratic unconstrained binary optimization (QUBO) problem, that\ncan be efficiently solved on Ising machines. The conversion leverages both\npenalty function and Rosenberg order reduction, who together eliminate equality\nconstraints and reduce high-order loss function into a quadratic one. With some\nassumptions, theoretical analysis shows the space complexity of our algorithm\nis $\\mathcal{O}(H^2L + HLN\\log H)$, quantifying the required number of Ising\nspins. Finally, the algorithm effectiveness is validated with a simulated Ising\nmachine on MNIST dataset. After annealing 700 ms, the classification accuracy\nachieves 98.3%. Among 100 runs, the success probability of finding the optimal\nsolution is 72%. Along with the increasing number of spins on Ising machine,\nour algorithm has the potential to train deeper neural networks.","terms":["cs.LG","cs.AI","cs.NE","quant-ph"]},{"titles":"From molecules to scaffolds to functional groups: building context-dependent molecular representation via multi-channel learning","summaries":"Reliable molecular property prediction is essential for various scientific\nendeavors and industrial applications, such as drug discovery. However, the\nscarcity of data, combined with the highly non-linear causal relationships\nbetween physicochemical and biological properties and conventional molecular\nfeaturization schemes, complicates the development of robust molecular machine\nlearning models. Self-supervised learning (SSL) has emerged as a popular\nsolution, utilizing large-scale, unannotated molecular data to learn a\nfoundational representation of chemical space that might be advantageous for\ndownstream tasks. Yet, existing molecular SSL methods largely overlook\ndomain-specific knowledge, such as molecular similarity and scaffold\nimportance, as well as the context of the target application when operating\nover the large chemical space. This paper introduces a novel learning framework\nthat leverages the knowledge of structural hierarchies within molecular\nstructures, embeds them through separate pre-training tasks over distinct\nchannels, and employs a task-specific channel selection to compose a\ncontext-dependent representation. Our approach demonstrates competitive\nperformance across various molecular property benchmarks and establishes some\nstate-of-the-art results. It further offers unprecedented advantages in\nparticularly challenging yet ubiquitous scenarios like activity cliffs with\nenhanced robustness and generalizability compared to other baselines.","terms":["cs.LG","physics.chem-ph","q-bio.QM"]},{"titles":"For SALE: State-Action Representation Learning for Deep Reinforcement Learning","summaries":"In the field of reinforcement learning (RL), representation learning is a\nproven tool for complex image-based tasks, but is often overlooked for\nenvironments with low-level states, such as physical control problems. This\npaper introduces SALE, a novel approach for learning embeddings that model the\nnuanced interaction between state and action, enabling effective representation\nlearning from low-level states. We extensively study the design space of these\nembeddings and highlight important design considerations. We integrate SALE and\nan adaptation of checkpoints for RL into TD3 to form the TD7 algorithm, which\nsignificantly outperforms existing continuous control algorithms. On OpenAI gym\nbenchmark tasks, TD7 has an average performance gain of 276.7% and 50.7% over\nTD3 at 300k and 5M time steps, respectively, and works in both the online and\noffline settings.","terms":["cs.LG","cs.AI","stat.ML"]},{"titles":"Towards Symmetry-Aware Generation of Periodic Materials","summaries":"We consider the problem of generating periodic materials with deep models.\nWhile symmetry-aware molecule generation has been studied extensively, periodic\nmaterials possess different symmetries, which have not been completely captured\nby existing methods. In this work, we propose SyMat, a novel material\ngeneration approach that can capture physical symmetries of periodic material\nstructures. SyMat generates atom types and lattices of materials through\ngenerating atom type sets, lattice lengths and lattice angles with a\nvariational auto-encoder model. In addition, SyMat employs a score-based\ndiffusion model to generate atom coordinates of materials, in which a novel\nsymmetry-aware probabilistic model is used in the coordinate diffusion process.\nWe show that SyMat is theoretically invariant to all symmetry transformations\non materials and demonstrate that SyMat achieves promising performance on\nrandom generation and property optimization tasks. Our code is publicly\navailable as part of the AIRS library (https:\/\/github.com\/divelab\/AIRS).","terms":["cs.LG","cond-mat.mtrl-sci"]},{"titles":"Digital Typhoon: Long-term Satellite Image Dataset for the Spatio-Temporal Modeling of Tropical Cyclones","summaries":"This paper presents the official release of the Digital Typhoon dataset, the\nlongest typhoon satellite image dataset for 40+ years aimed at benchmarking\nmachine learning models for long-term spatio-temporal data. To build the\ndataset, we developed a workflow to create an infrared typhoon-centered image\nfor cropping using Lambert azimuthal equal-area projection referring to the\nbest track data. We also address data quality issues such as inter-satellite\ncalibration to create a homogeneous dataset. To take advantage of the dataset,\nwe organized machine learning tasks by the types and targets of inference, with\nother tasks for meteorological analysis, societal impact, and climate change.\nThe benchmarking results on the analysis, forecasting, and reanalysis for the\nintensity suggest that the dataset is challenging for recent deep learning\nmodels, due to many choices that affect the performance of various models. This\ndataset reduces the barrier for machine learning researchers to meet\nlarge-scale real-world events called tropical cyclones and develop machine\nlearning models that may contribute to advancing scientific knowledge on\ntropical cyclones as well as solving societal and sustainability issues such as\ndisaster reduction and climate change. The dataset is publicly available at\nhttp:\/\/agora.ex.nii.ac.jp\/digital-typhoon\/dataset\/ and\nhttps:\/\/github.com\/kitamoto-lab\/digital-typhoon\/.","terms":["cs.CV","cs.LG","physics.ao-ph"]},{"titles":"On the Computational Entanglement of Distant Features in Adversarial Machine Learning","summaries":"Adversarial examples in machine learning has emerged as a focal point of\nresearch due to their remarkable ability to deceive models with seemingly\ninconspicuous input perturbations, potentially resulting in severe\nconsequences. In this study, we embark on a comprehensive exploration of\nadversarial machine learning models, shedding light on their intrinsic\ncomplexity and interpretability. Our investigation reveals intriguing links\nbetween machine learning model complexity and Einstein's theory of special\nrelativity, all through the lens of entanglement. While our work does not\nprimarily center on quantum entanglement, we instead define the entanglement\ncorrelations we have discovered to be computational, and demonstrate that\ndistant feature samples can be entangled, strongly resembling entanglement\ncorrelation in the quantum realm. This revelation bestows fresh insights for\nunderstanding the phenomenon of emergent adversarial examples in modern machine\nlearning, potentially paving the way for more robust and interpretable models\nin this rapidly evolving field.","terms":["cs.LG","cs.IT","math.IT","physics.comp-ph"]},{"titles":"Transfer learning for atomistic simulations using GNNs and kernel mean embeddings","summaries":"Interatomic potentials learned using machine learning methods have been\nsuccessfully applied to atomistic simulations. However, accurate models require\nlarge training datasets, while generating reference calculations is\ncomputationally demanding. To bypass this difficulty, we propose a transfer\nlearning algorithm that leverages the ability of graph neural networks (GNNs)\nto represent chemical environments together with kernel mean embeddings. We\nextract a feature map from GNNs pre-trained on the OC20 dataset and use it to\nlearn the potential energy surface from system-specific datasets of catalytic\nprocesses. Our method is further enhanced by incorporating into the kernel the\nchemical species information, resulting in improved performance and\ninterpretability. We test our approach on a series of realistic datasets of\nincreasing complexity, showing excellent generalization and transferability\nperformance, and improving on methods that rely on GNNs or ridge regression\nalone, as well as similar fine-tuning approaches.","terms":["cs.LG","physics.chem-ph"]},{"titles":"MAAIP: Multi-Agent Adversarial Interaction Priors for imitation from fighting demonstrations for physics-based characters","summaries":"Simulating realistic interaction and motions for physics-based characters is\nof great interest for interactive applications, and automatic secondary\ncharacter animation in the movie and video game industries. Recent works in\nreinforcement learning have proposed impressive results for single character\nsimulation, especially the ones that use imitation learning based techniques.\nHowever, imitating multiple characters interactions and motions requires to\nalso model their interactions. In this paper, we propose a novel Multi-Agent\nGenerative Adversarial Imitation Learning based approach that generalizes the\nidea of motion imitation for one character to deal with both the interaction\nand the motions of the multiple physics-based characters. Two unstructured\ndatasets are given as inputs: 1) a single-actor dataset containing motions of a\nsingle actor performing a set of motions linked to a specific application, and\n2) an interaction dataset containing a few examples of interactions between\nmultiple actors. Based on these datasets, our system trains control policies\nallowing each character to imitate the interactive skills associated with each\nactor, while preserving the intrinsic style. This approach has been tested on\ntwo different fighting styles, boxing and full-body martial art, to demonstrate\nthe ability of the method to imitate different styles.","terms":["cs.CV","cs.AI","cs.GR","cs.LG","cs.RO","68U99","I.3.8; I.3.m"]},{"titles":"Hybrid quantum image classification and federated learning for hepatic steatosis diagnosis","summaries":"With the maturity achieved by deep learning techniques, intelligent systems\nthat can assist physicians in the daily interpretation of clinical images can\nplay a very important role. In addition, quantum techniques applied to deep\nlearning can enhance this performance, and federated learning techniques can\nrealize privacy-friendly collaborative learning among different participants,\nsolving privacy issues due to the use of sensitive data and reducing the number\nof data to be collected for each individual participant. We present in this\nstudy a hybrid quantum neural network that can be used to quantify\nnon-alcoholic liver steatosis and could be useful in the diagnostic process to\ndetermine a liver's suitability for transplantation; at the same time, we\npropose a federated learning approach based on a classical deep learning\nsolution to solve the same problem, but using a reduced data set in each part.\nThe liver steatosis image classification accuracy of the hybrid quantum neural\nnetwork, the hybrid quantum ResNet model, consisted of 5 qubits and more than\n100 variational gates, reaches 97%, which is 1.8% higher than its classical\ncounterpart, ResNet. Crucially, that even with a reduced dataset, our hybrid\napproach consistently outperformed its classical counterpart, indicating\nsuperior generalization and less potential for overfitting in medical\napplications. In addition, a federated approach with multiple clients, up to\n32, despite the lower accuracy, but still higher than 90%, would allow using,\nfor each participant, a very small dataset, i.e., up to one-thirtieth. Our\nwork, based over real-word clinical data can be regarded as a scalable and\ncollaborative starting point, could thus fulfill the need for an effective and\nreliable computer-assisted system that facilitates the daily diagnostic work of\nthe clinical pathologist.","terms":["cs.LG","cs.CV","quant-ph"]},{"titles":"Attention-based Models for Snow-Water Equivalent Prediction","summaries":"Snow Water-Equivalent (SWE) -- the amount of water available if snowpack is\nmelted -- is a key decision variable used by water management agencies to make\nirrigation, flood control, power generation and drought management decisions.\nSWE values vary spatiotemporally -- affected by weather, topography and other\nenvironmental factors. While daily SWE can be measured by Snow Telemetry\n(SNOTEL) stations with requisite instrumentation, such stations are spatially\nsparse requiring interpolation techniques to create spatiotemporally complete\ndata. While recent efforts have explored machine learning (ML) for SWE\nprediction, a number of recent ML advances have yet to be considered. The main\ncontribution of this paper is to explore one such ML advance, attention\nmechanisms, for SWE prediction. Our hypothesis is that attention has a unique\nability to capture and exploit correlations that may exist across locations or\nthe temporal spectrum (or both). We present a generic attention-based modeling\nframework for SWE prediction and adapt it to capture spatial attention and\ntemporal attention. Our experimental results on 323 SNOTEL stations in the\nWestern U.S. demonstrate that our attention-based models outperform other\nmachine learning approaches. We also provide key results highlighting the\ndifferences between spatial and temporal attention in this context and a\nroadmap toward deployment for generating spatially-complete SWE maps.","terms":["cs.LG","cs.AI","physics.ao-ph","I.2"]},{"titles":"AI Increases Global Access to Reliable Flood Forecasts","summaries":"Floods are one of the most common natural disasters, with a disproportionate\nimpact in developing countries that often lack dense streamflow gauge networks.\nAccurate and timely warnings are critical for mitigating flood risks, but\nhydrological simulation models typically must be calibrated to long data\nrecords in each watershed. Using AI, we achieve reliability in predicting\nextreme riverine events in ungauged watersheds at up to a 5-day lead time that\nis similar to or better than the reliability of nowcasts (0-day lead time) from\na current state of the art global modeling system (the Copernicus Emergency\nManagement Service Global Flood Awareness System). Additionally, we achieve\naccuracies over 5-year return period events that are similar to or better than\ncurrent accuracies over 1-year return period events. This means that AI can\nprovide flood warnings earlier and over larger and more impactful events in\nungauged basins. The model developed in this paper was incorporated into an\noperational early warning system that produces publicly available (free and\nopen) forecasts in real time in over 80 countries. This work highlights a need\nfor increasing the availability of hydrological data to continue to improve\nglobal access to reliable flood warnings.","terms":["cs.LG","cs.AI","physics.soc-ph"]},{"titles":"Multi-scale Time-stepping of Partial Differential Equations with Transformers","summaries":"Developing fast surrogates for Partial Differential Equations (PDEs) will\naccelerate design and optimization in almost all scientific and engineering\napplications. Neural networks have been receiving ever-increasing attention and\ndemonstrated remarkable success in computational modeling of PDEs, however;\ntheir prediction accuracy is not at the level of full deployment. In this work,\nwe utilize the transformer architecture, the backbone of numerous\nstate-of-the-art AI models, to learn the dynamics of physical systems as the\nmixing of spatial patterns learned by a convolutional autoencoder. Moreover, we\nincorporate the idea of multi-scale hierarchical time-stepping to increase the\nprediction speed and decrease accumulated error over time. Our model achieves\nsimilar or better results in predicting the time-evolution of Navier-Stokes\nequations compared to the powerful Fourier Neural Operator (FNO) and two\ntransformer-based neural operators OFormer and Galerkin Transformer.","terms":["cs.LG"]},{"titles":"On some limitations of data-driven weather forecasting models","summaries":"As in many other areas of engineering and applied science, Machine Learning\n(ML) is having a profound impact in the domain of Weather and Climate\nPrediction. A very recent development in this area has been the emergence of\nfully data-driven ML prediction models which routinely claim superior\nperformance to that of traditional physics-based models. In this work, we\nexamine some aspects of the forecasts produced by an exemplar of the current\ngeneration of ML models, Pangu-Weather, with a focus on the fidelity and\nphysical consistency of those forecasts and how these characteristics relate to\nperceived forecast performance. The main conclusion is that Pangu-Weather\nforecasts, and possibly those of similar ML models, do not have the fidelity\nand physical consistency of physics-based models and their advantage in\naccuracy on traditional deterministic metrics of forecast skill can be at least\npartly attributed to these peculiarities. Balancing forecast skill and physical\nconsistency of ML-driven predictions will be an important consideration for\nfuture ML models. However, and similarly to other modern post-processing\ntechnologies, the current ML models appear to be already able to add value to\nstandard NWP output for specific forecast applications and combined with their\nextremely low computational cost during deployment, are set to provide an\nadditional, useful source of forecast information. .","terms":["stat.ML","cs.LG","physics.ao-ph"]},{"titles":"Learning COVID-19 Regional Transmission Using Universal Differential Equations in a SIR model","summaries":"Highly-interconnected societies difficult to model the spread of infectious\ndiseases such as COVID-19. Single-region SIR models fail to account for\nincoming forces of infection and expanding them to a large number of\ninteracting regions involves many assumptions that do not hold in the real\nworld. We propose using Universal Differential Equations (UDEs) to capture the\ninfluence of neighboring regions and improve the model's predictions in a\ncombined SIR+UDE model. UDEs are differential equations totally or partially\ndefined by a deep neural network (DNN). We include an additive term to the SIR\nequations composed by a DNN that learns the incoming force of infection from\nthe other regions. The learning is performed using automatic differentiation\nand gradient descent to approach the change in the target system caused by the\nstate of the neighboring regions. We compared the proposed model using a\nsimulated COVID-19 outbreak against a single-region SIR and a fully data-driven\nmodel composed only of a DNN. The proposed UDE+SIR model generates predictions\nthat capture the outbreak dynamic more accurately, but a decay in performance\nis observed at the last stages of the outbreak. The single-area SIR and the\nfully data-driven approach do not capture the proper dynamics accurately. Once\nthe predictions were obtained, we employed the SINDy algorithm to substitute\nthe DNN with a regression, removing the black box element of the model with no\nconsiderable increase in the error levels.","terms":["cs.LG","physics.soc-ph"]},{"titles":"A Systematic Review of Deep Graph Neural Networks: Challenges, Classification, Architectures, Applications & Potential Utility in Bioinformatics","summaries":"In recent years, tasks of machine learning ranging from image processing &\naudio\/video analysis to natural language understanding have been transformed by\ndeep learning. The data content in all these scenarios are expressed via\nEuclidean space. However, a considerable amount of application data is\nstructured in non-Euclidean space and is expressed as graphs, e.g. dealing with\ncomplicated interactions & object interdependencies. Modelling physical\nsystems, learning molecular signatures, identifying protein interactions and\npredicting diseases involve utilising a model that can adapt from graph data.\nGraph neural networks (GNNs), specified as artificial-neural models, employ\nmessage transmission between graph nodes to represent graph dependencies and\nare primarily used in the non-Euclidean domain. Variants of GNN like Graph\nRecurrent Networks (GRN), Graph Auto Encoder (GAE), Graph Convolution Networks\n(GCN), Graph Adversarial Methods & Graph Reinforcement learning have exhibited\nbreakthrough productivity on a wide range of tasks, especially in the field of\nbioinformatics, in recent years as a result of the rapid collection of\nbiological network data. Apart from presenting all existing GNN models,\nmathematical analysis and comparison of the variants of all types of GNN have\nbeen highlighted in this survey. Graph neural networks are investigated for\ntheir potential real-world applications in various fields, focusing on\nBioinformatics. Furthermore, resources for evaluating graph neural network\nmodels and accessing open-source code & benchmark data sets are included.\nUltimately, we provide some (seven) proposals for future research in this\nrapidly evolving domain. GNNs have the potential to be an excellent tool for\nsolving a wide range of biological challenges in bioinformatics research, as\nthey are best represented as connected complex graphs.","terms":["cs.LG","cs.AI","q-bio.QM"]},{"titles":"TailorMe: Self-Supervised Learning of an Anatomically Constrained Volumetric Human Shape Model","summaries":"Human shape spaces have been extensively studied, as they are a core element\nof human shape and pose inference tasks. Classic methods for creating a human\nshape model register a surface template mesh to a database of 3D scans and use\ndimensionality reduction techniques, such as Principal Component Analysis, to\nlearn a compact representation. While these shape models enable global shape\nmodifications by correlating anthropometric measurements with the learned\nsubspace, they only provide limited localized shape control. We instead\nregister a volumetric anatomical template, consisting of skeleton bones and\nsoft tissue, to the surface scans of the CAESAR database. We further enlarge\nour training data to the full Cartesian product of all skeletons and all soft\ntissues using physically plausible volumetric deformation transfer. This data\nis then used to learn an anatomically constrained volumetric human shape model\nin a self-supervised fashion. The resulting TailorMe model enables shape\nsampling, localized shape manipulation, and fast inference from given surface\nscans.","terms":["cs.CV","cs.GR","I.3.0; I.5.1"]},{"titles":"Physics-Informed Generator-Encoder Adversarial Networks with Latent Space Matching for Stochastic Differential Equations","summaries":"We propose a new class of physics-informed neural networks, called\nPhysics-Informed Generator-Encoder Adversarial Networks, to effectively address\nthe challenges posed by forward, inverse, and mixed problems in stochastic\ndifferential equations. In these scenarios, while the governing equations are\nknown, the available data consist of only a limited set of snapshots for system\nparameters. Our model consists of two key components: the generator and the\nencoder, both updated alternately by gradient descent. In contrast to previous\napproaches of directly matching the approximated solutions with real snapshots,\nwe employ an indirect matching that operates within the lower-dimensional\nlatent feature space. This method circumvents challenges associated with\nhigh-dimensional inputs and complex data distributions, while yielding more\naccurate solutions compared to existing neural network solvers. In addition,\nthe approach also mitigates the training instability issues encountered in\nprevious adversarial frameworks in an efficient manner. Numerical results\nprovide compelling evidence of the effectiveness of the proposed method in\nsolving different types of stochastic differential equations.","terms":["cs.LG","cs.NA","math.NA"]},{"titles":"Separable PINN: Mitigating the Curse of Dimensionality in Physics-Informed Neural Networks","summaries":"Physics-informed neural networks (PINNs) have emerged as new data-driven PDE\nsolvers for both forward and inverse problems. While promising, the expensive\ncomputational costs to obtain solutions often restrict their broader\napplicability. We demonstrate that the computations in automatic\ndifferentiation (AD) can be significantly reduced by leveraging forward-mode AD\nwhen training PINN. However, a naive application of forward-mode AD to\nconventional PINNs results in higher computation, losing its practical benefit.\nTherefore, we propose a network architecture, called separable PINN (SPINN),\nwhich can facilitate forward-mode AD for more efficient computation. SPINN\noperates on a per-axis basis instead of point-wise processing in conventional\nPINNs, decreasing the number of network forward passes. Besides, while the\ncomputation and memory costs of standard PINNs grow exponentially along with\nthe grid resolution, that of our model is remarkably less susceptible,\nmitigating the curse of dimensionality. We demonstrate the effectiveness of our\nmodel in various PDE systems by significantly reducing the training run-time\nwhile achieving comparable accuracy. Project page:\nhttps:\/\/jwcho5576.github.io\/spinn\/","terms":["cs.LG"]},{"titles":"Modeling Dynamics over Meshes with Gauge Equivariant Nonlinear Message Passing","summaries":"Data over non-Euclidean manifolds, often discretized as surface meshes,\nnaturally arise in computer graphics and biological and physical systems. In\nparticular, solutions to partial differential equations (PDEs) over manifolds\ndepend critically on the underlying geometry. While graph neural networks have\nbeen successfully applied to PDEs, they do not incorporate surface geometry and\ndo not consider local gauge symmetries of the manifold. Alternatively, recent\nworks on gauge equivariant convolutional and attentional architectures on\nmeshes leverage the underlying geometry but underperform in modeling surface\nPDEs with complex nonlinear dynamics. To address these issues, we introduce a\nnew gauge equivariant architecture using nonlinear message passing. Our novel\narchitecture achieves higher performance than either convolutional or\nattentional networks on domains with highly complex and nonlinear dynamics.\nHowever, similar to the non-mesh case, design trade-offs favor convolutional,\nattentional, or message passing networks for different tasks; we investigate in\nwhich circumstances our message passing method provides the most benefit.","terms":["cs.LG"]},{"titles":"ACQUIRED: A Dataset for Answering Counterfactual Questions In Real-Life Videos","summaries":"Multimodal counterfactual reasoning is a vital yet challenging ability for AI\nsystems. It involves predicting the outcomes of hypothetical circumstances\nbased on vision and language inputs, which enables AI models to learn from\nfailures and explore hypothetical scenarios. Despite its importance, there are\nonly a few datasets targeting the counterfactual reasoning abilities of\nmultimodal models. Among them, they only cover reasoning over synthetic\nenvironments or specific types of events (e.g. traffic collisions), making them\nhard to reliably benchmark the model generalization ability in diverse\nreal-world scenarios and reasoning dimensions. To overcome these limitations,\nwe develop a video question answering dataset, ACQUIRED: it consists of 3.9K\nannotated videos, encompassing a wide range of event types and incorporating\nboth first and third-person viewpoints, which ensures a focus on real-world\ndiversity. In addition, each video is annotated with questions that span three\ndistinct dimensions of reasoning, including physical, social, and temporal,\nwhich can comprehensively evaluate the model counterfactual abilities along\nmultiple aspects. We benchmark our dataset against several state-of-the-art\nlanguage-only and multimodal models and experimental results demonstrate a\nsignificant performance gap (>13%) between models and humans. The findings\nsuggest that multimodal counterfactual reasoning remains an open challenge and\nACQUIRED is a comprehensive and reliable benchmark for inspiring future\nresearch in this direction.","terms":["cs.CV","cs.CL"]},{"titles":"Assist Is Just as Important as the Goal: Image Resurfacing to Aid Model's Robust Prediction","summaries":"Adversarial patches threaten visual AI models in the real world. The number\nof patches in a patch attack is variable and determines the attack's potency in\na specific environment. Most existing defenses assume a single patch in the\nscene, and the multiple patch scenarios are shown to overcome them. This paper\npresents a model-agnostic defense against patch attacks based on total\nvariation for image resurfacing (TVR). The TVR is an image-cleansing method\nthat processes images to remove probable adversarial regions. TVR can be\nutilized solely or augmented with a defended model, providing multi-level\nsecurity for robust prediction. TVR nullifies the influence of patches in a\nsingle image scan with no prior assumption on the number of patches in the\nscene. We validate TVR on the ImageNet-Patch benchmark dataset and with\nreal-world physical objects, demonstrating its ability to mitigate patch\nattack.","terms":["cs.CV","cs.CR"]},{"titles":"Beyond Ensemble Averages: Leveraging Climate Model Ensembles for Subseasonal Forecasting","summaries":"Producing high-quality forecasts of key climate variables such as temperature\nand precipitation on subseasonal time scales has long been a gap in operational\nforecasting. Recent studies have shown promising results using machine learning\n(ML) models to advance subseasonal forecasting (SSF), but several open\nquestions remain. First, several past approaches use the average of an ensemble\nof physics-based forecasts as an input feature of these models. However,\nensemble forecasts contain information that can aid prediction beyond only the\nensemble mean. Second, past methods have focused on average performance,\nwhereas forecasts of extreme events are far more important for planning and\nmitigation purposes. Third, climate forecasts correspond to a spatially-varying\ncollection of forecasts, and different methods account for spatial variability\nin the response differently. Trade-offs between different approaches may be\nmitigated with model stacking. This paper describes the application of a\nvariety of ML methods used to predict monthly average precipitation and two\nmeter temperature using physics-based predictions (ensemble forecasts) and\nobservational data such as relative humidity, pressure at sea level, or\ngeopotential height, two weeks in advance for the whole continental United\nStates. Regression, quantile regression, and tercile classification tasks using\nlinear models, random forests, convolutional neural networks, and stacked\nmodels are considered. The proposed models outperform common baselines such as\nhistorical averages (or quantiles) and ensemble averages (or quantiles). This\npaper further includes an investigation of feature importance, trade-offs\nbetween using the full ensemble or only the ensemble average, and different\nmodes of accounting for spatial variability.","terms":["cs.LG","physics.ao-ph"]},{"titles":"Bridging Machine Learning and Sciences: Opportunities and Challenges","summaries":"The application of machine learning in sciences has seen exciting advances in\nrecent years. As a widely applicable technique, anomaly detection has been long\nstudied in the machine learning community. Especially, deep neural nets-based\nout-of-distribution detection has made great progress for high-dimensional\ndata. Recently, these techniques have been showing their potential in\nscientific disciplines. We take a critical look at their applicative prospects\nincluding data universality, experimental protocols, model robustness, etc. We\ndiscuss examples that display transferable practices and domain-specific\nchallenges simultaneously, providing a starting point for establishing a novel\ninterdisciplinary research paradigm in the near future.","terms":["stat.ML","cs.LG","hep-ex","hep-ph","physics.data-an"]},{"titles":"Leveraging Automatic Personalised Nutrition: Food Image Recognition Benchmark and Dataset based on Nutrition Taxonomy","summaries":"Maintaining a healthy lifestyle has become increasingly challenging in\ntoday's sedentary society marked by poor eating habits. To address this issue,\nboth national and international organisations have made numerous efforts to\npromote healthier diets and increased physical activity. However, implementing\nthese recommendations in daily life can be difficult, as they are often generic\nand not tailored to individuals. This study presents the AI4Food-NutritionDB\ndatabase, the first nutrition database that incorporates food images and a\nnutrition taxonomy based on recommendations by national and international\nhealth authorities. The database offers a multi-level categorisation,\ncomprising 6 nutritional levels, 19 main categories (e.g., \"Meat\"), 73\nsubcategories (e.g., \"White Meat\"), and 893 specific food products (e.g.,\n\"Chicken\"). The AI4Food-NutritionDB opens the doors to new food computing\napproaches in terms of food intake frequency, quality, and categorisation.\nAlso, we present a standardised experimental protocol and benchmark including\nthree tasks based on the nutrition taxonomy (i.e., category, subcategory, and\nfinal product recognition). These resources are available to the research\ncommunity, including our deep learning models trained on AI4Food-NutritionDB,\nwhich can serve as pre-trained models, achieving accurate recognition results\nfor challenging food image databases.","terms":["cs.CV","cs.MM"]},{"titles":"Score-based Data Assimilation for a Two-Layer Quasi-Geostrophic Model","summaries":"Data assimilation addresses the problem of identifying plausible state\ntrajectories of dynamical systems given noisy or incomplete observations. In\ngeosciences, it presents challenges due to the high-dimensionality of\ngeophysical dynamical systems, often exceeding millions of dimensions. This\nwork assesses the scalability of score-based data assimilation (SDA), a novel\ndata assimilation method, in the context of such systems. We propose\nmodifications to the score network architecture aimed at significantly reducing\nmemory consumption and execution time. We demonstrate promising results for a\ntwo-layer quasi-geostrophic model.","terms":["stat.ML","cs.LG","physics.ao-ph"]},{"titles":"Generating QM1B with PySCF$_{\\text{IPU}}$","summaries":"The emergence of foundation models in Computer Vision and Natural Language\nProcessing have resulted in immense progress on downstream tasks. This progress\nwas enabled by datasets with billions of training examples. Similar benefits\nare yet to be unlocked for quantum chemistry, where the potential of deep\nlearning is constrained by comparatively small datasets with 100k to 20M\ntraining examples. These datasets are limited in size because the labels are\ncomputed using the accurate (but computationally demanding) predictions of\nDensity Functional Theory (DFT). Notably, prior DFT datasets were created using\nCPU supercomputers without leveraging hardware acceleration. In this paper, we\ntake a first step towards utilising hardware accelerators by introducing the\ndata generator PySCF$_{\\text{IPU}}$ using Intelligence Processing Units (IPUs).\nThis allowed us to create the dataset QM1B with one billion training examples\ncontaining 9-11 heavy atoms. We demonstrate that a simple baseline neural\nnetwork (SchNet 9M) improves its performance by simply increasing the amount of\ntraining data without additional inductive biases. To encourage future\nresearchers to use QM1B responsibly, we highlight several limitations of QM1B\nand emphasise the low-resolution of our DFT options, which also serves as\nmotivation for even larger, more accurate datasets. Code and dataset are\navailable on Github: http:\/\/github.com\/graphcore-research\/pyscf-ipu","terms":["cs.LG","physics.chem-ph","I.2.6; J.2"]},{"titles":"AI for Interpretable Chemistry: Predicting Radical Mechanistic Pathways via Contrastive Learning","summaries":"Deep learning-based reaction predictors have undergone significant\narchitectural evolution. However, their reliance on reactions from the US\nPatent Office results in a lack of interpretable predictions and limited\ngeneralization capability to other chemistry domains, such as radical and\natmospheric chemistry. To address these challenges, we introduce a new reaction\npredictor system, RMechRP, that leverages contrastive learning in conjunction\nwith mechanistic pathways, the most interpretable representation of chemical\nreactions. Specifically designed for radical reactions, RMechRP provides\ndifferent levels of interpretation of chemical reactions. We develop and train\nmultiple deep-learning models using RMechDB, a public database of radical\nreactions, to establish the first benchmark for predicting radical reactions.\nOur results demonstrate the effectiveness of RMechRP in providing accurate and\ninterpretable predictions of radical reactions, and its potential for various\napplications in atmospheric chemistry.","terms":["cs.LG","physics.chem-ph"]},{"titles":"Deep Learning for real-time neural decoding of grasp","summaries":"Neural decoding involves correlating signals acquired from the brain to\nvariables in the physical world like limb movement or robot control in Brain\nMachine Interfaces. In this context, this work starts from a specific\npre-existing dataset of neural recordings from monkey motor cortex and presents\na Deep Learning-based approach to the decoding of neural signals for grasp type\nclassification. Specifically, we propose here an approach that exploits LSTM\nnetworks to classify time series containing neural data (i.e., spike trains)\ninto classes representing the object being grasped. The main goal of the\npresented approach is to improve over state-of-the-art decoding accuracy\nwithout relying on any prior neuroscience knowledge, and leveraging only the\ncapability of deep learning models to extract correlations from data. The paper\npresents the results achieved for the considered dataset and compares them with\nprevious works on the same dataset, showing a significant improvement in\nclassification accuracy, even if considering simulated real-time decoding.","terms":["cs.LG","q-bio.NC"]},{"titles":"Gaussian Process Priors for Systems of Linear Partial Differential Equations with Constant Coefficients","summaries":"Partial differential equations (PDEs) are important tools to model physical\nsystems and including them into machine learning models is an important way of\nincorporating physical knowledge. Given any system of linear PDEs with constant\ncoefficients, we propose a family of Gaussian process (GP) priors, which we\ncall EPGP, such that all realizations are exact solutions of this system. We\napply the Ehrenpreis-Palamodov fundamental principle, which works as a\nnon-linear Fourier transform, to construct GP kernels mirroring standard\nspectral methods for GPs. Our approach can infer probable solutions of linear\nPDE systems from any data such as noisy measurements, or pointwise defined\ninitial and boundary conditions. Constructing EPGP-priors is algorithmic,\ngenerally applicable, and comes with a sparse version (S-EPGP) that learns the\nrelevant spectral frequencies and works better for big data sets. We\ndemonstrate our approach on three families of systems of PDEs, the heat\nequation, wave equation, and Maxwell's equations, where we improve upon the\nstate of the art in computation time and precision, in some experiments by\nseveral orders of magnitude.","terms":["stat.ML","cs.LG","cs.NA","math.AC","math.NA","60G15, 13N10, 13P25, 60-08, 35G35"]},{"titles":"Boosting Adversarial Transferability by Achieving Flat Local Maxima","summaries":"Transfer-based attack adopts the adversarial examples generated on the\nsurrogate model to attack various models, making it applicable in the physical\nworld and attracting increasing interest. Recently, various adversarial attacks\nhave emerged to boost adversarial transferability from different perspectives.\nIn this work, inspired by the observation that flat local minima are correlated\nwith good generalization, we assume and empirically validate that adversarial\nexamples at a flat local region tend to have good transferability by\nintroducing a penalized gradient norm to the original loss function. Since\ndirectly optimizing the gradient regularization norm is computationally\nexpensive and intractable for generating adversarial examples, we propose an\napproximation optimization method to simplify the gradient update of the\nobjective function. Specifically, we randomly sample an example and adopt a\nfirst-order procedure to approximate the curvature of Hessian\/vector product,\nwhich makes computing more efficient by interpolating two neighboring\ngradients. Meanwhile, in order to obtain a more stable gradient direction, we\nrandomly sample multiple examples and average the gradients of these examples\nto reduce the variance due to random sampling during the iterative process.\nExtensive experimental results on the ImageNet-compatible dataset show that the\nproposed method can generate adversarial examples at flat local regions, and\nsignificantly improve the adversarial transferability on either normally\ntrained models or adversarially trained models than the state-of-the-art\nattacks. Our codes are available at:\nhttps:\/\/github.com\/Trustworthy-AI-Group\/PGN.","terms":["cs.CV","cs.CR","cs.LG"]},{"titles":"Typing on Any Surface: A Deep Learning-based Method for Real-Time Keystroke Detection in Augmented Reality","summaries":"Frustrating text entry interface has been a major obstacle in participating\nin social activities in augmented reality (AR). Popular options, such as\nmid-air keyboard interface, wireless keyboards or voice input, either suffer\nfrom poor ergonomic design, limited accuracy, or are simply embarrassing to use\nin public. This paper proposes and validates a deep-learning based approach,\nthat enables AR applications to accurately predict keystrokes from the user\nperspective RGB video stream that can be captured by any AR headset. This\nenables a user to perform typing activities on any flat surface and eliminates\nthe need of a physical or virtual keyboard. A two-stage model, combing an\noff-the-shelf hand landmark extractor and a novel adaptive Convolutional\nRecurrent Neural Network (C-RNN), was trained using our newly built dataset.\nThe final model was capable of adaptive processing user-perspective video\nstreams at ~32 FPS. This base model achieved an overall accuracy of $91.05\\%$\nwhen typing 40 Words per Minute (wpm), which is how fast an average person\ntypes with two hands on a physical keyboard. The Normalised Levenshtein\nDistance also further confirmed the real-world applicability of that our\napproach. The promising results highlight the viability of our approach and the\npotential for our method to be integrated into various applications. We also\ndiscussed the limitations and future research required to bring such technique\ninto a production system.","terms":["cs.CV","cs.HC"]},{"titles":"Adversary ML Resilience in Autonomous Driving Through Human Centered Perception Mechanisms","summaries":"Physical adversarial attacks on road signs are continuously exploiting\nvulnerabilities in modern day autonomous vehicles (AVs) and impeding their\nability to correctly classify what type of road sign they encounter. Current\nmodels cannot generalize input data well, resulting in overfitting or\nunderfitting. In overfitting, the model memorizes the input data but cannot\ngeneralize to new scenarios. In underfitting, the model does not learn enough\nof the input data to accurately classify these road signs. This paper explores\nthe resilience of autonomous driving systems against three main physical\nadversarial attacks (tape, graffiti, illumination), specifically targeting\nobject classifiers. Several machine learning models were developed and\nevaluated on two distinct datasets: road signs (stop signs, speed limit signs,\ntraffic lights, and pedestrian crosswalk signs) and geometric shapes (octagons,\ncircles, squares, and triangles). The study compared algorithm performance\nunder different conditions, including clean and adversarial training and\ntesting on these datasets. To build robustness against attacks, defense\ntechniques like adversarial training and transfer learning were implemented.\nResults demonstrated transfer learning models played a crucial role in\nperformance by allowing knowledge gained from shape training to improve\ngeneralizability of road sign classification, despite the datasets being\ncompletely different. The paper suggests future research directions, including\nhuman-in-the-loop validation, security analysis, real-world testing, and\nexplainable AI for transparency. This study aims to contribute to improving\nsecurity and robustness of object classifiers in autonomous vehicles and\nmitigating adversarial example impacts on driving systems.","terms":["cs.CV","cs.LG","68","I.4.0"]},{"titles":"Physion++: Evaluating Physical Scene Understanding that Requires Online Inference of Different Physical Properties","summaries":"General physical scene understanding requires more than simply localizing and\nrecognizing objects -- it requires knowledge that objects can have different\nlatent properties (e.g., mass or elasticity), and that those properties affect\nthe outcome of physical events. While there has been great progress in physical\nand video prediction models in recent years, benchmarks to test their\nperformance typically do not require an understanding that objects have\nindividual physical properties, or at best test only those properties that are\ndirectly observable (e.g., size or color). This work proposes a novel dataset\nand benchmark, termed Physion++, that rigorously evaluates visual physical\nprediction in artificial systems under circumstances where those predictions\nrely on accurate estimates of the latent physical properties of objects in the\nscene. Specifically, we test scenarios where accurate prediction relies on\nestimates of properties such as mass, friction, elasticity, and deformability,\nand where the values of those properties can only be inferred by observing how\nobjects move and interact with other objects or fluids. We evaluate the\nperformance of a number of state-of-the-art prediction models that span a\nvariety of levels of learning vs. built-in knowledge, and compare that\nperformance to a set of human predictions. We find that models that have been\ntrained using standard regimes and datasets do not spontaneously learn to make\ninferences about latent properties, but also that models that encode objectness\nand physical states tend to make better predictions. However, there is still a\nhuge gap between all models and human performance, and all models' predictions\ncorrelate poorly with those made by humans, suggesting that no state-of-the-art\nmodel is learning to make physical predictions in a human-like way. Project\npage: https:\/\/dingmyu.github.io\/physion_v2\/","terms":["cs.CV","cs.AI","cs.GR","cs.RO"]},{"titles":"Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning","summaries":"In this paper, we propose a Disentangled Counterfactual Learning~(DCL)\napproach for physical audiovisual commonsense reasoning. The task aims to infer\nobjects' physics commonsense based on both video and audio input, with the main\nchallenge is how to imitate the reasoning ability of humans. Most of the\ncurrent methods fail to take full advantage of different characteristics in\nmulti-modal data, and lacking causal reasoning ability in models impedes the\nprogress of implicit physical knowledge inferring. To address these issues, our\nproposed DCL method decouples videos into static (time-invariant) and dynamic\n(time-varying) factors in the latent space by the disentangled sequential\nencoder, which adopts a variational autoencoder (VAE) to maximize the mutual\ninformation with a contrastive loss function. Furthermore, we introduce a\ncounterfactual learning module to augment the model's reasoning ability by\nmodeling physical knowledge relationships among different objects under\ncounterfactual intervention. Our proposed method is a plug-and-play module that\ncan be incorporated into any baseline. In experiments, we show that our\nproposed method improves baseline methods and achieves state-of-the-art\nperformance. Our source code is available at https:\/\/github.com\/Andy20178\/DCL.","terms":["cs.CV"]},{"titles":"Discrepancy Modeling Framework: Learning missing physics, modeling systematic residuals, and disambiguating between deterministic and random effects","summaries":"Physics-based and first-principles models pervade the engineering and\nphysical sciences, allowing for the ability to model the dynamics of complex\nsystems with a prescribed accuracy. The approximations used in deriving\ngoverning equations often result in discrepancies between the model and\nsensor-based measurements of the system, revealing the approximate nature of\nthe equations and\/or the signal-to-noise ratio of the sensor itself. In modern\ndynamical systems, such discrepancies between model and measurement can lead to\npoor quantification, often undermining the ability to produce accurate and\nprecise control algorithms. We introduce a discrepancy modeling framework to\nidentify the missing physics and resolve the model-measurement mismatch with\ntwo distinct approaches: (i) by learning a model for the evolution of\nsystematic state-space residual, and (ii) by discovering a model for the\ndeterministic dynamical error. Regardless of approach, a common suite of\ndata-driven model discovery methods can be used. The choice of method depends\non one's intent (e.g., mechanistic interpretability) for discrepancy modeling,\nsensor measurement characteristics (e.g., quantity, quality, resolution), and\nconstraints imposed by practical applications (e.g., modeling approaches using\nthe suite of data-driven modeling methods on three continuous dynamical systems\nunder varying signal-to-noise ratios. Finally, we emphasize structural\nshortcomings of each discrepancy modeling approach depending on error type. In\nsummary, if the true dynamics are unknown (i.e., an imperfect model), one\nshould learn a discrepancy model of the missing physics in the dynamical space.\nYet, if the true dynamics are known yet model-measurement mismatch still\nexists, one should learn a discrepancy model in the state space.","terms":["stat.ML","math.DS"]},{"titles":"Transfer learning for improved generalizability in causal physics-informed neural networks for beam simulations","summaries":"This paper introduces a novel methodology for simulating the dynamics of\nbeams on elastic foundations. Specifically, Euler-Bernoulli and Timoshenko beam\nmodels on the Winkler foundation are simulated using a transfer learning\napproach within a causality-respecting physics-informed neural network (PINN)\nframework. Conventional PINNs encounter challenges in handling large space-time\ndomains, even for problems with closed-form analytical solutions. A\ncausality-respecting PINN loss function is employed to overcome this\nlimitation, effectively capturing the underlying physics. However, it is\nobserved that the causality-respecting PINN lacks generalizability. We propose\nusing solutions to similar problems instead of training from scratch by\nemploying transfer learning while adhering to causality to accelerate\nconvergence and ensure accurate results across diverse scenarios. Numerical\nexperiments on the Euler-Bernoulli beam highlight the efficacy of the proposed\napproach for various initial conditions, including those with noise in the\ninitial data. Furthermore, the potential of the proposed method is demonstrated\nfor the Timoshenko beam in an extended spatial and temporal domain. Several\ncomparisons suggest that the proposed method accurately captures the inherent\ndynamics, outperforming the state-of-the-art physics-informed methods under\nstandard $L^2$-norm metric and accelerating convergence.","terms":["cs.LG"]},{"titles":"Real-Time Magnetic Tracking and Diagnosis of COVID-19 via Machine Learning","summaries":"The COVID-19 pandemic underscored the importance of reliable, noninvasive\ndiagnostic tools for robust public health interventions. In this work, we fused\nmagnetic respiratory sensing technology (MRST) with machine learning (ML) to\ncreate a diagnostic platform for real-time tracking and diagnosis of COVID-19\nand other respiratory diseases. The MRST precisely captures breathing patterns\nthrough three specific breath testing protocols: normal breath, holding breath,\nand deep breath. We collected breath data from both COVID-19 patients and\nhealthy subjects in Vietnam using this platform, which then served to train and\nvalidate ML models. Our evaluation encompassed multiple ML algorithms,\nincluding support vector machines and deep learning models, assessing their\nability to diagnose COVID-19. Our multi-model validation methodology ensures a\nthorough comparison and grants the adaptability to select the most optimal\nmodel, striking a balance between diagnostic precision with model\ninterpretability. The findings highlight the exceptional potential of our\ndiagnostic tool in pinpointing respiratory anomalies, achieving over 90%\naccuracy. This innovative sensor technology can be seamlessly integrated into\nhealthcare settings for patient monitoring, marking a significant enhancement\nfor the healthcare infrastructure.","terms":["cs.LG","physics.ins-det","physics.med-ph"]},{"titles":"Event-based Background-Oriented Schlieren","summaries":"Schlieren imaging is an optical technique to observe the flow of transparent\nmedia, such as air or water, without any particle seeding. However,\nconventional frame-based techniques require both high spatial and temporal\nresolution cameras, which impose bright illumination and expensive computation\nlimitations. Event cameras offer potential advantages (high dynamic range, high\ntemporal resolution, and data efficiency) to overcome such limitations due to\ntheir bio-inspired sensing principle. This paper presents a novel technique for\nperceiving air convection using events and frames by providing the first\ntheoretical analysis that connects event data and schlieren. We formulate the\nproblem as a variational optimization one combining the linearized event\ngeneration model with a physically-motivated parameterization that estimates\nthe temporal derivative of the air density. The experiments with accurately\naligned frame- and event camera data reveal that the proposed method enables\nevent cameras to obtain on par results with existing frame-based optical flow\ntechniques. Moreover, the proposed method works under dark conditions where\nframe-based schlieren fails, and also enables slow-motion analysis by\nleveraging the event camera's advantages. Our work pioneers and opens a new\nstack of event camera applications, as we publish the source code as well as\nthe first schlieren dataset with high-quality frame and event data.\nhttps:\/\/github.com\/tub-rip\/event_based_bos","terms":["cs.CV","eess.IV"]},{"titles":"Feature-oriented Deep Learning Framework for Pulmonary Cone-beam CT (CBCT) Enhancement with Multi-task Customized Perceptual Loss","summaries":"Cone-beam computed tomography (CBCT) is routinely collected during\nimage-guided radiation therapy (IGRT) to provide updated patient anatomy\ninformation for cancer treatments. However, CBCT images often suffer from\nstreaking artifacts and noise caused by under-rate sampling projections and\nlow-dose exposure, resulting in low clarity and information loss. While recent\ndeep learning-based CBCT enhancement methods have shown promising results in\nsuppressing artifacts, they have limited performance on preserving anatomical\ndetails since conventional pixel-to-pixel loss functions are incapable of\ndescribing detailed anatomy. To address this issue, we propose a novel\nfeature-oriented deep learning framework that translates low-quality CBCT\nimages into high-quality CT-like imaging via a multi-task customized\nfeature-to-feature perceptual loss function. The framework comprises two main\ncomponents: a multi-task learning feature-selection network(MTFS-Net) for\ncustomizing the perceptual loss function; and a CBCT-to-CT translation network\nguided by feature-to-feature perceptual loss, which uses advanced generative\nmodels such as U-Net, GAN and CycleGAN. Our experiments showed that the\nproposed framework can generate synthesized CT (sCT) images for the lung that\nachieved a high similarity to CT images, with an average SSIM index of 0.9869\nand an average PSNR index of 39.9621. The sCT images also achieved visually\npleasing performance with effective artifacts suppression, noise reduction, and\ndistinctive anatomical details preservation. Our experiment results indicate\nthat the proposed framework outperforms the state-of-the-art models for\npulmonary CBCT enhancement. This framework holds great promise for generating\nhigh-quality anatomical imaging from CBCT that is suitable for various clinical\napplications.","terms":["cs.CV","physics.med-ph"]},{"titles":"A Spatial-Temporal Transformer based Framework For Human Pose Assessment And Correction in Education Scenarios","summaries":"Human pose assessment and correction play a crucial role in applications\nacross various fields, including computer vision, robotics, sports analysis,\nhealthcare, and entertainment. In this paper, we propose a Spatial-Temporal\nTransformer based Framework (STTF) for human pose assessment and correction in\neducation scenarios such as physical exercises and science experiment. The\nframework comprising skeletal tracking, pose estimation, posture assessment,\nand posture correction modules to educate students with professional,\nquick-to-fix feedback. We also create a pose correction method to provide\ncorrective feedback in the form of visual aids. We test the framework with our\nown dataset. It comprises (a) new recordings of five exercises, (b) existing\nrecordings found on the internet of the same exercises, and (c) corrective\nfeedback on the recordings by professional athletes and teachers. Results show\nthat our model can effectively measure and comment on the quality of students'\nactions. The STTF leverages the power of transformer models to capture spatial\nand temporal dependencies in human poses, enabling accurate assessment and\neffective correction of students' movements.","terms":["cs.CV","cs.AI"]},{"titles":"Discussing the Spectra of Physics-Enhanced Machine Learning via a Survey on Structural Mechanics Applications","summaries":"The intersection of physics and machine learning has given rise to a paradigm\nthat we refer to here as physics-enhanced machine learning (PEML), aiming to\nimprove the capabilities and reduce the individual shortcomings of data- or\nphysics-only methods. In this paper, the spectrum of physics-enhanced machine\nlearning methods, expressed across the defining axes of physics and data, is\ndiscussed by engaging in a comprehensive exploration of its characteristics,\nusage, and motivations. In doing so, this paper offers a survey of recent\napplications and developments of PEML techniques, revealing the potency of PEML\nin addressing complex challenges. We further demonstrate application of select\nsuch schemes on the simple working example of a single-degree-of-freedom\nDuffing oscillator, which allows to highlight the individual characteristics\nand motivations of different `genres' of PEML approaches. To promote\ncollaboration and transparency, and to provide practical examples for the\nreader, the code of these working examples is provided alongside this paper. As\na foundational contribution, this paper underscores the significance of PEML in\npushing the boundaries of scientific and engineering research, underpinned by\nthe synergy of physical insights and machine learning capabilities.","terms":["cs.LG"]},{"titles":"Learning Interpretable Low-dimensional Representation via Physical Symmetry","summaries":"Interpretable representation learning has been playing a key role in creative\nintelligent systems. In the music domain, current learning algorithms can\nsuccessfully learn various features such as pitch, timbre, chord, texture, etc.\nHowever, most methods rely heavily on music domain knowledge. It remains an\nopen question what general computational principles give rise to interpretable\nrepresentations, especially low-dim factors that agree with human perception.\nIn this study, we take inspiration from modern physics and use physical\nsymmetry as a self-consistency constraint for the latent space. Specifically,\nit requires the prior model that characterises the dynamics of the latent\nstates to be equivariant with respect to certain group transformations. We show\nthat physical symmetry leads the model to learn a linear pitch factor from\nunlabelled monophonic music audio in a self-supervised fashion. In addition,\nthe same methodology can be applied to computer vision, learning a 3D Cartesian\nspace from videos of a simple moving object without labels. Furthermore,\nphysical symmetry naturally leads to representation augmentation, a new\ntechnique which improves sample efficiency.","terms":["cs.LG","cs.AI"]},{"titles":"Adversarial Examples in the Physical World: A Survey","summaries":"Deep neural networks (DNNs) have demonstrated high vulnerability to\nadversarial examples. Besides the attacks in the digital world, the practical\nimplications of adversarial examples in the physical world present significant\nchallenges and safety concerns. However, current research on physical\nadversarial examples (PAEs) lacks a comprehensive understanding of their unique\ncharacteristics, leading to limited significance and understanding. In this\npaper, we address this gap by thoroughly examining the characteristics of PAEs\nwithin a practical workflow encompassing training, manufacturing, and\nre-sampling processes. By analyzing the links between physical adversarial\nattacks, we identify manufacturing and re-sampling as the primary sources of\ndistinct attributes and particularities in PAEs. Leveraging this knowledge, we\ndevelop a comprehensive analysis and classification framework for PAEs based on\ntheir specific characteristics, covering over 100 studies on physical-world\nadversarial examples. Furthermore, we investigate defense strategies against\nPAEs and identify open challenges and opportunities for future research. We aim\nto provide a fresh, thorough, and systematic understanding of PAEs, thereby\npromoting the development of robust adversarial learning and its application in\nopen-world scenarios.","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"ClimSim: A large multi-scale dataset for hybrid physics-ML climate emulation","summaries":"Modern climate projections lack adequate spatial and temporal resolution due\nto computational constraints. A consequence is inaccurate and imprecise\npredictions of critical processes such as storms. Hybrid methods that combine\nphysics with machine learning (ML) have introduced a new generation of higher\nfidelity climate simulators that can sidestep Moore's Law by outsourcing\ncompute-hungry, short, high-resolution simulations to ML emulators. However,\nthis hybrid ML-physics simulation approach requires domain-specific treatment\nand has been inaccessible to ML experts because of lack of training data and\nrelevant, easy-to-use workflows. We present ClimSim, the largest-ever dataset\ndesigned for hybrid ML-physics research. It comprises multi-scale climate\nsimulations, developed by a consortium of climate scientists and ML\nresearchers. It consists of 5.7 billion pairs of multivariate input and output\nvectors that isolate the influence of locally-nested, high-resolution,\nhigh-fidelity physics on a host climate simulator's macro-scale physical state.\n  The dataset is global in coverage, spans multiple years at high sampling\nfrequency, and is designed such that resulting emulators are compatible with\ndownstream coupling into operational climate simulators. We implement a range\nof deterministic and stochastic regression baselines to highlight the ML\nchallenges and their scoring. The data\n(https:\/\/huggingface.co\/datasets\/LEAP\/ClimSim_high-res) and code\n(https:\/\/leap-stc.github.io\/ClimSim) are released openly to support the\ndevelopment of hybrid ML-physics and high-fidelity climate simulations for the\nbenefit of science and society.","terms":["cs.LG","physics.ao-ph"]},{"titles":"A Unified Framework to Enforce, Discover, and Promote Symmetry in Machine Learning","summaries":"Symmetry is present throughout nature and continues to play an increasingly\ncentral role in physics and machine learning. Fundamental symmetries, such as\nPoincar\\'{e} invariance, allow physical laws discovered in laboratories on\nEarth to be extrapolated to the farthest reaches of the universe. Symmetry is\nessential to achieving this extrapolatory power in machine learning\napplications. For example, translation invariance in image classification\nallows models with fewer parameters, such as convolutional neural networks, to\nbe trained on smaller data sets and achieve state-of-the-art performance. In\nthis paper, we provide a unifying theoretical and methodological framework for\nincorporating symmetry into machine learning models in three ways: 1. enforcing\nknown symmetry when training a model; 2. discovering unknown symmetries of a\ngiven model or data set; and 3. promoting symmetry during training by learning\na model that breaks symmetries within a user-specified group of candidates when\nthere is sufficient evidence in the data. We show that these tasks can be cast\nwithin a common mathematical framework whose central object is the Lie\nderivative associated with fiber-linear Lie group actions on vector bundles. We\nextend and unify several existing results by showing that enforcing and\ndiscovering symmetry are linear-algebraic tasks that are dual with respect to\nthe bilinear structure of the Lie derivative. We also propose a novel way to\npromote symmetry by introducing a class of convex regularization functions\nbased on the Lie derivative and nuclear norm relaxation to penalize symmetry\nbreaking during training of machine learning models. We explain how these ideas\ncan be applied to a wide range of machine learning models including basis\nfunction regression, dynamical systems discovery, multilayer perceptrons, and\nneural networks acting on spatial fields such as images.","terms":["cs.LG","cs.NA","math.DG","math.NA","15B30, 22E15, 22E70, 47D03, 54H15, 57S99, 5808, 58D19, 58K70, 65F55,\n  68Q32, 68T07, 70G65, 70H33, 90C25"]},{"titles":"Multi-task Deep Convolutional Network to Predict Sea Ice Concentration and Drift in the Arctic Ocean","summaries":"Forecasting sea ice concentration (SIC) and sea ice drift (SID) in the Arctic\nOcean is of great significance as the Arctic environment has been changed by\nthe recent warming climate. Given that physical sea ice models require high\ncomputational costs with complex parameterization, deep learning techniques can\neffectively replace the physical model and improve the performance of sea ice\nprediction. This study proposes a novel multi-task fully conventional network\narchitecture named hierarchical information-sharing U-net (HIS-Unet) to predict\ndaily SIC and SID. Instead of learning SIC and SID separately at each branch,\nwe allow the SIC and SID layers to share their information and assist each\nother's prediction through the weighting attention modules (WAMs).\nConsequently, our HIS-Unet outperforms other statistical approaches, sea ice\nphysical models, and neural networks without such information-sharing units.\nThe improvement of HIS-Unet is obvious both for SIC and SID prediction when and\nwhere sea ice conditions change seasonally, which implies that the information\nsharing through WAMs allows the model to learn the sudden changes of SIC and\nSID. The weight values of the WAMs imply that SIC information plays a more\ncritical role in SID prediction, compared to that of SID information in SIC\nprediction, and information sharing is more active in sea ice edges (seasonal\nsea ice) than in the central Arctic (multi-year sea ice).","terms":["cs.LG","cs.CV","physics.ao-ph"]},{"titles":"A Hybrid 3D Eddy Detection Technique Based on Sea Surface Height and Velocity Field","summaries":"Eddy detection is a critical task for ocean scientists to understand and\nanalyze ocean circulation. In this paper, we introduce a hybrid eddy detection\napproach that combines sea surface height (SSH) and velocity fields with\ngeometric criteria defining eddy behavior. Our approach searches for SSH minima\nand maxima, which oceanographers expect to find at the center of eddies.\nGeometric criteria are used to verify expected velocity field properties, such\nas net rotation and symmetry, by tracing velocity components along a circular\npath surrounding each eddy center. Progressive searches outward and into deeper\nlayers yield each eddy's 3D region of influence. Isolation of each eddy\nstructure from the dataset, using it's cylindrical footprint, facilitates\nvisualization of internal eddy structures using horizontal velocity, vertical\nvelocity, temperature and salinity. A quantitative comparison of Okubo-Weiss\nvorticity (OW) thresholding, the standard winding angle, and this new\nSSH-velocity hybrid methods of eddy detection as applied to the Red Sea dataset\nsuggests that detection results are highly dependent on the choices of method,\nthresholds, and criteria. Our new SSH-velocity hybrid detection approach has\nthe advantages of providing eddy structures with verified rotation properties,\n3D visualization of the internal structure of physical properties, and rapid\nefficient estimations of eddy footprints without calculating streamlines. Our\napproach combines visualization of internal structure and tracking overall\nmovement to support the study of the transport mechanisms key to understanding\nthe interaction of nutrient distribution and ocean circulation. Our method is\napplied to three different datasets to showcase the generality of its\napplication.","terms":["cs.CV"]},{"titles":"StairNet: Visual Recognition of Stairs for Human-Robot Locomotion","summaries":"Human-robot walking with prosthetic legs and exoskeletons, especially over\ncomplex terrains such as stairs, remains a significant challenge. Egocentric\nvision has the unique potential to detect the walking environment prior to\nphysical interactions, which can improve transitions to and from stairs. This\nmotivated us to create the StairNet initiative to support the development of\nnew deep learning models for visual sensing and recognition of stairs, with an\nemphasis on lightweight and efficient neural networks for onboard real-time\ninference. In this study, we present an overview of the development of our\nlarge-scale dataset with over 515,000 manually labeled images, as well as our\ndevelopment of different deep learning models (e.g., 2D and 3D CNN, hybrid CNN\nand LSTM, and ViT networks) and training methods (e.g., supervised learning\nwith temporal data and semi-supervised learning with unlabeled images) using\nour new dataset. We consistently achieved high classification accuracy (i.e.,\nup to 98.8%) with different designs, offering trade-offs between model accuracy\nand size. When deployed on mobile devices with GPU and NPU accelerators, our\ndeep learning models achieved inference speeds up to 2.8 ms. We also deployed\nour models on custom-designed CPU-powered smart glasses. However, limitations\nin the embedded hardware yielded slower inference speeds of 1.5 seconds,\npresenting a trade-off between human-centered design and performance. Overall,\nwe showed that StairNet can be an effective platform to develop and study new\nvisual perception systems for human-robot locomotion with applications in\nexoskeleton and prosthetic leg control.","terms":["cs.CV"]},{"titles":"Diffusion Reconstruction of Ultrasound Images with Informative Uncertainty","summaries":"Despite its wide use in medicine, ultrasound imaging faces several challenges\nrelated to its poor signal-to-noise ratio and several sources of noise and\nartefacts. Enhancing ultrasound image quality involves balancing concurrent\nfactors like contrast, resolution, and speckle preservation. In recent years,\nthere has been progress both in model-based and learning-based approaches to\nimprove ultrasound image reconstruction. Bringing the best from both worlds, we\npropose a hybrid approach leveraging advances in diffusion models. To this end,\nwe adapt Denoising Diffusion Restoration Models (DDRM) to incorporate\nultrasound physics through a linear direct model and an unsupervised\nfine-tuning of the prior diffusion model. We conduct comprehensive experiments\non simulated, in-vitro, and in-vivo data, demonstrating the efficacy of our\napproach in achieving high-quality image reconstructions from a single plane\nwave input and in comparison to state-of-the-art methods. Finally, given the\nstochastic nature of the method, we analyse in depth the statistical properties\nof single and multiple-sample reconstructions, experimentally show the\ninformativeness of their variance, and provide an empirical model relating this\nbehaviour to speckle noise. The code and data are available at: (upon\nacceptance).","terms":["cs.CV","cs.LG"]},{"titles":"Generative Learning of Continuous Data by Tensor Networks","summaries":"Beyond their origin in modeling many-body quantum systems, tensor networks\nhave emerged as a promising class of models for solving machine learning\nproblems, notably in unsupervised generative learning. While possessing many\ndesirable features arising from their quantum-inspired nature, tensor network\ngenerative models have previously been largely restricted to binary or\ncategorical data, limiting their utility in real-world modeling problems. We\novercome this by introducing a new family of tensor network generative models\nfor continuous data, which are capable of learning from distributions\ncontaining continuous random variables. We develop our method in the setting of\nmatrix product states, first deriving a universal expressivity theorem proving\nthe ability of this model family to approximate any reasonably smooth\nprobability density function with arbitrary precision. We then benchmark the\nperformance of this model on several synthetic and real-world datasets, finding\nthat the model learns and generalizes well on distributions of continuous and\ndiscrete variables. We develop methods for modeling different data domains, and\nintroduce a trainable compression layer which is found to increase model\nperformance given limited memory or computational resources. Overall, our\nmethods give important theoretical and empirical evidence of the efficacy of\nquantum-inspired methods for the rapidly growing field of generative learning.","terms":["cs.LG","cond-mat.stat-mech","quant-ph","stat.ML"]},{"titles":"Requirement falsification for cyber-physical systems using generative models","summaries":"We present the OGAN algorithm for automatic requirement falsification of\ncyber-physical systems. System inputs and output are represented as piecewise\nconstant signals over time while requirements are expressed in signal temporal\nlogic. OGAN can find inputs that are counterexamples for the safety of a system\nrevealing design, software, or hardware defects before the system is taken into\noperation. The OGAN algorithm works by training a generative machine learning\nmodel to produce such counterexamples. It executes tests atomically and does\nnot require any previous model of the system under test. We evaluate OGAN using\nthe ARCH-COMP benchmark problems, and the experimental results show that\ngenerative models are a viable method for requirement falsification. OGAN can\nbe applied to new systems with little effort, has few requirements for the\nsystem under test, and exhibits state-of-the-art CPS falsification efficiency\nand effectiveness.","terms":["cs.LG","cs.SE"]},{"titles":"AMPose: Alternately Mixed Global-Local Attention Model for 3D Human Pose Estimation","summaries":"The graph convolutional networks (GCNs) have been applied to model the\nphysically connected and non-local relations among human joints for 3D human\npose estimation (HPE). In addition, the purely Transformer-based models\nrecently show promising results in video-based 3D HPE. However, the\nsingle-frame method still needs to model the physically connected relations\namong joints because the feature representations transformed only by global\nrelations via the Transformer neglect information on the human skeleton. To\ndeal with this problem, we propose a novel method in which the Transformer\nencoder and GCN blocks are alternately stacked, namely AMPose, to combine the\nglobal and physically connected relations among joints towards HPE. In the\nAMPose, the Transformer encoder is applied to connect each joint with all the\nother joints, while GCNs are applied to capture information on physically\nconnected relations. The effectiveness of our proposed method is evaluated on\nthe Human3.6M dataset. Our model also shows better generalization ability by\ntesting on the MPI-INF-3DHP dataset. Code can be retrieved at\nhttps:\/\/github.com\/erikervalid\/AMPose.","terms":["cs.CV","cs.MM"]},{"titles":"Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory","summaries":"This book aims to provide an introduction to the topic of deep learning\nalgorithms. We review essential components of deep learning algorithms in full\nmathematical detail including different artificial neural network (ANN)\narchitectures (such as fully-connected feedforward ANNs, convolutional ANNs,\nrecurrent ANNs, residual ANNs, and ANNs with batch normalization) and different\noptimization algorithms (such as the basic stochastic gradient descent (SGD)\nmethod, accelerated methods, and adaptive methods). We also cover several\ntheoretical aspects of deep learning algorithms such as approximation\ncapacities of ANNs (including a calculus for ANNs), optimization theory\n(including Kurdyka-{\\L}ojasiewicz inequalities), and generalization errors. In\nthe last part of the book some deep learning approximation methods for PDEs are\nreviewed including physics-informed neural networks (PINNs) and deep Galerkin\nmethods. We hope that this book will be useful for students and scientists who\ndo not yet have any background in deep learning at all and would like to gain a\nsolid foundation as well as for practitioners who would like to obtain a firmer\nmathematical understanding of the objects and methods considered in deep\nlearning.","terms":["cs.LG","cs.AI","cs.NA","math.NA","math.PR","stat.ML","68T07"]},{"titles":"Separable Physics-Informed Neural Networks","summaries":"Physics-informed neural networks (PINNs) have recently emerged as promising\ndata-driven PDE solvers showing encouraging results on various PDEs. However,\nthere is a fundamental limitation of training PINNs to solve multi-dimensional\nPDEs and approximate highly complex solution functions. The number of training\npoints (collocation points) required on these challenging PDEs grows\nsubstantially, but it is severely limited due to the expensive computational\ncosts and heavy memory overhead. To overcome this issue, we propose a network\narchitecture and training algorithm for PINNs. The proposed method, separable\nPINN (SPINN), operates on a per-axis basis to significantly reduce the number\nof network propagations in multi-dimensional PDEs unlike point-wise processing\nin conventional PINNs. We also propose using forward-mode automatic\ndifferentiation to reduce the computational cost of computing PDE residuals,\nenabling a large number of collocation points (>10^7) on a single commodity\nGPU. The experimental results show drastically reduced computational costs (62x\nin wall-clock time, 1,394x in FLOPs given the same number of collocation\npoints) in multi-dimensional PDEs while achieving better accuracy. Furthermore,\nwe present that SPINN can solve a chaotic (2+1)-d Navier-Stokes equation\nsignificantly faster than the best-performing prior method (9 minutes vs 10\nhours in a single GPU), maintaining accuracy. Finally, we showcase that SPINN\ncan accurately obtain the solution of a highly nonlinear and multi-dimensional\nPDE, a (3+1)-d Navier-Stokes equation. For visualized results and code, please\nsee https:\/\/jwcho5576.github.io\/spinn.github.io\/.","terms":["cs.LG","cs.AI"]},{"titles":"Self-supervised Pre-training for Precipitation Post-processor","summaries":"Securing sufficient forecast lead time for local precipitation is essential\nfor preventing hazardous weather events. Nonetheless, global warming-induced\nclimate change is adding to the challenge of accurately predicting severe\nprecipitation events, such as heavy rainfall. In this work, we propose a deep\nlearning-based precipitation post-processor approach to numerical weather\nprediction (NWP) models. The precipitation post-processor consists of (i)\nself-supervised pre-training, where parameters of encoder are pre-trained on\nthe reconstruction of masked variables of the atmospheric physics domain, and\n(ii) transfer learning on precipitation segmentation tasks (target domain) from\nthe pre-trained encoder. We also introduce a heuristic labeling approach for\neffectively training class-imbalanced datasets. Our experiment results in\nprecipitation correction for regional NWP show that the proposed method\noutperforms other approaches.","terms":["cs.LG","cs.AI"]},{"titles":"Understanding and Visualizing Droplet Distributions in Simulations of Shallow Clouds","summaries":"Thorough analysis of local droplet-level interactions is crucial to better\nunderstand the microphysical processes in clouds and their effect on the global\nclimate. High-accuracy simulations of relevant droplet size distributions from\nLarge Eddy Simulations (LES) of bin microphysics challenge current analysis\ntechniques due to their high dimensionality involving three spatial dimensions,\ntime, and a continuous range of droplet sizes. Utilizing the compact latent\nrepresentations from Variational Autoencoders (VAEs), we produce novel and\nintuitive visualizations for the organization of droplet sizes and their\nevolution over time beyond what is possible with clustering techniques. This\ngreatly improves interpretation and allows us to examine aerosol-cloud\ninteractions by contrasting simulations with different aerosol concentrations.\nWe find that the evolution of the droplet spectrum is similar across aerosol\nlevels but occurs at different paces. This similarity suggests that\nprecipitation initiation processes are alike despite variations in onset times.","terms":["cs.LG","physics.ao-ph","physics.flu-dyn"]},{"titles":"Refined Equivalent Pinhole Model for Large-scale 3D Reconstruction from Spaceborne CCD Imagery","summaries":"In this study, we present a large-scale earth surface reconstruction pipeline\nfor linear-array charge-coupled device (CCD) satellite imagery. While\nmainstream satellite image-based reconstruction approaches perform\nexceptionally well, the rational functional model (RFM) is subject to several\nlimitations. For example, the RFM has no rigorous physical interpretation and\ndiffers significantly from the pinhole imaging model; hence, it cannot be\ndirectly applied to learning-based 3D reconstruction networks and to more novel\nreconstruction pipelines in computer vision. Hence, in this study, we introduce\na method in which the RFM is equivalent to the pinhole camera model (PCM),\nmeaning that the internal and external parameters of the pinhole camera are\nused instead of the rational polynomial coefficient parameters. We then derive\nan error formula for this equivalent pinhole model for the first time,\ndemonstrating the influence of the image size on the accuracy of the\nreconstruction. In addition, we propose a polynomial image refinement model\nthat minimizes equivalent errors via the least squares method. The experiments\nwere conducted using four image datasets: WHU-TLC, DFC2019, ISPRS-ZY3, and GF7.\nThe results demonstrated that the reconstruction accuracy was proportional to\nthe image size. Our polynomial image refinement model significantly enhanced\nthe accuracy and completeness of the reconstruction, and achieved more\nsignificant improvements for larger-scale images.","terms":["cs.CV","eess.IV"]},{"titles":"$p$-Poisson surface reconstruction in curl-free flow from point clouds","summaries":"The aim of this paper is the reconstruction of a smooth surface from an\nunorganized point cloud sampled by a closed surface, with the preservation of\ngeometric shapes, without any further information other than the point cloud.\nImplicit neural representations (INRs) have recently emerged as a promising\napproach to surface reconstruction. However, the reconstruction quality of\nexisting methods relies on ground truth implicit function values or surface\nnormal vectors. In this paper, we show that proper supervision of partial\ndifferential equations and fundamental properties of differential vector fields\nare sufficient to robustly reconstruct high-quality surfaces. We cast the\n$p$-Poisson equation to learn a signed distance function (SDF) and the\nreconstructed surface is implicitly represented by the zero-level set of the\nSDF. For efficient training, we develop a variable splitting structure by\nintroducing a gradient of the SDF as an auxiliary variable and impose the\n$p$-Poisson equation directly on the auxiliary variable as a hard constraint.\nBased on the curl-free property of the gradient field, we impose a curl-free\nconstraint on the auxiliary variable, which leads to a more faithful\nreconstruction. Experiments on standard benchmark datasets show that the\nproposed INR provides a superior and robust reconstruction. The code is\navailable at \\url{https:\/\/github.com\/Yebbi\/PINC}.","terms":["cs.CV","cs.CG","math-ph","math.MP"]},{"titles":"LinFlo-Net: A two-stage deep learning method to generate simulation ready meshes of the heart","summaries":"We present a deep learning model to automatically generate computer models of\nthe human heart from patient imaging data with an emphasis on its capability to\ngenerate thin-walled cardiac structures. Our method works by deforming a\ntemplate mesh to fit the cardiac structures to the given image. Compared with\nprior deep learning methods that adopted this approach, our framework is\ndesigned to minimize mesh self-penetration, which typically arises when\ndeforming surface meshes separated by small distances. We achieve this by using\na two-stage diffeomorphic deformation process along with a novel loss function\nderived from the kinematics of motion that penalizes surface contact and\ninterpenetration. Our model demonstrates comparable accuracy with\nstate-of-the-art methods while additionally producing meshes free of\nself-intersections. The resultant meshes are readily usable in physics based\nsimulation, minimizing the need for post-processing and cleanup.","terms":["cs.CV","cs.LG"]},{"titles":"Debias Coarsely, Sample Conditionally: Statistical Downscaling through Optimal Transport and Probabilistic Diffusion Models","summaries":"We introduce a two-stage probabilistic framework for statistical downscaling\nusing unpaired data. Statistical downscaling seeks a probabilistic map to\ntransform low-resolution data from a biased coarse-grained numerical scheme to\nhigh-resolution data that is consistent with a high-fidelity scheme. Our\nframework tackles the problem by composing two transformations: (i) a debiasing\nstep via an optimal transport map, and (ii) an upsampling step achieved by a\nprobabilistic diffusion model with a posteriori conditional sampling. This\napproach characterizes a conditional distribution without needing paired data,\nand faithfully recovers relevant physical statistics from biased samples. We\ndemonstrate the utility of the proposed approach on one- and two-dimensional\nfluid flow problems, which are representative of the core difficulties present\nin numerical simulations of weather and climate. Our method produces realistic\nhigh-resolution outputs from low-resolution inputs, by upsampling resolutions\nof 8x and 16x. Moreover, our procedure correctly matches the statistics of\nphysical quantities, even when the low-frequency content of the inputs and\noutputs do not match, a crucial but difficult-to-satisfy assumption needed by\ncurrent state-of-the-art alternatives. Code for this work is available at:\nhttps:\/\/github.com\/google-research\/swirl-dynamics\/tree\/main\/swirl_dynamics\/projects\/probabilistic_diffusion.","terms":["cs.LG","physics.app-ph"]},{"titles":"Stabilized Neural Differential Equations for Learning Dynamics with Explicit Constraints","summaries":"Many successful methods to learn dynamical systems from data have recently\nbeen introduced. However, ensuring that the inferred dynamics preserve known\nconstraints, such as conservation laws or restrictions on the allowed system\nstates, remains challenging. We propose stabilized neural differential\nequations (SNDEs), a method to enforce arbitrary manifold constraints for\nneural differential equations. Our approach is based on a stabilization term\nthat, when added to the original dynamics, renders the constraint manifold\nprovably asymptotically stable. Due to its simplicity, our method is compatible\nwith all common neural differential equation (NDE) models and broadly\napplicable. In extensive empirical evaluations, we demonstrate that SNDEs\noutperform existing methods while broadening the types of constraints that can\nbe incorporated into NDE training.","terms":["cs.LG","physics.comp-ph","stat.ML"]},{"titles":"Multiscale Feature Attribution for Outliers","summaries":"Machine learning techniques can automatically identify outliers in massive\ndatasets, much faster and more reproducible than human inspection ever could.\nBut finding such outliers immediately leads to the question: which features\nrender this input anomalous? We propose a new feature attribution method,\nInverse Multiscale Occlusion, that is specifically designed for outliers, for\nwhich we have little knowledge of the type of features we want to identify and\nexpect that the model performance is questionable because anomalous test data\nlikely exceed the limits of the training data. We demonstrate our method on\noutliers detected in galaxy spectra from the Dark Energy Survey Instrument and\nfind its results to be much more interpretable than alternative attribution\napproaches.","terms":["cs.LG","astro-ph.IM","cs.AI"]},{"titles":"Emotional Theory of Mind: Bridging Fast Visual Processing with Slow Linguistic Reasoning","summaries":"The emotional theory of mind problem in images is an emotion recognition\ntask, specifically asking \"How does the person in the bounding box feel?\"\nFacial expressions, body pose, contextual information and implicit commonsense\nknowledge all contribute to the difficulty of the task, making this task\ncurrently one of the hardest problems in affective computing. The goal of this\nwork is to evaluate the emotional commonsense knowledge embedded in recent\nlarge vision language models (CLIP, LLaVA) and large language models (GPT-3.5)\non the Emotions in Context (EMOTIC) dataset. In order to evaluate a purely\ntext-based language model on images, we construct \"narrative captions\" relevant\nto emotion perception, using a set of 872 physical social signal descriptions\nrelated to 26 emotional categories, along with 224 labels for emotionally\nsalient environmental contexts, sourced from writer's guides for character\nexpressions and settings. We evaluate the use of the resulting captions in an\nimage-to-language-to-emotion task. Experiments using zero-shot vision-language\nmodels on EMOTIC show that combining \"fast\" and \"slow\" reasoning is a promising\nway forward to improve emotion recognition systems. Nevertheless, a gap remains\nin the zero-shot emotional theory of mind task compared to prior work trained\non the EMOTIC dataset.","terms":["cs.CV"]},{"titles":"The Acquisition of Physical Knowledge in Generative Neural Networks","summaries":"As children grow older, they develop an intuitive understanding of the\nphysical processes around them. Their physical understanding develops in\nstages, moving along developmental trajectories which have been mapped out\nextensively in previous empirical research. Here, we investigate how the\nlearning trajectories of deep generative neural networks compare to children's\ndevelopmental trajectories using physical understanding as a testbed. We\noutline an approach that allows us to examine two distinct hypotheses of human\ndevelopment - stochastic optimization and complexity increase. We find that\nwhile our models are able to accurately predict a number of physical processes,\ntheir learning trajectories under both hypotheses do not follow the\ndevelopmental trajectories of children.","terms":["cs.LG","q-bio.NC"]},{"titles":"Sim2Real for Environmental Neural Processes","summaries":"Machine learning (ML)-based weather models have recently undergone rapid\nimprovements. These models are typically trained on gridded reanalysis data\nfrom numerical data assimilation systems. However, reanalysis data comes with\nlimitations, such as assumptions about physical laws and low spatiotemporal\nresolution. The gap between reanalysis and reality has sparked growing interest\nin training ML models directly on observations such as weather stations.\nModelling scattered and sparse environmental observations requires scalable and\nflexible ML architectures, one of which is the convolutional conditional neural\nprocess (ConvCNP). ConvCNPs can learn to condition on both gridded and\noff-the-grid context data to make uncertainty-aware predictions at target\nlocations. However, the sparsity of real observations presents a challenge for\ndata-hungry deep learning models like the ConvCNP. One potential solution is\n'Sim2Real': pre-training on reanalysis and fine-tuning on observational data.\nWe analyse Sim2Real with a ConvCNP trained to interpolate surface air\ntemperature over Germany, using varying numbers of weather stations for\nfine-tuning. On held-out weather stations, Sim2Real training substantially\noutperforms the same model architecture trained only with reanalysis data or\nonly with station data, showing that reanalysis data can serve as a stepping\nstone for learning from real observations. Sim2Real could thus enable more\naccurate models for weather prediction and climate monitoring.","terms":["cs.LG","physics.ao-ph"]},{"titles":"Perception Test: A Diagnostic Benchmark for Multimodal Video Models","summaries":"We propose a novel multimodal video benchmark - the Perception Test - to\nevaluate the perception and reasoning skills of pre-trained multimodal models\n(e.g. Flamingo, SeViLA, or GPT-4). Compared to existing benchmarks that focus\non computational tasks (e.g. classification, detection or tracking), the\nPerception Test focuses on skills (Memory, Abstraction, Physics, Semantics) and\ntypes of reasoning (descriptive, explanatory, predictive, counterfactual)\nacross video, audio, and text modalities, to provide a comprehensive and\nefficient evaluation tool. The benchmark probes pre-trained models for their\ntransfer capabilities, in a zero-shot \/ few-shot or limited finetuning regime.\nFor these purposes, the Perception Test introduces 11.6k real-world videos, 23s\naverage length, designed to show perceptually interesting situations, filmed by\naround 100 participants worldwide. The videos are densely annotated with six\ntypes of labels (multiple-choice and grounded video question-answers, object\nand point tracks, temporal action and sound segments), enabling both language\nand non-language evaluations. The fine-tuning and validation splits of the\nbenchmark are publicly available (CC-BY license), in addition to a challenge\nserver with a held-out test split. Human baseline results compared to\nstate-of-the-art video QA models show a substantial gap in performance (91.4%\nvs 46.2%), suggesting that there is significant room for improvement in\nmultimodal video understanding.\n  Dataset, baseline code, and challenge server are available at\nhttps:\/\/github.com\/deepmind\/perception_test","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"Can input reconstruction be used to directly estimate uncertainty of a regression U-Net model? -- Application to proton therapy dose prediction for head and neck cancer patients","summaries":"Estimating the uncertainty of deep learning models in a reliable and\nefficient way has remained an open problem, where many different solutions have\nbeen proposed in the literature. Most common methods are based on Bayesian\napproximations, like Monte Carlo dropout (MCDO) or Deep ensembling (DE), but\nthey have a high inference time (i.e. require multiple inference passes) and\nmight not work for out-of-distribution detection (OOD) data (i.e. similar\nuncertainty for in-distribution (ID) and OOD). In safety critical environments,\nlike medical applications, accurate and fast uncertainty estimation methods,\nable to detect OOD data, are crucial, since wrong predictions can jeopardize\npatients safety. In this study, we present an alternative direct uncertainty\nestimation method and apply it for a regression U-Net architecture. The method\nconsists in the addition of a branch from the bottleneck which reconstructs the\ninput. The input reconstruction error can be used as a surrogate of the model\nuncertainty. For the proof-of-concept, our method is applied to proton therapy\ndose prediction in head and neck cancer patients. Accuracy, time-gain, and OOD\ndetection are analyzed for our method in this particular application and\ncompared with the popular MCDO and DE. The input reconstruction method showed a\nhigher Pearson correlation coefficient with the prediction error (0.620) than\nDE and MCDO (between 0.447 and 0.612). Moreover, our method allows an easier\nidentification of OOD (Z-score of 34.05). It estimates the uncertainty\nsimultaneously to the regression task, therefore requires less time or\ncomputational resources.","terms":["cs.LG","cs.AI","physics.med-ph"]},{"titles":"Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems","summaries":"Deep equivariant models use symmetries to improve sample efficiency and\ngeneralization. However, the assumption of perfect symmetry in many of these\nmodels can sometimes be restrictive, especially when the data does not\nperfectly align with such symmetries. Thus, we introduce relaxed octahedral\ngroup convolution for modeling 3D physical systems in this paper. This flexible\nconvolution technique provably allows the model to both maintain the highest\nlevel of equivariance that is consistent with data and discover the subtle\nsymmetry-breaking factors in the physical systems. Empirical results validate\nthat our approach can not only provide insights into the symmetry-breaking\nfactors in phase transitions but also achieves superior performance in fluid\nsuper-resolution tasks.","terms":["cs.LG","cs.AI"]},{"titles":"Operator Learning Enhanced Physics-informed Neural Networks for Solving Partial Differential Equations Characterized by Sharp Solutions","summaries":"Physics-informed Neural Networks (PINNs) have been shown as a promising\napproach for solving both forward and inverse problems of partial differential\nequations (PDEs). Meanwhile, the neural operator approach, including methods\nsuch as Deep Operator Network (DeepONet) and Fourier neural operator (FNO), has\nbeen introduced and extensively employed in approximating solution of PDEs.\nNevertheless, to solve problems consisting of sharp solutions poses a\nsignificant challenge when employing these two approaches. To address this\nissue, we propose in this work a novel framework termed Operator Learning\nEnhanced Physics-informed Neural Networks (OL-PINN). Initially, we utilize\nDeepONet to learn the solution operator for a set of smooth problems relevant\nto the PDEs characterized by sharp solutions. Subsequently, we integrate the\npre-trained DeepONet with PINN to resolve the target sharp solution problem. We\nshowcase the efficacy of OL-PINN by successfully addressing various problems,\nsuch as the nonlinear diffusion-reaction equation, the Burgers equation and the\nincompressible Navier-Stokes equation at high Reynolds number. Compared with\nthe vanilla PINN, the proposed method requires only a small number of residual\npoints to achieve a strong generalization capability. Moreover, it\nsubstantially enhances accuracy, while also ensuring a robust training process.\nFurthermore, OL-PINN inherits the advantage of PINN for solving inverse\nproblems. To this end, we apply the OL-PINN approach for solving problems with\nonly partial boundary conditions, which usually cannot be solved by the\nclassical numerical methods, showing its capacity in solving ill-posed problems\nand consequently more complex inverse problems.","terms":["cs.LG","cs.NA","math.NA"]},{"titles":"TensorNet: Cartesian Tensor Representations for Efficient Learning of Molecular Potentials","summaries":"The development of efficient machine learning models for molecular systems\nrepresentation is becoming crucial in scientific research. We introduce\nTensorNet, an innovative O(3)-equivariant message-passing neural network\narchitecture that leverages Cartesian tensor representations. By using\nCartesian tensor atomic embeddings, feature mixing is simplified through matrix\nproduct operations. Furthermore, the cost-effective decomposition of these\ntensors into rotation group irreducible representations allows for the separate\nprocessing of scalars, vectors, and tensors when necessary. Compared to\nhigher-rank spherical tensor models, TensorNet demonstrates state-of-the-art\nperformance with significantly fewer parameters. For small molecule potential\nenergies, this can be achieved even with a single interaction layer. As a\nresult of all these properties, the model's computational cost is substantially\ndecreased. Moreover, the accurate prediction of vector and tensor molecular\nquantities on top of potential energies and forces is possible. In summary,\nTensorNet's framework opens up a new space for the design of state-of-the-art\nequivariant models.","terms":["cs.LG","physics.chem-ph","physics.comp-ph"]},{"titles":"Introducing instance label correlation in multiple instance learning. Application to cancer detection on histopathological images","summaries":"In the last years, the weakly supervised paradigm of multiple instance\nlearning (MIL) has become very popular in many different areas. A paradigmatic\nexample is computational pathology, where the lack of patch-level labels for\nwhole-slide images prevents the application of supervised models. Probabilistic\nMIL methods based on Gaussian Processes (GPs) have obtained promising results\ndue to their excellent uncertainty estimation capabilities. However, these are\ngeneral-purpose MIL methods that do not take into account one important fact:\nin (histopathological) images, the labels of neighboring patches are expected\nto be correlated. In this work, we extend a state-of-the-art GP-based MIL\nmethod, which is called VGPMIL-PR, to exploit such correlation. To do so, we\ndevelop a novel coupling term inspired by the statistical physics Ising model.\nWe use variational inference to estimate all the model parameters.\nInterestingly, the VGPMIL-PR formulation is recovered when the weight that\nregulates the strength of the Ising term vanishes. The performance of the\nproposed method is assessed in two real-world problems of prostate cancer\ndetection. We show that our model achieves better results than other\nstate-of-the-art probabilistic MIL methods. We also provide different\nvisualizations and analysis to gain insights into the influence of the novel\nIsing term. These insights are expected to facilitate the application of the\nproposed model to other research areas.","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"Approximate information maximization for bandit games","summaries":"Entropy maximization and free energy minimization are general physical\nprinciples for modeling the dynamics of various physical systems. Notable\nexamples include modeling decision-making within the brain using the\nfree-energy principle, optimizing the accuracy-complexity trade-off when\naccessing hidden variables with the information bottleneck principle (Tishby et\nal., 2000), and navigation in random environments using information\nmaximization (Vergassola et al., 2007). Built on this principle, we propose a\nnew class of bandit algorithms that maximize an approximation to the\ninformation of a key variable within the system. To this end, we develop an\napproximated analytical physics-based representation of an entropy to forecast\nthe information gain of each action and greedily choose the one with the\nlargest information gain. This method yields strong performances in classical\nbandit settings. Motivated by its empirical success, we prove its asymptotic\noptimality for the two-armed bandit problem with Gaussian rewards. Owing to its\nability to encompass the system's properties in a global physical functional,\nthis approach can be efficiently adapted to more complex bandit settings,\ncalling for further investigation of information maximization approaches for\nmulti-armed bandit problems.","terms":["stat.ML","cs.LG"]},{"titles":"From Continuous Dynamics to Graph Neural Networks: Neural Diffusion and Beyond","summaries":"Graph neural networks (GNNs) have demonstrated significant promise in\nmodelling relational data and have been widely applied in various fields of\ninterest. The key mechanism behind GNNs is the so-called message passing where\ninformation is being iteratively aggregated to central nodes from their\nneighbourhood. Such a scheme has been found to be intrinsically linked to a\nphysical process known as heat diffusion, where the propagation of GNNs\nnaturally corresponds to the evolution of heat density. Analogizing the process\nof message passing to the heat dynamics allows to fundamentally understand the\npower and pitfalls of GNNs and consequently informs better model design.\nRecently, there emerges a plethora of works that proposes GNNs inspired from\nthe continuous dynamics formulation, in an attempt to mitigate the known\nlimitations of GNNs, such as oversmoothing and oversquashing. In this survey,\nwe provide the first systematic and comprehensive review of studies that\nleverage the continuous perspective of GNNs. To this end, we introduce\nfoundational ingredients for adapting continuous dynamics to GNNs, along with a\ngeneral framework for the design of graph neural dynamics. We then review and\ncategorize existing works based on their driven mechanisms and underlying\ndynamics. We also summarize how the limitations of classic GNNs can be\naddressed under the continuous framework. We conclude by identifying multiple\nopen research directions.","terms":["cs.LG","cs.AI","stat.ML"]},{"titles":"A practical PINN framework for multi-scale problems with multi-magnitude loss terms","summaries":"For multi-scale problems, the conventional physics-informed neural networks\n(PINNs) face some challenges in obtaining available predictions. In this paper,\nbased on PINNs, we propose a practical deep learning framework for multi-scale\nproblems by reconstructing the loss function and associating it with special\nneural network architectures. New PINN methods derived from the improved PINN\nframework differ from the conventional PINN method mainly in two aspects.\nFirst, the new methods use a novel loss function by modifying the standard loss\nfunction through a (grouping) regularization strategy. The regularization\nstrategy implements a different power operation on each loss term so that all\nloss terms composing the loss function are of approximately the same order of\nmagnitude, which makes all loss terms be optimized synchronously during the\noptimization process. Second, for the multi-frequency or high-frequency\nproblems, in addition to using the modified loss function, new methods upgrade\nthe neural network architecture from the common fully-connected neural network\nto special network architectures such as the Fourier feature architecture, and\nthe integrated architecture developed by us. The combination of the above two\ntechniques leads to a significant improvement in the computational accuracy of\nmulti-scale problems. Several challenging numerical examples demonstrate the\neffectiveness of the proposed methods. The proposed methods not only\nsignificantly outperform the conventional PINN method in terms of computational\nefficiency and computational accuracy, but also compare favorably with the\nstate-of-the-art methods in the recent literature. The improved PINN framework\nfacilitates better application of PINNs to multi-scale problems.","terms":["cs.LG"]},{"titles":"ODE-based Recurrent Model-free Reinforcement Learning for POMDPs","summaries":"Neural ordinary differential equations (ODEs) are widely recognized as the\nstandard for modeling physical mechanisms, which help to perform approximate\ninference in unknown physical or biological environments. In partially\nobservable (PO) environments, how to infer unseen information from raw\nobservations puzzled the agents. By using a recurrent policy with a compact\ncontext, context-based reinforcement learning provides a flexible way to\nextract unobservable information from historical transitions. To help the agent\nextract more dynamics-related information, we present a novel ODE-based\nrecurrent model combines with model-free reinforcement learning (RL) framework\nto solve partially observable Markov decision processes (POMDPs). We\nexperimentally demonstrate the efficacy of our methods across various PO\ncontinuous control and meta-RL tasks. Furthermore, our experiments illustrate\nthat our method is robust against irregular observations, owing to the ability\nof ODEs to model irregularly-sampled time series.","terms":["cs.LG","cs.AI"]},{"titles":"Physics-Driven ML-Based Modelling for Correcting Inverse Estimation","summaries":"When deploying machine learning estimators in science and engineering (SAE)\ndomains, it is critical to avoid failed estimations that can have disastrous\nconsequences, e.g., in aero engine design. This work focuses on detecting and\ncorrecting failed state estimations before adopting them in SAE inverse\nproblems, by utilizing simulations and performance metrics guided by physical\nlaws. We suggest to flag a machine learning estimation when its physical model\nerror exceeds a feasible threshold, and propose a novel approach, GEESE, to\ncorrect it through optimization, aiming at delivering both low error and high\nefficiency. The key designs of GEESE include (1) a hybrid surrogate error model\nto provide fast error estimations to reduce simulation cost and to enable\ngradient based backpropagation of error feedback, and (2) two generative models\nto approximate the probability distributions of the candidate states for\nsimulating the exploitation and exploration behaviours. All three models are\nconstructed as neural networks. GEESE is tested on three real-world SAE inverse\nproblems and compared to a number of state-of-the-art optimization\/search\napproaches. Results show that it fails the least number of times in terms of\nfinding a feasible state correction, and requires physical evaluations less\nfrequently in general.","terms":["cs.LG","cs.NE","78M50, 68T05"]},{"titles":"A foundational neural operator that continuously learns without forgetting","summaries":"Machine learning has witnessed substantial growth, leading to the development\nof advanced artificial intelligence models crafted to address a wide range of\nreal-world challenges spanning various domains, such as computer vision,\nnatural language processing, and scientific computing. Nevertheless, the\ncreation of custom models for each new task remains a resource-intensive\nundertaking, demanding considerable computational time and memory resources. In\nthis study, we introduce the concept of the Neural Combinatorial Wavelet Neural\nOperator (NCWNO) as a foundational model for scientific computing. This model\nis specifically designed to excel in learning from a diverse spectrum of\nphysics and continuously adapt to the solution operators associated with\nparametric partial differential equations (PDEs). The NCWNO leverages a gated\nstructure that employs local wavelet experts to acquire shared features across\nmultiple physical systems, complemented by a memory-based ensembling approach\namong these local wavelet experts. This combination enables rapid adaptation to\nnew challenges. The proposed foundational model offers two key advantages: (i)\nit can simultaneously learn solution operators for multiple parametric PDEs,\nand (ii) it can swiftly generalize to new parametric PDEs with minimal\nfine-tuning. The proposed NCWNO is the first foundational operator learning\nalgorithm distinguished by its (i) robustness against catastrophic forgetting,\n(ii) the maintenance of positive transfer for new parametric PDEs, and (iii)\nthe facilitation of knowledge transfer across dissimilar tasks. Through an\nextensive set of benchmark examples, we demonstrate that the NCWNO can\noutperform task-specific baseline operator learning frameworks with minimal\nhyperparameter tuning at the prediction stage. We also show that with minimal\nfine-tuning, the NCWNO performs accurate combinatorial learning of new\nparametric PDEs.","terms":["cs.LG"]},{"titles":"A Data-driven Recommendation Framework for Optimal Walker Designs","summaries":"The rapidly advancing fields of statistical modeling and machine learning\nhave significantly enhanced data-driven design and optimization. This paper\nfocuses on leveraging these design algorithms to optimize a medical walker, an\nintegral part of gait rehabilitation and physiological therapy of the lower\nextremities. To achieve the desirable qualities of a walker, we train a\npredictive machine-learning model to identify trade-offs between performance\nobjectives, thus enabling the use of efficient optimization algorithms. To do\nthis, we use an Automated Machine Learning model utilizing a stacked-ensemble\napproach shown to outperform traditional ML models. However, training a\npredictive model requires vast amounts of data for accuracy. Due to limited\npublicly available walker designs, this paper presents a dataset of more than\n5,000 parametric walker designs with performance values to assess mass,\nstructural integrity, and stability. These performance values include\ndisplacement vectors for the given load case, stress coefficients, mass, and\nother physical properties. We also introduce a novel method of systematically\ncalculating the stability index of a walker. We use MultiObjective\nCounterfactuals for Design (MCD), a novel genetic-based optimization algorithm,\nto explore the diverse 16-dimensional design space and search for\nhigh-performing designs based on numerous objectives. This paper presents\npotential walker designs that demonstrate up to a 30% mass reduction while\nincreasing structural stability and integrity. This work takes a step toward\nthe improved development of assistive mobility devices.","terms":["cs.LG","cs.NE"]},{"titles":"Domain Agnostic Fourier Neural Operators","summaries":"Fourier neural operators (FNOs) can learn highly nonlinear mappings between\nfunction spaces, and have recently become a popular tool for learning responses\nof complex physical systems. However, to achieve good accuracy and efficiency,\nFNOs rely on the Fast Fourier transform (FFT), which is restricted to modeling\nproblems on rectangular domains. To lift such a restriction and permit FFT on\nirregular geometries as well as topology changes, we introduce domain agnostic\nFourier neural operator (DAFNO), a novel neural operator architecture for\nlearning surrogates with irregular geometries and evolving domains. The key\nidea is to incorporate a smoothed characteristic function in the integral layer\narchitecture of FNOs, and leverage FFT to achieve rapid computations, in such a\nway that the geometric information is explicitly encoded in the architecture.\nIn our empirical evaluation, DAFNO has achieved state-of-the-art accuracy as\ncompared to baseline neural operator models on two benchmark datasets of\nmaterial modeling and airfoil simulation. To further demonstrate the capability\nand generalizability of DAFNO in handling complex domains with topology\nchanges, we consider a brittle material fracture evolution problem. With only\none training crack simulation sample, DAFNO has achieved generalizability to\nunseen loading scenarios and substantially different crack patterns from the\ntrained scenario. Our code and data accompanying this paper are available at\nhttps:\/\/github.com\/ningliu-iga\/DAFNO.","terms":["cs.LG","cond-mat.mtrl-sci","stat.ML"]},{"titles":"LagrangeBench: A Lagrangian Fluid Mechanics Benchmarking Suite","summaries":"Machine learning has been successfully applied to grid-based PDE modeling in\nvarious scientific applications. However, learned PDE solvers based on\nLagrangian particle discretizations, which are the preferred approach to\nproblems with free surfaces or complex physics, remain largely unexplored. We\npresent LagrangeBench, the first benchmarking suite for Lagrangian particle\nproblems, focusing on temporal coarse-graining. In particular, our contribution\nis: (a) seven new fluid mechanics datasets (four in 2D and three in 3D)\ngenerated with the Smoothed Particle Hydrodynamics (SPH) method including the\nTaylor-Green vortex, lid-driven cavity, reverse Poiseuille flow, and dam break,\neach of which includes different physics like solid wall interactions or free\nsurface, (b) efficient JAX-based API with various recent training strategies\nand three neighbor search routines, and (c) JAX implementation of established\nGraph Neural Networks (GNNs) like GNS and SEGNN with baseline results. Finally,\nto measure the performance of learned surrogates we go beyond established\nposition errors and introduce physical metrics like kinetic energy MSE and\nSinkhorn distance for the particle distribution. Our codebase is available at\nhttps:\/\/github.com\/tumaer\/lagrangebench .","terms":["cs.LG","physics.flu-dyn"]},{"titles":"Accelerating Molecular Graph Neural Networks via Knowledge Distillation","summaries":"Recent advances in graph neural networks (GNNs) have enabled more\ncomprehensive modeling of molecules and molecular systems, thereby enhancing\nthe precision of molecular property prediction and molecular simulations.\nNonetheless, as the field has been progressing to bigger and more complex\narchitectures, state-of-the-art GNNs have become largely prohibitive for many\nlarge-scale applications. In this paper, we explore the utility of knowledge\ndistillation (KD) for accelerating molecular GNNs. To this end, we devise KD\nstrategies that facilitate the distillation of hidden representations in\ndirectional and equivariant GNNs, and evaluate their performance on the\nregression task of energy and force prediction. We validate our protocols\nacross different teacher-student configurations and datasets, and demonstrate\nthat they can consistently boost the predictive accuracy of student models\nwithout any modifications to their architecture. Moreover, we conduct\ncomprehensive optimization of various components of our framework, and\ninvestigate the potential of data augmentation to further enhance performance.\nAll in all, we manage to close the gap in predictive accuracy between teacher\nand student models by as much as 96.7% and 62.5% for energy and force\nprediction respectively, while fully preserving the inference throughput of the\nmore lightweight models.","terms":["cs.LG","physics.chem-ph"]},{"titles":"Using convolutional neural networks for stereological characterization of 3D hetero-aggregates based on synthetic STEM data","summaries":"The structural characterization of hetero-aggregates in 3D is of great\ninterest, e.g., for deriving process-structure or structure-property\nrelationships. However, since 3D imaging techniques are often difficult to\nperform as well as time and cost intensive, a characterization of\nhetero-aggregates based on 2D image data is desirable, but often non-trivial.\nTo overcome the issues of characterizing 3D structures from 2D measurements, a\nmethod is presented that relies on machine learning combined with methods of\nspatial stochastic modeling, where the latter are utilized for the generation\nof synthetic training data. This kind of training data has the advantage that\ntime-consuming experiments for the synthesis of differently structured\nmaterials followed by their 3D imaging can be avoided. More precisely, a\nparametric stochastic 3D model is presented, from which a wide spectrum of\nvirtual hetero-aggregates can be generated. Additionally, the virtual\nstructures are passed to a physics-based simulation tool in order to generate\nvirtual scanning transmission electron microscopy (STEM) images. The preset\nparameters of the 3D model together with the simulated STEM images serve as a\ndatabase for the training of convolutional neural networks, which can be used\nto determine the parameters of the underlying 3D model and, consequently, to\npredict 3D structures of hetero-aggregates from 2D STEM images. Furthermore, an\nerror analysis is performed to evaluate the prediction power of the trained\nneural networks with respect to structural descriptors, e.g. the\nhetero-coordination number.","terms":["cs.CV","eess.IV"]},{"titles":"Multi-fidelity Design of Porous Microstructures for Thermofluidic Applications","summaries":"As modern electronic devices are increasingly miniaturized and integrated,\ntheir performance relies more heavily on effective thermal management.\nTwo-phase cooling methods enhanced by porous surfaces, which capitalize on\nthin-film evaporation atop structured porous surfaces, are emerging as\npotential solutions. In such porous structures, the optimum heat dissipation\ncapacity relies on two competing objectives that depend on mass and heat\ntransfer. The computational costs of evaluating these objectives, the high\ndimensionality of the design space which a voxelated microstructure\nrepresentation, and the manufacturability constraints hinder the optimization\nprocess for thermal management. We address these challenges by developing a\ndata-driven framework for designing optimal porous microstructures for cooling\napplications. In our framework we leverage spectral density functions (SDFs) to\nencode the design space via a handful of interpretable variables and, in turn,\nefficiently search it. We develop physics-based formulas to quantify the\nthermofluidic properties and feasibility of candidate designs via offline\nsimulations. To decrease the reliance on expensive simulations, we generate\nmulti-fidelity data and build emulators to find Pareto-optimal designs. We\napply our approach to a canonical problem on evaporator wick design and obtain\nfin-like topologies in the optimal microstructures which are also\ncharacteristics often observed in industrial applications.","terms":["cs.LG","physics.flu-dyn"]},{"titles":"Turbulence in Focus: Benchmarking Scaling Behavior of 3D Volumetric Super-Resolution with BLASTNet 2.0 Data","summaries":"Analysis of compressible turbulent flows is essential for applications\nrelated to propulsion, energy generation, and the environment. Here, we present\nBLASTNet 2.0, a 2.2 TB network-of-datasets containing 744 full-domain samples\nfrom 34 high-fidelity direct numerical simulations, which addresses the current\nlimited availability of 3D high-fidelity reacting and non-reacting compressible\nturbulent flow simulation data. With this data, we benchmark a total of 49\nvariations of five deep learning approaches for 3D super-resolution - which can\nbe applied for improving scientific imaging, simulations, turbulence models, as\nwell as in computer vision applications. We perform neural scaling analysis on\nthese models to examine the performance of different machine learning (ML)\napproaches, including two scientific ML techniques. We demonstrate that (i)\npredictive performance can scale with model size and cost, (ii) architecture\nmatters significantly, especially for smaller models, and (iii) the benefits of\nphysics-based losses can persist with increasing model size. The outcomes of\nthis benchmark study are anticipated to offer insights that can aid the design\nof 3D super-resolution models, especially for turbulence models, while this\ndata is expected to foster ML methods for a broad range of flow physics\napplications. This data is publicly available with download links and browsing\ntools consolidated at https:\/\/blastnet.github.io.","terms":["cs.LG","cs.CV","physics.comp-ph","physics.flu-dyn"]},{"titles":"A general learning scheme for classical and quantum Ising machines","summaries":"An Ising machine is any hardware specifically designed for finding the ground\nstate of the Ising model. Relevant examples are coherent Ising machines and\nquantum annealers. In this paper, we propose a new machine learning model that\nis based on the Ising structure and can be efficiently trained using gradient\ndescent. We provide a mathematical characterization of the training process,\nwhich is based upon optimizing a loss function whose partial derivatives are\nnot explicitly calculated but estimated by the Ising machine itself. Moreover,\nwe present some experimental results on the training and execution of the\nproposed learning model. These results point out new possibilities offered by\nIsing machines for different learning tasks. In particular, in the quantum\nrealm, the quantum resources are used for both the execution and the training\nof the model, providing a promising perspective in quantum machine learning.","terms":["cs.LG","quant-ph"]},{"titles":"PlantPlotGAN: A Physics-Informed Generative Adversarial Network for Plant Disease Prediction","summaries":"Monitoring plantations is crucial for crop management and producing healthy\nharvests. Unmanned Aerial Vehicles (UAVs) have been used to collect\nmultispectral images that aid in this monitoring. However, given the number of\nhectares to be monitored and the limitations of flight, plant disease signals\nbecome visually clear only in the later stages of plant growth and only if the\ndisease has spread throughout a significant portion of the plantation. This\nlimited amount of relevant data hampers the prediction models, as the\nalgorithms struggle to generalize patterns with unbalanced or unrealistic\naugmented datasets effectively. To address this issue, we propose PlantPlotGAN,\na physics-informed generative model capable of creating synthetic multispectral\nplot images with realistic vegetation indices. These indices served as a proxy\nfor disease detection and were used to evaluate if our model could help\nincrease the accuracy of prediction models. The results demonstrate that the\nsynthetic imagery generated from PlantPlotGAN outperforms state-of-the-art\nmethods regarding the Fr\\'echet inception distance. Moreover, prediction models\nachieve higher accuracy metrics when trained with synthetic and original\nimagery for earlier plant disease detection compared to the training processes\nbased solely on real imagery.","terms":["cs.CV","cs.LG","eess.IV"]},{"titles":"Guided Data Augmentation for Offline Reinforcement Learning and Imitation Learning","summaries":"Learning from demonstration (LfD) is a popular technique that uses expert\ndemonstrations to learn robot control policies. However, the difficulty in\nacquiring expert-quality demonstrations limits the applicability of LfD\nmethods: real-world data collection is often costly, and the quality of the\ndemonstrations depends greatly on the demonstrator's abilities and safety\nconcerns. A number of works have leveraged data augmentation (DA) to\ninexpensively generate additional demonstration data, but most DA works\ngenerate augmented data in a random fashion and ultimately produce highly\nsuboptimal data. In this work, we propose Guided Data Augmentation (GuDA), a\nhuman-guided DA framework that generates expert-quality augmented data. The key\ninsight of GuDA is that while it may be difficult to demonstrate the sequence\nof actions required to produce expert data, a user can often easily identify\nwhen an augmented trajectory segment represents task progress. Thus, the user\ncan impose a series of simple rules on the DA process to automatically generate\naugmented samples that approximate expert behavior. To extract a policy from\nGuDA, we use off-the-shelf offline reinforcement learning and behavior cloning\nalgorithms. We evaluate GuDA on a physical robot soccer task as well as\nsimulated D4RL navigation tasks, a simulated autonomous driving task, and a\nsimulated soccer task. Empirically, we find that GuDA enables learning from a\nsmall set of potentially suboptimal demonstrations and substantially\noutperforms a DA strategy that samples augmented data randomly.","terms":["cs.LG","cs.RO"]},{"titles":"FLARE: Fast Learning of Animatable and Relightable Mesh Avatars","summaries":"Our goal is to efficiently learn personalized animatable 3D head avatars from\nvideos that are geometrically accurate, realistic, relightable, and compatible\nwith current rendering systems. While 3D meshes enable efficient processing and\nare highly portable, they lack realism in terms of shape and appearance. Neural\nrepresentations, on the other hand, are realistic but lack compatibility and\nare slow to train and render. Our key insight is that it is possible to\nefficiently learn high-fidelity 3D mesh representations via differentiable\nrendering by exploiting highly-optimized methods from traditional computer\ngraphics and approximating some of the components with neural networks. To that\nend, we introduce FLARE, a technique that enables the creation of animatable\nand relightable mesh avatars from a single monocular video. First, we learn a\ncanonical geometry using a mesh representation, enabling efficient\ndifferentiable rasterization and straightforward animation via learned\nblendshapes and linear blend skinning weights. Second, we follow\nphysically-based rendering and factor observed colors into intrinsic albedo,\nroughness, and a neural representation of the illumination, allowing the\nlearned avatars to be relit in novel scenes. Since our input videos are\ncaptured on a single device with a narrow field of view, modeling the\nsurrounding environment light is non-trivial. Based on the split-sum\napproximation for modeling specular reflections, we address this by\napproximating the pre-filtered environment map with a multi-layer perceptron\n(MLP) modulated by the surface roughness, eliminating the need to explicitly\nmodel the light. We demonstrate that our mesh-based avatar formulation,\ncombined with learned deformation, material, and lighting MLPs, produces\navatars with high-quality geometry and appearance, while also being efficient\nto train and render compared to existing approaches.","terms":["cs.CV"]},{"titles":"Joint-Relation Transformer for Multi-Person Motion Prediction","summaries":"Multi-person motion prediction is a challenging problem due to the dependency\nof motion on both individual past movements and interactions with other people.\nTransformer-based methods have shown promising results on this task, but they\nmiss the explicit relation representation between joints, such as skeleton\nstructure and pairwise distance, which is crucial for accurate interaction\nmodeling. In this paper, we propose the Joint-Relation Transformer, which\nutilizes relation information to enhance interaction modeling and improve\nfuture motion prediction. Our relation information contains the relative\ndistance and the intra-\/inter-person physical constraints. To fuse relation and\njoint information, we design a novel joint-relation fusion layer with\nrelation-aware attention to update both features. Additionally, we supervise\nthe relation information by forecasting future distance. Experiments show that\nour method achieves a 13.4% improvement of 900ms VIM on 3DPW-SoMoF\/RC and\n17.8%\/12.0% improvement of 3s MPJPE on CMU-Mpcap\/MuPoTS-3D dataset.","terms":["cs.CV"]},{"titles":"Jigsaw: Learning to Assemble Multiple Fractured Objects","summaries":"Automated assembly of 3D fractures is essential in orthopedics, archaeology,\nand our daily life. This paper presents Jigsaw, a novel framework for\nassembling physically broken 3D objects from multiple pieces. Our approach\nleverages hierarchical features of global and local geometry to match and align\nthe fracture surfaces. Our framework consists of four components: (1) front-end\npoint feature extractor with attention layers, (2) surface segmentation to\nseparate fracture and original parts, (3) multi-parts matching to find\ncorrespondences among fracture surface points, and (4) robust global alignment\nto recover the global poses of the pieces. We show how to jointly learn\nsegmentation and matching and seamlessly integrate feature matching and\nrigidity constraints. We evaluate Jigsaw on the Breaking Bad dataset and\nachieve superior performance compared to state-of-the-art methods. Our method\nalso generalizes well to diverse fracture modes, objects, and unseen instances.\nTo the best of our knowledge, this is the first learning-based method designed\nspecifically for 3D fracture assembly over multiple pieces. Our code is\navailable at https:\/\/jiaxin-lu.github.io\/Jigsaw\/.","terms":["cs.CV"]},{"titles":"The Eigenlearning Framework: A Conservation Law Perspective on Kernel Regression and Wide Neural Networks","summaries":"We derive simple closed-form estimates for the test risk and other\ngeneralization metrics of kernel ridge regression (KRR). Relative to prior\nwork, our derivations are greatly simplified and our final expressions are more\nreadily interpreted. These improvements are enabled by our identification of a\nsharp conservation law which limits the ability of KRR to learn any orthonormal\nbasis of functions. Test risk and other objects of interest are expressed\ntransparently in terms of our conserved quantity evaluated in the kernel\neigenbasis. We use our improved framework to: i) provide a theoretical\nexplanation for the \"deep bootstrap\" of Nakkiran et al (2020), ii) generalize a\nprevious result regarding the hardness of the classic parity problem, iii)\nfashion a theoretical tool for the study of adversarial robustness, and iv)\ndraw a tight analogy between KRR and a well-studied system in statistical\nphysics.","terms":["cs.LG","stat.ML"]},{"titles":"Efficient Numerical Algorithm for Large-Scale Damped Natural Gradient Descent","summaries":"We propose a new algorithm for efficiently solving the damped Fisher matrix\nin large-scale scenarios where the number of parameters significantly exceeds\nthe number of available samples. This problem is fundamental for natural\ngradient descent and stochastic reconfiguration. Our algorithm is based on\nCholesky decomposition and is generally applicable. Benchmark results show that\nthe algorithm is significantly faster than existing methods.","terms":["cs.LG","cs.NA","math.NA","physics.comp-ph"]},{"titles":"What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement","summaries":"The question of what makes a data distribution suitable for deep learning is\na fundamental open problem. Focusing on locally connected neural networks (a\nprevalent family of architectures that includes convolutional and recurrent\nneural networks as well as local self-attention models), we address this\nproblem by adopting theoretical tools from quantum physics. Our main\ntheoretical result states that a certain locally connected neural network is\ncapable of accurate prediction over a data distribution if and only if the data\ndistribution admits low quantum entanglement under certain canonical partitions\nof features. As a practical application of this result, we derive a\npreprocessing method for enhancing the suitability of a data distribution to\nlocally connected neural networks. Experiments with widespread models over\nvarious datasets demonstrate our findings. We hope that our use of quantum\nentanglement will encourage further adoption of tools from physics for formally\nreasoning about the relation between deep learning and real-world data.","terms":["cs.LG","cs.AI","quant-ph"]},{"titles":"The statistical thermodynamics of generative diffusion models","summaries":"Generative diffusion models have achieved spectacular performance in many\nareas of generative modeling. While the fundamental ideas behind these models\ncome from non-equilibrium physics, in this paper we show that many aspects of\nthese models can be understood using the tools of equilibrium statistical\nmechanics. Using this reformulation, we show that generative diffusion models\nundergo second-order phase transitions corresponding to symmetry breaking\nphenomena. We argue that this lead to a form of instability that lies at the\nheart of their generative capabilities and that can be described by a set of\nmean field critical exponents. We conclude by analyzing recent work connecting\ndiffusion models and associative memory networks in view of the thermodynamic\nformulations.","terms":["stat.ML","cs.LG"]},{"titles":"Circuit as Set of Points","summaries":"As the size of circuit designs continues to grow rapidly, artificial\nintelligence technologies are being extensively used in Electronic Design\nAutomation (EDA) to assist with circuit design. Placement and routing are the\nmost time-consuming parts of the physical design process, and how to quickly\nevaluate the placement has become a hot research topic. Prior works either\ntransformed circuit designs into images using hand-crafted methods and then\nused Convolutional Neural Networks (CNN) to extract features, which are limited\nby the quality of the hand-crafted methods and could not achieve end-to-end\ntraining, or treated the circuit design as a graph structure and used Graph\nNeural Networks (GNN) to extract features, which require time-consuming\npreprocessing. In our work, we propose a novel perspective for circuit design\nby treating circuit components as point clouds and using Transformer-based\npoint cloud perception methods to extract features from the circuit. This\napproach enables direct feature extraction from raw data without any\npreprocessing, allows for end-to-end training, and results in high performance.\nExperimental results show that our method achieves state-of-the-art performance\nin congestion prediction tasks on both the CircuitNet and ISPD2015 datasets, as\nwell as in design rule check (DRC) violation prediction tasks on the CircuitNet\ndataset. Our method establishes a bridge between the relatively mature point\ncloud perception methods and the fast-developing EDA algorithms, enabling us to\nleverage more collective intelligence to solve this task. To facilitate the\nresearch of open EDA design, source codes and pre-trained models are released\nat https:\/\/github.com\/hustvl\/circuitformer.","terms":["cs.CV"]},{"titles":"Improving the Timing Resolution of Positron Emission Tomography Detectors Using Boosted Learning -- A Residual Physics Approach","summaries":"Artificial intelligence (AI) is entering medical imaging, mainly enhancing\nimage reconstruction. Nevertheless, improvements throughout the entire\nprocessing, from signal detection to computation, potentially offer significant\nbenefits. This work presents a novel and versatile approach to detector\noptimization using machine learning (ML) and residual physics. We apply the\nconcept to positron emission tomography (PET), intending to improve the\ncoincidence time resolution (CTR). PET visualizes metabolic processes in the\nbody by detecting photons with scintillation detectors. Improved CTR\nperformance offers the advantage of reducing radioactive dose exposure for\npatients. Modern PET detectors with sophisticated concepts and read-out\ntopologies represent complex physical and electronic systems requiring\ndedicated calibration techniques. Traditional methods primarily depend on\nanalytical formulations successfully describing the main detector\ncharacteristics. However, when accounting for higher-order effects, additional\ncomplexities arise matching theoretical models to experimental reality. Our\nwork addresses this challenge by combining traditional calibration with AI and\nresidual physics, presenting a highly promising approach. We present a residual\nphysics-based strategy using gradient tree boosting and physics-guided data\ngeneration. The explainable AI framework SHapley Additive exPlanations (SHAP)\nwas used to identify known physical effects with learned patterns. In addition,\nthe models were tested against basic physical laws. We were able to improve the\nCTR significantly (more than 20%) for clinically relevant detectors of 19 mm\nheight, reaching CTRs of 185 ps (450-550 keV).","terms":["cs.LG","physics.ins-det","physics.med-ph"]},{"titles":"Comparison of Cross-Entropy, Dice, and Focal Loss for Sea Ice Type Segmentation","summaries":"Up-to-date sea ice charts are crucial for safer navigation in ice-infested\nwaters. Recently, Convolutional Neural Network (CNN) models show the potential\nto accelerate the generation of ice maps for large regions. However, results\nfrom CNN models still need to undergo scrutiny as higher metrics performance\nnot always translate to adequate outputs. Sea ice type classes are imbalanced,\nrequiring special treatment during training. We evaluate how three different\nloss functions, some developed for imbalanced class problems, affect the\nperformance of CNN models trained to predict the dominant ice type in\nSentinel-1 images. Despite the fact that Dice and Focal loss produce higher\nmetrics, results from cross-entropy seem generally more physically consistent.","terms":["cs.CV","eess.IV"]},{"titles":"Diffusion in Diffusion: Cyclic One-Way Diffusion for Text-Vision-Conditioned Generation","summaries":"Originating from the diffusion phenomenon in physics that describes particle\nmovement, the diffusion generative models inherit the characteristics of\nstochastic random walk in the data space along the denoising trajectory.\nHowever, the intrinsic mutual interference among image regions contradicts the\nneed for practical downstream application scenarios where the preservation of\nlow-level pixel information from given conditioning is desired (e.g.,\ncustomization tasks like personalized generation and inpainting based on a\nuser-provided single image). In this work, we investigate the diffusion\n(physics) in diffusion (machine learning) properties and propose our Cyclic\nOne-Way Diffusion (COW) method to control the direction of diffusion phenomenon\ngiven a pre-trained frozen diffusion model for versatile customization\napplication scenarios, where the low-level pixel information from the\nconditioning needs to be preserved. Notably, unlike most current methods that\nincorporate additional conditions by fine-tuning the base text-to-image\ndiffusion model or learning auxiliary networks, our method provides a novel\nperspective to understand the task needs and is applicable to a wider range of\ncustomization scenarios in a learning-free manner. Extensive experiment results\nshow that our proposed COW can achieve more flexible customization based on\nstrict visual conditions in different application settings.","terms":["cs.CV"]},{"titles":"Transferring a molecular foundation model for polymer property predictions","summaries":"Transformer-based large language models have remarkable potential to\naccelerate design optimization for applications such as drug development and\nmaterials discovery. Self-supervised pretraining of transformer models requires\nlarge-scale datasets, which are often sparsely populated in topical areas such\nas polymer science. State-of-the-art approaches for polymers conduct data\naugmentation to generate additional samples but unavoidably incurs extra\ncomputational costs. In contrast, large-scale open-source datasets are\navailable for small molecules and provide a potential solution to data scarcity\nthrough transfer learning. In this work, we show that using transformers\npretrained on small molecules and fine-tuned on polymer properties achieve\ncomparable accuracy to those trained on augmented polymer datasets for a series\nof benchmark prediction tasks.","terms":["cs.LG","physics.chem-ph"]},{"titles":"Investigating the usefulness of Quantum Blur","summaries":"Though some years remain before quantum computation can fully outperform\nconventional computation, it already provides resources that can be used for\nexploratory purposes in various fields. This includes certain tasks for\nprocedural generation in computer games, music and art. The so-called `Quantum\nBlur' method represents the first step on this journey, providing a simple\nproof-of-principle example of how quantum software can be useful in these areas\ntoday. Here we analyse the `Quantum Blur' method and compare it to conventional\nblur effects. This investigation was guided by discussions with the most\nprominent user of the method, to determine which features were found most\nuseful. In particular we determine how these features depend on the quantum\nphenomena of superposition and entanglement.","terms":["cs.CV","cs.AI","cs.ET","quant-ph"]},{"titles":"Stochastic Latent Transformer: Efficient Modelling of Stochastically Forced Zonal Jets","summaries":"We introduce the 'Stochastic Latent Transformer', a probabilistic deep\nlearning approach for efficient reduced-order modelling of stochastic partial\ndifferential equations (SPDEs). Despite recent advances in deep learning for\nfluid mechanics, limited research has explored modelling stochastically driven\nflows - which play a crucial role in understanding a broad spectrum of\nphenomena, from jets on giant planets to ocean circulation and the variability\nof midlatitude weather. The model architecture consists of a\nstochastically-forced transformer, paired with a translation-equivariant\nautoencoder, that we demonstrate is capable of reproducing system dynamics\nacross various integration periods. We demonstrate its effectiveness applied to\na well-researched zonal jet system, with the neural network achieving a\nfive-order-of-magnitude speedup compared to numerical integration. This\nfacilitates the cost-effective generation of large ensembles, enabling the\nexploration of statistical questions concerning probabilities of spontaneous\ntransition events.","terms":["cs.LG","physics.ao-ph","physics.flu-dyn","68T07, 37N10, 35R60"]},{"titles":"TSONN: Time-stepping-oriented neural network for solving partial differential equations","summaries":"Deep neural networks (DNNs), especially physics-informed neural networks\n(PINNs), have recently become a new popular method for solving forward and\ninverse problems governed by partial differential equations (PDEs). However,\nthese methods still face challenges in achieving stable training and obtaining\ncorrect results in many problems, since minimizing PDE residuals with PDE-based\nsoft constraint make the problem ill-conditioned. Different from all existing\nmethods that directly minimize PDE residuals, this work integrates\ntime-stepping method with deep learning, and transforms the original\nill-conditioned optimization problem into a series of well-conditioned\nsub-problems over given pseudo time intervals. The convergence of model\ntraining is significantly improved by following the trajectory of the pseudo\ntime-stepping process, yielding a robust optimization-based PDE solver. Our\nresults show that the proposed method achieves stable training and correct\nresults in many problems that standard PINNs fail to solve, requiring only a\nsimple modification on the loss function. In addition, we demonstrate several\nnovel properties and advantages of time-stepping methods within the framework\nof neural network-based optimization approach, in comparison to traditional\ngrid-based numerical method. Specifically, explicit scheme allows significantly\nlarger time step, while implicit scheme can be implemented as straightforwardly\nas explicit scheme.","terms":["cs.LG","physics.comp-ph"]},{"titles":"Grokking in Linear Estimators -- A Solvable Model that Groks without Understanding","summaries":"Grokking is the intriguing phenomenon where a model learns to generalize long\nafter it has fit the training data. We show both analytically and numerically\nthat grokking can surprisingly occur in linear networks performing linear tasks\nin a simple teacher-student setup with Gaussian inputs. In this setting, the\nfull training dynamics is derived in terms of the training and generalization\ndata covariance matrix. We present exact predictions on how the grokking time\ndepends on input and output dimensionality, train sample size, regularization,\nand network initialization. We demonstrate that the sharp increase in\ngeneralization accuracy may not imply a transition from \"memorization\" to\n\"understanding\", but can simply be an artifact of the accuracy measure. We\nprovide empirical verification for our calculations, along with preliminary\nresults indicating that some predictions also hold for deeper networks, with\nnon-linear activations.","terms":["stat.ML","cond-mat.dis-nn","cs.LG","math-ph","math.MP"]},{"titles":"Dynamic Decision Frequency with Continuous Options","summaries":"In classic reinforcement learning algorithms, agents make decisions at\ndiscrete and fixed time intervals. The duration between decisions becomes a\ncrucial hyperparameter, as setting it too short may increase the problem's\ndifficulty by requiring the agent to make numerous decisions to achieve its\ngoal while setting it too long can result in the agent losing control over the\nsystem. However, physical systems do not necessarily require a constant control\nfrequency, and for learning agents, it is often preferable to operate with a\nlow frequency when possible and a high frequency when necessary. We propose a\nframework called Continuous-Time Continuous-Options (CTCO), where the agent\nchooses options as sub-policies of variable durations. These options are\ntime-continuous and can interact with the system at any desired frequency\nproviding a smooth change of actions. We demonstrate the effectiveness of CTCO\nby comparing its performance to classical RL and temporal-abstraction RL\nmethods on simulated continuous control tasks with various action-cycle times.\nWe show that our algorithm's performance is not affected by the choice of\nenvironment interaction frequency. Furthermore, we demonstrate the efficacy of\nCTCO in facilitating exploration in a real-world visual reaching task for a 7\nDOF robotic arm with sparse rewards.","terms":["cs.LG","cs.AI"]},{"titles":"Brain-Inspired Reservoir Computing Using Memristors with Tunable Dynamics and Short-Term Plasticity","summaries":"Recent advancements in reservoir computing research have created a demand for\nanalog devices with dynamics that can facilitate the physical implementation of\nreservoirs, promising faster information processing while consuming less energy\nand occupying a smaller area footprint. Studies have demonstrated that dynamic\nmemristors, with nonlinear and short-term memory dynamics, are excellent\ncandidates as information-processing devices or reservoirs for temporal\nclassification and prediction tasks. Previous implementations relied on\nnominally identical memristors that applied the same nonlinear transformation\nto the input data, which is not enough to achieve a rich state space. To\naddress this limitation, researchers either diversified the data encoding\nacross multiple memristors or harnessed the stochastic device-to-device\nvariability among the memristors. However, this approach requires additional\npre-processing steps and leads to synchronization issues. Instead, it is\npreferable to encode the data once and pass it through a reservoir layer\nconsisting of memristors with distinct dynamics. Here, we demonstrate that\nion-channel-based memristors with voltage-dependent dynamics can be\ncontrollably and predictively tuned through voltage or adjustment of the ion\nchannel concentration to exhibit diverse dynamic properties. We show, through\nexperiments and simulations, that reservoir layers constructed with a small\nnumber of distinct memristors exhibit significantly higher predictive and\nclassification accuracies with a single data encoding. We found that for a\nsecond-order nonlinear dynamical system prediction task, the varied memristor\nreservoir experimentally achieved a normalized mean square error of 0.0015\nusing only five distinct memristors. Moreover, in a neural activity\nclassification task, a reservoir of just three distinct memristors\nexperimentally attained an accuracy of 96.5%.","terms":["cs.LG"]},{"titles":"Imperfect Digital Twin Assisted Low Cost Reinforcement Training for Multi-UAV Networks","summaries":"Deep Reinforcement Learning (DRL) is widely used to optimize the performance\nof multi-UAV networks. However, the training of DRL relies on the frequent\ninteractions between the UAVs and the environment, which consumes lots of\nenergy due to the flying and communication of UAVs in practical experiments.\nInspired by the growing digital twin (DT) technology, which can simulate the\nperformance of algorithms in the digital space constructed by coping features\nof the physical space, the DT is introduced to reduce the costs of practical\ntraining, e.g., energy and hardware purchases. Different from previous\nDT-assisted works with an assumption of perfect reflecting real physics by\nvirtual digital, we consider an imperfect DT model with deviations for\nassisting the training of multi-UAV networks. Remarkably, to trade off the\ntraining cost, DT construction cost, and the impact of deviations of DT on\ntraining, the natural and virtually generated UAV mixing deployment method is\nproposed. Two cascade neural networks (NN) are used to optimize the joint\nnumber of virtually generated UAVs, the DT construction cost, and the\nperformance of multi-UAV networks. These two NNs are trained by unsupervised\nand reinforcement learning, both low-cost label-free training methods.\nSimulation results show the training cost can significantly decrease while\nguaranteeing the training performance. This implies that an efficient decision\ncan be made with imperfect DTs in multi-UAV networks.","terms":["cs.LG","cs.SY","eess.SY"]},{"titles":"FuXi-Extreme: Improving extreme rainfall and wind forecasts with diffusion model","summaries":"Significant advancements in the development of machine learning (ML) models\nfor weather forecasting have produced remarkable results. State-of-the-art\nML-based weather forecast models, such as FuXi, have demonstrated superior\nstatistical forecast performance in comparison to the high-resolution forecasts\n(HRES) of the European Centre for Medium-Range Weather Forecasts (ECMWF).\nHowever, ML models face a common challenge: as forecast lead times increase,\nthey tend to generate increasingly smooth predictions, leading to an\nunderestimation of the intensity of extreme weather events. To address this\nchallenge, we developed the FuXi-Extreme model, which employs a denoising\ndiffusion probabilistic model (DDPM) to restore finer-scale details in the\nsurface forecast data generated by the FuXi model in 5-day forecasts. An\nevaluation of extreme total precipitation ($\\textrm{TP}$), 10-meter wind speed\n($\\textrm{WS10}$), and 2-meter temperature ($\\textrm{T2M}$) illustrates the\nsuperior performance of FuXi-Extreme over both FuXi and HRES. Moreover, when\nevaluating tropical cyclone (TC) forecasts based on International Best Track\nArchive for Climate Stewardship (IBTrACS) dataset, both FuXi and FuXi-Extreme\nshows superior performance in TC track forecasts compared to HRES, but they\nshow inferior performance in TC intensity forecasts in comparison to HRES.","terms":["cs.LG","physics.ao-ph","stat.AP"]},{"titles":"Efficient deep data assimilation with sparse observations and time-varying sensors","summaries":"Variational Data Assimilation (DA) has been broadly used in engineering\nproblems for field reconstruction and prediction by performing a weighted\ncombination of multiple sources of noisy data. In recent years, the integration\nof deep learning (DL) techniques in DA has shown promise in improving the\nefficiency and accuracy in high-dimensional dynamical systems. Nevertheless,\nexisting deep DA approaches face difficulties in dealing with unstructured\nobservation data, especially when the placement and number of sensors are\ndynamic over time. We introduce a novel variational DA scheme, named\nVoronoi-tessellation Inverse operator for VariatIonal Data assimilation\n(VIVID), that incorporates a DL inverse operator into the assimilation\nobjective function. By leveraging the capabilities of the Voronoi-tessellation\nand convolutional neural networks, VIVID is adept at handling sparse,\nunstructured, and time-varying sensor data. Furthermore, the incorporation of\nthe DL inverse operator establishes a direct link between observation and state\nspace, leading to a reduction in the number of minimization steps required for\nDA. Additionally, VIVID can be seamlessly integrated with Proper Orthogonal\nDecomposition (POD) to develop an end-to-end reduced-order DA scheme, which can\nfurther expedite field reconstruction. Numerical experiments in a fluid\ndynamics system demonstrate that VIVID can significantly outperform existing DA\nand DL algorithms. The robustness of VIVID is also accessed through the\napplication of various levels of prior error, the utilization of varying\nnumbers of sensors, and the misspecification of error covariance in DA.","terms":["cs.LG","math-ph","math.MP"]},{"titles":"Image Segmentation using U-Net Architecture for Powder X-ray Diffraction Images","summaries":"Scientific researchers frequently use the in situ synchrotron high-energy\npowder X-ray diffraction (XRD) technique to examine the crystallographic\nstructures of materials in functional devices such as rechargeable battery\nmaterials. We propose a method for identifying artifacts in experimental XRD\nimages. The proposed method uses deep learning convolutional neural network\narchitectures, such as tunable U-Nets to identify the artifacts. In particular,\nthe predicted artifacts are evaluated against the corresponding ground truth\n(manually implemented) using the overall true positive rate or recall. The\nresult demonstrates that the U-Nets can consistently produce great recall\nperformance at 92.4% on the test dataset, which is not included in the\ntraining, with a 34% reduction in average false positives in comparison to the\nconventional method. The U-Nets also reduce the time required to identify and\nseparate artifacts by more than 50%. Furthermore, the exclusion of the\nartifacts shows major changes in the integrated 1D XRD pattern, enhancing\nfurther analysis of the post-processing XRD data.","terms":["cs.LG","hep-ex"]},{"titles":"Breaking the Curse of Dimensionality in Deep Neural Networks by Learning Invariant Representations","summaries":"Artificial intelligence, particularly the subfield of machine learning, has\nseen a paradigm shift towards data-driven models that learn from and adapt to\ndata. This has resulted in unprecedented advancements in various domains such\nas natural language processing and computer vision, largely attributed to deep\nlearning, a special class of machine learning models. Deep learning arguably\nsurpasses traditional approaches by learning the relevant features from raw\ndata through a series of computational layers.\n  This thesis explores the theoretical foundations of deep learning by studying\nthe relationship between the architecture of these models and the inherent\nstructures found within the data they process. In particular, we ask What\ndrives the efficacy of deep learning algorithms and allows them to beat the\nso-called curse of dimensionality-i.e. the difficulty of generally learning\nfunctions in high dimensions due to the exponentially increasing need for data\npoints with increased dimensionality? Is it their ability to learn relevant\nrepresentations of the data by exploiting their structure? How do different\narchitectures exploit different data structures? In order to address these\nquestions, we push forward the idea that the structure of the data can be\neffectively characterized by its invariances-i.e. aspects that are irrelevant\nfor the task at hand.\n  Our methodology takes an empirical approach to deep learning, combining\nexperimental studies with physics-inspired toy models. These simplified models\nallow us to investigate and interpret the complex behaviors we observe in deep\nlearning systems, offering insights into their inner workings, with the\nfar-reaching goal of bridging the gap between theory and practice.","terms":["cs.LG"]},{"titles":"KirchhoffNet: A Circuit Bridging Message Passing and Continuous-Depth Models","summaries":"In this paper, we exploit a fundamental principle of analog electronic\ncircuitry, Kirchhoff's current law, to introduce a unique class of neural\nnetwork models that we refer to as KirchhoffNet. KirchhoffNet establishes close\nconnections with message passing neural networks and continuous-depth networks.\nWe demonstrate that even in the absence of any traditional layers (such as\nconvolution, pooling, or linear layers), KirchhoffNet attains 98.86% test\naccuracy on the MNIST dataset, comparable with state of the art (SOTA) results.\nWhat makes KirchhoffNet more intriguing is its potential in the realm of\nhardware. Contemporary deep neural networks are conventionally deployed on\nGPUs. In contrast, KirchhoffNet can be physically realized by an analog\nelectronic circuit. Moreover, we justify that irrespective of the number of\nparameters within a KirchhoffNet, its forward calculation can always be\ncompleted within 1\/f seconds, with f representing the hardware's clock\nfrequency. This characteristic introduces a promising technology for\nimplementing ultra-large-scale neural networks.","terms":["cs.LG","cs.AI","cs.AR"]},{"titles":"Can bin-wise scaling improve consistency and adaptivity of prediction uncertainty for machine learning regression ?","summaries":"Binwise Variance Scaling (BVS) has recently been proposed as a post hoc\nrecalibration method for prediction uncertainties of machine learning\nregression problems that is able of more efficient corrections than uniform\nvariance (or temperature) scaling. The original version of BVS uses\nuncertainty-based binning, which is aimed to improve calibration conditionally\non uncertainty, i.e. consistency. I explore here several adaptations of BVS, in\nparticular with alternative loss functions and a binning scheme based on an\ninput-feature (X) in order to improve adaptivity, i.e. calibration conditional\non X. The performances of BVS and its proposed variants are tested on a\nbenchmark dataset for the prediction of atomization energies and compared to\nthe results of isotonic regression.","terms":["stat.ML","cs.LG","physics.chem-ph","physics.data-an"]},{"titles":"Physics-Informed with Power-Enhanced Residual Network for Interpolation and Inverse Problems","summaries":"This paper introduces a novel neural network structure called the\nPower-Enhancing residual network, designed to improve interpolation\ncapabilities for both smooth and non-smooth functions in 2D and 3D settings. By\nadding power terms to residual elements, the architecture boosts the network's\nexpressive power. The study explores network depth, width, and optimization\nmethods, showing the architecture's adaptability and performance advantages.\nConsistently, the results emphasize the exceptional accuracy of the proposed\nPower-Enhancing residual network, particularly for non-smooth functions.\nReal-world examples also confirm its superiority over plain neural network in\nterms of accuracy, convergence, and efficiency. The study also looks at the\nimpact of deeper network. Moreover, the proposed architecture is also applied\nto solving the inverse Burgers' equation, demonstrating superior performance.\nIn conclusion, the Power-Enhancing residual network offers a versatile solution\nthat significantly enhances neural network capabilities. The codes implemented\nare available at: \\url{https:\/\/github.com\/CMMAi\/ResNet_for_PINN}.","terms":["cs.LG","cs.CV","math.AP"]},{"titles":"GUPNet++: Geometry Uncertainty Propagation Network for Monocular 3D Object Detection","summaries":"Geometry plays a significant role in monocular 3D object detection. It can be\nused to estimate object depth by using the perspective projection between\nobject's physical size and 2D projection in the image plane, which can\nintroduce mathematical priors into deep models. However, this projection\nprocess also introduces error amplification, where the error of the estimated\nheight is amplified and reflected into the projected depth. It leads to\nunreliable depth inferences and also impairs training stability. To tackle this\nproblem, we propose a novel Geometry Uncertainty Propagation Network (GUPNet++)\nby modeling geometry projection in a probabilistic manner. This ensures depth\npredictions are well-bounded and associated with a reasonable uncertainty. The\nsignificance of introducing such geometric uncertainty is two-fold: (1). It\nmodels the uncertainty propagation relationship of the geometry projection\nduring training, improving the stability and efficiency of the end-to-end model\nlearning. (2). It can be derived to a highly reliable confidence to indicate\nthe quality of the 3D detection result, enabling more reliable detection\ninference. Experiments show that the proposed approach not only obtains\n(state-of-the-art) SOTA performance in image-based monocular 3D detection but\nalso demonstrates superiority in efficacy with a simplified framework.","terms":["cs.CV","cs.LG"]},{"titles":"Using Slisemap to interpret physical data","summaries":"Manifold visualisation techniques are commonly used to visualise\nhigh-dimensional datasets in physical sciences. In this paper we apply a\nrecently introduced manifold visualisation method, called Slise, on datasets\nfrom physics and chemistry. Slisemap combines manifold visualisation with\nexplainable artificial intelligence. Explainable artificial intelligence is\nused to investigate the decision processes of black box machine learning models\nand complex simulators. With Slisemap we find an embedding such that data items\nwith similar local explanations are grouped together. Hence, Slisemap gives us\nan overview of the different behaviours of a black box model. This makes\nSlisemap into a supervised manifold visualisation method, where the patterns in\nthe embedding reflect a target property. In this paper we show how Slisemap can\nbe used and evaluated on physical data and that Slisemap is helpful in finding\nmeaningful information on classification and regression models trained on these\ndatasets.","terms":["cs.LG","cs.AI","cs.HC"]},{"titles":"Ghost on the Shell: An Expressive Representation of General 3D Shapes","summaries":"The creation of photorealistic virtual worlds requires the accurate modeling\nof 3D surface geometry for a wide range of objects. For this, meshes are\nappealing since they 1) enable fast physics-based rendering with realistic\nmaterial and lighting, 2) support physical simulation, and 3) are\nmemory-efficient for modern graphics pipelines. Recent work on reconstructing\nand statistically modeling 3D shape, however, has critiqued meshes as being\ntopologically inflexible. To capture a wide range of object shapes, any 3D\nrepresentation must be able to model solid, watertight, shapes as well as thin,\nopen, surfaces. Recent work has focused on the former, and methods for\nreconstructing open surfaces do not support fast reconstruction with material\nand lighting or unconditional generative modelling. Inspired by the observation\nthat open surfaces can be seen as islands floating on watertight surfaces, we\nparameterize open surfaces by defining a manifold signed distance field on\nwatertight templates. With this parameterization, we further develop a\ngrid-based and differentiable representation that parameterizes both watertight\nand non-watertight meshes of arbitrary topology. Our new representation, called\nGhost-on-the-Shell (G-Shell), enables two important applications:\ndifferentiable rasterization-based reconstruction from multiview images and\ngenerative modelling of non-watertight meshes. We empirically demonstrate that\nG-Shell achieves state-of-the-art performance on non-watertight mesh\nreconstruction and generation tasks, while also performing effectively for\nwatertight meshes.","terms":["cs.CV","cs.GR","cs.LG"]},{"titles":"A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems","summaries":"Although decades of effort have been devoted to building Physical-Conceptual\n(PC) models for predicting the time-series evolution of geoscientific systems,\nrecent work shows that Machine Learning (ML) based Gated Recurrent Neural\nNetwork technology can be used to develop models that are much more accurate.\nHowever, the difficulty of extracting physical understanding from ML-based\nmodels complicates their utility for enhancing scientific knowledge regarding\nsystem structure and function. Here, we propose a physically-interpretable Mass\nConserving Perceptron (MCP) as a way to bridge the gap between PC-based and\nML-based modeling approaches. The MCP exploits the inherent isomorphism between\nthe directed graph structures underlying both PC models and GRNNs to explicitly\nrepresent the mass-conserving nature of physical processes while enabling the\nfunctional nature of such processes to be directly learned (in an interpretable\nmanner) from available data using off-the-shelf ML technology. As a proof of\nconcept, we investigate the functional expressivity (capacity) of the MCP,\nexplore its ability to parsimoniously represent the rainfall-runoff (RR)\ndynamics of the Leaf River Basin, and demonstrate its utility for scientific\nhypothesis testing. To conclude, we discuss extensions of the concept to enable\nML-based physical-conceptual representation of the coupled nature of\nmass-energy-information flows through geoscientific systems.","terms":["cs.LG","cs.AI"]},{"titles":"Applications of ML-Based Surrogates in Bayesian Approaches to Inverse Problems","summaries":"Neural networks have become a powerful tool as surrogate models to provide\nnumerical solutions for scientific problems with increased computational\nefficiency. This efficiency can be advantageous for numerically challenging\nproblems where time to solution is important or when evaluation of many similar\nanalysis scenarios is required. One particular area of scientific interest is\nthe setting of inverse problems, where one knows the forward dynamics of a\nsystem are described by a partial differential equation and the task is to\ninfer properties of the system given (potentially noisy) observations of these\ndynamics. We consider the inverse problem of inferring the location of a wave\nsource on a square domain, given a noisy solution to the 2-D acoustic wave\nequation. Under the assumption of Gaussian noise, a likelihood function for\nsource location can be formulated, which requires one forward simulation of the\nsystem per evaluation. Using a standard neural network as a surrogate model\nmakes it computationally feasible to evaluate this likelihood several times,\nand so Markov Chain Monte Carlo methods can be used to evaluate the posterior\ndistribution of the source location. We demonstrate that this method can\naccurately infer source-locations from noisy data.","terms":["stat.ML","cs.LG"]},{"titles":"Burgers' pinns with implicit euler transfer learning","summaries":"The Burgers equation is a well-established test case in the computational\nmodeling of several phenomena such as fluid dynamics, gas dynamics, shock\ntheory, cosmology, and others. In this work, we present the application of\nPhysics-Informed Neural Networks (PINNs) with an implicit Euler transfer\nlearning approach to solve the Burgers equation. The proposed approach consists\nin seeking a time-discrete solution by a sequence of Artificial Neural Networks\n(ANNs). At each time step, the previous ANN transfers its knowledge to the next\nnetwork model, which learns the current time solution by minimizing a loss\nfunction based on the implicit Euler approximation of the Burgers equation. The\napproach is tested for two benchmark problems: the first with an exact solution\nand the other with an alternative analytical solution. In comparison to the\nusual PINN models, the proposed approach has the advantage of requiring smaller\nneural network architectures with similar accurate results and potentially\ndecreasing computational costs.","terms":["cs.LG","cs.NA","math.NA"]},{"titles":"Cousins Of The Vendi Score: A Family Of Similarity-Based Diversity Metrics For Science And Machine Learning","summaries":"Measuring diversity accurately is important for many scientific fields,\nincluding machine learning (ML), ecology, and chemistry. The Vendi Score was\nintroduced as a generic similarity-based diversity metric that extends the Hill\nnumber of order q=1 by leveraging ideas from quantum statistical mechanics.\nContrary to many diversity metrics in ecology, the Vendi Score accounts for\nsimilarity and does not require knowledge of the prevalence of the categories\nin the collection to be evaluated for diversity. However, the Vendi Score\ntreats each item in a given collection with a level of sensitivity proportional\nto the item's prevalence. This is undesirable in settings where there is a\nsignificant imbalance in item prevalence. In this paper, we extend the other\nHill numbers using similarity to provide flexibility in allocating sensitivity\nto rare or common items. This leads to a family of diversity metrics -- Vendi\nscores with different levels of sensitivity -- that can be used in a variety of\napplications. We study the properties of the scores in a synthetic controlled\nsetting where the ground truth diversity is known. We then test their utility\nin improving molecular simulations via Vendi Sampling. Finally, we use the\nVendi scores to better understand the behavior of image generative models in\nterms of memorization, duplication, diversity, and sample quality.","terms":["cs.LG","physics.chem-ph","q-bio.PE"]},{"titles":"Projected Stochastic Gradient Descent with Quantum Annealed Binary Gradients","summaries":"We present, QP-SBGD, a novel layer-wise stochastic optimiser tailored towards\ntraining neural networks with binary weights, known as binary neural networks\n(BNNs), on quantum hardware. BNNs reduce the computational requirements and\nenergy consumption of deep learning models with minimal loss in accuracy.\nHowever, training them in practice remains to be an open challenge. Most known\nBNN-optimisers either rely on projected updates or binarise weights\npost-training. Instead, QP-SBGD approximately maps the gradient onto binary\nvariables, by solving a quadratic constrained binary optimisation. Under\npractically reasonable assumptions, we show that this update rule converges\nwith a rate of $\\mathcal{O}(1 \/ \\sqrt{T})$. Moreover, we show how the\n$\\mathcal{NP}$-hard projection can be effectively executed on an adiabatic\nquantum annealer, harnessing recent advancements in quantum computation. We\nalso introduce a projected version of this update rule and prove that if a\nfixed point exists in the binary variable space, the modified updates will\nconverge to it. Last but not least, our algorithm is implemented layer-wise,\nmaking it suitable to train larger networks on resource-limited quantum\nhardware. Through extensive evaluations, we show that QP-SBGD outperforms or is\non par with competitive and well-established baselines such as BinaryConnect,\nsignSGD and ProxQuant when optimising the Rosenbrock function, training BNNs as\nwell as binary graph neural networks.","terms":["cs.CV","cs.LG","quant-ph"]},{"titles":"Improving day-ahead Solar Irradiance Time Series Forecasting by Leveraging Spatio-Temporal Context","summaries":"Solar power harbors immense potential in mitigating climate change by\nsubstantially reducing CO$_{2}$ emissions. Nonetheless, the inherent\nvariability of solar irradiance poses a significant challenge for seamlessly\nintegrating solar power into the electrical grid. While the majority of prior\nresearch has centered on employing purely time series-based methodologies for\nsolar forecasting, only a limited number of studies have taken into account\nfactors such as cloud cover or the surrounding physical context. In this paper,\nwe put forth a deep learning architecture designed to harness spatio-temporal\ncontext using satellite data, to attain highly accurate \\textit{day-ahead}\ntime-series forecasting for any given station, with a particular emphasis on\nforecasting Global Horizontal Irradiance (GHI). We also suggest a methodology\nto extract a distribution for each time step prediction, which can serve as a\nvery valuable measure of uncertainty attached to the forecast. When evaluating\nmodels, we propose a testing scheme in which we separate particularly difficult\nexamples from easy ones, in order to capture the model performances in crucial\nsituations, which in the case of this study are the days suffering from varying\ncloudy conditions. Furthermore, we present a new multi-modal dataset gathering\nsatellite imagery over a large zone and time series for solar irradiance and\nother related physical variables from multiple geographically diverse solar\nstations. Our approach exhibits robust performance in solar irradiance\nforecasting, including zero-shot generalization tests at unobserved solar\nstations, and holds great promise in promoting the effective integration of\nsolar power into the grid.","terms":["cs.LG"]},{"titles":"Fast 2D Bicephalous Convolutional Autoencoder for Compressing 3D Time Projection Chamber Data","summaries":"High-energy large-scale particle colliders produce data at high speed in the\norder of 1 terabytes per second in nuclear physics and petabytes per second in\nhigh-energy physics. Developing real-time data compression algorithms to reduce\nsuch data at high throughput to fit permanent storage has drawn increasing\nattention. Specifically, at the newly constructed sPHENIX experiment at the\nRelativistic Heavy Ion Collider (RHIC), a time projection chamber is used as\nthe main tracking detector, which records particle trajectories in a volume of\na three-dimensional (3D) cylinder. The resulting data are usually very sparse\nwith occupancy around 10.8%. Such sparsity presents a challenge to conventional\nlearning-free lossy compression algorithms, such as SZ, ZFP, and MGARD. The 3D\nconvolutional neural network (CNN)-based approach, Bicephalous Convolutional\nAutoencoder (BCAE), outperforms traditional methods both in compression rate\nand reconstruction accuracy. BCAE can also utilize the computation power of\ngraphical processing units suitable for deployment in a modern heterogeneous\nhigh-performance computing environment. This work introduces two BCAE variants:\nBCAE++ and BCAE-2D. BCAE++ achieves a 15% better compression ratio and a 77%\nbetter reconstruction accuracy measured in mean absolute error compared with\nBCAE. BCAE-2D treats the radial direction as the channel dimension of an image,\nresulting in a 3x speedup in compression throughput. In addition, we\ndemonstrate an unbalanced autoencoder with a larger decoder can improve\nreconstruction accuracy without significantly sacrificing throughput. Lastly,\nwe observe both the BCAE++ and BCAE-2D can benefit more from using\nhalf-precision mode in throughput (76-79% increase) without loss in\nreconstruction accuracy. The source code and links to data and pretrained\nmodels can be found at https:\/\/github.com\/BNL-DAQ-LDRD\/NeuralCompression_v2.","terms":["stat.ML","cs.LG","hep-ex","nucl-ex"]},{"titles":"Symplectic Learning for Hamiltonian Neural Networks","summaries":"Machine learning methods are widely used in the natural sciences to model and\npredict physical systems from observation data. Yet, they are often used as\npoorly understood \"black boxes,\" disregarding existing mathematical structure\nand invariants of the problem. Recently, the proposal of Hamiltonian Neural\nNetworks (HNNs) took a first step towards a unified \"gray box\" approach, using\nphysical insight to improve performance for Hamiltonian systems. In this paper,\nwe explore a significantly improved training method for HNNs, exploiting the\nsymplectic structure of Hamiltonian systems with a different loss function.\nThis frees the loss from an artificial lower bound. We mathematically guarantee\nthe existence of an exact Hamiltonian function which the HNN can learn. This\nallows us to prove and numerically analyze the errors made by HNNs which, in\nturn, renders them fully explainable. Finally, we present a novel post-training\ncorrection to obtain the true Hamiltonian only from discretized observation\ndata, up to an arbitrary order.","terms":["cs.LG","cs.NA","math.NA"]},{"titles":"Learning curves for deep structured Gaussian feature models","summaries":"In recent years, significant attention in deep learning theory has been\ndevoted to analyzing when models that interpolate their training data can still\ngeneralize well to unseen examples. Many insights have been gained from\nstudying models with multiple layers of Gaussian random features, for which one\ncan compute precise generalization asymptotics. However, few works have\nconsidered the effect of weight anisotropy; most assume that the random\nfeatures are generated using independent and identically distributed Gaussian\nweights, and allow only for structure in the input data. Here, we use the\nreplica trick from statistical physics to derive learning curves for models\nwith many layers of structured Gaussian features. We show that allowing\ncorrelations between the rows of the first layer of features can aid\ngeneralization, while structure in later layers is generally detrimental. Our\nresults shed light on how weight structure affects generalization in a simple\nclass of solvable models.","terms":["stat.ML","cond-mat.dis-nn","cs.LG"]},{"titles":"Hindsight Learning for MDPs with Exogenous Inputs","summaries":"Many resource management problems require sequential decision-making under\nuncertainty, where the only uncertainty affecting the decision outcomes are\nexogenous variables outside the control of the decision-maker. We model these\nproblems as Exo-MDPs (Markov Decision Processes with Exogenous Inputs) and\ndesign a class of data-efficient algorithms for them termed Hindsight Learning\n(HL). Our HL algorithms achieve data efficiency by leveraging a key insight:\nhaving samples of the exogenous variables, past decisions can be revisited in\nhindsight to infer counterfactual consequences that can accelerate policy\nimprovements. We compare HL against classic baselines in the multi-secretary\nand airline revenue management problems. We also scale our algorithms to a\nbusiness-critical cloud resource management problem -- allocating Virtual\nMachines (VMs) to physical machines, and simulate their performance with real\ndatasets from a large public cloud provider. We find that HL algorithms\noutperform domain-specific heuristics, as well as state-of-the-art\nreinforcement learning methods.","terms":["cs.LG","stat.ML","68Q32","I.2.6"]},{"titles":"Relit-NeuLF: Efficient Relighting and Novel View Synthesis via Neural 4D Light Field","summaries":"In this paper, we address the problem of simultaneous relighting and novel\nview synthesis of a complex scene from multi-view images with a limited number\nof light sources. We propose an analysis-synthesis approach called Relit-NeuLF.\nFollowing the recent neural 4D light field network (NeuLF), Relit-NeuLF first\nleverages a two-plane light field representation to parameterize each ray in a\n4D coordinate system, enabling efficient learning and inference. Then, we\nrecover the spatially-varying bidirectional reflectance distribution function\n(SVBRDF) of a 3D scene in a self-supervised manner. A DecomposeNet learns to\nmap each ray to its SVBRDF components: albedo, normal, and roughness. Based on\nthe decomposed BRDF components and conditioning light directions, a RenderNet\nlearns to synthesize the color of the ray. To self-supervise the SVBRDF\ndecomposition, we encourage the predicted ray color to be close to the\nphysically-based rendering result using the microfacet model. Comprehensive\nexperiments demonstrate that the proposed method is efficient and effective on\nboth synthetic data and real-world human face data, and outperforms the\nstate-of-the-art results. We publicly released our code on GitHub. You can find\nit here: https:\/\/github.com\/oppo-us-research\/RelitNeuLF","terms":["cs.CV"]},{"titles":"Bullying10K: A Large-Scale Neuromorphic Dataset towards Privacy-Preserving Bullying Recognition","summaries":"The prevalence of violence in daily life poses significant threats to\nindividuals' physical and mental well-being. Using surveillance cameras in\npublic spaces has proven effective in proactively deterring and preventing such\nincidents. However, concerns regarding privacy invasion have emerged due to\ntheir widespread deployment. To address the problem, we leverage Dynamic Vision\nSensors (DVS) cameras to detect violent incidents and preserve privacy since it\ncaptures pixel brightness variations instead of static imagery. We introduce\nthe Bullying10K dataset, encompassing various actions, complex movements, and\nocclusions from real-life scenarios. It provides three benchmarks for\nevaluating different tasks: action recognition, temporal action localization,\nand pose estimation. With 10,000 event segments, totaling 12 billion events and\n255 GB of data, Bullying10K contributes significantly by balancing violence\ndetection and personal privacy persevering. And it also poses a challenge to\nthe neuromorphic dataset. It will serve as a valuable resource for training and\ndeveloping privacy-protecting video systems. The Bullying10K opens new\npossibilities for innovative approaches in these domains.","terms":["cs.CV"]},{"titles":"Poster: Real-Time Object Substitution for Mobile Diminished Reality with Edge Computing","summaries":"Diminished Reality (DR) is considered as the conceptual counterpart to\nAugmented Reality (AR), and has recently gained increasing attention from both\nindustry and academia. Unlike AR which adds virtual objects to the real world,\nDR allows users to remove physical content from the real world. When combined\nwith object replacement technology, it presents an further exciting avenue for\nexploration within the metaverse. Although a few researches have been conducted\non the intersection of object substitution and DR, there is no real-time object\nsubstitution for mobile diminished reality architecture with high quality. In\nthis paper, we propose an end-to-end architecture to facilitate immersive and\nreal-time scene construction for mobile devices with edge computing.","terms":["cs.CV","cs.LG","cs.NI"]},{"titles":"Inferring Relational Potentials in Interacting Systems","summaries":"Systems consisting of interacting agents are prevalent in the world, ranging\nfrom dynamical systems in physics to complex biological networks. To build\nsystems which can interact robustly in the real world, it is thus important to\nbe able to infer the precise interactions governing such systems. Existing\napproaches typically discover such interactions by explicitly modeling the\nfeed-forward dynamics of the trajectories. In this work, we propose Neural\nInteraction Inference with Potentials (NIIP) as an alternative approach to\ndiscover such interactions that enables greater flexibility in trajectory\nmodeling: it discovers a set of relational potentials, represented as energy\nfunctions, which when minimized reconstruct the original trajectory. NIIP\nassigns low energy to the subset of trajectories which respect the relational\nconstraints observed. We illustrate that with these representations NIIP\ndisplays unique capabilities in test-time. First, it allows trajectory\nmanipulation, such as interchanging interaction types across separately trained\nmodels, as well as trajectory forecasting. Additionally, it allows adding\nexternal hand-crafted potentials at test-time. Finally, NIIP enables the\ndetection of out-of-distribution samples and anomalies without explicit\ntraining. Website: https:\/\/energy-based-model.github.io\/interaction-potentials.","terms":["cs.LG"]},{"titles":"Clifford Group Equivariant Neural Networks","summaries":"We introduce Clifford Group Equivariant Neural Networks: a novel approach for\nconstructing $\\mathrm{O}(n)$- and $\\mathrm{E}(n)$-equivariant models. We\nidentify and study the $\\textit{Clifford group}$, a subgroup inside the\nClifford algebra tailored to achieve several favorable properties. Primarily,\nthe group's action forms an orthogonal automorphism that extends beyond the\ntypical vector space to the entire Clifford algebra while respecting the\nmultivector grading. This leads to several non-equivalent subrepresentations\ncorresponding to the multivector decomposition. Furthermore, we prove that the\naction respects not just the vector space structure of the Clifford algebra but\nalso its multiplicative structure, i.e., the geometric product. These findings\nimply that every polynomial in multivectors, An advantage worth mentioning is\nthat we obtain expressive layers that can elegantly generalize to inner-product\nspaces of any dimension. We demonstrate, notably from a single core\nimplementation, state-of-the-art performance on several distinct tasks,\nincluding a three-dimensional $n$-body experiment, a four-dimensional\nLorentz-equivariant high-energy physics experiment, and a five-dimensional\nconvex hull experiment.","terms":["cs.LG","cs.AI"]},{"titles":"Unifying O(3) Equivariant Neural Networks Design with Tensor-Network Formalism","summaries":"Many learning tasks, including learning potential energy surfaces from ab\ninitio calculations, involve global spatial symmetries and permutational\nsymmetry between atoms or general particles. Equivariant graph neural networks\nare a standard approach to such problems, with one of the most successful\nmethods employing tensor products between various tensors that transform under\nthe spatial group. However, as the number of different tensors and the\ncomplexity of relationships between them increase, maintaining parsimony and\nequivariance becomes increasingly challenging. In this paper, we propose using\nfusion diagrams, a technique widely employed in simulating SU($2$)-symmetric\nquantum many-body problems, to design new equivariant components for\nequivariant neural networks. This results in a diagrammatic approach to\nconstructing novel neural network architectures. When applied to particles\nwithin a given local neighborhood, the resulting components, which we term\n\"fusion blocks,\" serve as universal approximators of any continuous equivariant\nfunction defined in the neighborhood. We incorporate a fusion block into\npre-existing equivariant architectures (Cormorant and MACE), leading to\nimproved performance with fewer parameters on a range of challenging chemical\nproblems. Furthermore, we apply group-equivariant neural networks to study\nnon-adiabatic molecular dynamics of stilbene cis-trans isomerization. Our\napproach, which combines tensor networks with equivariant neural networks,\nsuggests a potentially fruitful direction for designing more expressive\nequivariant neural networks.","terms":["cs.LG","cs.AI","quant-ph","stat.ML"]},{"titles":"MMGP: a Mesh Morphing Gaussian Process-based machine learning method for regression of physical problems under non-parameterized geometrical variability","summaries":"When learning simulations for modeling physical phenomena in industrial\ndesigns, geometrical variabilities are of prime interest. While classical\nregression techniques prove effective for parameterized geometries, practical\nscenarios often involve the absence of shape parametrization during the\ninference stage, leaving us with only mesh discretizations as available data.\nLearning simulations from such mesh-based representations poses significant\nchallenges, with recent advances relying heavily on deep graph neural networks\nto overcome the limitations of conventional machine learning approaches.\nDespite their promising results, graph neural networks exhibit certain\ndrawbacks, including their dependency on extensive datasets and limitations in\nproviding built-in predictive uncertainties or handling large meshes. In this\nwork, we propose a machine learning method that do not rely on graph neural\nnetworks. Complex geometrical shapes and variations with fixed topology are\ndealt with using well-known mesh morphing onto a common support, combined with\nclassical dimensionality reduction techniques and Gaussian processes. The\nproposed methodology can easily deal with large meshes without the need for\nexplicit shape parameterization and provides crucial predictive uncertainties,\nwhich are essential for informed decision-making. In the considered numerical\nexperiments, the proposed method is competitive with respect to existing graph\nneural networks, regarding training efficiency and accuracy of the predictions.","terms":["cs.LG"]},{"titles":"MLMOD: Machine Learning Methods for Data-Driven Modeling in LAMMPS","summaries":"MLMOD is a software package for incorporating machine learning approaches and\nmodels into simulations of microscale mechanics and molecular dynamics in\nLAMMPS. Recent machine learning approaches provide promising data-driven\napproaches for learning representations for system behaviors from experimental\ndata and high fidelity simulations. The package faciliates learning and using\ndata-driven models for (i) dynamics of the system at larger spatial-temporal\nscales (ii) interactions between system components, (iii) features yielding\ncoarser degrees of freedom, and (iv) features for new quantities of interest\ncharacterizing system behaviors. MLMOD provides hooks in LAMMPS for (i)\nmodeling dynamics and time-step integration, (ii) modeling interactions, and\n(iii) computing quantities of interest characterizing system states. The\npackage allows for use of machine learning methods with general model classes\nincluding Neural Networks, Gaussian Process Regression, Kernel Models, and\nother approaches. Here we discuss our prototype C++\/Python package, aims, and\nexample usage. The package is integrated currently with the mesocale and\nmolecular dynamics simulation package LAMMPS and PyTorch. For related papers,\nexamples, updates, and additional information see\nhttps:\/\/github.com\/atzberg\/mlmod and http:\/\/atzberger.org\/.","terms":["cs.LG","cond-mat.mes-hall","cond-mat.soft","cs.NA","math.NA"]},{"titles":"A Sparse Bayesian Learning for Diagnosis of Nonstationary and Spatially Correlated Faults with Application to Multistation Assembly Systems","summaries":"Sensor technology developments provide a basis for effective fault diagnosis\nin manufacturing systems. However, the limited number of sensors due to\nphysical constraints or undue costs hinders the accurate diagnosis in the\nactual process. In addition, time-varying operational conditions that generate\nnonstationary process faults and the correlation information in the process\nrequire to consider for accurate fault diagnosis in the manufacturing systems.\nThis article proposes a novel fault diagnosis method: clustering spatially\ncorrelated sparse Bayesian learning (CSSBL), and explicitly demonstrates its\napplicability in a multistation assembly system that is vulnerable to the above\nchallenges. Specifically, the method is based on a practical assumption that it\nwill likely have a few process faults (sparse). In addition, the hierarchical\nstructure of CSSBL has several parameterized prior distributions to address the\nabove challenges. As posterior distributions of process faults do not have\nclosed form, this paper derives approximate posterior distributions through\nVariational Bayes inference. The proposed method's efficacy is provided through\nnumerical and real-world case studies utilizing an actual autobody assembly\nsystem. The generalizability of the proposed method allows the technique to be\napplied in fault diagnosis in other domains, including communication and\nhealthcare systems.","terms":["cs.LG","stat.AP"]},{"titles":"Exponential weight averaging as damped harmonic motion","summaries":"The exponential moving average (EMA) is a commonly used statistic for\nproviding stable estimates of stochastic quantities in deep learning\noptimization. Recently, EMA has seen considerable use in generative models,\nwhere it is computed with respect to the model weights, and significantly\nimproves the stability of the inference model during and after training. While\nthe practice of weight averaging at the end of training is well-studied and\nknown to improve estimates of local optima, the benefits of EMA over the course\nof training is less understood. In this paper, we derive an explicit connection\nbetween EMA and a damped harmonic system between two particles, where one\nparticle (the EMA weights) is drawn to the other (the model weights) via an\nidealized zero-length spring. We then leverage this physical analogy to analyze\nthe effectiveness of EMA, and propose an improved training algorithm, which we\ncall BELAY. Finally, we demonstrate theoretically and empirically several\nadvantages enjoyed by BELAY over standard EMA.","terms":["cs.LG","math.OC"]},{"titles":"Analyzing the contribution of different passively collected data to predict Stress and Depression","summaries":"The possibility of recognizing diverse aspects of human behavior and\nenvironmental context from passively captured data motivates its use for mental\nhealth assessment. In this paper, we analyze the contribution of different\npassively collected sensor data types (WiFi, GPS, Social interaction, Phone\nLog, Physical Activity, Audio, and Academic features) to predict daily\nselfreport stress and PHQ-9 depression score. First, we compute 125 mid-level\nfeatures from the original raw data. These 125 features include groups of\nfeatures from the different sensor data types. Then, we evaluate the\ncontribution of each feature type by comparing the performance of Neural\nNetwork models trained with all features against Neural Network models trained\nwith specific feature groups. Our results show that WiFi features (which encode\nmobility patterns) and Phone Log features (which encode information correlated\nwith sleep patterns), provide significative information for stress and\ndepression prediction.","terms":["cs.LG"]},{"titles":"Physics-aware Machine Learning Revolutionizes Scientific Paradigm for Machine Learning and Process-based Hydrology","summaries":"Accurate hydrological understanding and water cycle prediction are crucial\nfor addressing scientific and societal challenges associated with the\nmanagement of water resources, particularly under the dynamic influence of\nanthropogenic climate change. Existing reviews predominantly concentrate on the\ndevelopment of machine learning (ML) in this field, yet there is a clear\ndistinction between hydrology and ML as separate paradigms. Here, we introduce\nphysics-aware ML as a transformative approach to overcome the perceived barrier\nand revolutionize both fields. Specifically, we present a comprehensive review\nof the physics-aware ML methods, building a structured community (PaML) of\nexisting methodologies that integrate prior physical knowledge or physics-based\nmodeling into ML. We systematically analyze these PaML methodologies with\nrespect to four aspects: physical data-guided ML, physics-informed ML,\nphysics-embedded ML, and physics-aware hybrid learning. PaML facilitates\nML-aided hypotheses, accelerating insights from big data and fostering\nscientific discoveries. We first conduct a systematic review of hydrology in\nPaML, including rainfall-runoff hydrological processes and hydrodynamic\nprocesses, and highlight the most promising and challenging directions for\ndifferent objectives and PaML methods. Finally, a new PaML-based hydrology\nplatform, termed HydroPML, is released as a foundation for hydrological\napplications. HydroPML enhances the explainability and causality of ML and lays\nthe groundwork for the digital water cycle's realization. The HydroPML platform\nis publicly available at https:\/\/hydropml.github.io\/.","terms":["cs.LG","cs.AI","physics.flu-dyn"]},{"titles":"Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models","summaries":"Multimodal large language models (MLLMs) have shown great potential in\nperception and interpretation tasks, but their capabilities in predictive\nreasoning remain under-explored. To address this gap, we introduce a novel\nbenchmark that assesses the predictive reasoning capabilities of MLLMs across\ndiverse scenarios. Our benchmark targets three important domains: abstract\npattern reasoning, human activity prediction, and physical interaction\nprediction. We further develop three evaluation methods powered by large\nlanguage model to robustly quantify a model's performance in predicting and\nreasoning the future based on multi-visual context. Empirical experiments\nconfirm the soundness of the proposed benchmark and evaluation methods via\nrigorous testing and reveal pros and cons of current popular MLLMs in the task\nof predictive reasoning. Lastly, our proposed benchmark provides a standardized\nevaluation framework for MLLMs and can facilitate the development of more\nadvanced models that can reason and predict over complex long sequence of\nmultimodal input.","terms":["cs.CV"]},{"titles":"Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook","summaries":"Temporal data, notably time series and spatio-temporal data, are prevalent in\nreal-world applications. They capture dynamic system measurements and are\nproduced in vast quantities by both physical and virtual sensors. Analyzing\nthese data types is vital to harnessing the rich information they encompass and\nthus benefits a wide range of downstream tasks. Recent advances in large\nlanguage and other foundational models have spurred increased use of these\nmodels in time series and spatio-temporal data mining. Such methodologies not\nonly enable enhanced pattern recognition and reasoning across diverse domains\nbut also lay the groundwork for artificial general intelligence capable of\ncomprehending and processing common temporal data. In this survey, we offer a\ncomprehensive and up-to-date review of large models tailored (or adapted) for\ntime series and spatio-temporal data, spanning four key facets: data types,\nmodel categories, model scopes, and application areas\/tasks. Our objective is\nto equip practitioners with the knowledge to develop applications and further\nresearch in this underexplored domain. We primarily categorize the existing\nliterature into two major clusters: large models for time series analysis\n(LM4TS) and spatio-temporal data mining (LM4STD). On this basis, we further\nclassify research based on model scopes (i.e., general vs. domain-specific) and\napplication areas\/tasks. We also provide a comprehensive collection of\npertinent resources, including datasets, model assets, and useful tools,\ncategorized by mainstream applications. This survey coalesces the latest\nstrides in large model-centric research on time series and spatio-temporal\ndata, underscoring the solid foundations, current advances, practical\napplications, abundant resources, and future research opportunities.","terms":["cs.LG","cs.AI"]},{"titles":"Meta-learning of Physics-informed Neural Networks for Efficiently Solving Newly Given PDEs","summaries":"We propose a neural network-based meta-learning method to efficiently solve\npartial differential equation (PDE) problems. The proposed method is designed\nto meta-learn how to solve a wide variety of PDE problems, and uses the\nknowledge for solving newly given PDE problems. We encode a PDE problem into a\nproblem representation using neural networks, where governing equations are\nrepresented by coefficients of a polynomial function of partial derivatives,\nand boundary conditions are represented by a set of point-condition pairs. We\nuse the problem representation as an input of a neural network for predicting\nsolutions, which enables us to efficiently predict problem-specific solutions\nby the forwarding process of the neural network without updating model\nparameters. To train our model, we minimize the expected error when adapted to\na PDE problem based on the physics-informed neural network framework, by which\nwe can evaluate the error even when solutions are unknown. We demonstrate that\nour proposed method outperforms existing methods in predicting solutions of PDE\nproblems.","terms":["stat.ML","cs.AI","cs.LG"]},{"titles":"CoarsenConf: Equivariant Coarsening with Aggregated Attention for Molecular Conformer Generation","summaries":"Molecular conformer generation (MCG) is an important task in cheminformatics\nand drug discovery. The ability to efficiently generate low-energy 3D\nstructures can avoid expensive quantum mechanical simulations, leading to\naccelerated virtual screenings and enhanced structural exploration. Several\ngenerative models have been developed for MCG, but many struggle to\nconsistently produce high-quality conformers. To address these issues, we\nintroduce CoarsenConf, which coarse-grains molecular graphs based on torsional\nangles and integrates them into an SE(3)-equivariant hierarchical variational\nautoencoder. Through equivariant coarse-graining, we aggregate the fine-grained\natomic coordinates of subgraphs connected via rotatable bonds, creating a\nvariable-length coarse-grained latent representation. Our model uses a novel\naggregated attention mechanism to restore fine-grained coordinates from the\ncoarse-grained latent representation, enabling efficient generation of accurate\nconformers. Furthermore, we evaluate the chemical and biochemical quality of\nour generated conformers on multiple downstream applications, including\nproperty prediction and oracle-based protein docking. Overall, CoarsenConf\ngenerates more accurate conformer ensembles compared to prior generative\nmodels.","terms":["cs.LG","physics.chem-ph","q-bio.BM"]},{"titles":"CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation","summaries":"Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks\nbut lack an intuitive interface for consistent image-to-image (I2I)\ntranslation. Various methods have been explored to address this issue,\nincluding mask-based methods, attention-based methods, and image-conditioning.\nHowever, it remains a critical challenge to enable unpaired I2I translation\nwith pre-trained DMs while maintaining satisfying consistency. This paper\nintroduces Cyclenet, a novel but simple method that incorporates cycle\nconsistency into DMs to regularize image manipulation. We validate Cyclenet on\nunpaired I2I tasks of different granularities. Besides the scene and object\nlevel translation, we additionally contribute a multi-domain I2I translation\ndataset to study the physical state changes of objects. Our empirical studies\nshow that Cyclenet is superior in translation consistency and quality, and can\ngenerate high-quality images for out-of-domain distributions with a simple\nchange of the textual prompt. Cyclenet is a practical framework, which is\nrobust even with very limited training data (around 2k) and requires minimal\ncomputational resources (1 GPU) to train. Project homepage:\nhttps:\/\/cyclenetweb.github.io\/","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"Generative Marginalization Models","summaries":"We introduce marginalization models (MaMs), a new family of generative models\nfor high-dimensional discrete data. They offer scalable and flexible generative\nmodeling with tractable likelihoods by explicitly modeling all induced marginal\ndistributions. Marginalization models enable fast evaluation of arbitrary\nmarginal probabilities with a single forward pass of the neural network, which\novercomes a major limitation of methods with exact marginal inference, such as\nautoregressive models (ARMs). We propose scalable methods for learning the\nmarginals, grounded in the concept of \"marginalization self-consistency\".\nUnlike previous methods, MaMs support scalable training of any-order generative\nmodels for high-dimensional problems under the setting of energy-based\ntraining, where the goal is to match the learned distribution to a given\ndesired probability (specified by an unnormalized (log) probability function\nsuch as energy function or reward function). We demonstrate the effectiveness\nof the proposed model on a variety of discrete data distributions, including\nbinary images, language, physical systems, and molecules, for maximum\nlikelihood and energy-based training settings. MaMs achieve orders of magnitude\nspeedup in evaluating the marginal probabilities on both settings. For\nenergy-based training tasks, MaMs enable any-order generative modeling of\nhigh-dimensional problems beyond the capability of previous methods. Code is at\nhttps:\/\/github.com\/PrincetonLIPS\/MaM.","terms":["cs.LG","cs.AI"]},{"titles":"Neural Likelihood Approximation for Integer Valued Time Series Data","summaries":"Stochastic processes defined on integer valued state spaces are popular\nwithin the physical and biological sciences. These models are necessary for\ncapturing the dynamics of small systems where the individual nature of the\npopulations cannot be ignored and stochastic effects are important. The\ninference of the parameters of such models, from time series data, is difficult\ndue to intractability of the likelihood; current methods, based on simulations\nof the underlying model, can be so computationally expensive as to be\nprohibitive. In this paper we construct a neural likelihood approximation for\ninteger valued time series data using causal convolutions, which allows us to\nevaluate the likelihood of the whole time series in parallel. We demonstrate\nour method by performing inference on a number of ecological and\nepidemiological models, showing that we can accurately approximate the true\nposterior while achieving significant computational speed ups in situations\nwhere current methods struggle.","terms":["stat.ML","cs.LG"]},{"titles":"CAT: Closed-loop Adversarial Training for Safe End-to-End Driving","summaries":"Driving safety is a top priority for autonomous vehicles. Orthogonal to prior\nwork handling accident-prone traffic events by algorithm designs at the policy\nlevel, we investigate a Closed-loop Adversarial Training (CAT) framework for\nsafe end-to-end driving in this paper through the lens of environment\naugmentation. CAT aims to continuously improve the safety of driving agents by\ntraining the agent on safety-critical scenarios that are dynamically generated\nover time. A novel resampling technique is developed to turn log-replay\nreal-world driving scenarios into safety-critical ones via probabilistic\nfactorization, where the adversarial traffic generation is modeled as the\nmultiplication of standard motion prediction sub-problems. Consequently, CAT\ncan launch more efficient physical attacks compared to existing safety-critical\nscenario generation methods and yields a significantly less computational cost\nin the iterative learning pipeline. We incorporate CAT into the MetaDrive\nsimulator and validate our approach on hundreds of driving scenarios imported\nfrom real-world driving datasets. Experimental results demonstrate that CAT can\neffectively generate adversarial scenarios countering the agent being trained.\nAfter training, the agent can achieve superior driving safety in both\nlog-replay and safety-critical traffic scenarios on the held-out test set. Code\nand data are available at https:\/\/metadriverse.github.io\/cat.","terms":["cs.LG"]},{"titles":"Reduction of rain-induced errors for wind speed estimation on SAR observations using convolutional neural networks","summaries":"Synthetic Aperture Radar is known to be able to provide high-resolution\nestimates of surface wind speed. These estimates usually rely on a Geophysical\nModel Function (GMF) that has difficulties accounting for non-wind processes\nsuch as rain events. Convolutional neural network, on the other hand, have the\ncapacity to use contextual information and have demonstrated their ability to\ndelimit rainfall areas. By carefully building a large dataset of SAR\nobservations from the Copernicus Sentinel-1 mission, collocated with both GMF\nand atmospheric model wind speeds as well as rainfall estimates, we were able\nto train a wind speed estimator with reduced errors under rain. Collocations\nwith in-situ wind speed measurements from buoys show a root mean square error\nthat is reduced by 27% (resp. 45%) under rainfall estimated at more than 1 mm\/h\n(resp. 3 mm\/h). These results demonstrate the capacity of deep learning models\nto correct rain-related errors in SAR products.","terms":["cs.CV","physics.ao-ph"]},{"titles":"REVAMP: Automated Simulations of Adversarial Attacks on Arbitrary Objects in Realistic Scenes","summaries":"Deep Learning models, such as those used in an autonomous vehicle are\nvulnerable to adversarial attacks where an attacker could place an adversarial\nobject in the environment, leading to mis-classification. Generating these\nadversarial objects in the digital space has been extensively studied, however\nsuccessfully transferring these attacks from the digital realm to the physical\nrealm has proven challenging when controlling for real-world environmental\nfactors. In response to these limitations, we introduce REVAMP, an easy-to-use\nPython library that is the first-of-its-kind tool for creating attack scenarios\nwith arbitrary objects and simulating realistic environmental factors,\nlighting, reflection, and refraction. REVAMP enables researchers and\npractitioners to swiftly explore various scenarios within the digital realm by\noffering a wide range of configurable options for designing experiments and\nusing differentiable rendering to reproduce physically plausible adversarial\nobjects. We will demonstrate and invite the audience to try REVAMP to produce\nan adversarial texture on a chosen object while having control over various\nscene parameters. The audience will choose a scene, an object to attack, the\ndesired attack class, and the number of camera positions to use. Then, in real\ntime, we show how this altered texture causes the chosen object to be\nmis-classified, showcasing the potential of REVAMP in real-world scenarios.\nREVAMP is open-source and available at https:\/\/github.com\/poloclub\/revamp.","terms":["cs.LG","cs.CV"]},{"titles":"Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness","summaries":"As researchers strive to narrow the gap between machine intelligence and\nhuman through the development of artificial intelligence technologies, it is\nimperative that we recognize the critical importance of trustworthiness in\nopen-world, which has become ubiquitous in all aspects of daily life for\neveryone. However, several challenges may create a crisis of trust in current\nartificial intelligence systems that need to be bridged: 1) Insufficient\nexplanation of predictive results; 2) Inadequate generalization for learning\nmodels; 3) Poor adaptability to uncertain environments. Consequently, we\nexplore a neural program to bridge trustworthiness and open-world learning,\nextending from single-modal to multi-modal scenarios for readers. 1) To enhance\ndesign-level interpretability, we first customize trustworthy networks with\nspecific physical meanings; 2) We then design environmental well-being\ntask-interfaces via flexible learning regularizers for improving the\ngeneralization of trustworthy learning; 3) We propose to increase the\nrobustness of trustworthy learning by integrating open-world recognition losses\nwith agent mechanisms. Eventually, we enhance various trustworthy properties\nthrough the establishment of design-level explainability, environmental\nwell-being task-interfaces and open-world recognition programs. These designed\nopen-world protocols are applicable across a wide range of surroundings, under\nopen-world multimedia recognition scenarios with significant performance\nimprovements observed.","terms":["stat.ML","cs.LG"]},{"titles":"ShapeGraFormer: GraFormer-Based Network for Hand-Object Reconstruction from a Single Depth Map","summaries":"3D reconstruction of hand-object manipulations is important for emulating\nhuman actions. Most methods dealing with challenging object manipulation\nscenarios, focus on hands reconstruction in isolation, ignoring physical and\nkinematic constraints due to object contact. Some approaches produce more\nrealistic results by jointly reconstructing 3D hand-object interactions.\nHowever, they focus on coarse pose estimation or rely upon known hand and\nobject shapes. We propose the first approach for realistic 3D hand-object shape\nand pose reconstruction from a single depth map. Unlike previous work, our\nvoxel-based reconstruction network regresses the vertex coordinates of a hand\nand an object and reconstructs more realistic interaction. Our pipeline\nadditionally predicts voxelized hand-object shapes, having a one-to-one mapping\nto the input voxelized depth. Thereafter, we exploit the graph nature of the\nhand and object shapes, by utilizing the recent GraFormer network with\npositional embedding to reconstruct shapes from template meshes. In addition,\nwe show the impact of adding another GraFormer component that refines the\nreconstructed shapes based on the hand-object interactions and its ability to\nreconstruct more accurate object shapes. We perform an extensive evaluation on\nthe HO-3D and DexYCB datasets and show that our method outperforms existing\napproaches in hand reconstruction and produces plausible reconstructions for\nthe objects","terms":["cs.CV"]},{"titles":"Adversarial Training for Physics-Informed Neural Networks","summaries":"Physics-informed neural networks have shown great promise in solving partial\ndifferential equations. However, due to insufficient robustness, vanilla PINNs\noften face challenges when solving complex PDEs, especially those involving\nmulti-scale behaviors or solutions with sharp or oscillatory characteristics.\nTo address these issues, based on the projected gradient descent adversarial\nattack, we proposed an adversarial training strategy for PINNs termed by\nAT-PINNs. AT-PINNs enhance the robustness of PINNs by fine-tuning the model\nwith adversarial samples, which can accurately identify model failure locations\nand drive the model to focus on those regions during training. AT-PINNs can\nalso perform inference with temporal causality by selecting the initial\ncollocation points around temporal initial values. We implement AT-PINNs to the\nelliptic equation with multi-scale coefficients, Poisson equation with\nmulti-peak solutions, Burgers equation with sharp solutions and the Allen-Cahn\nequation. The results demonstrate that AT-PINNs can effectively locate and\nreduce failure regions. Moreover, AT-PINNs are suitable for solving complex\nPDEs, since locating failure regions through adversarial attacks is independent\nof the size of failure regions or the complexity of the distribution.","terms":["cs.LG","cs.NA","math.NA"]},{"titles":"Quantum Acceleration of Infinite Horizon Average-Reward Reinforcement Learning","summaries":"This paper investigates the potential of quantum acceleration in addressing\ninfinite horizon Markov Decision Processes (MDPs) to enhance average reward\noutcomes. We introduce an innovative quantum framework for the agent's\nengagement with an unknown MDP, extending the conventional interaction\nparadigm. Our approach involves the design of an optimism-driven tabular\nReinforcement Learning algorithm that harnesses quantum signals acquired by the\nagent through efficient quantum mean estimation techniques. Through thorough\ntheoretical analysis, we demonstrate that the quantum advantage in mean\nestimation leads to exponential advancements in regret guarantees for infinite\nhorizon Reinforcement Learning. Specifically, the proposed Quantum algorithm\nachieves a regret bound of $\\tilde{\\mathcal{O}}(1)$, a significant improvement\nover the $\\tilde{\\mathcal{O}}(\\sqrt{T})$ bound exhibited by classical\ncounterparts.","terms":["cs.LG","cs.AI","quant-ph"]},{"titles":"Bi-fidelity Variational Auto-encoder for Uncertainty Quantification","summaries":"Quantifying the uncertainty of quantities of interest (QoIs) from physical\nsystems is a primary objective in model validation. However, achieving this\ngoal entails balancing the need for computational efficiency with the\nrequirement for numerical accuracy. To address this trade-off, we propose a\nnovel bi-fidelity formulation of variational auto-encoders (BF-VAE) designed to\nestimate the uncertainty associated with a QoI from low-fidelity (LF) and\nhigh-fidelity (HF) samples of the QoI. This model allows for the approximation\nof the statistics of the HF QoI by leveraging information derived from its LF\ncounterpart. Specifically, we design a bi-fidelity auto-regressive model in the\nlatent space that is integrated within the VAE's probabilistic encoder-decoder\nstructure. An effective algorithm is proposed to maximize the variational lower\nbound of the HF log-likelihood in the presence of limited HF data, resulting in\nthe synthesis of HF realizations with a reduced computational cost.\nAdditionally, we introduce the concept of the bi-fidelity information\nbottleneck (BF-IB) to provide an information-theoretic interpretation of the\nproposed BF-VAE model. Our numerical results demonstrate that BF-VAE leads to\nconsiderably improved accuracy, as compared to a VAE trained using only HF\ndata, when limited HF data is available.","terms":["stat.ML","cs.LG","cs.NA","math.NA"]},{"titles":"A Computational Framework for Solving Wasserstein Lagrangian Flows","summaries":"The dynamical formulation of the optimal transport can be extended through\nvarious choices of the underlying geometry ($\\textit{kinetic energy}$), and the\nregularization of density paths ($\\textit{potential energy}$). These\ncombinations yield different variational problems ($\\textit{Lagrangians}$),\nencompassing many variations of the optimal transport problem such as the\nSchr\\\"odinger bridge, unbalanced optimal transport, and optimal transport with\nphysical constraints, among others. In general, the optimal density path is\nunknown, and solving these variational problems can be computationally\nchallenging. Leveraging the dual formulation of the Lagrangians, we propose a\nnovel deep learning based framework approaching all of these problems from a\nunified perspective. Our method does not require simulating or backpropagating\nthrough the trajectories of the learned dynamics, and does not need access to\noptimal couplings. We showcase the versatility of the proposed framework by\noutperforming previous approaches for the single-cell trajectory inference,\nwhere incorporating prior knowledge into the dynamics is crucial for correct\npredictions.","terms":["cs.LG","math.OC","stat.ML"]},{"titles":"Learning to Sample Better","summaries":"These lecture notes provide an introduction to recent advances in generative\nmodeling methods based on the dynamical transportation of measures, by means of\nwhich samples from a simple base measure are mapped to samples from a target\nmeasure of interest. Special emphasis is put on the applications of these\nmethods to Monte-Carlo (MC) sampling techniques, such as importance sampling\nand Markov Chain Monte-Carlo (MCMC) schemes. In this context, it is shown how\nthe maps can be learned variationally using data generated by MC sampling, and\nhow they can in turn be used to improve such sampling in a positive feedback\nloop.","terms":["cs.LG","stat.ML"]},{"titles":"Patch of Invisibility: Naturalistic Physical Black-Box Adversarial Attacks on Object Detectors","summaries":"Adversarial attacks on deep-learning models have been receiving increased\nattention in recent years. Work in this area has mostly focused on\ngradient-based techniques, so-called ``white-box'' attacks, wherein the\nattacker has access to the targeted model's internal parameters; such an\nassumption is usually unrealistic in the real world. Some attacks additionally\nuse the entire pixel space to fool a given model, which is neither practical\nnor physical (i.e., real-world). On the contrary, we propose herein a direct,\nblack-box, gradient-free method that uses the learned image manifold of a\npretrained generative adversarial network (GAN) to generate naturalistic\nphysical adversarial patches for object detectors. To our knowledge this is the\nfirst and only method that performs black-box physical attacks directly on\nobject-detection models, which results with a model-agnostic attack. We show\nthat our proposed method works both digitally and physically. We compared our\napproach against four different black-box attacks with different\nconfigurations. Our approach outperformed all other approaches that were tested\nin our experiments by a large margin.","terms":["cs.CV","cs.AI","cs.NE"]},{"titles":"MoConVQ: Unified Physics-Based Motion Control via Scalable Discrete Representations","summaries":"In this work, we present MoConVQ, a novel unified framework for physics-based\nmotion control leveraging scalable discrete representations. Building upon\nvector quantized variational autoencoders (VQ-VAE) and model-based\nreinforcement learning, our approach effectively learns motion embeddings from\na large, unstructured dataset spanning tens of hours of motion examples. The\nresultant motion representation not only captures diverse motion skills but\nalso offers a robust and intuitive interface for various applications. We\ndemonstrate the versatility of MoConVQ through several applications: universal\ntracking control from various motion sources, interactive character control\nwith latent motion representations using supervised learning, physics-based\nmotion generation from natural language descriptions using the GPT framework,\nand, most interestingly, seamless integration with large language models (LLMs)\nwith in-context learning to tackle complex and abstract tasks.","terms":["cs.CV","cs.GR"]},{"titles":"Combat Urban Congestion via Collaboration: Heterogeneous GNN-based MARL for Coordinated Platooning and Traffic Signal Control","summaries":"Over the years, reinforcement learning has emerged as a popular approach to\ndevelop signal control and vehicle platooning strategies either independently\nor in a hierarchical way. However, jointly controlling both in real-time to\nalleviate traffic congestion presents new challenges, such as the inherent\nphysical and behavioral heterogeneity between signal control and platooning, as\nwell as coordination between them. This paper proposes an innovative solution\nto tackle these challenges based on heterogeneous graph multi-agent\nreinforcement learning and traffic theories. Our approach involves: 1)\ndesigning platoon and signal control as distinct reinforcement learning agents\nwith their own set of observations, actions, and reward functions to optimize\ntraffic flow; 2) designing coordination by incorporating graph neural networks\nwithin multi-agent reinforcement learning to facilitate seamless information\nexchange among agents on a regional scale. We evaluate our approach through\nSUMO simulation, which shows a convergent result in terms of various\ntransportation metrics and better performance over sole signal or platooning\ncontrol.","terms":["cs.LG","cs.MA"]},{"titles":"Neural Packing: from Visual Sensing to Reinforcement Learning","summaries":"We present a novel learning framework to solve the transport-and-packing\n(TAP) problem in 3D. It constitutes a full solution pipeline from partial\nobservations of input objects via RGBD sensing and recognition to final box\nplacement, via robotic motion planning, to arrive at a compact packing in a\ntarget container. The technical core of our method is a neural network for TAP,\ntrained via reinforcement learning (RL), to solve the NP-hard combinatorial\noptimization problem. Our network simultaneously selects an object to pack and\ndetermines the final packing location, based on a judicious encoding of the\ncontinuously evolving states of partially observed source objects and available\nspaces in the target container, using separate encoders both enabled with\nattention mechanisms. The encoded feature vectors are employed to compute the\nmatching scores and feasibility masks of different pairings of box selection\nand available space configuration for packing strategy optimization. Extensive\nexperiments, including ablation studies and physical packing execution by a\nreal robot (Universal Robot UR5e), are conducted to evaluate our method in\nterms of its design choices, scalability, generalizability, and comparisons to\nbaselines, including the most recent RL-based TAP solution. We also contribute\nthe first benchmark for TAP which covers a variety of input settings and\ndifficulty levels.","terms":["cs.LG","cs.GR","cs.RO"]},{"titles":"Correcting model misspecification in physics-informed neural networks (PINNs)","summaries":"Data-driven discovery of governing equations in computational science has\nemerged as a new paradigm for obtaining accurate physical models and as a\npossible alternative to theoretical derivations. The recently developed\nphysics-informed neural networks (PINNs) have also been employed to learn\ngoverning equations given data across diverse scientific disciplines. Despite\nthe effectiveness of PINNs for discovering governing equations, the physical\nmodels encoded in PINNs may be misspecified in complex systems as some of the\nphysical processes may not be fully understood, leading to the poor accuracy of\nPINN predictions. In this work, we present a general approach to correct the\nmisspecified physical models in PINNs for discovering governing equations,\ngiven some sparse and\/or noisy data. Specifically, we first encode the assumed\nphysical models, which may be misspecified, then employ other deep neural\nnetworks (DNNs) to model the discrepancy between the imperfect models and the\nobservational data. Due to the expressivity of DNNs, the proposed method is\ncapable of reducing the computational errors caused by the model\nmisspecification and thus enables the applications of PINNs in complex systems\nwhere the physical processes are not exactly known. Furthermore, we utilize the\nBayesian PINNs (B-PINNs) and\/or ensemble PINNs to quantify uncertainties\narising from noisy and\/or gappy data in the discovered governing equations. A\nseries of numerical examples including non-Newtonian channel and cavity flows\ndemonstrate that the added DNNs are capable of correcting the model\nmisspecification in PINNs and thus reduce the discrepancy between the physical\nmodels and the observational data. We envision that the proposed approach will\nextend the applications of PINNs for discovering governing equations in\nproblems where the physico-chemical or biological processes are not well\nunderstood.","terms":["cs.LG","physics.comp-ph"]},{"titles":"Exploring hyperelastic material model discovery for human brain cortex: multivariate analysis vs. artificial neural network approaches","summaries":"Traditional computational methods, such as the finite element analysis, have\nprovided valuable insights into uncovering the underlying mechanisms of brain\nphysical behaviors. However, precise predictions of brain physics require\neffective constitutive models to represent the intricate mechanical properties\nof brain tissue. In this study, we aimed to identify the most favorable\nconstitutive material model for human brain tissue. To achieve this, we applied\nartificial neural network and multiple regression methods to a generalization\nof widely accepted classic models, and compared the results obtained from these\ntwo approaches. To evaluate the applicability and efficacy of the model, all\nsetups were kept consistent across both methods, except for the approach to\nprevent potential overfitting. Our results demonstrate that artificial neural\nnetworks are capable of automatically identifying accurate constitutive models\nfrom given admissible estimators. Nonetheless, the five-term and two-term\nneural network models trained under single-mode and multi-mode loading\nscenarios, were found to be suboptimal and could be further simplified into\ntwo-term and single-term, respectively, with higher accuracy using multiple\nregression. Our findings highlight the importance of hyperparameters for the\nartificial neural network and emphasize the necessity for detailed\ncross-validations of regularization parameters to ensure optimal selection at a\nglobal level in the development of material constitutive models. This study\nvalidates the applicability and accuracy of artificial neural network to\nautomatically discover constitutive material models with proper regularization\nas well as the benefits in model simplification without compromising accuracy\nfor traditional multivariable regression.","terms":["cs.LG"]},{"titles":"TraM-NeRF: Tracing Mirror and Near-Perfect Specular Reflections through Neural Radiance Fields","summaries":"Implicit representations like Neural Radiance Fields (NeRF) showed impressive\nresults for photorealistic rendering of complex scenes with fine details.\nHowever, ideal or near-perfectly specular reflecting objects such as mirrors,\nwhich are often encountered in various indoor scenes, impose ambiguities and\ninconsistencies in the representation of the reconstructed scene leading to\nsevere artifacts in the synthesized renderings. In this paper, we present a\nnovel reflection tracing method tailored for the involved volume rendering\nwithin NeRF that takes these mirror-like objects into account while avoiding\nthe cost of straightforward but expensive extensions through standard path\ntracing. By explicitly modeling the reflection behavior using physically\nplausible materials and estimating the reflected radiance with Monte-Carlo\nmethods within the volume rendering formulation, we derive efficient strategies\nfor importance sampling and the transmittance computation along rays from only\nfew samples. We show that our novel method enables the training of consistent\nrepresentations of such challenging scenes and achieves superior results in\ncomparison to previous state-of-the-art approaches.","terms":["cs.CV","cs.GR"]},{"titles":"Certainty In, Certainty Out: REVQCs for Quantum Machine Learning","summaries":"The field of Quantum Machine Learning (QML) has emerged recently in the hopes\nof finding new machine learning protocols or exponential speedups for classical\nones. Apart from problems with vanishing gradients and efficient encoding\nmethods, these speedups are hard to find because the sampling nature of quantum\ncomputers promotes either simulating computations classically or running them\nmany times on quantum computers in order to use approximate expectation values\nin gradient calculations. In this paper, we make a case for setting high\nsingle-sample accuracy as a primary goal. We discuss the statistical theory\nwhich enables highly accurate and precise sample inference, and propose a\nmethod of reversed training towards this end. We show the effectiveness of this\ntraining method by assessing several effective variational quantum circuits\n(VQCs), trained in both the standard and reversed directions, on random binary\nsubsets of the MNIST and MNIST Fashion datasets, on which our method provides\nan increase of $10-15\\%$ in single-sample inference accuracy.","terms":["cs.LG","quant-ph","I.2.6; I.6.5"]},{"titles":"HelmSim: Learning Helmholtz Dynamics for Interpretable Fluid Simulation","summaries":"Fluid simulation is a long-standing challenge due to the intrinsic\nhigh-dimensional non-linear dynamics. Previous methods usually utilize the\nnon-linear modeling capability of deep models to directly estimate velocity\nfields for future prediction. However, skipping over inherent physical\nproperties but directly learning superficial velocity fields will overwhelm the\nmodel from generating precise or physics-reliable results. In this paper, we\npropose the HelmSim toward an accurate and interpretable simulator for fluid.\nInspired by the Helmholtz theorem, we design a HelmDynamic block to learn the\nHelmholtz dynamics, which decomposes fluid dynamics into more solvable\ncurl-free and divergence-free parts, physically corresponding to potential and\nstream functions of fluid. By embedding the HelmDynamic block into a Multiscale\nIntegration Network, HelmSim can integrate learned Helmholtz dynamics along\ntemporal dimension in multiple spatial scales to yield future fluid. Comparing\nwith previous velocity estimating methods, HelmSim is faithfully derived from\nHelmholtz theorem and ravels out complex fluid dynamics with physically\ninterpretable evidence. Experimentally, our proposed HelmSim achieves the\nconsistent state-of-the-art in both numerical simulated and real-world observed\nbenchmarks, even for scenarios with complex boundaries.","terms":["cs.LG"]},{"titles":"Population-based wind farm monitoring based on a spatial autoregressive approach","summaries":"An important challenge faced by wind farm operators is to reduce operation\nand maintenance cost. Structural health monitoring provides a means of cost\nreduction through minimising unnecessary maintenance trips as well as\nprolonging turbine service life. Population-based structural health monitoring\ncan further reduce the cost of health monitoring systems by implementing one\nsystem for multiple structures (i.e.~turbines). At the same time, shared data\nwithin a population of structures may improve the predictions of structural\nbehaviour. To monitor turbine performance at a population\/farm level, an\nimportant initial step is to construct a model that describes the behaviour of\nall turbines under normal conditions. This paper proposes a population-level\nmodel that explicitly captures the spatial and temporal correlations (between\nturbines) induced by the wake effect. The proposed model is a Gaussian\nprocess-based spatial autoregressive model, named here a GP-SPARX model. This\napproach is developed since (a) it reflects our physical understanding of the\nwake effect, and (b) it benefits from a stochastic data-based learner. A case\nstudy is provided to demonstrate the capability of the GP-SPARX model in\ncapturing spatial and temporal variations as well as its potential\napplicability in a health monitoring system.","terms":["cs.LG","physics.flu-dyn"]},{"titles":"Equivariant Matrix Function Neural Networks","summaries":"Graph Neural Networks (GNNs), especially message-passing neural networks\n(MPNNs), have emerged as powerful architectures for learning on graphs in\ndiverse applications. However, MPNNs face challenges when modeling non-local\ninteractions in systems such as large conjugated molecules, metals, or\namorphous materials. Although Spectral GNNs and traditional neural networks\nsuch as recurrent neural networks and transformers mitigate these challenges,\nthey often lack extensivity, adaptability, generalizability, computational\nefficiency, or fail to capture detailed structural relationships or symmetries\nin the data. To address these concerns, we introduce Matrix Function Neural\nNetworks (MFNs), a novel architecture that parameterizes non-local interactions\nthrough analytic matrix equivariant functions. Employing resolvent expansions\noffers a straightforward implementation and the potential for linear scaling\nwith system size. The MFN architecture achieves state-of-the-art performance in\nstandard graph benchmarks, such as the ZINC and TU datasets, and is able to\ncapture intricate non-local interactions in quantum systems, paving the way to\nnew state-of-the-art force fields.","terms":["stat.ML","cond-mat.mtrl-sci","cs.LG","physics.chem-ph"]},{"titles":"Looping LOCI: Developing Object Permanence from Videos","summaries":"Recent compositional scene representation learning models have become\nremarkably good in segmenting and tracking distinct objects within visual\nscenes. Yet, many of these models require that objects are continuously, at\nleast partially, visible. Moreover, they tend to fail on intuitive physics\ntests, which infants learn to solve over the first months of their life. Our\ngoal is to advance compositional scene representation algorithms with an\nembedded algorithm that fosters the progressive learning of intuitive physics,\nakin to infant development. As a fundamental component for such an algorithm,\nwe introduce Loci-Looped, which advances a recently published unsupervised\nobject location, identification, and tracking neural network architecture\n(Loci, Traub et al., ICLR 2023) with an internal processing loop. The loop is\ndesigned to adaptively blend pixel-space information with anticipations\nyielding information-fused activities as percepts. Moreover, it is designed to\nlearn compositional representations of both individual object dynamics and\nbetween-objects interaction dynamics. We show that Loci-Looped learns to track\nobjects through extended periods of object occlusions, indeed simulating their\nhidden trajectories and anticipating their reappearance, without the need for\nan explicit history buffer. We even find that Loci-Looped surpasses\nstate-of-the-art models on the ADEPT and the CLEVRER dataset, when confronted\nwith object occlusions or temporary sensory data interruptions. This indicates\nthat Loci-Looped is able to learn the physical concepts of object permanence\nand inertia in a fully unsupervised emergent manner. We believe that even\nfurther architectural advancements of the internal loop - also in other\ncompositional scene representation learning models - can be developed in the\nnear future.","terms":["cs.CV"]},{"titles":"Machine learning in physics: a short guide","summaries":"Machine learning is a rapidly growing field with the potential to\nrevolutionize many areas of science, including physics. This review provides a\nbrief overview of machine learning in physics, covering the main concepts of\nsupervised, unsupervised, and reinforcement learning, as well as more\nspecialized topics such as causal inference, symbolic regression, and deep\nlearning. We present some of the principal applications of machine learning in\nphysics and discuss the associated challenges and perspectives.","terms":["cs.LG","cond-mat.stat-mech","physics.app-ph"]},{"titles":"Deep learning modelling of tip clearance variations on multi-stage axial compressors aerodynamics","summaries":"Application of deep learning methods to physical simulations such as CFD\n(Computational Fluid Dynamics) for turbomachinery applications, have been so\nfar of limited industrial relevance. This paper demonstrates the development\nand application of a deep learning framework for real-time predictions of the\nimpact of tip clearance variations on the flow field and aerodynamic\nperformance of multi-stage axial compressors in gas turbines. The proposed\narchitecture is proven to be scalable to industrial applications, and achieves\nin real-time accuracy comparable to the CFD benchmark. The deployed model, is\nreadily integrated within the manufacturing and build process of gas turbines,\nthus providing the opportunity to analytically assess the impact on performance\nand potentially reduce requirements for expensive physical tests.","terms":["cs.LG","cs.CE","physics.flu-dyn"]},{"titles":"How to Learn and Generalize From Three Minutes of Data: Physics-Constrained and Uncertainty-Aware Neural Stochastic Differential Equations","summaries":"We present a framework and algorithms to learn controlled dynamics models\nusing neural stochastic differential equations (SDEs) -- SDEs whose drift and\ndiffusion terms are both parametrized by neural networks. We construct the\ndrift term to leverage a priori physics knowledge as inductive bias, and we\ndesign the diffusion term to represent a distance-aware estimate of the\nuncertainty in the learned model's predictions -- it matches the system's\nunderlying stochasticity when evaluated on states near those from the training\ndataset, and it predicts highly stochastic dynamics when evaluated on states\nbeyond the training regime. The proposed neural SDEs can be evaluated quickly\nenough for use in model predictive control algorithms, or they can be used as\nsimulators for model-based reinforcement learning. Furthermore, they make\naccurate predictions over long time horizons, even when trained on small\ndatasets that cover limited regions of the state space. We demonstrate these\ncapabilities through experiments on simulated robotic systems, as well as by\nusing them to model and control a hexacopter's flight dynamics: A neural SDE\ntrained using only three minutes of manually collected flight data results in a\nmodel-based control policy that accurately tracks aggressive trajectories that\npush the hexacopter's velocity and Euler angles to nearly double the maximum\nvalues observed in the training dataset.","terms":["cs.LG","cs.RO","cs.SY","eess.SY"]},{"titles":"Specialized Deep Residual Policy Safe Reinforcement Learning-Based Controller for Complex and Continuous State-Action Spaces","summaries":"Traditional controllers have limitations as they rely on prior knowledge\nabout the physics of the problem, require modeling of dynamics, and struggle to\nadapt to abnormal situations. Deep reinforcement learning has the potential to\naddress these problems by learning optimal control policies through exploration\nin an environment. For safety-critical environments, it is impractical to\nexplore randomly, and replacing conventional controllers with black-box models\nis also undesirable. Also, it is expensive in continuous state and action\nspaces, unless the search space is constrained. To address these challenges we\npropose a specialized deep residual policy safe reinforcement learning with a\ncycle of learning approach adapted for complex and continuous state-action\nspaces. Residual policy learning allows learning a hybrid control architecture\nwhere the reinforcement learning agent acts in synchronous collaboration with\nthe conventional controller. The cycle of learning initiates the policy through\nthe expert trajectory and guides the exploration around it. Further, the\nspecialization through the input-output hidden Markov model helps to optimize\npolicy that lies within the region of interest (such as abnormality), where the\nreinforcement learning agent is required and is activated. The proposed\nsolution is validated on the Tennessee Eastman process control.","terms":["cs.LG","cs.AI","cs.SY","eess.SY"]},{"titles":"Reconstructing 3D Human Pose from RGB-D Data with Occlusions","summaries":"We propose a new method to reconstruct the 3D human body from RGB-D images\nwith occlusions. The foremost challenge is the incompleteness of the RGB-D data\ndue to occlusions between the body and the environment, leading to implausible\nreconstructions that suffer from severe human-scene penetration. To reconstruct\na semantically and physically plausible human body, we propose to reduce the\nsolution space based on scene information and prior knowledge. Our key idea is\nto constrain the solution space of the human body by considering the occluded\nbody parts and visible body parts separately: modeling all plausible poses\nwhere the occluded body parts do not penetrate the scene, and constraining the\nvisible body parts using depth data. Specifically, the first component is\nrealized by a neural network that estimates the candidate region named the\n\"free zone\", a region carved out of the open space within which it is safe to\nsearch for poses of the invisible body parts without concern for penetration.\nThe second component constrains the visible body parts using the \"truncated\nshadow volume\" of the scanned body point cloud. Furthermore, we propose to use\na volume matching strategy, which yields better performance than surface\nmatching, to match the human body with the confined region. We conducted\nexperiments on the PROX dataset, and the results demonstrate that our method\nproduces more accurate and plausible results compared with other methods.","terms":["cs.CV","cs.GR"]},{"titles":"QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum-Classical Neural Network","summaries":"Supply chain management relies on accurate backorder prediction for\noptimizing inventory control, reducing costs, and enhancing customer\nsatisfaction. However, traditional machine-learning models struggle with\nlarge-scale datasets and complex relationships, hindering real-world data\ncollection. This research introduces a novel methodological framework for\nsupply chain backorder prediction, addressing the challenge of handling large\ndatasets. Our proposed model, QAmplifyNet, employs quantum-inspired techniques\nwithin a quantum-classical neural network to predict backorders effectively on\nshort and imbalanced datasets. Experimental evaluations on a benchmark dataset\ndemonstrate QAmplifyNet's superiority over classical models, quantum ensembles,\nquantum neural networks, and deep reinforcement learning. Its proficiency in\nhandling short, imbalanced datasets makes it an ideal solution for supply chain\nmanagement. To enhance model interpretability, we use Explainable Artificial\nIntelligence techniques. Practical implications include improved inventory\ncontrol, reduced backorders, and enhanced operational efficiency. QAmplifyNet\nseamlessly integrates into real-world supply chain management systems, enabling\nproactive decision-making and efficient resource allocation. Future work\ninvolves exploring additional quantum-inspired techniques, expanding the\ndataset, and investigating other supply chain applications. This research\nunlocks the potential of quantum computing in supply chain optimization and\npaves the way for further exploration of quantum-inspired machine learning\nmodels in supply chain management. Our framework and QAmplifyNet model offer a\nbreakthrough approach to supply chain backorder prediction, providing superior\nperformance and opening new avenues for leveraging quantum-inspired techniques\nin supply chain management.","terms":["cs.LG","cs.AI","quant-ph"]},{"titles":"Model Inversion Attacks on Homogeneous and Heterogeneous Graph Neural Networks","summaries":"Recently, Graph Neural Networks (GNNs), including Homogeneous Graph Neural\nNetworks (HomoGNNs) and Heterogeneous Graph Neural Networks (HeteGNNs), have\nmade remarkable progress in many physical scenarios, especially in\ncommunication applications. Despite achieving great success, the privacy issue\nof such models has also received considerable attention. Previous studies have\nshown that given a well-fitted target GNN, the attacker can reconstruct the\nsensitive training graph of this model via model inversion attacks, leading to\nsignificant privacy worries for the AI service provider. We advocate that the\nvulnerability comes from the target GNN itself and the prior knowledge about\nthe shared properties in real-world graphs. Inspired by this, we propose a\nnovel model inversion attack method on HomoGNNs and HeteGNNs, namely HomoGMI\nand HeteGMI. Specifically, HomoGMI and HeteGMI are gradient-descent-based\noptimization methods that aim to maximize the cross-entropy loss on the target\nGNN and the $1^{st}$ and $2^{nd}$-order proximities on the reconstructed graph.\nNotably, to the best of our knowledge, HeteGMI is the first attempt to perform\nmodel inversion attacks on HeteGNNs. Extensive experiments on multiple\nbenchmarks demonstrate that the proposed method can achieve better performance\nthan the competitors.","terms":["cs.LG","cs.CV"]},{"titles":"Notes on Applicability of Explainable AI Methods to Machine Learning Models Using Features Extracted by Persistent Homology","summaries":"Data analysis that uses the output of topological data analysis as input for\nmachine learning algorithms has been the subject of extensive research. This\napproach offers a means of capturing the global structure of data. Persistent\nhomology (PH), a common methodology within the field of TDA, has found\nwide-ranging applications in machine learning. One of the key reasons for the\nsuccess of the PH-ML pipeline lies in the deterministic nature of feature\nextraction conducted through PH. The ability to achieve satisfactory levels of\naccuracy with relatively simple downstream machine learning models, when\nprocessing these extracted features, underlines the pipeline's superior\ninterpretability. However, it must be noted that this interpretation has\nencountered issues. Specifically, it fails to accurately reflect the feasible\nparameter region in the data generation process, and the physical or chemical\nconstraints that restrict this process. Against this backdrop, we explore the\npotential application of explainable AI methodologies to this PH-ML pipeline.\nWe apply this approach to the specific problem of predicting gas adsorption in\nmetal-organic frameworks and demonstrate that it can yield suggestive results.\nThe codes to reproduce our results are available at\nhttps:\/\/github.com\/naofumihama\/xai_ph_ml","terms":["cs.LG","cs.AI"]},{"titles":"Gender-Based Comparative Study of Type 2 Diabetes Risk Factors in Kolkata, India: A Machine Learning Approach","summaries":"Type 2 diabetes mellitus represents a prevalent and widespread global health\nconcern, necessitating a comprehensive assessment of its risk factors. This\nstudy aimed towards learning whether there is any differential impact of age,\nLifestyle, BMI and Waist to height ratio on the risk of Type 2 diabetes\nmellitus in males and females in Kolkata, West Bengal, India based on a sample\nobserved from the out-patient consultation department of Belle Vue Clinic in\nKolkata. Various machine learning models like Logistic Regression, Random\nForest, and Support Vector Classifier, were used to predict the risk of\ndiabetes, and performance was compared based on different predictors. Our\nfindings indicate a significant age-related increase in risk of diabetes for\nboth males and females. Although exercising and BMI was found to have\nsignificant impact on the risk of Type 2 diabetes in males, in females both\nturned out to be statistically insignificant. For both males and females,\npredictive models based on WhtR demonstrated superior performance in risk\nassessment compared to those based on BMI. This study sheds light on the\ngender-specific differences in the risk factors for Type 2 diabetes, offering\nvaluable insights that can be used towards more targeted healthcare\ninterventions and public health strategies.","terms":["cs.LG","physics.med-ph","physics.soc-ph"]},{"titles":"Ensemble learning for blending gridded satellite and gauge-measured precipitation data","summaries":"Regression algorithms are regularly used for improving the accuracy of\nsatellite precipitation products. In this context, satellite precipitation and\ntopography data are the predictor variables, and gauged-measured precipitation\ndata are the dependent variables. Alongside this, it is increasingly recognised\nin many fields that combinations of algorithms through ensemble learning can\nlead to substantial predictive performance improvements. Still, a sufficient\nnumber of ensemble learners for improving the accuracy of satellite\nprecipitation products and their large-scale comparison are currently missing\nfrom the literature. In this study, we work towards filling in this specific\ngap by proposing 11 new ensemble learners in the field and by extensively\ncomparing them. We apply the ensemble learners to monthly data from the\nPERSIANN (Precipitation Estimation from Remotely Sensed Information using\nArtificial Neural Networks) and IMERG (Integrated Multi-satellitE Retrievals\nfor GPM) gridded datasets that span over a 15-year period and over the entire\nthe contiguous United States (CONUS). We also use gauge-measured precipitation\ndata from the Global Historical Climatology Network monthly database, version 2\n(GHCNm). The ensemble learners combine the predictions of six machine learning\nregression algorithms (base learners), namely the multivariate adaptive\nregression splines (MARS), multivariate adaptive polynomial splines\n(poly-MARS), random forests (RF), gradient boosting machines (GBM), extreme\ngradient boosting (XGBoost) and Bayesian regularized neural networks (BRNN),\nand each of them is based on a different combiner. The combiners include the\nequal-weight combiner, the median combiner, two best learners and seven\nvariants of a sophisticated stacking method. The latter stacks a regression\nalgorithm on top of the base learners to combine their independent\npredictions...","terms":["cs.LG","physics.ao-ph","stat.AP","stat.ME"]},{"titles":"Machine Learning for Urban Air Quality Analytics: A Survey","summaries":"The increasing air pollution poses an urgent global concern with far-reaching\nconsequences, such as premature mortality and reduced crop yield, which\nsignificantly impact various aspects of our daily lives. Accurate and timely\nanalysis of air pollution is crucial for understanding its underlying\nmechanisms and implementing necessary precautions to mitigate potential\nsocio-economic losses. Traditional analytical methodologies, such as\natmospheric modeling, heavily rely on domain expertise and often make\nsimplified assumptions that may not be applicable to complex air pollution\nproblems. In contrast, Machine Learning (ML) models are able to capture the\nintrinsic physical and chemical rules by automatically learning from a large\namount of historical observational data, showing great promise in various air\nquality analytical tasks. In this article, we present a comprehensive survey of\nML-based air quality analytics, following a roadmap spanning from data\nacquisition to pre-processing, and encompassing various analytical tasks such\nas pollution pattern mining, air quality inference, and forecasting. Moreover,\nwe offer a systematic categorization and summary of existing methodologies and\napplications, while also providing a list of publicly available air quality\ndatasets to ease the research in this direction. Finally, we identify several\npromising future research directions. This survey can serve as a valuable\nresource for professionals seeking suitable solutions for their specific\nchallenges and advancing their research at the cutting edge.","terms":["cs.LG"]},{"titles":"ALA: Naturalness-aware Adversarial Lightness Attack","summaries":"Most researchers have tried to enhance the robustness of DNNs by revealing\nand repairing the vulnerability of DNNs with specialized adversarial examples.\nParts of the attack examples have imperceptible perturbations restricted by Lp\nnorm. However, due to their high-frequency property, the adversarial examples\ncan be defended by denoising methods and are hard to realize in the physical\nworld. To avoid the defects, some works have proposed unrestricted attacks to\ngain better robustness and practicality. It is disappointing that these\nexamples usually look unnatural and can alert the guards. In this paper, we\npropose Adversarial Lightness Attack (ALA), a white-box unrestricted\nadversarial attack that focuses on modifying the lightness of the images. The\nshape and color of the samples, which are crucial to human perception, are\nbarely influenced. To obtain adversarial examples with a high attack success\nrate, we propose unconstrained enhancement in terms of the light and shade\nrelationship in images. To enhance the naturalness of images, we craft the\nnaturalness-aware regularization according to the range and distribution of\nlight. The effectiveness of ALA is verified on two popular datasets for\ndifferent tasks (i.e., ImageNet for image classification and Places-365 for\nscene recognition).","terms":["cs.CV","cs.AI"]},{"titles":"Hypernetwork-based Meta-Learning for Low-Rank Physics-Informed Neural Networks","summaries":"In various engineering and applied science applications, repetitive numerical\nsimulations of partial differential equations (PDEs) for varying input\nparameters are often required (e.g., aircraft shape optimization over many\ndesign parameters) and solvers are required to perform rapid execution. In this\nstudy, we suggest a path that potentially opens up a possibility for\nphysics-informed neural networks (PINNs), emerging deep-learning-based solvers,\nto be considered as one such solver. Although PINNs have pioneered a proper\nintegration of deep-learning and scientific computing, they require repetitive\ntime-consuming training of neural networks, which is not suitable for\nmany-query scenarios. To address this issue, we propose a lightweight low-rank\nPINNs containing only hundreds of model parameters and an associated\nhypernetwork-based meta-learning algorithm, which allows efficient\napproximation of solutions of PDEs for varying ranges of PDE input parameters.\nMoreover, we show that the proposed method is effective in overcoming a\nchallenging issue, known as \"failure modes\" of PINNs.","terms":["cs.LG","cs.NA","math.NA","physics.comp-ph"]},{"titles":"Physical Adversarial Attacks for Surveillance: A Survey","summaries":"Modern automated surveillance techniques are heavily reliant on deep learning\nmethods. Despite the superior performance, these learning systems are\ninherently vulnerable to adversarial attacks - maliciously crafted inputs that\nare designed to mislead, or trick, models into making incorrect predictions. An\nadversary can physically change their appearance by wearing adversarial\nt-shirts, glasses, or hats or by specific behavior, to potentially avoid\nvarious forms of detection, tracking and recognition of surveillance systems;\nand obtain unauthorized access to secure properties and assets. This poses a\nsevere threat to the security and safety of modern surveillance systems. This\npaper reviews recent attempts and findings in learning and designing physical\nadversarial attacks for surveillance applications. In particular, we propose a\nframework to analyze physical adversarial attacks and provide a comprehensive\nsurvey of physical adversarial attacks on four key surveillance tasks:\ndetection, identification, tracking, and action recognition under this\nframework. Furthermore, we review and analyze strategies to defend against the\nphysical adversarial attacks and the methods for evaluating the strengths of\nthe defense. The insights in this paper present an important step in building\nresilience within surveillance systems to physical adversarial attacks.","terms":["cs.CV","cs.AI"]},{"titles":"Learning In-between Imagery Dynamics via Physical Latent Spaces","summaries":"We present a framework designed to learn the underlying dynamics between two\nimages observed at consecutive time steps. The complex nature of image data and\nthe lack of temporal information pose significant challenges in capturing the\nunique evolving patterns. Our proposed method focuses on estimating the\nintermediary stages of image evolution, allowing for interpretability through\nlatent dynamics while preserving spatial correlations with the image. By\nincorporating a latent variable that follows a physical model expressed in\npartial differential equations (PDEs), our approach ensures the\ninterpretability of the learned model and provides insight into corresponding\nimage dynamics. We demonstrate the robustness and effectiveness of our learning\nframework through a series of numerical tests using geoscientific imagery data.","terms":["cs.LG","cs.CV","stat.ML","37M05, 62F99, 68T45"]},{"titles":"MEMTRACK: A Deep Learning-Based Approach to Microrobot Tracking in Dense and Low-Contrast Environments","summaries":"Tracking microrobots is challenging, considering their minute size and high\nspeed. As the field progresses towards developing microrobots for biomedical\napplications and conducting mechanistic studies in physiologically relevant\nmedia (e.g., collagen), this challenge is exacerbated by the dense surrounding\nenvironments with feature size and shape comparable to microrobots. Herein, we\nreport Motion Enhanced Multi-level Tracker (MEMTrack), a robust pipeline for\ndetecting and tracking microrobots using synthetic motion features, deep\nlearning-based object detection, and a modified Simple Online and Real-time\nTracking (SORT) algorithm with interpolation for tracking. Our object detection\napproach combines different models based on the object's motion pattern. We\ntrained and validated our model using bacterial micro-motors in collagen\n(tissue phantom) and tested it in collagen and aqueous media. We demonstrate\nthat MEMTrack accurately tracks even the most challenging bacteria missed by\nskilled human annotators, achieving precision and recall of 77% and 48% in\ncollagen and 94% and 35% in liquid media, respectively. Moreover, we show that\nMEMTrack can quantify average bacteria speed with no statistically significant\ndifference from the laboriously-produced manual tracking data. MEMTrack\nrepresents a significant contribution to microrobot localization and tracking,\nand opens the potential for vision-based deep learning approaches to microrobot\ncontrol in dense and low-contrast settings. All source code for training and\ntesting MEMTrack and reproducing the results of the paper have been made\npublicly available https:\/\/github.com\/sawhney-medha\/MEMTrack.","terms":["cs.CV","physics.bio-ph","q-bio.QM"]},{"titles":"Learning nonlinear integral operators via Recurrent Neural Networks and its application in solving Integro-Differential Equations","summaries":"In this paper, we propose using LSTM-RNNs (Long Short-Term Memory-Recurrent\nNeural Networks) to learn and represent nonlinear integral operators that\nappear in nonlinear integro-differential equations (IDEs). The LSTM-RNN\nrepresentation of the nonlinear integral operator allows us to turn a system of\nnonlinear integro-differential equations into a system of ordinary differential\nequations for which many efficient solvers are available. Furthermore, because\nthe use of LSTM-RNN representation of the nonlinear integral operator in an IDE\neliminates the need to perform a numerical integration in each numerical time\nevolution step, the overall temporal cost of the LSTM-RNN-based IDE solver can\nbe reduced to $O(n_T)$ from $O(n_T^2)$ if a $n_T$-step trajectory is to be\ncomputed. We illustrate the efficiency and robustness of this LSTM-RNN-based\nnumerical IDE solver with a model problem. Additionally, we highlight the\ngeneralizability of the learned integral operator by applying it to IDEs driven\nby different external forces. As a practical application, we show how this\nmethodology can effectively solve the Dyson's equation for quantum many-body\nsystems.","terms":["cs.LG","math.DS","physics.comp-ph"]},{"titles":"Beyond the Pixel: a Photometrically Calibrated HDR Dataset for Luminance and Color Prediction","summaries":"Light plays an important role in human well-being. However, most computer\nvision tasks treat pixels without considering their relationship to physical\nluminance. To address this shortcoming, we introduce the Laval Photometric\nIndoor HDR Dataset, the first large-scale photometrically calibrated dataset of\nhigh dynamic range 360{\\deg} panoramas. Our key contribution is the calibration\nof an existing, uncalibrated HDR Dataset. We do so by accurately capturing RAW\nbracketed exposures simultaneously with a professional photometric measurement\ndevice (chroma meter) for multiple scenes across a variety of lighting\nconditions. Using the resulting measurements, we establish the calibration\ncoefficients to be applied to the HDR images. The resulting dataset is a rich\nrepresentation of indoor scenes which displays a wide range of illuminance and\ncolor, and varied types of light sources. We exploit the dataset to introduce\nthree novel tasks, where: per-pixel luminance, per-pixel color and planar\nilluminance can be predicted from a single input image. Finally, we also\ncapture another smaller photometric dataset with a commercial 360{\\deg} camera,\nto experiment on generalization across cameras. We are optimistic that the\nrelease of our datasets and associated code will spark interest in physically\naccurate light estimation within the community. Dataset and code are available\nat https:\/\/lvsn.github.io\/beyondthepixel\/.","terms":["cs.CV"]},{"titles":"Optimal Scheduling of Electric Vehicle Charging with Deep Reinforcement Learning considering End Users Flexibility","summaries":"The rapid growth of decentralized energy resources and especially Electric\nVehicles (EV), that are expected to increase sharply over the next decade, will\nput further stress on existing power distribution networks, increasing the need\nfor higher system reliability and flexibility. In an attempt to avoid\nunnecessary network investments and to increase the controllability over\ndistribution networks, network operators develop demand response (DR) programs\nthat incentivize end users to shift their consumption in return for financial\nor other benefits. Artificial intelligence (AI) methods are in the research\nforefront for residential load scheduling applications, mainly due to their\nhigh accuracy, high computational speed and lower dependence on the physical\ncharacteristics of the models under development. The aim of this work is to\nidentify households' EV cost-reducing charging policy under a Time-of-Use\ntariff scheme, with the use of Deep Reinforcement Learning, and more\nspecifically Deep Q-Networks (DQN). A novel end users flexibility potential\nreward is inferred from historical data analysis, where households with solar\npower generation have been used to train and test the designed algorithm. The\nsuggested DQN EV charging policy can lead to more than 20% of savings in end\nusers electricity bills.","terms":["cs.LG","cs.AI"]},{"titles":"GDL-DS: A Benchmark for Geometric Deep Learning under Distribution Shifts","summaries":"Geometric deep learning (GDL) has gained significant attention in various\nscientific fields, chiefly for its proficiency in modeling data with intricate\ngeometric structures. Yet, very few works have delved into its capability of\ntackling the distribution shift problem, a prevalent challenge in many relevant\napplications. To bridge this gap, we propose GDL-DS, a comprehensive benchmark\ndesigned for evaluating the performance of GDL models in scenarios with\ndistribution shifts. Our evaluation datasets cover diverse scientific domains\nfrom particle physics and materials science to biochemistry, and encapsulate a\nbroad spectrum of distribution shifts including conditional, covariate, and\nconcept shifts. Furthermore, we study three levels of information access from\nthe out-of-distribution (OOD) testing data, including no OOD information, only\nOOD features without labels, and OOD features with a few labels. Overall, our\nbenchmark results in 30 different experiment settings, and evaluates 3 GDL\nbackbones and 11 learning algorithms in each setting. A thorough analysis of\nthe evaluation results is provided, poised to illuminate insights for DGL\nresearchers and domain practitioners who are to use DGL in their applications.","terms":["cs.LG","cs.AI"]},{"titles":"On Scale Space Radon Transform, Properties and Application in CT Image Reconstruction","summaries":"Since the Radon transform (RT) consists in a line integral function, some\nmodeling assumptions are made on Computed Tomography (CT) system, making image\nreconstruction analytical methods, such as Filtered Backprojection (FBP),\nsensitive to artifacts and noise. In the other hand, recently, a new integral\ntransform, called Scale Space Radon Transform (SSRT), is introduced where, RT\nis a particular case. Thanks to its interesting properties, such as good scale\nspace behavior, the SSRT has known number of new applications. In this paper,\nwith the aim to improve the reconstructed image quality for these methods, we\npropose to model the X-ray beam with the Scale Space Radon Transform (SSRT)\nwhere, the assumptions done on the physical dimensions of the CT system\nelements reflect better the reality. After depicting the basic properties and\nthe inversion of SSRT, the FBP algorithm is used to reconstruct the image from\nthe SSRT sinogram where the RT spectrum used in FBP is replaced by SSRT and the\nGaussian kernel, expressed in their frequency domain. PSNR and SSIM, as quality\nmeasures, are used to compare RT and SSRT-based image reconstruction on\nShepp-Logan head and anthropomorphic abdominal phantoms. The first findings\nshow that the SSRT-based method outperforms the methods based on RT,\nespecially, when the number of projections is reduced, making it more\nappropriate for applications requiring low-dose radiation, such as medical\nX-ray CT. While SSRT-FBP and RT-FBP have utmost the same runtime, the\nexperiments show that SSRT-FBP is more robust to Poisson-Gaussian noise\ncorrupting CT data.","terms":["cs.CV"]},{"titles":"Reinforcement Learning of Display Transfer Robots in Glass Flow Control Systems: A Physical Simulation-Based Approach","summaries":"A flow control system is a critical concept for increasing the production\ncapacity of manufacturing systems. To solve the scheduling optimization problem\nrelated to the flow control with the aim of improving productivity, existing\nmethods depend on a heuristic design by domain human experts. Therefore, the\nmethods require correction, monitoring, and verification by using real\nequipment. As system designs increase in complexity, the monitoring time\nincreases, which decreases the probability of arriving at the optimal design.\nAs an alternative approach to the heuristic design of flow control systems, the\nuse of deep reinforcement learning to solve the scheduling optimization problem\nhas been considered. Although the existing research on reinforcement learning\nhas yielded excellent performance in some areas, the applicability of the\nresults to actual FAB such as display and semiconductor manufacturing processes\nis not evident so far. To this end, we propose a method to implement a physical\nsimulation environment and devise a feasible flow control system design using a\ntransfer robot in display manufacturing through reinforcement learning. We\npresent a model and parameter setting to build a virtual environment for\ndifferent display transfer robots, and training methods of reinforcement\nlearning on the environment to obtain an optimal scheduling of glass flow\ncontrol systems. Its feasibility was verified by using different types of\nrobots used in the actual process.","terms":["cs.LG","cs.RO","cs.SY","eess.SY"]},{"titles":"Dynamic Appearance Particle Neural Radiance Field","summaries":"Neural Radiance Fields (NeRFs) have shown great potential in modelling 3D\nscenes. Dynamic NeRFs extend this model by capturing time-varying elements,\ntypically using deformation fields. The existing dynamic NeRFs employ a similar\nEulerian representation for both light radiance and deformation fields. This\nleads to a close coupling of appearance and motion and lacks a physical\ninterpretation. In this work, we propose Dynamic Appearance Particle Neural\nRadiance Field (DAP-NeRF), which introduces particle-based representation to\nmodel the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists\nof superposition of a static field and a dynamic field. The dynamic field is\nquantised as a collection of {\\em appearance particles}, which carries the\nvisual information of a small dynamic element in the scene and is equipped with\na motion model. All components, including the static field, the visual features\nand motion models of the particles, are learned from monocular videos without\nany prior geometric knowledge of the scene. We develop an efficient\ncomputational framework for the particle-based model. We also construct a new\ndataset to evaluate motion modelling. Experimental results show that DAP-NeRF\nis an effective technique to capture not only the appearance but also the\nphysically meaningful motions in a 3D dynamic scene.","terms":["cs.CV"]},{"titles":"On Training Derivative-Constrained Neural Networks","summaries":"We refer to the setting where the (partial) derivatives of a neural network's\n(NN's) predictions with respect to its inputs are used as additional training\nsignal as a derivative-constrained (DC) NN. This situation is common in\nphysics-informed settings in the natural sciences. We propose an integrated\nRELU (IReLU) activation function to improve training of DC NNs. We also\ninvestigate denormalization and label rescaling to help stabilize DC training.\nWe evaluate our methods on physics-informed settings including quantum\nchemistry and Scientific Machine Learning (SciML) tasks. We demonstrate that\nexisting architectures with IReLU activations combined with denormalization and\nlabel rescaling better incorporate training signal provided by derivative\nconstraints.","terms":["cs.LG","cs.AI"]},{"titles":"A Complete Recipe for Diffusion Generative Models","summaries":"Score-based Generative Models (SGMs) have demonstrated exceptional synthesis\noutcomes across various tasks. However, the current design landscape of the\nforward diffusion process remains largely untapped and often relies on physical\nheuristics or simplifying assumptions. Utilizing insights from the development\nof scalable Bayesian posterior samplers, we present a complete recipe for\nformulating forward processes in SGMs, ensuring convergence to the desired\ntarget distribution. Our approach reveals that several existing SGMs can be\nseen as specific manifestations of our framework. Building upon this method, we\nintroduce Phase Space Langevin Diffusion (PSLD), which relies on score-based\nmodeling within an augmented space enriched by auxiliary variables akin to\nphysical phase space. Empirical results exhibit the superior sample quality and\nimproved speed-quality trade-off of PSLD compared to various competing\napproaches on established image synthesis benchmarks. Remarkably, PSLD achieves\nsample quality akin to state-of-the-art SGMs (FID: 2.10 for unconditional\nCIFAR-10 generation). Lastly, we demonstrate the applicability of PSLD in\nconditional synthesis using pre-trained score networks, offering an appealing\nalternative as an SGM backbone for future advancements. Code and model\ncheckpoints can be accessed at \\url{https:\/\/github.com\/mandt-lab\/PSLD}.","terms":["cs.LG","cs.CV","stat.ML"]},{"titles":"Feature Learning and Generalization in Deep Networks with Orthogonal Weights","summaries":"Fully-connected deep neural networks with weights initialized from\nindependent Gaussian distributions can be tuned to criticality, which prevents\nthe exponential growth or decay of signals propagating through the network.\nHowever, such networks still exhibit fluctuations that grow linearly with the\ndepth of the network, which may impair the training of networks with width\ncomparable to depth. We show analytically that rectangular networks with tanh\nactivations and weights initialized from the ensemble of orthogonal matrices\nhave corresponding preactivation fluctuations which are independent of depth,\nto leading order in inverse width. Moreover, we demonstrate numerically that,\nat initialization, all correlators involving the neural tangent kernel (NTK)\nand its descendants at leading order in inverse width -- which govern the\nevolution of observables during training -- saturate at a depth of $\\sim 20$,\nrather than growing without bound as in the case of Gaussian initializations.\nWe speculate that this structure preserves finite-width feature learning while\nreducing overall noise, thus improving both generalization and training speed.\nWe provide some experimental justification by relating empirical measurements\nof the NTK to the superior performance of deep nonlinear orthogonal networks\ntrained under full-batch gradient descent on the MNIST and CIFAR-10\nclassification tasks.","terms":["cs.LG","hep-ph","hep-th","stat.ML"]},{"titles":"Graph Transformer Network for Flood Forecasting with Heterogeneous Covariates","summaries":"Floods can be very destructive causing heavy damage to life, property, and\nlivelihoods. Global climate change and the consequent sea-level rise have\nincreased the occurrence of extreme weather events, resulting in elevated and\nfrequent flood risk. Therefore, accurate and timely flood forecasting in\ncoastal river systems is critical to facilitate good flood management. However,\nthe computational tools currently used are either slow or inaccurate. In this\npaper, we propose a Flood prediction tool using Graph Transformer Network\n(FloodGTN) for river systems. More specifically, FloodGTN learns the\nspatio-temporal dependencies of water levels at different monitoring stations\nusing Graph Neural Networks (GNNs) and an LSTM. It is currently implemented to\nconsider external covariates such as rainfall, tide, and the settings of\nhydraulic structures (e.g., outflows of dams, gates, pumps, etc.) along the\nriver. We use a Transformer to learn the attention given to external covariates\nin computing water levels. We apply the FloodGTN tool to data from the South\nFlorida Water Management District, which manages a coastal area prone to\nfrequent storms and hurricanes. Experimental results show that FloodGTN\noutperforms the physics-based model (HEC-RAS) by achieving higher accuracy with\n70% improvement while speeding up run times by at least 500x.","terms":["cs.LG"]},{"titles":"Unsupervised Learning of Sea Surface Height Interpolation from Multi-variate Simulated Satellite Observations","summaries":"Satellite-based remote sensing missions have revolutionized our understanding\nof the Ocean state and dynamics. Among them, spaceborne altimetry provides\nvaluable measurements of Sea Surface Height (SSH), which is used to estimate\nsurface geostrophic currents. However, due to the sensor technology employed,\nimportant gaps occur in SSH observations. Complete SSH maps are produced by the\naltimetry community using linear Optimal Interpolations (OI) such as the\nwidely-used Data Unification and Altimeter Combination System (DUACS). However,\nOI is known for producing overly smooth fields and thus misses some\nmesostructures and eddies. On the other hand, Sea Surface Temperature (SST)\nproducts have much higher data coverage and SST is physically linked to\ngeostrophic currents through advection. We design a realistic twin experiment\nto emulate the satellite observations of SSH and SST to evaluate interpolation\nmethods. We introduce a deep learning network able to use SST information, and\na trainable in two settings: one where we have no access to ground truth during\ntraining and one where it is accessible. Our investigation involves a\ncomparative analysis of the aforementioned network when trained using either\nsupervised or unsupervised loss functions. We assess the quality of SSH\nreconstructions and further evaluate the network's performance in terms of eddy\ndetection and physical properties. We find that it is possible, even in an\nunsupervised setting to use SST to improve reconstruction performance compared\nto SST-agnostic interpolations. We compare our reconstructions to DUACS's and\nreport a decrease of 41\\% in terms of root mean squared error.","terms":["cs.LG"]},{"titles":"Surrogate modeling for stochastic crack growth processes in structural health monitoring applications","summaries":"Fatigue crack growth is one of the most common types of deterioration in\nmetal structures with significant implications on their reliability. Recent\nadvances in Structural Health Monitoring (SHM) have motivated the use of\nstructural response data to predict future crack growth under uncertainty, in\norder to enable a transition towards predictive maintenance. Accurately\nrepresenting different sources of uncertainty in stochastic crack growth (SCG)\nprocesses is a non-trivial task. The present work builds on previous research\non physics-based SCG modeling under both material and load-related uncertainty.\nThe aim here is to construct computationally efficient, probabilistic surrogate\nmodels for SCG processes that successfully encode these different sources of\nuncertainty. An approach inspired by latent variable modeling is employed that\nutilizes Gaussian Process (GP) regression models to enable the surrogates to be\nused to generate prior distributions for different Bayesian SHM tasks as the\napplication of interest. Implementation is carried out in a numerical setting\nand model performance is assessed for two fundamental crack SHM problems;\nnamely crack length monitoring (damage quantification) and crack growth\nmonitoring (damage prognosis).","terms":["stat.ML","cs.AI","cs.LG"]},{"titles":"VDT: General-purpose Video Diffusion Transformers via Mask Modeling","summaries":"This work introduces Video Diffusion Transformer (VDT), which pioneers the\nuse of transformers in diffusion-based video generation. It features\ntransformer blocks with modularized temporal and spatial attention modules to\nleverage the rich spatial-temporal representation inherited in transformers. We\nalso propose a unified spatial-temporal mask modeling mechanism, seamlessly\nintegrated with the model, to cater to diverse video generation scenarios. VDT\noffers several appealing benefits. 1) It excels at capturing temporal\ndependencies to produce temporally consistent video frames and even simulate\nthe physics and dynamics of 3D objects over time. 2) It facilitates flexible\nconditioning information, \\eg, simple concatenation in the token space,\neffectively unifying different token lengths and modalities. 3) Pairing with\nour proposed spatial-temporal mask modeling mechanism, it becomes a\ngeneral-purpose video diffuser for harnessing a range of tasks, including\nunconditional generation, video prediction, interpolation, animation, and\ncompletion, etc. Extensive experiments on these tasks spanning various\nscenarios, including autonomous driving, natural weather, human action, and\nphysics-based simulation, demonstrate the effectiveness of VDT. Additionally,\nwe present comprehensive studies on how \\model handles conditioning information\nwith the mask modeling mechanism, which we believe will benefit future research\nand advance the field. Project page: https:VDT-2023.github.io","terms":["cs.CV"]},{"titles":"Learning Physical Models that Can Respect Conservation Laws","summaries":"Recent work in scientific machine learning (SciML) has focused on\nincorporating partial differential equation (PDE) information into the learning\nprocess. Much of this work has focused on relatively \"easy\" PDE operators\n(e.g., elliptic and parabolic), with less emphasis on relatively \"hard\" PDE\noperators (e.g., hyperbolic). Within numerical PDEs, the latter problem class\nrequires control of a type of volume element or conservation constraint, which\nis known to be challenging. Delivering on the promise of SciML requires\nseamlessly incorporating both types of problems into the learning process. To\naddress this issue, we propose ProbConserv, a framework for incorporating\nconservation constraints into a generic SciML architecture. To do so,\nProbConserv combines the integral form of a conservation law with a Bayesian\nupdate. We provide a detailed analysis of ProbConserv on learning with the\nGeneralized Porous Medium Equation (GPME), a widely-applicable parameterized\nfamily of PDEs that illustrates the qualitative properties of both easier and\nharder PDEs. ProbConserv is effective for easy GPME variants, performing well\nwith state-of-the-art competitors; and for harder GPME variants it outperforms\nother approaches that do not guarantee volume conservation. ProbConserv\nseamlessly enforces physical conservation constraints, maintains probabilistic\nuncertainty quantification (UQ), and deals well with shocks and\nheteroscedasticities. In each case, it achieves superior predictive performance\non downstream tasks.","terms":["cs.LG","cs.NA","math.AP","math.NA"]},{"titles":"A Machine Learning Approach for Modelling Parking Duration in Urban Land-use","summaries":"Parking is an inevitable issue in the fast-growing developing countries.\nIncreasing number of vehicles require more and more urban land to be allocated\nfor parking. However, a little attention has been conferred to the parking\nissues in developing countries like India. This study proposes a model for\nanalysing the influence of car users' socioeconomic and travel characteristics\non parking duration. Specifically, artificial neural networks (ANNs) is\ndeployed to capture the interrelationship between driver characteristics and\nparking duration. ANNs are highly efficient in learning and recognizing\nconnections between parameters for best prediction of an outcome. Since,\nutility of ANNs has been critically limited due to its Black Box nature, the\nstudy involves the use of Garson algorithm and Local interpretable\nmodel-agnostic explanations (LIME) for model interpretations. LIME shows the\nprediction for any classification, by approximating it locally with the\ndeveloped interpretable model. This study is based on microdata collected\non-site through interview surveys considering two land-uses: office-business\nand market\/shopping. Results revealed the higher probability of prediction\nthrough LIME and therefore, the methodology can be adopted ubiquitously.\nFurther, the policy implications are discussed based on the results for both\nland-uses. This unique study could lead to enhanced parking policy and\nmanagement to achieve the sustainability goals.","terms":["stat.ML","cs.LG","physics.soc-ph"]},{"titles":"A Variational Autoencoder Framework for Robust, Physics-Informed Cyberattack Recognition in Industrial Cyber-Physical Systems","summaries":"Cybersecurity of Industrial Cyber-Physical Systems is drawing significant\nconcerns as data communication increasingly leverages wireless networks. A lot\nof data-driven methods were develope for detecting cyberattacks, but few are\nfocused on distinguishing them from equipment faults. In this paper, we develop\na data-driven framework that can be used to detect, diagnose, and localize a\ntype of cyberattack called covert attacks on networked industrial control\nsystems. The framework has a hybrid design that combines a variational\nautoencoder (VAE), a recurrent neural network (RNN), and a Deep Neural Network\n(DNN). This data-driven framework considers the temporal behavior of a generic\nphysical system that extracts features from the time series of the sensor\nmeasurements that can be used for detecting covert attacks, distinguishing them\nfrom equipment faults, as well as localize the attack\/fault. We evaluate the\nperformance of the proposed method through a realistic simulation study on a\nnetworked power transmission system as a typical example of ICS. We compare the\nperformance of the proposed method with the traditional model-based method to\nshow its applicability and efficacy.","terms":["cs.LG","cs.SY","eess.SY"]},{"titles":"What Does Stable Diffusion Know about the 3D Scene?","summaries":"Recent advances in generative models like Stable Diffusion enable the\ngeneration of highly photo-realistic images. Our objective in this paper is to\nprobe the diffusion network to determine to what extent it 'understands'\ndifferent properties of the 3D scene depicted in an image. To this end, we make\nthe following contributions: (i) We introduce a protocol to evaluate whether a\nnetwork models a number of physical 'properties' of the 3D scene by probing for\nexplicit features that represent these properties. The probes are applied on\ndatasets of real images with annotations for the property. (ii) We apply this\nprotocol to properties covering scene geometry, scene material, support\nrelations, lighting, and view dependent measures. (iii) We find that Stable\nDiffusion is good at a number of properties including scene geometry, support\nrelations, shadows and depth, but less performant for occlusion. (iv) We also\napply the probes to other models trained at large-scale, including DINO and\nCLIP, and find their performance inferior to that of Stable Diffusion.","terms":["cs.CV"]},{"titles":"Branched Latent Neural Maps","summaries":"We introduce Branched Latent Neural Maps (BLNMs) to learn finite dimensional\ninput-output maps encoding complex physical processes. A BLNM is defined by a\nsimple and compact feedforward partially-connected neural network that\nstructurally disentangles inputs with different intrinsic roles, such as the\ntime variable from model parameters of a differential equation, while\ntransferring them into a generic field of interest. BLNMs leverage latent\noutputs to enhance the learned dynamics and break the curse of dimensionality\nby showing excellent generalization properties with small training datasets and\nshort training times on a single processor. Indeed, their generalization error\nremains comparable regardless of the adopted discretization during the testing\nphase. Moreover, the partial connections significantly reduce the number of\ntunable parameters. We show the capabilities of BLNMs in a challenging test\ncase involving electrophysiology simulations in a biventricular cardiac model\nof a pediatric patient with hypoplastic left heart syndrome. The model includes\na 1D Purkinje network for fast conduction and a 3D heart-torso geometry.\nSpecifically, we trained BLNMs on 150 in silico generated 12-lead\nelectrocardiograms (ECGs) while spanning 7 model parameters, covering\ncell-scale and organ-level. Although the 12-lead ECGs manifest very fast\ndynamics with sharp gradients, after automatic hyperparameter tuning the\noptimal BLNM, trained in less than 3 hours on a single CPU, retains just 7\nhidden layers and 19 neurons per layer. The resulting mean square error is on\nthe order of $10^{-4}$ on a test dataset comprised of 50 electrophysiology\nsimulations. In the online phase, the BLNM allows for 5000x faster real-time\nsimulations of cardiac electrophysiology on a single core standard computer and\ncan be used to solve inverse problems via global optimization in a few seconds\nof computational time.","terms":["cs.LG","cs.NA","eess.SP","math.NA"]},{"titles":"Deep Learning for Automatic Detection and Facial Recognition in Japanese Macaques: Illuminating Social Networks","summaries":"Individual identification plays a pivotal role in ecology and ethology,\nnotably as a tool for complex social structures understanding. However,\ntraditional identification methods often involve invasive physical tags and can\nprove both disruptive for animals and time-intensive for researchers. In recent\nyears, the integration of deep learning in research offered new methodological\nperspectives through automatization of complex tasks. Harnessing object\ndetection and recognition technologies is increasingly used by researchers to\nachieve identification on video footage. This study represents a preliminary\nexploration into the development of a non-invasive tool for face detection and\nindividual identification of Japanese macaques (Macaca fuscata) through deep\nlearning. The ultimate goal of this research is, using identifications done on\nthe dataset, to automatically generate a social network representation of the\nstudied population. The current main results are promising: (i) the creation of\na Japanese macaques' face detector (Faster-RCNN model), reaching a 82.2%\naccuracy and (ii) the creation of an individual recognizer for K{\\=o}jima\nisland macaques population (YOLOv8n model), reaching a 83% accuracy. We also\ncreated a K{\\=o}jima population social network by traditional methods, based on\nco-occurrences on videos. Thus, we provide a benchmark against which the\nautomatically generated network will be assessed for reliability. These\npreliminary results are a testament to the potential of this innovative\napproach to provide the scientific community with a tool for tracking\nindividuals and social network studies in Japanese macaques.","terms":["cs.CV","cs.LG","cs.SI"]},{"titles":"TANGO: Time-Reversal Latent GraphODE for Multi-Agent Dynamical Systems","summaries":"Learning complex multi-agent system dynamics from data is crucial across many\ndomains, such as in physical simulations and material modeling. Extended from\npurely data-driven approaches, existing physics-informed approaches such as\nHamiltonian Neural Network strictly follow energy conservation law to introduce\ninductive bias, making their learning more sample efficiently. However, many\nreal-world systems do not strictly conserve energy, such as spring systems with\nfrictions. Recognizing this, we turn our attention to a broader physical\nprinciple: Time-Reversal Symmetry, which depicts that the dynamics of a system\nshall remain invariant when traversed back over time. It still helps to\npreserve energies for conservative systems and in the meanwhile, serves as a\nstrong inductive bias for non-conservative, reversible systems. To inject such\ninductive bias, in this paper, we propose a simple-yet-effective\nself-supervised regularization term as a soft constraint that aligns the\nforward and backward trajectories predicted by a continuous graph neural\nnetwork-based ordinary differential equation (GraphODE). It effectively imposes\ntime-reversal symmetry to enable more accurate model predictions across a wider\nrange of dynamical systems under classical mechanics. In addition, we further\nprovide theoretical analysis to show that our regularization essentially\nminimizes higher-order Taylor expansion terms during the ODE integration steps,\nwhich enables our model to be more noise-tolerant and even applicable to\nirreversible systems. Experimental results on a variety of physical systems\ndemonstrate the effectiveness of our proposed method. Particularly, it achieves\nan MSE improvement of 11.5 % on a challenging chaotic triple-pendulum systems.","terms":["cs.LG","cs.AI"]},{"titles":"Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach","summaries":"Graph neural networks (GNNs) are vulnerable to adversarial perturbations,\nincluding those that affect both node features and graph topology. This paper\ninvestigates GNNs derived from diverse neural flows, concentrating on their\nconnection to various stability notions such as BIBO stability, Lyapunov\nstability, structural stability, and conservative stability. We argue that\nLyapunov stability, despite its common use, does not necessarily ensure\nadversarial robustness. Inspired by physics principles, we advocate for the use\nof conservative Hamiltonian neural flows to construct GNNs that are robust to\nadversarial attacks. The adversarial robustness of different neural flow GNNs\nis empirically compared on several benchmark datasets under a variety of\nadversarial attacks. Extensive numerical experiments demonstrate that GNNs\nleveraging conservative Hamiltonian flows with Lyapunov stability substantially\nimprove robustness against adversarial perturbations. The implementation code\nof experiments is available at\nhttps:\/\/github.com\/zknus\/NeurIPS-2023-HANG-Robustness.","terms":["cs.LG"]},{"titles":"Transfer learning-based physics-informed convolutional neural network for simulating flow in porous media with time-varying controls","summaries":"A physics-informed convolutional neural network is proposed to simulate two\nphase flow in porous media with time-varying well controls. While most of\nPICNNs in existing literatures worked on parameter-to-state mapping, our\nproposed network parameterizes the solution with time-varying controls to\nestablish a control-to-state regression. Firstly, finite volume scheme is\nadopted to discretize flow equations and formulate loss function that respects\nmass conservation laws. Neumann boundary conditions are seamlessly incorporated\ninto the semi-discretized equations so no additional loss term is needed. The\nnetwork architecture comprises two parallel U-Net structures, with network\ninputs being well controls and outputs being the system states. To capture the\ntime-dependent relationship between inputs and outputs, the network is well\ndesigned to mimic discretized state space equations. We train the network\nprogressively for every timestep, enabling it to simultaneously predict oil\npressure and water saturation at each timestep. After training the network for\none timestep, we leverage transfer learning techniques to expedite the training\nprocess for subsequent timestep. The proposed model is used to simulate\noil-water porous flow scenarios with varying reservoir gridblocks and aspects\nincluding computation efficiency and accuracy are compared against\ncorresponding numerical approaches. The results underscore the potential of\nPICNN in effectively simulating systems with numerous grid blocks, as\ncomputation time does not scale with model dimensionality. We assess the\ntemporal error using 10 different testing controls with variation in magnitude\nand another 10 with higher alternation frequency with proposed control-to-state\narchitecture. Our observations suggest the need for a more robust and reliable\nmodel when dealing with controls that exhibit significant variations in\nmagnitude or frequency.","terms":["cs.LG","cs.CE"]},{"titles":"Robust Digital-Twin Localization via An RGBD-based Transformer Network and A Comprehensive Evaluation on a Mobile Dataset","summaries":"The potential of digital-twin technology, involving the creation of precise\ndigital replicas of physical objects, to reshape AR experiences in 3D object\ntracking and localization scenarios is significant. However, enabling robust 3D\nobject tracking in dynamic mobile AR environments remains a formidable\nchallenge. These scenarios often require a more robust pose estimator capable\nof handling the inherent sensor-level measurement noise. In this paper,\nrecognizing the challenges of comprehensive solutions in existing literature,\nwe propose a transformer-based 6DoF pose estimator designed to achieve\nstate-of-the-art accuracy under real-world noisy data. To systematically\nvalidate the new solution's performance against the prior art, we also\nintroduce a novel RGBD dataset called Digital Twin Tracking Dataset v2 (DTTD2),\nwhich is focused on digital-twin object tracking scenarios. Expanded from an\nexisting DTTD v1 (DTTD1), the new dataset adds digital-twin data captured using\na cutting-edge mobile RGBD sensor suite on Apple iPhone 14 Pro, expanding the\napplicability of our approach to iPhone sensor data. Through extensive\nexperimentation and in-depth analysis, we illustrate the effectiveness of our\nmethods under significant depth data errors, surpassing the performance of\nexisting baselines. Code and dataset are made publicly available at:\nhttps:\/\/github.com\/augcog\/DTTD2","terms":["cs.CV"]},{"titles":"Quantum Learning Theory Beyond Batch Binary Classification","summaries":"Arunachalam and de Wolf (2018) showed that the sample complexity of quantum\nbatch learning of boolean functions, in the realizable and agnostic settings,\nhas the same form and order as the corresponding classical sample complexities.\nIn this paper, we extend this, ostensibly surprising, message to batch\nmulticlass learning, online boolean learning, and online multiclass learning.\nFor our online learning results, we first consider an adaptive adversary\nvariant of the classical model of Dawid and Tewari (2022). Then, we introduce\nthe first (to the best of our knowledge) model of online learning with quantum\nexamples.","terms":["cs.LG","cs.CC","quant-ph","stat.ML"]},{"titles":"A Bayesian framework for discovering interpretable Lagrangian of dynamical systems from data","summaries":"Learning and predicting the dynamics of physical systems requires a profound\nunderstanding of the underlying physical laws. Recent works on learning\nphysical laws involve generalizing the equation discovery frameworks to the\ndiscovery of Hamiltonian and Lagrangian of physical systems. While the existing\nmethods parameterize the Lagrangian using neural networks, we propose an\nalternate framework for learning interpretable Lagrangian descriptions of\nphysical systems from limited data using the sparse Bayesian approach. Unlike\nexisting neural network-based approaches, the proposed approach (a) yields an\ninterpretable description of Lagrangian, (b) exploits Bayesian learning to\nquantify the epistemic uncertainty due to limited data, (c) automates the\ndistillation of Hamiltonian from the learned Lagrangian using Legendre\ntransformation, and (d) provides ordinary (ODE) and partial differential\nequation (PDE) based descriptions of the observed systems. Six different\nexamples involving both discrete and continuous system illustrates the efficacy\nof the proposed approach.","terms":["stat.ML","cs.LG"]},{"titles":"DockGame: Cooperative Games for Multimeric Rigid Protein Docking","summaries":"Protein interactions and assembly formation are fundamental to most\nbiological processes. Predicting the assembly structure from constituent\nproteins -- referred to as the protein docking task -- is thus a crucial step\nin protein design applications. Most traditional and deep learning methods for\ndocking have focused mainly on binary docking, following either a search-based,\nregression-based, or generative modeling paradigm. In this paper, we focus on\nthe less-studied multimeric (i.e., two or more proteins) docking problem. We\nintroduce DockGame, a novel game-theoretic framework for docking -- we view\nprotein docking as a cooperative game between proteins, where the final\nassembly structure(s) constitute stable equilibria w.r.t. the underlying game\npotential. Since we do not have access to the true potential, we consider two\napproaches - i) learning a surrogate game potential guided by physics-based\nenergy functions and computing equilibria by simultaneous gradient updates, and\nii) sampling from the Gibbs distribution of the true potential by learning a\ndiffusion generative model over the action spaces (rotations and translations)\nof all proteins. Empirically, on the Docking Benchmark 5.5 (DB5.5) dataset,\nDockGame has much faster runtimes than traditional docking methods, can\ngenerate multiple plausible assembly structures, and achieves comparable\nperformance to existing binary docking baselines, despite solving the harder\ntask of coordinating multiple protein chains.","terms":["cs.LG"]},{"titles":"Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models","summaries":"We present Step-Back Prompting, a simple prompting technique that enables\nLLMs to do abstractions to derive high-level concepts and first principles from\ninstances containing specific details. Using the concepts and principles to\nguide the reasoning steps, LLMs significantly improve their abilities in\nfollowing a correct reasoning path towards the solution. We conduct experiments\nof Step-Back Prompting with PaLM-2L models and observe substantial performance\ngains on a wide range of challenging reasoning-intensive tasks including STEM,\nKnowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting\nimproves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%,\nTimeQA by 27%, and MuSiQue by 7%.","terms":["cs.LG","cs.AI","cs.CL"]},{"titles":"Transformers and Large Language Models for Chemistry and Drug Discovery","summaries":"Language modeling has seen impressive progress over the last years, mainly\nprompted by the invention of the Transformer architecture, sparking a\nrevolution in many fields of machine learning, with breakthroughs in chemistry\nand biology. In this chapter, we explore how analogies between chemical and\nnatural language have inspired the use of Transformers to tackle important\nbottlenecks in the drug discovery process, such as retrosynthetic planning and\nchemical space exploration. The revolution started with models able to perform\nparticular tasks with a single type of data, like linearised molecular graphs,\nwhich then evolved to include other types of data, like spectra from analytical\ninstruments, synthesis actions, and human language. A new trend leverages\nrecent developments in large language models, giving rise to a wave of models\ncapable of solving generic tasks in chemistry, all facilitated by the\nflexibility of natural language. As we continue to explore and harness these\ncapabilities, we can look forward to a future where machine learning plays an\neven more integral role in accelerating scientific discovery.","terms":["cs.LG","physics.chem-ph"]},{"titles":"Augmenting Vision-Based Human Pose Estimation with Rotation Matrix","summaries":"Fitness applications are commonly used to monitor activities within the gym,\nbut they often fail to automatically track indoor activities inside the gym.\nThis study proposes a model that utilizes pose estimation combined with a novel\ndata augmentation method, i.e., rotation matrix. We aim to enhance the\nclassification accuracy of activity recognition based on pose estimation data.\nThrough our experiments, we experiment with different classification algorithms\nalong with image augmentation approaches. Our findings demonstrate that the SVM\nwith SGD optimization, using data augmentation with the Rotation Matrix, yields\nthe most accurate results, achieving a 96% accuracy rate in classifying five\nphysical activities. Conversely, without implementing the data augmentation\ntechniques, the baseline accuracy remains at a modest 64%.","terms":["cs.CV","cs.AI"]},{"titles":"Generative ensemble deep learning severe weather prediction from a deterministic convection-allowing model","summaries":"An ensemble post-processing method is developed for the probabilistic\nprediction of severe weather (tornadoes, hail, and wind gusts) over the\nconterminous United States (CONUS). The method combines conditional generative\nadversarial networks (CGANs), a type of deep generative model, with a\nconvolutional neural network (CNN) to post-process convection-allowing model\n(CAM) forecasts. The CGANs are designed to create synthetic ensemble members\nfrom deterministic CAM forecasts, and their outputs are processed by the CNN to\nestimate the probability of severe weather. The method is tested using\nHigh-Resolution Rapid Refresh (HRRR) 1--24 hr forecasts as inputs and Storm\nPrediction Center (SPC) severe weather reports as targets. The method produced\nskillful predictions with up to 20% Brier Skill Score (BSS) increases compared\nto other neural-network-based reference methods using a testing dataset of HRRR\nforecasts in 2021. For the evaluation of uncertainty quantification, the method\nis overconfident but produces meaningful ensemble spreads that can distinguish\ngood and bad forecasts. The quality of CGAN outputs is also evaluated. Results\nshow that the CGAN outputs behave similarly to a numerical ensemble; they\npreserved the inter-variable correlations and the contribution of influential\npredictors as in the original HRRR forecasts. This work provides a novel\napproach to post-process CAM output using neural networks that can be applied\nto severe weather prediction.","terms":["cs.LG","cs.AI","physics.ao-ph"]},{"titles":"Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks","summaries":"Backward propagation (BP) is widely used to compute the gradients in neural\nnetwork training. However, it is hard to implement BP on edge devices due to\nthe lack of hardware and software resources to support automatic\ndifferentiation. This has tremendously increased the design complexity and\ntime-to-market of on-device training accelerators. This paper presents a\ncompletely BP-free framework that only requires forward propagation to train\nrealistic neural networks. Our technical contributions are three-fold. Firstly,\nwe present a tensor-compressed variance reduction approach to greatly improve\nthe scalability of zeroth-order (ZO) optimization, making it feasible to handle\na network size that is beyond the capability of previous ZO approaches.\nSecondly, we present a hybrid gradient evaluation approach to improve the\nefficiency of ZO training. Finally, we extend our BP-free training framework to\nphysics-informed neural networks (PINNs) by proposing a sparse-grid approach to\nestimate the derivatives in the loss function without using BP. Our BP-free\ntraining only loses little accuracy on the MNIST dataset compared with standard\nfirst-order training. We also demonstrate successful results in training a PINN\nfor solving a 20-dim Hamiltonian-Jacobi-Bellman PDE. This memory-efficient and\nBP-free approach may serve as a foundation for the near-future on-device\ntraining on many resource-constraint platforms (e.g., FPGA, ASIC,\nmicro-controllers, and photonic chips).","terms":["cs.LG","cs.AI"]},{"titles":"A Low-Cost Lane-Following Algorithm for Cyber-Physical Robots","summaries":"Duckiebots are low-cost mobile robots that are widely used in the fields of\nresearch and education. Although there are existing self-driving algorithms for\nthe Duckietown platform, they are either too complex or perform too poorly to\nnavigate a multi-lane track. Moreover, it is essential to give memory and\ncomputational resources to a Duckiebot so it can perform additional tasks such\nas out-of-distribution input detection. In order to satisfy these constraints,\nwe built a low-cost autonomous driving algorithm capable of driving on a\ntwo-lane track. The algorithm uses traditional computer vision techniques to\nidentify the central lane on the track and obtain the relevant steering angle.\nThe steering is then controlled by a PID controller that smoothens the movement\nof the Duckiebot. The performance of the algorithm was compared to that of the\nNeurIPS 2018 AI Driving Olympics (AIDO) finalists, and it outperformed all but\none finalists. The two main contributions of our algorithm are its low\ncomputational requirements and very quick set-up, with ongoing efforts to make\nit more reliable.","terms":["cs.CV","cs.RO"]},{"titles":"An operator preconditioning perspective on training in physics-informed machine learning","summaries":"In this paper, we investigate the behavior of gradient descent algorithms in\nphysics-informed machine learning methods like PINNs, which minimize residuals\nconnected to partial differential equations (PDEs). Our key result is that the\ndifficulty in training these models is closely related to the conditioning of a\nspecific differential operator. This operator, in turn, is associated to the\nHermitian square of the differential operator of the underlying PDE. If this\noperator is ill-conditioned, it results in slow or infeasible training.\nTherefore, preconditioning this operator is crucial. We employ both rigorous\nmathematical analysis and empirical evaluations to investigate various\nstrategies, explaining how they better condition this critical operator, and\nconsequently improve training.","terms":["cs.LG"]},{"titles":"Climate-sensitive Urban Planning through Optimization of Tree Placements","summaries":"Climate change is increasing the intensity and frequency of many extreme\nweather events, including heatwaves, which results in increased thermal\ndiscomfort and mortality rates. While global mitigation action is undoubtedly\nnecessary, so is climate adaptation, e.g., through climate-sensitive urban\nplanning. Among the most promising strategies is harnessing the benefits of\nurban trees in shading and cooling pedestrian-level environments. Our work\ninvestigates the challenge of optimal placement of such trees. Physical\nsimulations can estimate the radiative and thermal impact of trees on human\nthermal comfort but induce high computational costs. This rules out\noptimization of tree placements over large areas and considering effects over\nlonger time scales. Hence, we employ neural networks to simulate the point-wise\nmean radiant temperatures--a driving factor of outdoor human thermal\ncomfort--across various time scales, spanning from daily variations to extended\ntime scales of heatwave events and even decades. To optimize tree placements,\nwe harness the innate local effect of trees within the iterated local search\nframework with tailored adaptations. We show the efficacy of our approach\nacross a wide spectrum of study areas and time scales. We believe that our\napproach is a step towards empowering decision-makers, urban designers and\nplanners to proactively and effectively assess the potential of urban trees to\nmitigate heat stress.","terms":["cs.CV","physics.ao-ph"]},{"titles":"An unsupervised machine-learning-based shock sensor for high-order supersonic flow solvers","summaries":"We present a novel unsupervised machine-learning sock sensor based on\nGaussian Mixture Models (GMMs). The proposed GMM sensor demonstrates remarkable\naccuracy in detecting shocks and is robust across diverse test cases with\nsignificantly less parameter tuning than other options. We compare the\nGMM-based sensor with state-of-the-art alternatives. All methods are integrated\ninto a high-order compressible discontinuous Galerkin solver, where two\nstabilization approaches are coupled to the sensor to provide examples of\npossible applications. The Sedov blast and double Mach reflection cases\ndemonstrate that our proposed sensor can enhance hybrid sub-cell\nflux-differencing formulations by providing accurate information of the nodes\nthat require low-order blending. Besides, supersonic test cases including high\nReynolds numbers showcase the sensor performance when used to introduce\nentropy-stable artificial viscosity to capture shocks, demonstrating the same\neffectiveness as fine-tuned state-of-the-art sensors. The adaptive nature and\nability to function without extensive training datasets make this GMM-based\nsensor suitable for complex geometries and varied flow configurations. Our\nstudy reveals the potential of unsupervised machine-learning methods,\nexemplified by this GMM sensor, to improve the robustness and efficiency of\nadvanced CFD codes.","terms":["cs.LG","physics.comp-ph","physics.flu-dyn"]},{"titles":"WeatherGNN: Exploiting Complicated Relationships in Numerical Weather Prediction Bias Correction","summaries":"Numerical weather prediction (NWP) may be inaccurate or biased due to\nincomplete atmospheric physical processes, insufficient spatial-temporal\nresolution, and inherent uncertainty of weather. Previous studies have\nattempted to correct biases by using handcrafted features and domain knowledge,\nor by applying general machine learning models naively. They do not fully\nexplore the complicated meteorologic interactions and spatial dependencies in\nthe atmosphere dynamically, which limits their applicability in NWP\nbias-correction. Specifically, weather factors interact with each other in\ncomplex ways, and these interactions can vary regionally. In addition, the\ninteractions between weather factors are further complicated by the spatial\ndependencies between regions, which are influenced by varied terrain and\natmospheric motions. To address these issues, we propose WeatherGNN, an NWP\nbias-correction method that utilizes Graph Neural Networks (GNN) to learn\nmeteorologic and geographic relationships in a unified framework. Our approach\nincludes a factor-wise GNN that captures meteorological interactions within\neach grid (a specific location) adaptively, and a fast hierarchical GNN that\ncaptures spatial dependencies between grids dynamically. Notably, the fast\nhierarchical GNN achieves linear complexity with respect to the number of\ngrids, enhancing model efficiency and scalability. Our experimental results on\ntwo real-world datasets demonstrate the superiority of WeatherGNN in comparison\nwith other SOTA methods, with an average improvement of 40.50\\% on RMSE\ncompared to the original NWP.","terms":["cs.LG"]},{"titles":"On sparse regression, Lp-regularization, and automated model discovery","summaries":"Sparse regression and feature extraction are the cornerstones of knowledge\ndiscovery from massive data. Their goal is to discover interpretable and\npredictive models that provide simple relationships among scientific variables.\nWhile the statistical tools for model discovery are well established in the\ncontext of linear regression, their generalization to nonlinear regression in\nmaterial modeling is highly problem-specific and insufficiently understood.\nHere we explore the potential of neural networks for automatic model discovery\nand induce sparsity by a hybrid approach that combines two strategies:\nregularization and physical constraints. We integrate the concept of Lp\nregularization for subset selection with constitutive neural networks that\nleverage our domain knowledge in kinematics and thermodynamics. We train our\nnetworks with both, synthetic and real data, and perform several thousand\ndiscovery runs to infer common guidelines and trends: L2 regularization or\nridge regression is unsuitable for model discovery; L1 regularization or lasso\npromotes sparsity, but induces strong bias; only L0 regularization allows us to\ntransparently fine-tune the trade-off between interpretability and\npredictability, simplicity and accuracy, and bias and variance. With these\ninsights, we demonstrate that Lp regularized constitutive neural networks can\nsimultaneously discover both, interpretable models and physically meaningful\nparameters. We anticipate that our findings will generalize to alternative\ndiscovery techniques such as sparse and symbolic regression, and to other\ndomains such as biology, chemistry, or medicine. Our ability to automatically\ndiscover material models from data could have tremendous applications in\ngenerative material design and open new opportunities to manipulate matter,\nalter properties of existing materials, and discover new materials with\nuser-defined properties.","terms":["cs.LG","65, 74","I.6; J.2"]},{"titles":"Bayesian Optimisation for Sequential Experimental Design with Applications in Additive Manufacturing","summaries":"Bayesian optimization (BO) is an approach to globally optimizing black-box\nobjective functions that are expensive to evaluate. BO-powered experimental\ndesign has found wide application in materials science, chemistry, experimental\nphysics, drug development, etc. This work aims to bring attention to the\nbenefits of applying BO in designing experiments and to provide a BO manual,\ncovering both methodology and software, for the convenience of anyone who wants\nto apply or learn BO. In particular, we briefly explain the BO technique,\nreview all the applications of BO in additive manufacturing, compare and\nexemplify the features of different open BO libraries, unlock new potential\napplications of BO to other types of data (e.g., preferential output). This\narticle is aimed at readers with some understanding of Bayesian methods, but\nnot necessarily with knowledge of additive manufacturing; the software\nperformance overview and implementation instructions are instrumental for any\nexperimental-design practitioner. Moreover, our review in the field of additive\nmanufacturing highlights the current knowledge and technological trends of BO.\nThis article has a supplementary material online.","terms":["cs.LG","cs.CE"]},{"titles":"SEEDS: Emulation of Weather Forecast Ensembles with Diffusion Models","summaries":"Uncertainty quantification is crucial to decision-making. A prominent example\nis probabilistic forecasting in numerical weather prediction. The dominant\napproach to representing uncertainty in weather forecasting is to generate an\nensemble of forecasts. This is done by running many physics-based simulations\nunder different conditions, which is a computationally costly process. We\npropose to amortize the computational cost by emulating these forecasts with\ndeep generative diffusion models learned from historical data. The learned\nmodels are highly scalable with respect to high-performance computing\naccelerators and can sample hundreds to tens of thousands of realistic weather\nforecasts at low cost. When designed to emulate operational ensemble forecasts,\nthe generated ones are similar to physics-based ensembles in important\nstatistical properties and predictive skill. When designed to correct biases\npresent in the operational forecasting system, the generated ensembles show\nimproved probabilistic forecast metrics. They are more reliable and forecast\nprobabilities of extreme weather events more accurately. While this work\ndemonstrates the utility of the methodology by focusing on weather forecasting,\nthe generative artificial intelligence methodology can be extended for\nuncertainty quantification in climate modeling, where we believe the generation\nof very large ensembles of climate projections will play an increasingly\nimportant role in climate risk assessment.","terms":["cs.LG","physics.ao-ph"]},{"titles":"Investigating the Ability of PINNs To Solve Burgers' PDE Near Finite-Time BlowUp","summaries":"Physics Informed Neural Networks (PINNs) have been achieving ever newer feats\nof solving complicated PDEs numerically while offering an attractive trade-off\nbetween accuracy and speed of inference. A particularly challenging aspect of\nPDEs is that there exist simple PDEs which can evolve into singular solutions\nin finite time starting from smooth initial conditions. In recent times some\nstriking experiments have suggested that PINNs might be good at even detecting\nsuch finite-time blow-ups. In this work, we embark on a program to investigate\nthis stability of PINNs from a rigorous theoretical viewpoint. Firstly, we\nderive generalization bounds for PINNs for Burgers' PDE, in arbitrary\ndimensions, under conditions that allow for a finite-time blow-up. Then we\ndemonstrate via experiments that our bounds are significantly correlated to the\n$\\ell_2$-distance of the neurally found surrogate from the true blow-up\nsolution, when computed on sequences of PDEs that are getting increasingly\nclose to a blow-up.","terms":["cs.LG","cs.NA","math.AP","math.NA"]},{"titles":"Unsupervisedly Prompting AlphaFold2 for Few-Shot Learning of Accurate Folding Landscape and Protein Structure Prediction","summaries":"Data-driven predictive methods which can efficiently and accurately transform\nprotein sequences into biologically active structures are highly valuable for\nscientific research and medical development. Determining accurate folding\nlandscape using co-evolutionary information is fundamental to the success of\nmodern protein structure prediction methods. As the state of the art,\nAlphaFold2 has dramatically raised the accuracy without performing explicit\nco-evolutionary analysis. Nevertheless, its performance still shows strong\ndependence on available sequence homologs. Based on the interrogation on the\ncause of such dependence, we presented EvoGen, a meta generative model, to\nremedy the underperformance of AlphaFold2 for poor MSA targets. By prompting\nthe model with calibrated or virtually generated homologue sequences, EvoGen\nhelps AlphaFold2 fold accurately in low-data regime and even achieve\nencouraging performance with single-sequence predictions. Being able to make\naccurate predictions with few-shot MSA not only generalizes AlphaFold2 better\nfor orphan sequences, but also democratizes its use for high-throughput\napplications. Besides, EvoGen combined with AlphaFold2 yields a probabilistic\nstructure generation method which could explore alternative conformations of\nprotein sequences, and the task-aware differentiable algorithm for sequence\ngeneration will benefit other related tasks including protein design.","terms":["cs.LG","cs.AI","physics.bio-ph"]},{"titles":"ASM: Adaptive Skinning Model for High-Quality 3D Face Modeling","summaries":"The research fields of parametric face model and 3D face reconstruction have\nbeen extensively studied. However, a critical question remains unanswered: how\nto tailor the face model for specific reconstruction settings. We argue that\nreconstruction with multi-view uncalibrated images demands a new model with\nstronger capacity. Our study shifts attention from data-dependent 3D Morphable\nModels (3DMM) to an understudied human-designed skinning model. We propose\nAdaptive Skinning Model (ASM), which redefines the skinning model with more\ncompact and fully tunable parameters. With extensive experiments, we\ndemonstrate that ASM achieves significantly improved capacity than 3DMM, with\nthe additional advantage of model size and easy implementation for new\ntopology. We achieve state-of-the-art performance with ASM for multi-view\nreconstruction on the Florence MICC Coop benchmark. Our quantitative analysis\ndemonstrates the importance of a high-capacity model for fully exploiting\nabundant information from multi-view input in reconstruction. Furthermore, our\nmodel with physical-semantic parameters can be directly utilized for real-world\napplications, such as in-game avatar creation. As a result, our work opens up\nnew research direction for parametric face model and facilitates future\nresearch on multi-view reconstruction.","terms":["cs.CV"]},{"titles":"Robust Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers","summaries":"Deep neural networks (DNNs) can be manipulated to exhibit specific behaviors\nwhen exposed to specific trigger patterns, without affecting their performance\non benign samples, dubbed backdoor attack. Some recent research has focused on\ndesigning invisible triggers for backdoor attacks to ensure visual\nstealthiness, while showing high effectiveness, even under backdoor defense.\nHowever, we find that these carefully designed invisible triggers are often\nsensitive to visual distortion during inference, such as Gaussian blurring or\nenvironmental variations in physical scenarios. This phenomenon could\nsignificantly undermine the practical effectiveness of attacks, but has been\nrarely paid attention to and thoroughly investigated. To address this\nlimitation, we define a novel trigger called the Visible, Semantic,\nSample-Specific, and Compatible trigger (VSSC trigger), to achieve effective,\nstealthy and robust to visual distortion simultaneously. To implement it, we\ndevelop an innovative approach by utilizing the powerful capabilities of large\nlanguage models for choosing the suitable trigger and text-guided image editing\ntechniques for generating the poisoned image with the trigger. Extensive\nexperimental results and analysis validate the effectiveness, stealthiness and\nrobustness of the VSSC trigger. It demonstrates superior robustness to\ndistortions compared with most digital backdoor attacks and allows more\nefficient and flexible trigger integration compared to physical backdoor\nattacks. We hope that the proposed VSSC trigger and implementation approach\ncould inspire future studies on designing more practical triggers in backdoor\nattacks.","terms":["cs.CV","cs.CR"]},{"titles":"Multi-Resolution Active Learning of Fourier Neural Operators","summaries":"Fourier Neural Operator (FNO) is a popular operator learning framework, which\nnot only achieves the state-of-the-art performance in many tasks, but also is\nhighly efficient in training and prediction. However, collecting training data\nfor the FNO is a costly bottleneck in practice, because it often demands\nexpensive physical simulations. To overcome this problem, we propose\nMulti-Resolution Active learning of FNO (MRA-FNO), which can dynamically select\nthe input functions and resolutions to lower the data cost as much as possible\nwhile optimizing the learning efficiency. Specifically, we propose a\nprobabilistic multi-resolution FNO and use ensemble Monte-Carlo to develop an\neffective posterior inference algorithm. To conduct active learning, we\nmaximize a utility-cost ratio as the acquisition function to acquire new\nexamples and resolutions at each step. We use moment matching and the matrix\ndeterminant lemma to enable tractable, efficient utility computation.\nFurthermore, we develop a cost annealing framework to avoid over-penalizing\nhigh-resolution queries at the early stage. The over-penalization is severe\nwhen the cost difference is significant between the resolutions, which renders\nactive learning often stuck at low-resolution queries and inferior performance.\nOur method overcomes this problem and applies to general multi-fidelity active\nlearning and optimization problems. We have shown the advantage of our method\nin several benchmark operator learning tasks.","terms":["cs.LG"]},{"titles":"Crystal-GFN: sampling crystals with desirable properties and constraints","summaries":"Accelerating material discovery holds the potential to greatly help mitigate\nthe climate crisis. Discovering new solid-state crystals such as\nelectrocatalysts, ionic conductors or photovoltaics can have a crucial impact,\nfor instance, in improving the efficiency of renewable energy production and\nstorage. In this paper, we introduce Crystal-GFlowNet, a generative model of\ncrystal structures that sequentially samples a crystal's composition, space\ngroup and lattice parameters. This domain-inspired approach enables the\nflexible incorporation of physical and geometrical constraints, as well as the\nuse of any available predictive model of a desired property as an objective\nfunction. We evaluate the capabilities of Crystal-GFlowNet by using as\nobjective the formation energy of a crystal structure, as predicted by a new\nproxy model trained on MatBench. The results demonstrate that Crystal-GFlowNet\nis able to sample diverse crystals with low formation energy.","terms":["cs.LG"]},{"titles":"Randomized Sparse Neural Galerkin Schemes for Solving Evolution Equations with Deep Networks","summaries":"Training neural networks sequentially in time to approximate solution fields\nof time-dependent partial differential equations can be beneficial for\npreserving causality and other physics properties; however, the\nsequential-in-time training is numerically challenging because training errors\nquickly accumulate and amplify over time. This work introduces Neural Galerkin\nschemes that update randomized sparse subsets of network parameters at each\ntime step. The randomization avoids overfitting locally in time and so helps\nprevent the error from accumulating quickly over the sequential-in-time\ntraining, which is motivated by dropout that addresses a similar issue of\noverfitting due to neuron co-adaptation. The sparsity of the update reduces the\ncomputational costs of training without losing expressiveness because many of\nthe network parameters are redundant locally at each time step. In numerical\nexperiments with a wide range of evolution equations, the proposed scheme with\nrandomized sparse updates is up to two orders of magnitude more accurate at a\nfixed computational budget and up to two orders of magnitude faster at a fixed\naccuracy than schemes with dense updates.","terms":["cs.LG","cs.NA","math.NA","stat.ML"]},{"titles":"PMNN:Physical Model-driven Neural Network for solving time-fractional differential equations","summaries":"In this paper, an innovative Physical Model-driven Neural Network (PMNN)\nmethod is proposed to solve time-fractional differential equations. It\nestablishes a temporal iteration scheme based on physical model-driven neural\nnetworks which effectively combines deep neural networks (DNNs) with\ninterpolation approximation of fractional derivatives. Specifically, once the\nfractional differential operator is discretized, DNNs are employed as a bridge\nto integrate interpolation approximation techniques with differential\nequations. On the basis of this integration, we construct a neural-based\niteration scheme. Subsequently, by training DNNs to learn this temporal\niteration scheme, approximate solutions to the differential equations can be\nobtained. The proposed method aims to preserve the intrinsic physical\ninformation within the equations as far as possible. It fully utilizes the\npowerful fitting capability of neural networks while maintaining the efficiency\nof the difference schemes for fractional differential equations. Moreover, we\nvalidate the efficiency and accuracy of PMNN through several numerical\nexperiments.","terms":["cs.LG","cs.AI","cs.NA","math.NA"]},{"titles":"How does Transformer model evolve to learn diverse chemical structures?","summaries":"Recent years have seen rapid development of descriptor generation based on\nrepresentation learning of extremely diverse molecules, especially those that\napply natural language processing (NLP) models to SMILES, a literal\nrepresentation of molecular structure. However, little research has been done\non how these models understand chemical structure. To address this black box,\nwe investigated the relationship between the learning progress of SMILES and\nchemical structure using a representative NLP model, the Transformer. The\nresults suggest that while the Transformer learns partial structures of\nmolecules quickly, it requires extended training to understand overall\nstructures. Consistently, the accuracy of molecular property predictions using\ndescriptors generated from models at different learning steps was similar from\nthe beginning to the end of training. Furthermore, we found that the\nTransformer requires particularly long training to learn chirality and\nsometimes stagnates with low translation accuracy due to misunderstanding of\nenantiomers. These findings are expected to deepen the understanding of NLP\nmodels in chemistry.","terms":["cs.LG","cs.CL","physics.chem-ph","q-bio.BM","J.2; I.2.7"]},{"titles":"Modeling non-uniform uncertainty in Reaction Prediction via Boosting and Dropout","summaries":"Reaction prediction has been recognized as a critical task in synthetic\nchemistry, where the goal is to predict the outcome of a reaction based on the\ngiven reactants. With the widespread adoption of generative models, the\nVariational Autoencoder(VAE) framework has typically been employed to tackle\nchallenges in reaction prediction, where the reactants are encoded as a\ncondition for the decoder, which then generates the product. Despite\neffectiveness, these conditional VAE (CVAE) models still fail to adequately\naccount for the inherent uncertainty in reaction prediction, which primarily\nstems from the stochastic reaction process. The principal limitations are\ntwofold. Firstly, in these CVAE models, the prior is independent of the\nreactants, leading to a default wide and assumed uniform distribution variance\nof the generated product. Secondly, reactants with analogous molecular\nrepresentations are presumed to undergo similar electronic transition\nprocesses, thereby producing similar products. This hinders the ability to\nmodel diverse reaction mechanisms effectively. Since the variance in outcomes\nis inherently non-uniform, we are thus motivated to develop a framework that\ngenerates reaction products with non-uniform uncertainty. Firstly, we eliminate\nthe latent variable in previous CVAE models to mitigate uncontrol-label noise.\nInstead, we introduce randomness into product generation via boosting to\nensemble diverse models and cover the range of potential outcomes, and through\ndropout to secure models with minor variations. Additionally, we design a\nranking method to union the predictions from boosting and dropout, prioritizing\nthe most plausible products. Experimental results on the largest reaction\nprediction benchmark USPTO-MIT show the superior performance of our proposed\nmethod in modeling the non-uniform uncertainty compared to baselines.","terms":["cs.LG","physics.chem-ph"]},{"titles":"HybrUR: A Hybrid Physical-Neural Solution for Unsupervised Underwater Image Restoration","summaries":"Robust vision restoration of underwater images remains a challenge. Owing to\nthe lack of well-matched underwater and in-air images, unsupervised methods\nbased on the cyclic generative adversarial framework have been widely\ninvestigated in recent years. However, when using an end-to-end unsupervised\napproach with only unpaired image data, mode collapse could occur, and the\ncolor correction of the restored images is usually poor. In this paper, we\npropose a data- and physics-driven unsupervised architecture to perform\nunderwater image restoration from unpaired underwater and in-air images. For\neffective color correction and quality enhancement, an underwater image\ndegeneration model must be explicitly constructed based on the optically\nunambiguous physics law. Thus, we employ the Jaffe-McGlamery degeneration\ntheory to design a generator and use neural networks to model the process of\nunderwater visual degeneration. Furthermore, we impose physical constraints on\nthe scene depth and degeneration factors for backscattering estimation to avoid\nthe vanishing gradient problem during the training of the hybrid\nphysical-neural model. Experimental results show that the proposed method can\nbe used to perform high-quality restoration of unconstrained underwater images\nwithout supervision. On multiple benchmarks, the proposed method outperforms\nseveral state-of-the-art supervised and unsupervised approaches. We demonstrate\nthat our method yields encouraging results in real-world applications.","terms":["cs.CV","eess.IV"]},{"titles":"Universal Humanoid Motion Representations for Physics-Based Control","summaries":"We present a universal motion representation that encompasses a comprehensive\nrange of motor skills for physics-based humanoid control. Due to the\nhigh-dimensionality of humanoid control as well as the inherent difficulties in\nreinforcement learning, prior methods have focused on learning skill embeddings\nfor a narrow range of movement styles (e.g. locomotion, game characters) from\nspecialized motion datasets. This limited scope hampers its applicability in\ncomplex tasks. Our work closes this gap, significantly increasing the coverage\nof motion representation space. To achieve this, we first learn a motion\nimitator that can imitate all of human motion from a large, unstructured motion\ndataset. We then create our motion representation by distilling skills directly\nfrom the imitator. This is achieved using an encoder-decoder structure with a\nvariational information bottleneck. Additionally, we jointly learn a prior\nconditioned on proprioception (humanoid's own pose and velocities) to improve\nmodel expressiveness and sampling efficiency for downstream tasks. Sampling\nfrom the prior, we can generate long, stable, and diverse human motions. Using\nthis latent space for hierarchical RL, we show that our policies solve tasks\nusing natural and realistic human behavior. We demonstrate the effectiveness of\nour motion representation by solving generative tasks (e.g. strike, terrain\ntraversal) and motion tracking using VR controllers.","terms":["cs.CV","cs.GR","cs.RO"]},{"titles":"Generative Diffusion From An Action Principle","summaries":"Generative diffusion models synthesize new samples by reversing a diffusive\nprocess that converts a given data set to generic noise. This is accomplished\nby training a neural network to match the gradient of the log of the\nprobability distribution of a given data set, also called the score. By casting\nreverse diffusion as an optimal control problem, we show that score matching\ncan be derived from an action principle, like the ones commonly used in\nphysics. We use this insight to demonstrate the connection between different\nclasses of diffusion models.","terms":["cs.LG","physics.class-ph"]},{"titles":"Adjustable Robust Reinforcement Learning for Online 3D Bin Packing","summaries":"Designing effective policies for the online 3D bin packing problem (3D-BPP)\nhas been a long-standing challenge, primarily due to the unpredictable nature\nof incoming box sequences and stringent physical constraints. While current\ndeep reinforcement learning (DRL) methods for online 3D-BPP have shown\npromising results in optimizing average performance over an underlying box\nsequence distribution, they often fail in real-world settings where some\nworst-case scenarios can materialize. Standard robust DRL algorithms tend to\noverly prioritize optimizing the worst-case performance at the expense of\nperformance under normal problem instance distribution. To address these\nissues, we first introduce a permutation-based attacker to investigate the\npractical robustness of both DRL-based and heuristic methods proposed for\nsolving online 3D-BPP. Then, we propose an adjustable robust reinforcement\nlearning (AR2L) framework that allows efficient adjustment of robustness\nweights to achieve the desired balance of the policy's performance in average\nand worst-case environments. Specifically, we formulate the objective function\nas a weighted sum of expected and worst-case returns, and derive the lower\nperformance bound by relating to the return under a mixture dynamics. To\nrealize this lower bound, we adopt an iterative procedure that searches for the\nassociated mixture dynamics and improves the corresponding policy. We integrate\nthis procedure into two popular robust adversarial algorithms to develop the\nexact and approximate AR2L algorithms. Experiments demonstrate that AR2L is\nversatile in the sense that it improves policy robustness while maintaining an\nacceptable level of performance for the nominal case.","terms":["cs.LG","cs.AI"]},{"titles":"Gradient Descent Provably Solves Nonlinear Tomographic Reconstruction","summaries":"In computed tomography (CT), the forward model consists of a linear Radon\ntransform followed by an exponential nonlinearity based on the attenuation of\nlight according to the Beer-Lambert Law. Conventional reconstruction often\ninvolves inverting this nonlinearity as a preprocessing step and then solving a\nconvex inverse problem. However, this nonlinear measurement preprocessing\nrequired to use the Radon transform is poorly conditioned in the vicinity of\nhigh-density materials, such as metal. This preprocessing makes CT\nreconstruction methods numerically sensitive and susceptible to artifacts near\nhigh-density regions. In this paper, we study a technique where the signal is\ndirectly reconstructed from raw measurements through the nonlinear forward\nmodel. Though this optimization is nonconvex, we show that gradient descent\nprovably converges to the global optimum at a geometric rate, perfectly\nreconstructing the underlying signal with a near minimal number of random\nmeasurements. We also prove similar results in the under-determined setting\nwhere the number of measurements is significantly smaller than the dimension of\nthe signal. This is achieved by enforcing prior structural information about\nthe signal through constraints on the optimization variables. We illustrate the\nbenefits of direct nonlinear CT reconstruction with cone-beam CT experiments on\nsynthetic and real 3D volumes. We show that this approach reduces metal\nartifacts compared to a commercial reconstruction of a human skull with metal\ndental crowns.","terms":["cs.CV","math.OC","physics.med-ph"]},{"titles":"Critical Initialization of Wide and Deep Neural Networks through Partial Jacobians: General Theory and Applications","summaries":"Deep neural networks are notorious for defying theoretical treatment.\nHowever, when the number of parameters in each layer tends to infinity, the\nnetwork function is a Gaussian process (GP) and quantitatively predictive\ndescription is possible. Gaussian approximation allows one to formulate\ncriteria for selecting hyperparameters, such as variances of weights and\nbiases, as well as the learning rate. These criteria rely on the notion of\ncriticality defined for deep neural networks. In this work we describe a new\npractical way to diagnose criticality. We introduce \\emph{partial Jacobians} of\na network, defined as derivatives of preactivations in layer $l$ with respect\nto preactivations in layer $l_0\\leq l$. We derive recurrence relations for the\nnorms of partial Jacobians and utilize these relations to analyze criticality\nof deep fully connected neural networks with LayerNorm and\/or residual\nconnections. We derive and implement a simple and cheap numerical test that\nallows one to select optimal initialization for a broad class of deep neural\nnetworks; containing fully connected, convolutional and normalization layers.\nUsing these tools we show quantitatively that proper stacking of the LayerNorm\n(applied to preactivations) and residual connections leads to an architecture\nthat is critical for any initialization. Finally, we apply our methods to\nanalyze ResNet and MLP-Mixer architectures; demonstrating the\neverywhere-critical regime.","terms":["cs.LG","cond-mat.dis-nn","hep-th","stat.ML"]},{"titles":"USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields","summaries":"Neural Radiance Fields (NeRF) has received much attention recently due to its\nimpressive capability to represent 3D scene and synthesize novel view images.\nExisting works usually assume that the input images are captured by a global\nshutter camera. Thus, rolling shutter (RS) images cannot be trivially applied\nto an off-the-shelf NeRF algorithm for novel view synthesis. Rolling shutter\neffect would also affect the accuracy of the camera pose estimation (e.g. via\nCOLMAP), which further prevents the success of NeRF algorithm with RS images.\nIn this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance\nFields (USB-NeRF). USB-NeRF is able to correct rolling shutter distortions and\nrecover accurate camera motion trajectory simultaneously under the framework of\nNeRF, by modeling the physical image formation process of a RS camera.\nExperimental results demonstrate that USB-NeRF achieves better performance\ncompared to prior works, in terms of RS effect removal, novel view image\nsynthesis as well as camera motion estimation. Furthermore, our algorithm can\nalso be used to recover high-fidelity high frame-rate global shutter video from\na sequence of RS images.","terms":["cs.CV","cs.AI"]},{"titles":"Spectral-Bias and Kernel-Task Alignment in Physically Informed Neural Networks","summaries":"Physically informed neural networks (PINNs) are a promising emerging method\nfor solving differential equations. As in many other deep learning approaches,\nthe choice of PINN design and training protocol requires careful craftsmanship.\nHere, we suggest a comprehensive theoretical framework that sheds light on this\nimportant problem. Leveraging an equivalence between infinitely\nover-parameterized neural networks and Gaussian process regression (GPR), we\nderive an integro-differential equation that governs PINN prediction in the\nlarge data-set limit -- the neurally-informed equation. This equation augments\nthe original one by a kernel term reflecting architecture choices and allows\nquantifying implicit bias induced by the network via a spectral decomposition\nof the source term in the original differential equation.","terms":["stat.ML","cond-mat.dis-nn","cs.LG"]},{"titles":"Conditional Generative Models for Simulation of EMG During Naturalistic Movements","summaries":"Numerical models of electromyographic (EMG) signals have provided a huge\ncontribution to our fundamental understanding of human neurophysiology and\nremain a central pillar of motor neuroscience and the development of\nhuman-machine interfaces. However, whilst modern biophysical simulations based\non finite element methods are highly accurate, they are extremely\ncomputationally expensive and thus are generally limited to modelling static\nsystems such as isometrically contracting limbs. As a solution to this problem,\nwe propose a transfer learning approach, in which a conditional generative\nmodel is trained to mimic the output of an advanced numerical model. To this\nend, we present BioMime, a conditional generative neural network trained\nadversarially to generate motor unit activation potential waveforms under a\nwide variety of volume conductor parameters. We demonstrate the ability of such\na model to predictively interpolate between a much smaller number of numerical\nmodel's outputs with a high accuracy. Consequently, the computational load is\ndramatically reduced, which allows the rapid simulation of EMG signals during\ntruly dynamic and naturalistic movements.","terms":["cs.LG","cs.CE","eess.SP","physics.bio-ph"]},{"titles":"High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning","summaries":"A robot self-model is a task-agnostic representation of the robot's physical\nmorphology that can be used for motion planning tasks in absence of classical\ngeometric kinematic models. In particular, when the latter are hard to engineer\nor the robot's kinematics change unexpectedly, human-free self-modeling is a\nnecessary feature of truly autonomous agents. In this work, we leverage neural\nfields to allow a robot to self-model its kinematics as a neural-implicit query\nmodel learned only from 2D images annotated with camera poses and\nconfigurations. This enables significantly greater applicability than existing\napproaches which have been dependent on depth images or geometry knowledge. To\nthis end, alongside a curricular data sampling strategy, we propose a new\nencoder-based neural density field architecture for dynamic object-centric\nscenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF\nrobot test setup, the learned self-model achieves a Chamfer-L2 distance of 2%\nof the robot's workspace dimension. We demonstrate the capabilities of this\nmodel on a motion planning task as an exemplary downstream application.","terms":["cs.CV","cs.LG","cs.RO"]},{"titles":"Plug-and-Play Posterior Sampling under Mismatched Measurement and Prior Models","summaries":"Posterior sampling has been shown to be a powerful Bayesian approach for\nsolving imaging inverse problems. The recent plug-and-play unadjusted Langevin\nalgorithm (PnP-ULA) has emerged as a promising method for Monte Carlo sampling\nand minimum mean squared error (MMSE) estimation by combining physical\nmeasurement models with deep-learning priors specified using image denoisers.\nHowever, the intricate relationship between the sampling distribution of\nPnP-ULA and the mismatched data-fidelity and denoiser has not been\ntheoretically analyzed. We address this gap by proposing a posterior-L2\npseudometric and using it to quantify an explicit error bound for PnP-ULA under\nmismatched posterior distribution. We numerically validate our theory on\nseveral inverse problems such as sampling from Gaussian mixture models and\nimage deblurring. Our results suggest that the sensitivity of the sampling\ndistribution of PnP-ULA to a mismatch in the measurement model and the denoiser\ncan be precisely characterized.","terms":["stat.ML","cs.LG"]},{"titles":"AdvRain: Adversarial Raindrops to Attack Camera-based Smart Vision Systems","summaries":"Vision-based perception modules are increasingly deployed in many\napplications, especially autonomous vehicles and intelligent robots. These\nmodules are being used to acquire information about the surroundings and\nidentify obstacles. Hence, accurate detection and classification are essential\nto reach appropriate decisions and take appropriate and safe actions at all\ntimes. Current studies have demonstrated that \"printed adversarial attacks\",\nknown as physical adversarial attacks, can successfully mislead perception\nmodels such as object detectors and image classifiers. However, most of these\nphysical attacks are based on noticeable and eye-catching patterns for\ngenerated perturbations making them identifiable\/detectable by human eye or in\ntest drives. In this paper, we propose a camera-based inconspicuous adversarial\nattack (\\textbf{AdvRain}) capable of fooling camera-based perception systems\nover all objects of the same class. Unlike mask based fake-weather attacks that\nrequire access to the underlying computing hardware or image memory, our attack\nis based on emulating the effects of a natural weather condition (i.e.,\nRaindrops) that can be printed on a translucent sticker, which is externally\nplaced over the lens of a camera. To accomplish this, we provide an iterative\nprocess based on performing a random search aiming to identify critical\npositions to make sure that the performed transformation is adversarial for a\ntarget classifier. Our transformation is based on blurring predefined parts of\nthe captured image corresponding to the areas covered by the raindrop. We\nachieve a drop in average model accuracy of more than $45\\%$ and $40\\%$ on\nVGG19 for ImageNet and Resnet34 for Caltech-101, respectively, using only $20$\nraindrops.","terms":["cs.CV","cs.CR"]},{"titles":"Probabilistic Forecasting of Day-Ahead Electricity Prices and their Volatility with LSTMs","summaries":"Accurate forecasts of electricity prices are crucial for the management of\nelectric power systems and the development of smart applications. European\nelectricity prices have risen substantially and became highly volatile after\nthe Russian invasion of Ukraine, challenging established forecasting methods.\nHere, we present a Long Short-Term Memory (LSTM) model for the\nGerman-Luxembourg day-ahead electricity prices addressing these challenges. The\nrecurrent structure of the LSTM allows the model to adapt to trends, while the\njoint prediction of both mean and standard deviation enables a probabilistic\nprediction. Using a physics-inspired approach - superstatistics - to derive an\nexplanation for the statistics of prices, we show that the LSTM model\nfaithfully reproduces both prices and their volatility.","terms":["cs.LG","physics.data-an","physics.soc-ph"]},{"titles":"PINNacle: A Comprehensive Benchmark of Physics-Informed Neural Networks for Solving PDEs","summaries":"While significant progress has been made on Physics-Informed Neural Networks\n(PINNs), a comprehensive comparison of these methods across a wide range of\nPartial Differential Equations (PDEs) is still lacking. This study introduces\nPINNacle, a benchmarking tool designed to fill this gap. PINNacle provides a\ndiverse dataset, comprising over 20 distinct PDEs from various domains,\nincluding heat conduction, fluid dynamics, biology, and electromagnetics. These\nPDEs encapsulate key challenges inherent to real-world problems, such as\ncomplex geometry, multi-scale phenomena, nonlinearity, and high dimensionality.\nPINNacle also offers a user-friendly toolbox, incorporating about 10\nstate-of-the-art PINN methods for systematic evaluation and comparison. We have\nconducted extensive experiments with these methods, offering insights into\ntheir strengths and weaknesses. In addition to providing a standardized means\nof assessing performance, PINNacle also offers an in-depth analysis to guide\nfuture research, particularly in areas such as domain decomposition methods and\nloss reweighting for handling multi-scale problems and complex geometry. To the\nbest of our knowledge, it is the largest benchmark with a diverse and\ncomprehensive evaluation that will undoubtedly foster further research in\nPINNs.","terms":["cs.LG","cs.NA","math.NA","physics.comp-ph"]},{"titles":"PostRainBench: A comprehensive benchmark and a new model for precipitation forecasting","summaries":"Accurate precipitation forecasting is a vital challenge of both scientific\nand societal importance. Data-driven approaches have emerged as a widely used\nsolution for addressing this challenge. However, solely relying on data-driven\napproaches has limitations in modeling the underlying physics, making accurate\npredictions difficult. Coupling AI-based post-processing techniques with\ntraditional Numerical Weather Prediction (NWP) methods offers a more effective\nsolution for improving forecasting accuracy. Despite previous post-processing\nefforts, accurately predicting heavy rainfall remains challenging due to the\nimbalanced precipitation data across locations and complex relationships\nbetween multiple meteorological variables. To address these limitations, we\nintroduce the PostRainBench, a comprehensive multi-variable NWP post-processing\nbenchmark consisting of three datasets for NWP post-processing-based\nprecipitation forecasting. We propose CAMT, a simple yet effective Channel\nAttention Enhanced Multi-task Learning framework with a specially designed\nweighted loss function. Its flexible design allows for easy plug-and-play\nintegration with various backbones. Extensive experimental results on the\nproposed benchmark show that our method outperforms state-of-the-art methods by\n6.3%, 4.7%, and 26.8% in rain CSI on the three datasets respectively. Most\nnotably, our model is the first deep learning-based method to outperform\ntraditional Numerical Weather Prediction (NWP) approaches in extreme\nprecipitation conditions. It shows improvements of 15.6%, 17.4%, and 31.8% over\nNWP predictions in heavy rain CSI on respective datasets. These results\nhighlight the potential impact of our model in reducing the severe consequences\nof extreme weather events.","terms":["cs.LG","cs.CV"]},{"titles":"Formal and Practical Elements for the Certification of Machine Learning Systems","summaries":"Over the past decade, machine learning has demonstrated impressive results,\noften surpassing human capabilities in sensing tasks relevant to autonomous\nflight. Unlike traditional aerospace software, the parameters of machine\nlearning models are not hand-coded nor derived from physics but learned from\ndata. They are automatically adjusted during a training phase, and their values\ndo not usually correspond to physical requirements. As a result, requirements\ncannot be directly traced to lines of code, hindering the current bottom-up\naerospace certification paradigm. This paper attempts to address this gap by 1)\ndemystifying the inner workings and processes to build machine learning models,\n2) formally establishing theoretical guarantees given by those processes, and\n3) complementing these formal elements with practical considerations to develop\na complete certification argument for safety-critical machine learning systems.\nBased on a scalable statistical verifier, our proposed framework is\nmodel-agnostic and tool-independent, making it adaptable to many use cases in\nthe industry. We demonstrate results on a widespread application in autonomous\nflight: vision-based landing.","terms":["cs.LG"]},{"titles":"Landscape-Sketch-Step: An AI\/ML-Based Metaheuristic for Surrogate Optimization Problems","summaries":"In this paper, we introduce a new heuristics for global optimization in\nscenarios where extensive evaluations of the cost function are expensive,\ninaccessible, or even prohibitive. The method, which we call\nLandscape-Sketch-and-Step (LSS), combines Machine Learning, Stochastic\nOptimization, and Reinforcement Learning techniques, relying on historical\ninformation from previously sampled points to make judicious choices of\nparameter values where the cost function should be evaluated at. Unlike\noptimization by Replica Exchange Monte Carlo methods, the number of evaluations\nof the cost function required in this approach is comparable to that used by\nSimulated Annealing, quality that is especially important in contexts like\nhigh-throughput computing or high-performance computing tasks, where\nevaluations are either computationally expensive or take a long time to be\nperformed. The method also differs from standard Surrogate Optimization\ntechniques, for it does not construct a surrogate model that aims at\napproximating or reconstructing the objective function. We illustrate our\nmethod by applying it to low dimensional optimization problems (dimensions 1,\n2, 4, and 8) that mimick known difficulties of minimization on rugged energy\nlandscapes often seen in Condensed Matter Physics, where cost functions are\nrugged and plagued with local minima. When compared to classical Simulated\nAnnealing, the LSS shows an effective acceleration of the optimization process.","terms":["cs.LG","cond-mat.mtrl-sci","cs.AI","math.OC","math.PR","68T20, 90C56"]},{"titles":"Crossed-IoT device portability of Electromagnetic Side Channel Analysis: Challenges and Dataset","summaries":"IoT (Internet of Things) refers to the network of interconnected physical\ndevices, vehicles, home appliances, and other items embedded with sensors,\nsoftware, and connectivity, enabling them to collect and exchange data. IoT\nForensics is collecting and analyzing digital evidence from IoT devices to\ninvestigate cybercrimes, security breaches, and other malicious activities that\nmay have taken place on these connected devices. In particular, EM-SCA has\nbecome an essential tool for IoT forensics due to its ability to reveal\nconfidential information about the internal workings of IoT devices without\ninterfering these devices or wiretapping their networks. However, the accuracy\nand reliability of EM-SCA results can be limited by device variability,\nenvironmental factors, and data collection and processing methods. Besides,\nthere is very few research on these limitations that affects significantly the\naccuracy of EM-SCA approaches for the crossed-IoT device portability as well as\nlimited research on the possible solutions to address such challenge.\nTherefore, this empirical study examines the impact of device variability on\nthe accuracy and reliability of EM-SCA approaches, in particular\nmachine-learning (ML) based approaches for EM-SCA. We firstly presents the\nbackground, basic concepts and techniques used to evaluate the limitations of\ncurrent EM-SCA approaches and datasets. Our study then addresses one of the\nmost important limitation, which is caused by the multi-core architecture of\nthe processors (SoC). We present an approach to collect the EM-SCA datasets and\ndemonstrate the feasibility of using transfer learning to obtain more\nmeaningful and reliable results from EM-SCA in IoT forensics of crossed-IoT\ndevices. Our study moreover contributes a new dataset for using deep learning\nmodels in analysing Electromagnetic Side-Channel data with regards to the\ncross-device portability matter.","terms":["cs.LG","cs.CR"]},{"titles":"Numerical Weather Forecasting using Convolutional-LSTM with Attention and Context Matcher Mechanisms","summaries":"Numerical weather forecasting using high-resolution physical models often\nrequires extensive computational resources on supercomputers, which diminishes\ntheir wide usage in most real-life applications. As a remedy, applying deep\nlearning methods has revealed innovative solutions within this field. To this\nend, we introduce a novel deep learning architecture for forecasting\nhigh-resolution spatio-temporal weather data. Our approach extends the\nconventional encoder-decoder structure by integrating Convolutional Long-short\nTerm Memory and Convolutional Neural Networks. In addition, we incorporate\nattention and context matcher mechanisms into the model architecture. Our\nWeather Model achieves significant performance improvements compared to\nbaseline deep learning models, including ConvLSTM, TrajGRU, and U-Net. Our\nexperimental evaluation involves high-scale, real-world benchmark numerical\nweather datasets, namely the ERA5 hourly dataset on pressure levels and\nWeatherBench. Our results demonstrate substantial improvements in identifying\nspatial and temporal correlations with attention matrices focusing on distinct\nparts of the input series to model atmospheric circulations. We also compare\nour model with high-resolution physical models using the benchmark metrics and\nshow that our Weather Model is accurate and easy to interpret.","terms":["cs.LG","cs.AI","cs.CV"]},{"titles":"Physics-Informed Neural Networks for Accelerating Power System State Estimation","summaries":"State estimation is the cornerstone of the power system control center since\nit provides the operating condition of the system in consecutive time\nintervals. This work investigates the application of physics-informed neural\nnetworks (PINNs) for accelerating power systems state estimation in monitoring\nthe operation of power systems. Traditional state estimation techniques often\nrely on iterative algorithms that can be computationally intensive,\nparticularly for large-scale power systems. In this paper, a novel approach\nthat leverages the inherent physical knowledge of power systems through the\nintegration of PINNs is proposed. By incorporating physical laws as prior\nknowledge, the proposed method significantly reduces the computational\ncomplexity associated with state estimation while maintaining high accuracy.\nThe proposed method achieves up to 11% increase in accuracy, 75% reduction in\nstandard deviation of results, and 30% faster convergence, as demonstrated by\ncomprehensive experiments on the IEEE 14-bus system.","terms":["cs.LG","cs.SY","eess.SY"]},{"titles":"Learning characteristic parameters and dynamics of centrifugal pumps under multi-phase flow using physics-informed neural networks","summaries":"Electrical submersible pumps (ESP) are the second most used artificial\nlifting equipment in the oil and gas industry due to their high flow rates and\nboost pressures. They often have to handle multiphase flows, which usually\ncontain a mixture of hydrocarbons, water, and\/or sediments. Given these\ncircumstances, emulsions are commonly formed. It is a liquid-liquid flow\ncomposed of two immiscible fluids whose effective viscosity and density differ\nfrom the single phase separately. In this context, accurate modeling of ESP\nsystems is crucial for optimizing oil production and implementing control\nstrategies. However, real-time and direct measurement of fluid and system\ncharacteristics is often impractical due to time constraints and economy.\nHence, indirect methods are generally considered to estimate the system\nparameters. In this paper, we formulate a machine learning model based on\nPhysics-Informed Neural Networks (PINNs) to estimate crucial system parameters.\nIn order to study the efficacy of the proposed PINN model, we conduct\ncomputational studies using not only simulated but also experimental data for\ndifferent water-oil ratios. We evaluate the state variable's dynamics and\nunknown parameters for various combinations when only intake and discharge\npressure measurements are available. We also study structural and practical\nidentifiability analyses based on commonly available pressure measurements. The\nPINN model could reduce the requirement of expensive field laboratory tests\nused to estimate fluid properties.","terms":["cs.LG"]},{"titles":"Multiple Physics Pretraining for Physical Surrogate Models","summaries":"We introduce multiple physics pretraining (MPP), an autoregressive\ntask-agnostic pretraining approach for physical surrogate modeling. MPP\ninvolves training large surrogate models to predict the dynamics of multiple\nheterogeneous physical systems simultaneously by learning features that are\nbroadly useful across diverse physical tasks. In order to learn effectively in\nthis setting, we introduce a shared embedding and normalization strategy that\nprojects the fields of multiple systems into a single shared embedding space.\nWe validate the efficacy of our approach on both pretraining and downstream\ntasks over a broad fluid mechanics-oriented benchmark. We show that a single\nMPP-pretrained transformer is able to match or outperform task-specific\nbaselines on all pretraining sub-tasks without the need for finetuning. For\ndownstream tasks, we demonstrate that finetuning MPP-trained models results in\nmore accurate predictions across multiple time-steps on new physics compared to\ntraining from scratch or finetuning pretrained video foundation models. We\nopen-source our code and model weights trained at multiple scales for\nreproducibility and community experimentation.","terms":["cs.LG","cs.AI","stat.ML"]},{"titles":"ELUQuant: Event-Level Uncertainty Quantification in Deep Inelastic Scattering","summaries":"We introduce a physics-informed Bayesian Neural Network (BNN) with flow\napproximated posteriors using multiplicative normalizing flows (MNF) for\ndetailed uncertainty quantification (UQ) at the physics event-level. Our method\nis capable of identifying both heteroskedastic aleatoric and epistemic\nuncertainties, providing granular physical insights. Applied to Deep Inelastic\nScattering (DIS) events, our model effectively extracts the kinematic variables\n$x$, $Q^2$, and $y$, matching the performance of recent deep learning\nregression techniques but with the critical enhancement of event-level UQ. This\ndetailed description of the underlying uncertainty proves invaluable for\ndecision-making, especially in tasks like event filtering. It also allows for\nthe reduction of true inaccuracies without directly accessing the ground truth.\nA thorough DIS simulation using the H1 detector at HERA indicates possible\napplications for the future EIC. Additionally, this paves the way for related\ntasks such as data quality monitoring and anomaly detection. Remarkably, our\napproach effectively processes large samples at high rates.","terms":["cs.LG","hep-ex","nucl-ex","physics.data-an","stat.ML"]},{"titles":"Deep Stochastic Mechanics","summaries":"This paper introduces a novel deep-learning-based approach for numerical\nsimulation of a time-evolving Schr\\\"odinger equation inspired by stochastic\nmechanics and generative diffusion models. Unlike existing approaches, which\nexhibit computational complexity that scales exponentially in the problem\ndimension, our method allows us to adapt to the latent low-dimensional\nstructure of the wave function by sampling from the Markovian diffusion.\nDepending on the latent dimension, our method may have far lower computational\ncomplexity in higher dimensions. Moreover, we propose novel equations for\nstochastic quantum mechanics, resulting in linear computational complexity with\nrespect to the number of dimensions. Numerical simulations verify our\ntheoretical findings and show a significant advantage of our method compared to\nother deep-learning-based approaches used for quantum mechanics.","terms":["cs.LG","quant-ph","stat.ML"]},{"titles":"MUNCH: Modelling Unique 'N Controllable Heads","summaries":"The automated generation of 3D human heads has been an intriguing and\nchallenging task for computer vision researchers. Prevailing methods synthesize\nrealistic avatars but with limited control over the diversity and quality of\nrendered outputs and suffer from limited correlation between shape and texture\nof the character. We propose a method that offers quality, diversity, control,\nand realism along with explainable network design, all desirable features to\ngame-design artists in the domain. First, our proposed Geometry Generator\nidentifies disentangled latent directions and generate novel and diverse\nsamples. A Render Map Generator then learns to synthesize multiply high-fidelty\nphysically-based render maps including Albedo, Glossiness, Specular, and\nNormals. For artists preferring fine-grained control over the output, we\nintroduce a novel Color Transformer Model that allows semantic color control\nover generated maps. We also introduce quantifiable metrics called Uniqueness\nand Novelty and a combined metric to test the overall performance of our model.\nDemo for both shapes and textures can be found:\nhttps:\/\/munch-seven.vercel.app\/. We will release our model along with the\nsynthetic dataset.","terms":["cs.CV","cs.AI","cs.GR","cs.LG"]},{"titles":"Exact and soft boundary conditions in Physics-Informed Neural Networks for the Variable Coefficient Poisson equation","summaries":"Boundary conditions (BCs) are a key component in every Physics-Informed\nNeural Network (PINN). By defining the solution to partial differential\nequations (PDEs) along domain boundaries, BCs constrain the underlying boundary\nvalue problem (BVP) that a PINN tries to approximate. Without them, unique PDE\nsolutions may not exist and finding approximations with PINNs would be a\nchallenging, if not impossible task. This study examines how soft loss-based\nand exact distance function-based BC imposition approaches differ when applied\nin PINNs. The well known variable coefficient Poisson equation serves as the\ntarget PDE for all PINN models trained in this work. Besides comparing BC\nimposition approaches, the goal of this work is to also provide resources on\nhow to implement these PINNs in practice. To this end, Keras models with\nTensorflow backend as well as a Python notebook with code examples and\nstep-by-step explanations on how to build soft\/exact BC PINNs are published\nalongside this review.","terms":["cs.LG"]},{"titles":"QuATON: Quantization Aware Training of Optical Neurons","summaries":"Optical neural architectures (ONAs) use coding elements with optimized\nphysical parameters to perform intelligent measurements. However, fabricating\nONAs while maintaining design performances is challenging. Limitations in\nfabrication techniques often limit the realizable precision of the trained\nparameters. Physical constraints may also limit the range of values the\nphysical parameters can hold. Thus, ONAs should be trained within the\nimplementable constraints. However, such physics-based constraints reduce the\ntraining objective to a constrained optimization problem, making it harder to\noptimize with existing gradient-based methods. To alleviate these critical\nissues that degrade performance from simulation to realization we propose a\nphysics-informed quantization-aware training framework. Our approach accounts\nfor the physical constraints during the training process, leading to robust\ndesigns. We evaluate our approach on an ONA proposed in the literature, named a\ndiffractive deep neural network (D2NN), for all-optical phase imaging and for\nclassification of phase objects. With extensive experiments on different\nquantization levels and datasets, we show that our approach leads to ONA\ndesigns that are robust to quantization noise.","terms":["cs.LG","eess.IV","physics.optics"]},{"titles":"Robust Model-Based Optimization for Challenging Fitness Landscapes","summaries":"Protein design, a grand challenge of the day, involves optimization on a\nfitness landscape, and leading methods adopt a model-based approach where a\nmodel is trained on a training set (protein sequences and fitness) and proposes\ncandidates to explore next. These methods are challenged by sparsity of\nhigh-fitness samples in the training set, a problem that has been in the\nliterature. A less recognized but equally important problem stems from the\ndistribution of training samples in the design space: leading methods are not\ndesigned for scenarios where the desired optimum is in a region that is not\nonly poorly represented in training data, but also relatively far from the\nhighly represented low-fitness regions. We show that this problem of\n\"separation\" in the design space is a significant bottleneck in existing\nmodel-based optimization tools and propose a new approach that uses a novel VAE\nas its search model to overcome the problem. We demonstrate its advantage over\nprior methods in robustly finding improved samples, regardless of the imbalance\nand separation between low- and high-fitness training samples. Our\ncomprehensive benchmark on real and semi-synthetic protein datasets as well as\nsolution design for physics-informed neural networks, showcases the generality\nof our approach in discrete and continuous design spaces. Our implementation is\navailable at https:\/\/github.com\/sabagh1994\/PGVAE.","terms":["cs.LG","cs.AI"]},{"titles":"Stochastic force inference via density estimation","summaries":"Inferring dynamical models from low-resolution temporal data continues to be\na significant challenge in biophysics, especially within transcriptomics, where\nseparating molecular programs from noise remains an important open problem. We\nexplore a common scenario in which we have access to an adequate amount of\ncross-sectional samples at a few time-points, and assume that our samples are\ngenerated from a latent diffusion process. We propose an approach that relies\non the probability flow associated with an underlying diffusion process to\ninfer an autonomous, nonlinear force field interpolating between the\ndistributions. Given a prior on the noise model, we employ score-matching to\ndifferentiate the force field from the intrinsic noise. Using relevant\nbiophysical examples, we demonstrate that our approach can extract\nnon-conservative forces from non-stationary data, that it learns equilibrium\ndynamics when applied to steady-state data, and that it can do so with both\nadditive and multiplicative noise models.","terms":["cs.LG","physics.bio-ph","q-bio.QM"]},{"titles":"A Neural Scaling Law from Lottery Ticket Ensembling","summaries":"Neural scaling laws (NSL) refer to the phenomenon where model performance\nimproves with scale. Sharma & Kaplan analyzed NSL using approximation theory\nand predict that MSE losses decay as $N^{-\\alpha}$, $\\alpha=4\/d$, where $N$ is\nthe number of model parameters, and $d$ is the intrinsic input dimension.\nAlthough their theory works well for some cases (e.g., ReLU networks), we\nsurprisingly find that a simple 1D problem $y=x^2$ manifests a different\nscaling law ($\\alpha=1$) from their predictions ($\\alpha=4$). We opened the\nneural networks and found that the new scaling law originates from lottery\nticket ensembling: a wider network on average has more \"lottery tickets\", which\nare ensembled to reduce the variance of outputs. We support the ensembling\nmechanism by mechanistically interpreting single neural networks, as well as\nstudying them statistically. We attribute the $N^{-1}$ scaling law to the\n\"central limit theorem\" of lottery tickets. Finally, we discuss its potential\nimplications for large language models and statistical physics-type theories of\nlearning.","terms":["cs.LG","cs.AI","physics.data-an","stat.ML"]},{"titles":"Unifying supervised learning and VAEs -- coverage, systematics and goodness-of-fit in normalizing-flow based neural network models for astro-particle reconstructions","summaries":"Neural-network based predictions of event properties in astro-particle\nphysics are getting more and more common. However, in many cases the result is\njust utilized as a point prediction. Statistical uncertainties and coverage\n(1), systematic uncertainties (2) or a goodness-of-fit measure (3) are often\nnot calculated. Here we describe a certain choice of training and network\narchitecture that allows to incorporate all these properties into a single\nnetwork model. We show that a KL-divergence objective of the joint distribution\nof data and labels allows to unify supervised learning and variational\nautoencoders (VAEs) under one umbrella of stochastic variational inference. The\nunification motivates an extended supervised learning scheme which allows to\ncalculate a goodness-of-fit p-value for the neural network model. Conditional\nnormalizing flows amortized with a neural network are crucial in this\nconstruction. We discuss how they allow to rigorously define coverage for\nposteriors defined jointly on a product space, e.g. $\\mathbb{R}^n \\times\n\\mathcal{S}^m$, which encompasses posteriors over directions. Finally,\nsystematic uncertainties are naturally included in the variational viewpoint.\nThe proposed extended supervised training with amortized normalizing flows\nincorporates (1) coverage calculation, (2) systematics and (3) a\ngoodness-of-fit measure in a single machine-learning model. There are no\nconstraints on the shape of the involved distributions (e.g. Gaussianity) for\nthese properties to hold, in fact it works with complex multi-modal\ndistributions defined on product spaces like $\\mathbb{R}^n \\times\n\\mathcal{S}^m$. We see great potential for exploiting this per-event\ninformation in event selections or for fast astronomical alerts which require\nuncertainty guarantees.","terms":["cs.LG","astro-ph.HE","astro-ph.IM","hep-ex","stat.ML"]},{"titles":"Uncertainty Quantification in Inverse Models in Hydrology","summaries":"In hydrology, modeling streamflow remains a challenging task due to the\nlimited availability of basin characteristics information such as soil geology\nand geomorphology. These characteristics may be noisy due to measurement errors\nor may be missing altogether. To overcome this challenge, we propose a\nknowledge-guided, probabilistic inverse modeling method for recovering physical\ncharacteristics from streamflow and weather data, which are more readily\navailable. We compare our framework with state-of-the-art inverse models for\nestimating river basin characteristics. We also show that these estimates offer\nimprovement in streamflow modeling as opposed to using the original basin\ncharacteristic values. Our inverse model offers 3\\% improvement in R$^2$ for\nthe inverse model (basin characteristic estimation) and 6\\% for the forward\nmodel (streamflow prediction). Our framework also offers improved\nexplainability since it can quantify uncertainty in both the inverse and the\nforward model. Uncertainty quantification plays a pivotal role in improving the\nexplainability of machine learning models by providing additional insights into\nthe reliability and limitations of model predictions. In our analysis, we\nassess the quality of the uncertainty estimates. Compared to baseline\nuncertainty quantification methods, our framework offers 10\\% improvement in\nthe dispersion of epistemic uncertainty and 13\\% improvement in coverage rate.\nThis information can help stakeholders understand the level of uncertainty\nassociated with the predictions and provide a more comprehensive view of the\npotential outcomes.","terms":["cs.LG","cs.AI","stat.AP"]},{"titles":"Parallelizing non-linear sequential models over the sequence length","summaries":"Sequential models, such as Recurrent Neural Networks and Neural Ordinary\nDifferential Equations, have long suffered from slow training due to their\ninherent sequential nature. For many years this bottleneck has persisted, as\nmany thought sequential models could not be parallelized. We challenge this\nlong-held belief with our parallel algorithm that accelerates GPU evaluation of\nsequential models by up to 3 orders of magnitude faster without compromising\noutput accuracy. The algorithm does not need any special structure in the\nsequential models' architecture, making it applicable to a wide range of\narchitectures. Using our method, training sequential models can be more than 10\ntimes faster than the common sequential method without any meaningful\ndifference in the training results. Leveraging this accelerated training, we\ndiscovered the efficacy of the Gated Recurrent Unit in a long time series\nclassification problem with 17k time samples. By overcoming the training\nbottleneck, our work serves as the first step to unlock the potential of\nnon-linear sequential models for long sequence problems.","terms":["cs.LG","cs.DC","physics.comp-ph"]},{"titles":"Unsupervised Complex Semi-Binary Matrix Factorization for Activation Sequence Recovery of Quasi-Stationary Sources","summaries":"Advocating for a sustainable, resilient and human-centric industry, the three\npillars of Industry 5.0 call for an increased understanding of industrial\nprocesses and manufacturing systems, as well as their energy sustainability.\nOne of the most fundamental elements of comprehension is knowing when the\nsystems are operated, as this is key to locating energy intensive subsystems\nand operations. Such knowledge is often lacking in practice. Activation\nstatuses can be recovered from sensor data though. Some non-intrusive sensors\n(accelerometers, current sensors, etc.) acquire mixed signals containing\ninformation about multiple actuators at once. Despite their low cost as regards\nthe fleet of systems they monitor, additional signal processing is required to\nextract the individual activation sequences. To that end, sparse regression\ntechniques can extract leading dynamics in sequential data. Notorious\ndictionary learning algorithms have proven effective in this regard. This paper\nconsiders different industrial settings in which the identification of binary\nsubsystem activation sequences is sought. In this context, it is assumed that\neach sensor measures an extensive physical property, source signals are\nperiodic, quasi-stationary and independent, albeit these signals may be\ncorrelated and their noise distribution is arbitrary. Existing methods either\nrestrict these assumptions, e.g., by imposing orthogonality or noise\ncharacteristics, or lift them using additional assumptions, typically using\nnonlinear transforms.","terms":["cs.LG","eess.SP"]},{"titles":"Exploring Physical Latent Spaces for High-Resolution Flow Restoration","summaries":"We explore training deep neural network models in conjunction with physics\nsimulations via partial differential equations (PDEs), using the simulated\ndegrees of freedom as latent space for a neural network. In contrast to\nprevious work, this paper treats the degrees of freedom of the simulated space\npurely as tools to be used by the neural network. We demonstrate this concept\nfor learning reduced representations, as it is extremely challenging to\nfaithfully preserve correct solutions over long time-spans with traditional\nreduced representations, particularly for solutions with large amounts of small\nscale features. This work focuses on the use of such physical, reduced latent\nspace for the restoration of fine simulations, by training models that can\nmodify the content of the reduced physical states as much as needed to best\nsatisfy the learning objective. This autonomy allows the neural networks to\ndiscover alternate dynamics that significantly improve the performance in the\ngiven tasks. We demonstrate this concept for various fluid flows ranging from\ndifferent turbulence scenarios to rising smoke plumes.","terms":["cs.LG"]},{"titles":"A Framework for Interpretability in Machine Learning for Medical Imaging","summaries":"Interpretability for machine learning models in medical imaging (MLMI) is an\nimportant direction of research. However, there is a general sense of murkiness\nin what interpretability means. Why does the need for interpretability in MLMI\narise? What goals does one actually seek to address when interpretability is\nneeded? To answer these questions, we identify a need to formalize the goals\nand elements of interpretability in MLMI. By reasoning about real-world tasks\nand goals common in both medical image analysis and its intersection with\nmachine learning, we identify four core elements of interpretability:\nlocalization, visual recognizability, physical attribution, and transparency.\nOverall, this paper formalizes interpretability needs in the context of medical\nimaging, and our applied perspective clarifies concrete MLMI-specific goals and\nconsiderations in order to guide method design and improve real-world usage.\nOur goal is to provide practical and didactic information for model designers\nand practitioners, inspire developers of models in the medical imaging field to\nreason more deeply about what interpretability is achieving, and suggest future\ndirections of interpretability research.","terms":["cs.LG"]},{"titles":"CoDBench: A Critical Evaluation of Data-driven Models for Continuous Dynamical Systems","summaries":"Continuous dynamical systems, characterized by differential equations, are\nubiquitously used to model several important problems: plasma dynamics, flow\nthrough porous media, weather forecasting, and epidemic dynamics. Recently, a\nwide range of data-driven models has been used successfully to model these\nsystems. However, in contrast to established fields like computer vision,\nlimited studies are available analyzing the strengths and potential\napplications of different classes of these models that could steer\ndecision-making in scientific machine learning. Here, we introduce CodBench, an\nexhaustive benchmarking suite comprising 11 state-of-the-art data-driven models\nfor solving differential equations. Specifically, we comprehensively evaluate 4\ndistinct categories of models, viz., feed forward neural networks, deep\noperator regression models, frequency-based neural operators, and transformer\narchitectures against 8 widely applicable benchmark datasets encompassing\nchallenges from fluid and solid mechanics. We conduct extensive experiments,\nassessing the operators' capabilities in learning, zero-shot super-resolution,\ndata efficiency, robustness to noise, and computational efficiency.\nInterestingly, our findings highlight that current operators struggle with the\nnewer mechanics datasets, motivating the need for more robust neural operators.\nAll the datasets and codes will be shared in an easy-to-use fashion for the\nscientific community. We hope this resource will be an impetus for accelerated\nprogress and exploration in modeling dynamical systems.","terms":["cs.LG","cs.AI","physics.comp-ph"]},{"titles":"Prescribed Fire Modeling using Knowledge-Guided Machine Learning for Land Management","summaries":"In recent years, the increasing threat of devastating wildfires has\nunderscored the need for effective prescribed fire management. Process-based\ncomputer simulations have traditionally been employed to plan prescribed fires\nfor wildfire prevention. However, even simplified process models like QUIC-Fire\nare too compute-intensive to be used for real-time decision-making, especially\nwhen weather conditions change rapidly. Traditional ML methods used for fire\nmodeling offer computational speedup but struggle with physically inconsistent\npredictions, biased predictions due to class imbalance, biased estimates for\nfire spread metrics (e.g., burned area, rate of spread), and generalizability\nin out-of-distribution wind conditions. This paper introduces a novel machine\nlearning (ML) framework that enables rapid emulation of prescribed fires while\naddressing these concerns. By incorporating domain knowledge, the proposed\nmethod helps reduce physical inconsistencies in fuel density estimates in\ndata-scarce scenarios. To overcome the majority class bias in predictions, we\nleverage pre-existing source domain data to augment training data and learn the\nspread of fire more effectively. Finally, we overcome the problem of biased\nestimation of fire spread metrics by incorporating a hierarchical modeling\nstructure to capture the interdependence in fuel density and burned area.\nNotably, improvement in fire metric (e.g., burned area) estimates offered by\nour framework makes it useful for fire managers, who often rely on these fire\nmetric estimates to make decisions about prescribed burn management.\nFurthermore, our framework exhibits better generalization capabilities than the\nother ML-based fire modeling methods across diverse wind conditions and\nignition patterns.","terms":["cs.LG","cs.AI","stat.AP"]},{"titles":"Causality-informed Rapid Post-hurricane Building Damage Detection in Large Scale from InSAR Imagery","summaries":"Timely and accurate assessment of hurricane-induced building damage is\ncrucial for effective post-hurricane response and recovery efforts. Recently,\nremote sensing technologies provide large-scale optical or Interferometric\nSynthetic Aperture Radar (InSAR) imagery data immediately after a disastrous\nevent, which can be readily used to conduct rapid building damage assessment.\nCompared to optical satellite imageries, the Synthetic Aperture Radar can\npenetrate cloud cover and provide more complete spatial coverage of damaged\nzones in various weather conditions. However, these InSAR imageries often\ncontain highly noisy and mixed signals induced by co-occurring or co-located\nbuilding damage, flood, flood\/wind-induced vegetation changes, as well as\nanthropogenic activities, making it challenging to extract accurate building\ndamage information. In this paper, we introduced an approach for rapid\npost-hurricane building damage detection from InSAR imagery. This approach\nencoded complex causal dependencies among wind, flood, building damage, and\nInSAR imagery using a holistic causal Bayesian network. Based on the causal\nBayesian network, we further jointly inferred the large-scale unobserved\nbuilding damage by fusing the information from InSAR imagery with prior\nphysical models of flood and wind, without the need for ground truth labels.\nFurthermore, we validated our estimation results in a real-world devastating\nhurricane -- the 2022 Hurricane Ian. We gathered and annotated building damage\nground truth data in Lee County, Florida, and compared the introduced method's\nestimation results with the ground truth and benchmarked it against\nstate-of-the-art models to assess the effectiveness of our proposed method.\nResults show that our method achieves rapid and accurate detection of building\ndamage, with significantly reduced processing time compared to traditional\nmanual inspection methods.","terms":["cs.LG","cs.IR","eess.IV"]},{"titles":"The Benefit of Noise-Injection for Dynamic Gray-Box Model Creation","summaries":"Gray-box models offer significant benefit over black-box approaches for\nequipment emulator development for equipment since their integration of physics\nprovides more confidence in the model outside of the training domain. However,\nchallenges such as model nonlinearity, unmodeled dynamics, and local minima\nintroduce uncertainties into grey-box creation that contemporary approaches\nhave failed to overcome, leading to their under-performance compared with\nblack-box models. This paper seeks to address these uncertainties by injecting\nnoise into the training dataset. This noise injection enriches the dataset and\nprovides a measure of robustness against such uncertainties. A dynamic model\nfor a water-to-water heat exchanger has been used as a demonstration case for\nthis approach and tested using a pair of real devices with live data streaming.\nCompared to the unprocessed signal data, the application of noise injection\nresulted in a significant reduction in modeling error (root mean square error),\ndecreasing from 0.68 to 0.27{\\deg}C. This improvement amounts to a 60%\nenhancement when assessed on the training set, and improvements of 50% and 45%\nwhen validated against the test and validation sets, respectively.","terms":["cs.LG","cs.SY","eess.SY","93-10","I.6.3"]},{"titles":"A Comparison of Mesh-Free Differentiable Programming and Data-Driven Strategies for Optimal Control under PDE Constraints","summaries":"The field of Optimal Control under Partial Differential Equations (PDE)\nconstraints is rapidly changing under the influence of Deep Learning and the\naccompanying automatic differentiation libraries. Novel techniques like\nPhysics-Informed Neural Networks (PINNs) and Differentiable Programming (DP)\nare to be contrasted with established numerical schemes like Direct-Adjoint\nLooping (DAL). We present a comprehensive comparison of DAL, PINN, and DP using\na general-purpose mesh-free differentiable PDE solver based on Radial Basis\nFunctions. Under Laplace and Navier-Stokes equations, we found DP to be\nextremely effective as it produces the most accurate gradients; thriving even\nwhen DAL fails and PINNs struggle. Additionally, we provide a detailed\nbenchmark highlighting the limited conditions under which any of those methods\ncan be efficiently used. Our work provides a guide to Optimal Control\npractitioners and connects them further to the Deep Learning community.","terms":["cs.LG","cs.AI","math.OC"]},{"titles":"Making LLaMA SEE and Draw with SEED Tokenizer","summaries":"The great success of Large Language Models (LLMs) has expanded the potential\nof multimodality, contributing to the gradual evolution of General Artificial\nIntelligence (AGI). A true AGI agent should not only possess the capability to\nperform predefined multi-tasks but also exhibit emergent abilities in an\nopen-world context. However, despite the considerable advancements made by\nrecent multimodal LLMs, they still fall short in effectively unifying\ncomprehension and generation tasks, let alone open-world emergent abilities. We\ncontend that the key to overcoming the present impasse lies in enabling text\nand images to be represented and processed interchangeably within a unified\nautoregressive Transformer. To this end, we introduce SEED, an elaborate image\ntokenizer that empowers LLMs with the ability to SEE and Draw at the same time.\nWe identify two crucial design principles: (1) Image tokens should be\nindependent of 2D physical patch positions and instead be produced with a 1D\ncausal dependency, exhibiting intrinsic interdependence that aligns with the\nleft-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens\nshould capture high-level semantics consistent with the degree of semantic\nabstraction in words, and be optimized for both discriminativeness and\nreconstruction during the tokenizer training phase. With SEED tokens, LLM is\nable to perform scalable multimodal autoregression under its original training\nrecipe, i.e., next-word prediction. SEED-LLaMA is therefore produced by\nlarge-scale pretraining and instruction tuning on the interleaved textual and\nvisual data, demonstrating impressive performance on a broad range of\nmultimodal comprehension and generation tasks. More importantly, SEED-LLaMA has\nexhibited compositional emergent abilities such as multi-turn in-context\nmultimodal generation, acting like your AI assistant.","terms":["cs.CV"]},{"titles":"Intelligent Knee Sleeves: A Real-time Multimodal Dataset for 3D Lower Body Motion Estimation Using Smart Textile","summaries":"The kinematics of human movements and locomotion are closely linked to the\nactivation and contractions of muscles. To investigate this, we present a\nmultimodal dataset with benchmarks collected using a novel pair of Intelligent\nKnee Sleeves (Texavie MarsWear Knee Sleeves) for human pose estimation. Our\nsystem utilizes synchronized datasets that comprise time-series data from the\nKnee Sleeves and the corresponding ground truth labels from the visualized\nmotion capture camera system. We employ these to generate 3D human models\nsolely based on the wearable data of individuals performing different\nactivities. We demonstrate the effectiveness of this camera-free system and\nmachine learning algorithms in the assessment of various movements and\nexercises, including extension to unseen exercises and individuals. The results\nshow an average error of 7.21 degrees across all eight lower body joints when\ncompared to the ground truth, indicating the effectiveness and reliability of\nthe Knee Sleeve system for the prediction of different lower body joints beyond\nthe knees. The results enable human pose estimation in a seamless manner\nwithout being limited by visual occlusion or the field of view of cameras. Our\nresults show the potential of multimodal wearable sensing in a variety of\napplications from home fitness to sports, healthcare, and physical\nrehabilitation focusing on pose and movement estimation.","terms":["cs.CV","cs.AI"]},{"titles":"OceanNet: A principled neural operator-based digital twin for regional oceans","summaries":"While data-driven approaches demonstrate great potential in atmospheric\nmodeling and weather forecasting, ocean modeling poses distinct challenges due\nto complex bathymetry, land, vertical structure, and flow non-linearity. This\nstudy introduces OceanNet, a principled neural operator-based digital twin for\nocean circulation. OceanNet uses a Fourier neural operator and\npredictor-evaluate-corrector integration scheme to mitigate autoregressive\nerror growth and enhance stability over extended time scales. A spectral\nregularizer counteracts spectral bias at smaller scales. OceanNet is applied to\nthe northwest Atlantic Ocean western boundary current (the Gulf Stream),\nfocusing on the task of seasonal prediction for Loop Current eddies and the\nGulf Stream meander. Trained using historical sea surface height (SSH) data,\nOceanNet demonstrates competitive forecast skill by outperforming SSH\npredictions by an uncoupled, state-of-the-art dynamical ocean model forecast,\nreducing computation by 500,000 times. These accomplishments demonstrate the\npotential of physics-inspired deep neural operators as cost-effective\nalternatives to high-resolution numerical ocean models.","terms":["cs.LG","cs.AI","nlin.CD","physics.ao-ph"]},{"titles":"Physics-Informed Graph Neural Network for Dynamic Reconfiguration of Power Systems","summaries":"To maintain a reliable grid we need fast decision-making algorithms for\ncomplex problems like Dynamic Reconfiguration (DyR). DyR optimizes distribution\ngrid switch settings in real-time to minimize grid losses and dispatches\nresources to supply loads with available generation. DyR is a mixed-integer\nproblem and can be computationally intractable to solve for large grids and at\nfast timescales. We propose GraPhyR, a Physics-Informed Graph Neural Network\n(GNNs) framework tailored for DyR. We incorporate essential operational and\nconnectivity constraints directly within the GNN framework and train it\nend-to-end. Our results show that GraPhyR is able to learn to optimize the DyR\ntask.","terms":["cs.LG","cs.SY","eess.SY","math.OC","stat.ML"]},{"titles":"A quantum moving target segmentation algorithm for grayscale video","summaries":"The moving target segmentation (MTS) aims to segment out moving targets in\nthe video, however, the classical algorithm faces the huge challenge of\nreal-time processing in the current video era. Some scholars have successfully\ndemonstrated the quantum advantages in some video processing tasks, but not\nconcerning moving target segmentation. In this paper, a quantum moving target\nsegmentation algorithm for grayscale video is proposed, which can use quantum\nmechanism to simultaneously calculate the difference of all pixels in all\nadjacent frames and then quickly segment out the moving target. In addition, a\nfeasible quantum comparator is designed to distinguish the grayscale values\nwith the threshold. Then several quantum circuit units, including three-frame\ndifference, binarization and AND operation, are designed in detail, and then\nare combined together to construct the complete quantum circuits for segmenting\nthe moving target. For a quantum video with $2^m$ frames (every frame is a\n$2^n\\times 2^n$ image with $q$ grayscale levels), the complexity of our\nalgorithm can be reduced to O$(n^2 + q)$. Compared with the classic\ncounterpart, it is an exponential speedup, while its complexity is also\nsuperior to the existing quantum algorithms. Finally, the experiment is\nconducted on IBM Q to show the feasibility of our algorithm in the noisy\nintermediate-scale quantum (NISQ) era.","terms":["cs.CV","cs.ET","quant-ph"]},{"titles":"A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against Adversarial Attacks","summaries":"Benefiting from the rapid development of deep learning, 2D and 3D computer\nvision applications are deployed in many safe-critical systems, such as\nautopilot and identity authentication. However, deep learning models are not\ntrustworthy enough because of their limited robustness against adversarial\nattacks. The physically realizable adversarial attacks further pose fatal\nthreats to the application and human safety. Lots of papers have emerged to\ninvestigate the robustness and safety of deep learning models against\nadversarial attacks. To lead to trustworthy AI, we first construct a general\nthreat model from different perspectives and then comprehensively review the\nlatest progress of both 2D and 3D adversarial attacks. We extend the concept of\nadversarial examples beyond imperceptive perturbations and collate over 170\npapers to give an overview of deep learning model robustness against various\nadversarial attacks. To the best of our knowledge, we are the first to\nsystematically investigate adversarial attacks for 3D models, a flourishing\nfield applied to many real-world applications. In addition, we examine physical\nadversarial attacks that lead to safety violations. Last but not least, we\nsummarize present popular topics, give insights on challenges, and shed light\non future research on trustworthy AI.","terms":["cs.LG","cs.AI"]},{"titles":"Quantum image edge detection based on eight-direction Sobel operator for NEQR","summaries":"Quantum Sobel edge detection (QSED) is a kind of algorithm for image edge\ndetection using quantum mechanism, which can solve the real-time problem\nencountered by classical algorithms. However, the existing QSED algorithms only\nconsider two- or four-direction Sobel operator, which leads to a certain loss\nof edge detail information in some high-definition images. In this paper, a\nnovel QSED algorithm based on eight-direction Sobel operator is proposed, which\nnot only reduces the loss of edge information, but also simultaneously\ncalculates eight directions' gradient values of all pixel in a quantum image.\nIn addition, the concrete quantum circuits, which consist of gradient\ncalculation, non-maximum suppression, double threshold detection and edge\ntracking units, are designed in details. For a 2^n x 2^n image with q gray\nscale, the complexity of our algorithm can be reduced to O(n^2 + q^2), which is\nlower than other existing classical or quantum algorithms. And the simulation\nexperiment demonstrates that our algorithm can detect more edge information,\nespecially diagonal edges, than the two- and four-direction QSED algorithms.","terms":["cs.CV","cs.ET","quant-ph"]},{"titles":"Physical Adversarial Attack meets Computer Vision: A Decade Survey","summaries":"Despite the impressive achievements of Deep Neural Networks (DNNs) in\ncomputer vision, their vulnerability to adversarial attacks remains a critical\nconcern. Extensive research has demonstrated that incorporating sophisticated\nperturbations into input images can lead to a catastrophic degradation in DNNs'\nperformance. This perplexing phenomenon not only exists in the digital space\nbut also in the physical world. Consequently, it becomes imperative to evaluate\nthe security of DNNs-based systems to ensure their safe deployment in\nreal-world scenarios, particularly in security-sensitive applications. To\nfacilitate a profound understanding of this topic, this paper presents a\ncomprehensive overview of physical adversarial attacks. Firstly, we distill\nfour general steps for launching physical adversarial attacks. Building upon\nthis foundation, we uncover the pervasive role of artifacts carrying\nadversarial perturbations in the physical world. These artifacts influence each\nstep. To denote them, we introduce a new term: adversarial medium. Then, we\ntake the first step to systematically evaluate the performance of physical\nadversarial attacks, taking the adversarial medium as a first attempt. Our\nproposed evaluation metric, hiPAA, comprises six perspectives: Effectiveness,\nStealthiness, Robustness, Practicability, Aesthetics, and Economics. We also\nprovide comparative results across task categories, together with insightful\nobservations and suggestions for future research directions.","terms":["cs.CV"]},{"titles":"Quantum-Based Feature Selection for Multi-classification Problem in Complex Systems with Edge Computing","summaries":"The complex systems with edge computing require a huge amount of\nmulti-feature data to extract appropriate insights for their decision making,\nso it is important to find a feasible feature selection method to improve the\ncomputational efficiency and save the resource consumption. In this paper, a\nquantum-based feature selection algorithm for the multi-classification problem,\nnamely, QReliefF, is proposed, which can effectively reduce the complexity of\nalgorithm and improve its computational efficiency. First, all features of each\nsample are encoded into a quantum state by performing operations CMP and R_y,\nand then the amplitude estimation is applied to calculate the similarity\nbetween any two quantum states (i.e., two samples). According to the\nsimilarities, the Grover-Long method is utilized to find the nearest k neighbor\nsamples, and then the weight vector is updated. After a certain number of\niterations through the above process, the desired features can be selected with\nregards to the final weight vector and the threshold {\\tau}. Compared with the\nclassical ReliefF algorithm, our algorithm reduces the complexity of similarity\ncalculation from O(MN) to O(M), the complexity of finding the nearest neighbor\nfrom O(M) to O(sqrt(M)), and resource consumption from O(MN) to O(MlogN).\nMeanwhile, compared with the quantum Relief algorithm, our algorithm is\nsuperior in finding the nearest neighbor, reducing the complexity from O(M) to\nO(sqrt(M)). Finally, in order to verify the feasibility of our algorithm, a\nsimulation experiment based on Rigetti with a simple example is performed.","terms":["cs.LG","cs.AI","cs.ET","quant-ph"]},{"titles":"Human-Producible Adversarial Examples","summaries":"Visual adversarial examples have so far been restricted to pixel-level image\nmanipulations in the digital world, or have required sophisticated equipment\nsuch as 2D or 3D printers to be produced in the physical real world. We present\nthe first ever method of generating human-producible adversarial examples for\nthe real world that requires nothing more complicated than a marker pen. We\ncall them $\\textbf{adversarial tags}$. First, building on top of differential\nrendering, we demonstrate that it is possible to build potent adversarial\nexamples with just lines. We find that by drawing just $4$ lines we can disrupt\na YOLO-based model in $54.8\\%$ of cases; increasing this to $9$ lines disrupts\n$81.8\\%$ of the cases tested. Next, we devise an improved method for line\nplacement to be invariant to human drawing error. We evaluate our system\nthoroughly in both digital and analogue worlds and demonstrate that our tags\ncan be applied by untrained humans. We demonstrate the effectiveness of our\nmethod for producing real-world adversarial examples by conducting a user study\nwhere participants were asked to draw over printed images using digital\nequivalents as guides. We further evaluate the effectiveness of both targeted\nand untargeted attacks, and discuss various trade-offs and method limitations,\nas well as the practical and ethical implications of our work. The source code\nwill be released publicly.","terms":["cs.CV","cs.LG"]},{"titles":"Limitless stability for Graph Convolutional Networks","summaries":"This work establishes rigorous, novel and widely applicable stability\nguarantees and transferability bounds for graph convolutional networks --\nwithout reference to any underlying limit object or statistical distribution.\nCrucially, utilized graph-shift operators (GSOs) are not necessarily assumed to\nbe normal, allowing for the treatment of networks on both undirected- and for\nthe first time also directed graphs. Stability to node-level perturbations is\nrelated to an 'adequate (spectral) covering' property of the filters in each\nlayer. Stability to edge-level perturbations is related to Lipschitz constants\nand newly introduced semi-norms of filters. Results on stability to topological\nperturbations are obtained through recently developed mathematical-physics\nbased tools. As an important and novel example, it is showcased that graph\nconvolutional networks are stable under graph-coarse-graining procedures\n(replacing strongly-connected sub-graphs by single nodes) precisely if the GSO\nis the graph Laplacian and filters are regular at infinity. These new\ntheoretical results are supported by corresponding numerical investigations.","terms":["cs.LG","math.FA"]},{"titles":"The Underlying Scaling Laws and Universal Statistical Structure of Complex Datasets","summaries":"We study universal traits which emerge both in real-world complex datasets,\nas well as in artificially generated ones. Our approach is to analogize data to\na physical system and employ tools from statistical physics and Random Matrix\nTheory (RMT) to reveal their underlying structure. We focus on the\nfeature-feature covariance matrix, analyzing both its local and global\neigenvalue statistics. Our main observations are: (i) The power-law scalings\nthat the bulk of its eigenvalues exhibit are vastly different for uncorrelated\nnormally distributed data compared to real-world data, (ii) this scaling\nbehavior can be completely modeled by generating gaussian data with long range\ncorrelations, (iii) both generated and real-world datasets lie in the same\nuniversality class from the RMT perspective, as chaotic rather than integrable\nsystems, (iv) the expected RMT statistical behavior already manifests for\nempirical covariance matrices at dataset sizes significantly smaller than those\nconventionally used for real-world training, and can be related to the number\nof samples required to approximate the population power-law scaling behavior,\n(v) the Shannon entropy is correlated with local RMT structure and eigenvalues\nscaling, and substantially smaller in strongly correlated datasets compared to\nuncorrelated synthetic data, and requires fewer samples to reach the\ndistribution entropy. These findings show that with sufficient sample size, the\nGram matrix of natural image datasets can be well approximated by a Wishart\nrandom matrix with a simple covariance structure, opening the door to rigorous\nstudies of neural network dynamics and generalization which rely on the data\nGram matrix.","terms":["cs.LG","cond-mat.dis-nn","hep-th","math.PR","stat.ML"]},{"titles":"A hybrid quantum-classical conditional generative adversarial network algorithm for human-centered paradigm in cloud","summaries":"As an emerging field that aims to bridge the gap between human activities and\ncomputing systems, human-centered computing (HCC) in cloud, edge, fog has had a\nhuge impact on the artificial intelligence algorithms. The quantum generative\nadversarial network (QGAN) is considered to be one of the quantum machine\nlearning algorithms with great application prospects, which also should be\nimproved to conform to the human-centered paradigm. The generation process of\nQGAN is relatively random and the generated model does not conform to the\nhuman-centered concept, so it is not quite suitable for real scenarios. In\norder to solve these problems, a hybrid quantum-classical conditional\ngenerative adversarial network (QCGAN) algorithm is proposed, which is a\nknowledge-driven human-computer interaction computing mode that can be\nimplemented in cloud. The purposes of stabilizing the generation process and\nrealizing the interaction between human and computing process are achieved by\ninputting artificial conditional information in the generator and\ndiscriminator. The generator uses the parameterized quantum circuit with an\nall-to-all connected topology, which facilitates the tuning of network\nparameters during the training process. The discriminator uses the classical\nneural network, which effectively avoids the \"input bottleneck\" of quantum\nmachine learning. Finally, the BAS training set is selected to conduct\nexperiment on the quantum cloud computing platform. The result shows that the\nQCGAN algorithm can effectively converge to the Nash equilibrium point after\ntraining and perform human-centered classification generation tasks.","terms":["cs.LG","cs.ET","quant-ph"]},{"titles":"Augmented Physics-Informed Neural Networks (APINNs): A gating network-based soft domain decomposition methodology","summaries":"In this paper, we propose the augmented physics-informed neural network\n(APINN), which adopts soft and trainable domain decomposition and flexible\nparameter sharing to further improve the extended PINN (XPINN) as well as the\nvanilla PINN methods. In particular, a trainable gate network is employed to\nmimic the hard decomposition of XPINN, which can be flexibly fine-tuned for\ndiscovering a potentially better partition. It weight-averages several sub-nets\nas the output of APINN. APINN does not require complex interface conditions,\nand its sub-nets can take advantage of all training samples rather than just\npart of the training data in their subdomains. Lastly, each sub-net shares part\nof the common parameters to capture the similar components in each decomposed\nfunction. Furthermore, following the PINN generalization theory in Hu et al.\n[2021], we show that APINN can improve generalization by proper gate network\ninitialization and general domain & function decomposition. Extensive\nexperiments on different types of PDEs demonstrate how APINN improves the PINN\nand XPINN methods. Specifically, we present examples where XPINN performs\nsimilarly to or worse than PINN, so that APINN can significantly improve both.\nWe also show cases where XPINN is already better than PINN, so APINN can still\nslightly improve XPINN. Furthermore, we visualize the optimized gating networks\nand their optimization trajectories, and connect them with their performance,\nwhich helps discover the possibly optimal decomposition. Interestingly, if\ninitialized by different decomposition, the performances of corresponding\nAPINNs can differ drastically. This, in turn, shows the potential to design an\noptimal domain decomposition for the differential equation problem under\nconsideration.","terms":["cs.LG","cs.NA","math.DS","math.NA","stat.ML"]},{"titles":"Prior Mismatch and Adaptation in PnP-ADMM with a Nonconvex Convergence Analysis","summaries":"Plug-and-Play (PnP) priors is a widely-used family of methods for solving\nimaging inverse problems by integrating physical measurement models with image\npriors specified using image denoisers. PnP methods have been shown to achieve\nstate-of-the-art performance when the prior is obtained using powerful deep\ndenoisers. Despite extensive work on PnP, the topic of distribution mismatch\nbetween the training and testing data has often been overlooked in the PnP\nliterature. This paper presents a set of new theoretical and numerical results\non the topic of prior distribution mismatch and domain adaptation for\nalternating direction method of multipliers (ADMM) variant of PnP. Our\ntheoretical result provides an explicit error bound for PnP-ADMM due to the\nmismatch between the desired denoiser and the one used for inference. Our\nanalysis contributes to the work in the area by considering the mismatch under\nnonconvex data-fidelity terms and expansive denoisers. Our first set of\nnumerical results quantifies the impact of the prior distribution mismatch on\nthe performance of PnP-ADMM on the problem of image super-resolution. Our\nsecond set of numerical results considers a simple and effective domain\nadaption strategy that closes the performance gap due to the use of mismatched\ndenoisers. Our results suggest the relative robustness of PnP-ADMM to prior\ndistribution mismatch, while also showing that the performance gap can be\nsignificantly reduced with few training samples from the desired distribution.","terms":["cs.CV"]},{"titles":"Machine Learning Clifford invariants of ADE Coxeter elements","summaries":"There has been recent interest in novel Clifford geometric invariants of\nlinear transformations. This motivates the investigation of such invariants for\na certain type of geometric transformation of interest in the context of root\nsystems, reflection groups, Lie groups and Lie algebras: the Coxeter\ntransformations. We perform exhaustive calculations of all Coxeter\ntransformations for $A_8$, $D_8$ and $E_8$ for a choice of basis of simple\nroots and compute their invariants, using high-performance computing. This\ncomputational algebra paradigm generates a dataset that can then be mined using\ntechniques from data science such as supervised and unsupervised machine\nlearning. In this paper we focus on neural network classification and principal\ncomponent analysis. Since the output -- the invariants -- is fully determined\nby the choice of simple roots and the permutation order of the corresponding\nreflections in the Coxeter element, we expect huge degeneracy in the mapping.\nThis provides the perfect setup for machine learning, and indeed we see that\nthe datasets can be machine learned to very high accuracy. This paper is a\npump-priming study in experimental mathematics using Clifford algebras, showing\nthat such Clifford algebraic datasets are amenable to machine learning, and\nshedding light on relationships between these novel and other well-known\ngeometric invariants and also giving rise to analytic results.","terms":["cs.LG","hep-th","math-ph","math.GR","math.MP","math.RT"]},{"titles":"Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning","summaries":"Many processes in biology and drug discovery involve various 3D interactions\nbetween molecules, such as protein and protein, protein and small molecule,\netc. Given that different molecules are usually represented in different\ngranularity, existing methods usually encode each type of molecules\nindependently with different models, leaving it defective to learn the\nuniversal underlying interaction physics. In this paper, we first propose to\nuniversally represent an arbitrary 3D complex as a geometric graph of sets,\nshedding light on encoding all types of molecules with one model. We then\npropose a Generalist Equivariant Transformer (GET) to effectively capture both\ndomain-specific hierarchies and domain-agnostic interaction physics. To be\nspecific, GET consists of a bilevel attention module, a feed-forward module and\na layer normalization module, where each module is E(3) equivariant and\nspecialized for handling sets of variable sizes. Notably, in contrast to\nconventional pooling-based hierarchical models, our GET is able to retain\nfine-grained information of all levels. Extensive experiments on the\ninteractions between proteins, small molecules and RNA\/DNAs verify the\neffectiveness and generalization capability of our proposed method across\ndifferent domains.","terms":["cs.LG","q-bio.BM"]},{"titles":"HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World","summaries":"Building an interactive AI assistant that can perceive, reason, and\ncollaborate with humans in the real world has been a long-standing pursuit in\nthe AI community. This work is part of a broader research effort to develop\nintelligent agents that can interactively guide humans through performing tasks\nin the physical world. As a first step in this direction, we introduce\nHoloAssist, a large-scale egocentric human interaction dataset, where two\npeople collaboratively complete physical manipulation tasks. The task performer\nexecutes the task while wearing a mixed-reality headset that captures seven\nsynchronized data streams. The task instructor watches the performer's\negocentric video in real time and guides them verbally. By augmenting the data\nwith action and conversational annotations and observing the rich behaviors of\nvarious participants, we present key insights into how human assistants correct\nmistakes, intervene in the task completion procedure, and ground their\ninstructions to the environment. HoloAssist spans 166 hours of data captured by\n350 unique instructor-performer pairs. Furthermore, we construct and present\nbenchmarks on mistake detection, intervention type prediction, and hand\nforecasting, along with detailed analysis. We expect HoloAssist will provide an\nimportant resource for building AI assistants that can fluidly collaborate with\nhumans in the real world. Data can be downloaded at\nhttps:\/\/holoassist.github.io\/.","terms":["cs.CV"]},{"titles":"The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement","summaries":"Reinforcement learning (RL) for physical design of silicon chips in a Google\n2021 Nature paper stirred controversy due to poorly documented claims that\nraised eyebrows and drew critical media coverage. The paper withheld critical\nmethodology steps and most inputs needed to reproduce results. Our\nmeta-analysis shows how two separate evaluations filled in the gaps and\ndemonstrated that Google RL lags behind (i) human designers, (ii) a well-known\nalgorithm (Simulated Annealing), and (iii) generally-available commercial\nsoftware, while being slower; and in a 2023 open research contest, RL methods\nweren't in top 5. Crosschecked data indicate that the integrity of the Nature\npaper is substantially undermined owing to errors in conduct, analysis and\nreporting. Before publishing, Google rebuffed internal allegations of fraud. We\nnote policy implications and conclusions for chip design.","terms":["cs.LG","cs.AI","cs.AR","cs.CY"]},{"titles":"Controlling Continuous Relaxation for Combinatorial Optimization","summaries":"Recent advancements in combinatorial optimization (CO) problems emphasize the\npotential of graph neural networks (GNNs). The physics-inspired GNN (PI-GNN)\nsolver, which finds approximate solutions through unsupervised learning, has\nattracted significant attention for large-scale CO problems. Nevertheless,\nthere has been limited discussion on the performance of the PI-GNN solver for\nCO problems on relatively dense graphs where the performance of greedy\nalgorithms worsens. In addition, since the PI-GNN solver employs a relaxation\nstrategy, an artificial transformation from the continuous space back to the\noriginal discrete space is necessary after learning, potentially undermining\nthe robustness of the solutions. This paper numerically demonstrates that the\nPI-GNN solver can be trapped in a local solution, where all variables are zero,\nin the early stage of learning for CO problems on the dense graphs. Then, we\naddress these problems by controlling the continuity and discreteness of\nrelaxed variables while avoiding the local solution: (i) introducing a new\npenalty term that controls the continuity and discreteness of the relaxed\nvariables and eliminates the local solution; (ii) proposing a new continuous\nrelaxation annealing (CRA) strategy. This new annealing first prioritizes\ncontinuous solutions and intensifies exploration by leveraging the continuity\nwhile avoiding the local solution and then schedules the penalty term for\nprioritizing a discrete solution until the relaxed variables are almost\ndiscrete values, which eliminates the need for an artificial transformation\nfrom the continuous to the original discrete space. Empirically, better results\nare obtained for CO problems on the dense graphs, where the PI-GNN solver\nstruggles to find reasonable solutions, and for those on relatively sparse\ngraphs. Furthermore, the computational time scaling is identical to that of the\nPI-GNN solver.","terms":["stat.ML","cs.LG","stat.CO","stat.ME"]},{"titles":"Physics-Informed Induction Machine Modelling","summaries":"This rapid communication devises a Neural Induction Machine (NeuIM) model,\nwhich pilots the use of physics-informed machine learning to enable AI-based\nelectromagnetic transient simulations. The contributions are threefold: (1) a\nformation of NeuIM to represent the induction machine in phase domain; (2) a\nphysics-informed neural network capable of capturing fast and slow IM dynamics\neven in the absence of data; and (3) a data-physics-integrated hybrid NeuIM\napproach which is adaptive to various levels of data availability. Extensive\ncase studies validate the efficacy of NeuIM and in particular, its advantage\nover purely data-driven approaches.","terms":["cs.LG","cs.SY","eess.SY"]},{"titles":"Maximum Diffusion Reinforcement Learning","summaries":"The assumption that data are independent and identically distributed\nunderpins all machine learning. When data are collected sequentially from agent\nexperiences this assumption does not generally hold, as in reinforcement\nlearning. Here, we derive a method that overcomes these limitations by\nexploiting the statistical mechanics of ergodic processes, which we term\nmaximum diffusion reinforcement learning. By decorrelating agent experiences,\nour approach provably enables agents to learn continually in single-shot\ndeployments regardless of how they are initialized. Moreover, we prove our\napproach generalizes well-known maximum entropy techniques, and show that it\nrobustly exceeds state-of-the-art performance across popular benchmarks. Our\nresults at the nexus of physics, learning, and control pave the way towards\nmore transparent and reliable decision-making in reinforcement learning agents,\nsuch as locomoting robots and self-driving cars.","terms":["cs.LG","cond-mat.stat-mech","cs.AI","cs.RO"]},{"titles":"ProbVLM: Probabilistic Adapter for Frozen Vision-Language Models","summaries":"Large-scale vision-language models (VLMs) like CLIP successfully find\ncorrespondences between images and text. Through the standard deterministic\nmapping process, an image or a text sample is mapped to a single vector in the\nembedding space. This is problematic: as multiple samples (images or text) can\nabstract the same concept in the physical world, deterministic embeddings do\nnot reflect the inherent ambiguity in the embedding space. We propose ProbVLM,\na probabilistic adapter that estimates probability distributions for the\nembeddings of pre-trained VLMs via inter\/intra-modal alignment in a post-hoc\nmanner without needing large-scale datasets or computing. On four challenging\ndatasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the\nmulti-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify\nthe calibration of embedding uncertainties in retrieval tasks and show that\nProbVLM outperforms other methods. Furthermore, we propose active learning and\nmodel selection as two real-world downstream tasks for VLMs and show that the\nestimated uncertainty aids both tasks. Lastly, we present a novel technique for\nvisualizing the embedding distributions using a large-scale pre-trained latent\ndiffusion model. Code is available at https:\/\/github.com\/ExplainableML\/ProbVLM.","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"DarSwin: Distortion Aware Radial Swin Transformer","summaries":"Wide-angle lenses are commonly used in perception tasks requiring a large\nfield of view. Unfortunately, these lenses produce significant distortions\nmaking conventional models that ignore the distortion effects unable to adapt\nto wide-angle images. In this paper, we present a novel transformer-based model\nthat automatically adapts to the distortion produced by wide-angle lenses. We\nleverage the physical characteristics of such lenses, which are analytically\ndefined by the radial distortion profile (assumed to be known), to develop a\ndistortion aware radial swin transformer (DarSwin). In contrast to conventional\ntransformer-based architectures, DarSwin comprises a radial patch partitioning,\na distortion-based sampling technique for creating token embeddings, and an\nangular position encoding for radial patch merging. We validate our method on\nclassification tasks using synthetically distorted ImageNet data and show\nthrough extensive experiments that DarSwin can perform zero-shot adaptation to\nunseen distortions of different wide-angle lenses. Compared to other baselines,\nDarSwin achieves the best results (in terms of Top-1 accuracy) with significant\ngains when trained on bounded levels of distortions (very-low, low, medium, and\nhigh) and tested on all including out-of-distribution distortions. The code and\nmodels are publicly available at https:\/\/lvsn.github.io\/darswin\/","terms":["cs.CV","68T01"]},{"titles":"Generative Residual Diffusion Modeling for Km-scale Atmospheric Downscaling","summaries":"The state of the art for physical hazard prediction from weather and climate\nrequires expensive km-scale numerical simulations driven by coarser resolution\nglobal inputs. Here, a km-scale downscaling diffusion model is presented as a\ncost effective alternative. The model is trained from a regional\nhigh-resolution weather model over Taiwan, and conditioned on ERA5 reanalysis\ndata. To address the downscaling uncertainties, large resolution ratios (25km\nto 2km), different physics involved at different scales and predict channels\nthat are not in the input data, we employ a two-step approach\n(\\textit{ResDiff}) where a (UNet) regression predicts the mean in the first\nstep and a diffusion model predicts the residual in the second step.\n\\textit{ResDiff} exhibits encouraging skill in bulk RMSE and CRPS scores. The\npredicted spectra and distributions from ResDiff faithfully recover important\npower law relationships regulating damaging wind and rain extremes. Case\nstudies of coherent weather phenomena reveal appropriate multivariate\nrelationships reminiscent of learnt physics. This includes the sharp wind and\ntemperature variations that co-locate with intense rainfall in a cold front,\nand the extreme winds and rainfall bands that surround the eyewall of typhoons.\nSome evidence of simultaneous bias correction is found. A first attempt at\ndownscaling directly from an operational global forecast model successfully\nretains many of these benefits. The implication is that a new era of fully\nend-to-end, global-to-regional machine learning weather prediction is likely\nnear at hand.","terms":["cs.LG","physics.ao-ph"]},{"titles":"M-OFDFT: Overcoming the Barrier of Orbital-Free Density Functional Theory for Molecular Systems Using Deep Learning","summaries":"Orbital-free density functional theory (OFDFT) is a quantum chemistry\nformulation that has a lower cost scaling than the prevailing Kohn-Sham DFT,\nwhich is increasingly desired for contemporary molecular research. However, its\naccuracy is limited by the kinetic energy density functional, which is\nnotoriously hard to approximate for non-periodic molecular systems. In this\nwork, we propose M-OFDFT, an OFDFT approach capable of solving molecular\nsystems using a deep-learning functional model. We build the essential\nnonlocality into the model, which is made affordable by the concise density\nrepresentation as expansion coefficients under an atomic basis. With techniques\nto address unconventional learning challenges therein, M-OFDFT achieves a\ncomparable accuracy with Kohn-Sham DFT on a wide range of molecules untouched\nby OFDFT before. More attractively, M-OFDFT extrapolates well to molecules much\nlarger than those in training, which unleashes the appealing scaling for\nstudying large molecules including proteins, representing an advancement of the\naccuracy-efficiency trade-off frontier in quantum chemistry.","terms":["stat.ML","cs.LG","physics.chem-ph"]},{"titles":"Deep Single Models vs. Ensembles: Insights for a Fast Deployment of Parking Monitoring Systems","summaries":"Searching for available parking spots in high-density urban centers is a\nstressful task for drivers that can be mitigated by systems that know in\nadvance the nearest parking space available.\n  To this end, image-based systems offer cost advantages over other\nsensor-based alternatives (e.g., ultrasonic sensors), requiring less physical\ninfrastructure for installation and maintenance.\n  Despite recent deep learning advances, deploying intelligent parking\nmonitoring is still a challenge since most approaches involve collecting and\nlabeling large amounts of data, which is laborious and time-consuming. Our\nstudy aims to uncover the challenges in creating a global framework, trained\nusing publicly available labeled parking lot images, that performs accurately\nacross diverse scenarios, enabling the parking space monitoring as a\nready-to-use system to deploy in a new environment. Through exhaustive\nexperiments involving different datasets and deep learning architectures,\nincluding fusion strategies and ensemble methods, we found that models trained\non diverse datasets can achieve 95\\% accuracy without the burden of data\nannotation and model training on the target parking lot","terms":["cs.CV","cs.LG"]},{"titles":"A parsimonious, computationally efficient machine learning method for spatial regression","summaries":"We introduce the modified planar rotator method (MPRS), a physically inspired\nmachine learning method for spatial\/temporal regression. MPRS is a\nnon-parametric model which incorporates spatial or temporal correlations via\nshort-range, distance-dependent ``interactions'' without assuming a specific\nform for the underlying probability distribution. Predictions are obtained by\nmeans of a fully autonomous learning algorithm which employs equilibrium\nconditional Monte Carlo simulations. MPRS is able to handle scattered data and\narbitrary spatial dimensions. We report tests on various synthetic and\nreal-word data in one, two and three dimensions which demonstrate that the MPRS\nprediction performance (without parameter tuning) is competitive with standard\ninterpolation methods such as ordinary kriging and inverse distance weighting.\nIn particular, MPRS is a particularly effective gap-filling method for rough\nand non-Gaussian data (e.g., daily precipitation time series). MPRS shows\nsuperior computational efficiency and scalability for large samples. Massive\ndata sets involving millions of nodes can be processed in a few seconds on a\nstandard personal computer.","terms":["stat.ML","cs.LG"]},{"titles":"Improving Image Quality of Sparse-view Lung Cancer CT Images with a Convolutional Neural Network","summaries":"Purpose: To improve the image quality of sparse-view computed tomography (CT)\nimages with a U-Net for lung cancer detection and to determine the best\ntrade-off between number of views, image quality, and diagnostic confidence.\n  Methods: CT images from 41 subjects (34 with lung cancer, seven healthy) were\nretrospectively selected (01.2016-12.2018) and forward projected onto 2048-view\nsinograms. Six corresponding sparse-view CT data subsets at varying levels of\nundersampling were reconstructed from sinograms using filtered backprojection\nwith 16, 32, 64, 128, 256, and 512 views, respectively. A dual-frame U-Net was\ntrained and evaluated for each subsampling level on 8,658 images from 22\ndiseased subjects. A representative image per scan was selected from 19\nsubjects (12 diseased, seven healthy) for a single-blinded reader study. The\nselected slices, for all levels of subsampling, with and without\npost-processing by the U-Net model, were presented to three readers. Image\nquality and diagnostic confidence were ranked using pre-defined scales.\nSubjective nodule segmentation was evaluated utilizing sensitivity (Se) and\nDice Similarity Coefficient (DSC) with 95% confidence intervals (CI).\n  Results: The 64-projection sparse-view images resulted in Se = 0.89 and DSC =\n0.81 [0.75,0.86] while their counterparts, post-processed with the U-Net, had\nimproved metrics (Se = 0.94, DSC = 0.85 [0.82,0.87]). Fewer views lead to\ninsufficient quality for diagnostic purposes. For increased views, no\nsubstantial discrepancies were noted between the sparse-view and post-processed\nimages.\n  Conclusion: Projection views can be reduced from 2048 to 64 while maintaining\nimage quality and the confidence of the radiologists on a satisfactory level.","terms":["cs.CV","physics.med-ph"]},{"titles":"Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks","summaries":"Copulas are a powerful statistical tool that captures dependencies across\ndata dimensions. When applying Copulas, we can estimate multivariate\ndistribution functions by initially estimating independent marginals, an easy\ntask, and then a single copulating function, $C$, to connect the marginals, a\nhard task. For two-dimensional data, a copula is a two-increasing function of\nthe form $C: (u,v)\\in \\mathbf{I}^2 \\rightarrow \\mathbf{I}$, where $\\mathbf{I} =\n[0, 1]$. In this paper, we show how Neural Networks (NNs) can approximate any\ntwo-dimensional copula non-parametrically. Our approach, denoted as 2-Cats, is\ninspired by the Physics-Informed Neural Networks and Sobolev Training\nliterature. Not only do we show that we can estimate the output of a 2d Copula\nbetter than the state-of-the-art, our approach is non-parametric and respects\nthe mathematical properties of a Copula $C$.","terms":["cs.LG","cs.AI"]},{"titles":"High Throughput Training of Deep Surrogates from Large Ensemble Runs","summaries":"Recent years have seen a surge in deep learning approaches to accelerate\nnumerical solvers, which provide faithful but computationally intensive\nsimulations of the physical world. These deep surrogates are generally trained\nin a supervised manner from limited amounts of data slowly generated by the\nsame solver they intend to accelerate. We propose an open-source framework that\nenables the online training of these models from a large ensemble run of\nsimulations. It leverages multiple levels of parallelism to generate rich\ndatasets. The framework avoids I\/O bottlenecks and storage issues by directly\nstreaming the generated data. A training reservoir mitigates the inherent bias\nof streaming while maximizing GPU throughput. Experiment on training a fully\nconnected network as a surrogate for the heat equation shows the proposed\napproach enables training on 8TB of data in 2 hours with an accuracy improved\nby 47% and a batch throughput multiplied by 13 compared to a traditional\noffline procedure.","terms":["cs.LG","cs.AI","cs.DC"]},{"titles":"Object Motion Guided Human Motion Synthesis","summaries":"Modeling human behaviors in contextual environments has a wide range of\napplications in character animation, embodied AI, VR\/AR, and robotics. In\nreal-world scenarios, humans frequently interact with the environment and\nmanipulate various objects to complete daily tasks. In this work, we study the\nproblem of full-body human motion synthesis for the manipulation of large-sized\nobjects. We propose Object MOtion guided human MOtion synthesis (OMOMO), a\nconditional diffusion framework that can generate full-body manipulation\nbehaviors from only the object motion. Since naively applying diffusion models\nfails to precisely enforce contact constraints between the hands and the\nobject, OMOMO learns two separate denoising processes to first predict hand\npositions from object motion and subsequently synthesize full-body poses based\non the predicted hand positions. By employing the hand positions as an\nintermediate representation between the two denoising processes, we can\nexplicitly enforce contact constraints, resulting in more physically plausible\nmanipulation motions. With the learned model, we develop a novel system that\ncaptures full-body human manipulation motions by simply attaching a smartphone\nto the object being manipulated. Through extensive experiments, we demonstrate\nthe effectiveness of our proposed pipeline and its ability to generalize to\nunseen objects. Additionally, as high-quality human-object interaction datasets\nare scarce, we collect a large-scale dataset consisting of 3D object geometry,\nobject motion, and human motion. Our dataset contains human-object interaction\nmotion for 15 objects, with a total duration of approximately 10 hours.","terms":["cs.CV"]},{"titles":"DynaBench: A benchmark dataset for learning dynamical systems from low-resolution data","summaries":"Previous work on learning physical systems from data has focused on\nhigh-resolution grid-structured measurements. However, real-world knowledge of\nsuch systems (e.g. weather data) relies on sparsely scattered measuring\nstations. In this paper, we introduce a novel simulated benchmark dataset,\nDynaBench, for learning dynamical systems directly from sparsely scattered data\nwithout prior knowledge of the equations. The dataset focuses on predicting the\nevolution of a dynamical system from low-resolution, unstructured measurements.\nWe simulate six different partial differential equations covering a variety of\nphysical systems commonly used in the literature and evaluate several machine\nlearning models, including traditional graph neural networks and point cloud\nprocessing models, with the task of predicting the evolution of the system. The\nproposed benchmark dataset is expected to advance the state of art as an\nout-of-the-box easy-to-use tool for evaluating models in a setting where only\nunstructured low-resolution observations are available. The benchmark is\navailable at https:\/\/anonymous.4open.science\/r\/code-2022-dynabench\/.","terms":["cs.LG"]},{"titles":"Capturing the Diffusive Behavior of the Multiscale Linear Transport Equations by Asymptotic-Preserving Convolutional DeepONets","summaries":"In this paper, we introduce two types of novel Asymptotic-Preserving\nConvolutional Deep Operator Networks (APCONs) designed to address the\nmultiscale time-dependent linear transport problem. We observe that the vanilla\nphysics-informed DeepONets with modified MLP may exhibit instability in\nmaintaining the desired limiting macroscopic behavior. Therefore, this\nnecessitates the utilization of an asymptotic-preserving loss function. Drawing\ninspiration from the heat kernel in the diffusion equation, we propose a new\narchitecture called Convolutional Deep Operator Networks, which employ multiple\nlocal convolution operations instead of a global heat kernel, along with\npooling and activation operations in each filter layer. Our APCON methods\npossess a parameter count that is independent of the grid size and are capable\nof capturing the diffusive behavior of the linear transport problem. Finally,\nwe validate the effectiveness of our methods through several numerical\nexamples.","terms":["cs.LG"]},{"titles":"Cloth2Body: Generating 3D Human Body Mesh from 2D Clothing","summaries":"In this paper, we define and study a new Cloth2Body problem which has a goal\nof generating 3D human body meshes from a 2D clothing image. Unlike the\nexisting human mesh recovery problem, Cloth2Body needs to address new and\nemerging challenges raised by the partial observation of the input and the high\ndiversity of the output. Indeed, there are three specific challenges. First,\nhow to locate and pose human bodies into the clothes. Second, how to\neffectively estimate body shapes out of various clothing types. Finally, how to\ngenerate diverse and plausible results from a 2D clothing image. To this end,\nwe propose an end-to-end framework that can accurately estimate 3D body mesh\nparameterized by pose and shape from a 2D clothing image. Along this line, we\nfirst utilize Kinematics-aware Pose Estimation to estimate body pose\nparameters. 3D skeleton is employed as a proxy followed by an inverse\nkinematics module to boost the estimation accuracy. We additionally design an\nadaptive depth trick to align the re-projected 3D mesh better with 2D clothing\nimage by disentangling the effects of object size and camera extrinsic. Next,\nwe propose Physics-informed Shape Estimation to estimate body shape parameters.\n3D shape parameters are predicted based on partial body measurements estimated\nfrom RGB image, which not only improves pixel-wise human-cloth alignment, but\nalso enables flexible user editing. Finally, we design Evolution-based pose\ngeneration method, a skeleton transplanting method inspired by genetic\nalgorithms to generate diverse reasonable poses during inference. As shown by\nexperimental results on both synthetic and real-world data, the proposed\nframework achieves state-of-the-art performance and can effectively recover\nnatural and diverse 3D body meshes from 2D images that align well with\nclothing.","terms":["cs.CV"]},{"titles":"Instance-Agnostic Geometry and Contact Dynamics Learning","summaries":"This work presents an instance-agnostic learning framework that fuses vision\nwith dynamics to simultaneously learn shape, pose trajectories, and physical\nproperties via the use of geometry as a shared representation. Unlike many\ncontact learning approaches that assume motion capture input and a known shape\nprior for the collision model, our proposed framework learns an object's\ngeometric and dynamic properties from RGBD video, without requiring either\ncategory-level or instance-level shape priors. We integrate a vision system,\nBundleSDF, with a dynamics system, ContactNets, and propose a cyclic training\npipeline to use the output from the dynamics module to refine the poses and the\ngeometry from the vision module, using perspective reprojection. Experiments\ndemonstrate our framework's ability to learn the geometry and dynamics of rigid\nand convex objects and improve upon the current tracking framework.","terms":["cs.CV","cs.LG","cs.RO"]},{"titles":"DPA-WNO: A gray box model for a class of stochastic mechanics problem","summaries":"The well-known governing physics in science and engineering is often based on\ncertain assumptions and approximations. Therefore, analyses and designs carried\nout based on these equations are also approximate. The emergence of data-driven\nmodels has, to a certain degree, addressed this challenge; however, the purely\ndata-driven models often (a) lack interpretability, (b) are data-hungry, and\n(c) do not generalize beyond the training window. Operator learning has\nrecently been proposed as a potential alternative to address the aforementioned\nchallenges; however, the challenges are still persistent. We here argue that\none of the possible solutions resides in data-physics fusion, where the\ndata-driven model is used to correct\/identify the missing physics. To that end,\nwe propose a novel Differentiable Physics Augmented Wavelet Neural Operator\n(DPA-WNO). The proposed DPA-WNO blends a differentiable physics solver with the\nWavelet Neural Operator (WNO), where the role of WNO is to model the missing\nphysics. This empowers the proposed framework to exploit the capability of WNO\nto learn from data while retaining the interpretability and generalizability\nassociated with physics-based solvers. We illustrate the applicability of the\nproposed approach in solving time-dependent uncertainty quantification problems\ndue to randomness in the initial condition. Four benchmark uncertainty\nquantification and reliability analysis examples from various fields of science\nand engineering are solved using the proposed approach. The results presented\nillustrate interesting features of the proposed approach.","terms":["cs.LG"]},{"titles":"Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems","summaries":"Anomaly detection is critical to ensure the security of cyber-physical\nsystems (CPS). However, due to the increasing complexity of attacks and CPS\nthemselves, anomaly detection in CPS is becoming more and more challenging. In\nour previous work, we proposed a digital twin-based anomaly detection method,\ncalled ATTAIN, which takes advantage of both historical and real-time data of\nCPS. However, such data vary significantly in terms of difficulty. Therefore,\nsimilar to human learning processes, deep learning models (e.g., ATTAIN) can\nbenefit from an easy-to-difficult curriculum. To this end, in this paper, we\npresent a novel approach, named digitaL twin-based Anomaly deTecTion wIth\nCurriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum\nlearning to optimize its learning paradigm. LATTICE attributes each sample with\na difficulty score, before being fed into a training scheduler. The training\nscheduler samples batches of training data based on these difficulty scores\nsuch that learning from easy to difficult data can be performed. To evaluate\nLATTICE, we use five publicly available datasets collected from five real-world\nCPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art\nanomaly detectors. Evaluation results show that LATTICE outperforms the three\nbaselines and ATTAIN by 0.906%-2.367% in terms of the F1 score. LATTICE also,\non average, reduces the training time of ATTAIN by 4.2% on the five datasets\nand is on par with the baselines in terms of detection delay time.","terms":["cs.LG","cs.CR","cs.SE"]},{"titles":"Assessment of Local Climate Zone Products via Simplified Classification Rule with 3D Building Maps","summaries":"This study assesses the performance of a global Local Climate Zone (LCZ)\nproduct. We examined the built-type classes of LCZs in three major metropolitan\nareas within the U.S. A reference LCZ was constructed using a simple rule-based\nmethod based on high-resolution 3D building maps. Our evaluation demonstrated\nthat the global LCZ product struggles to differentiate classes that demand\nprecise building footprint information (Classes 6 and 9), and classes that\nnecessitate the identification of subtle differences in building elevation\n(Classes 4-6). Additionally, we identified inconsistent tendencies, where the\ndistribution of classes skews differently across different cities, suggesting\nthe presence of a data distribution shift problem in the machine learning-based\nLCZ classifier. Our findings shed light on the uncertainties in global LCZ\nmaps, help identify the LCZ classes that are the most challenging to\ndistinguish, and offer insight into future plans for LCZ development and\nvalidation.","terms":["cs.CV","physics.ao-ph"]},{"titles":"Node-Aligned Graph-to-Graph Generation for Retrosynthesis Prediction","summaries":"Single-step retrosynthesis is a crucial task in organic chemistry and drug\ndesign, requiring the identification of required reactants to synthesize a\nspecific compound. with the advent of computer-aided synthesis planning, there\nis growing interest in using machine-learning techniques to facilitate the\nprocess. Existing template-free machine learning-based models typically utilize\ntransformer structures and represent molecules as ID sequences. However, these\nmethods often face challenges in fully leveraging the extensive topological\ninformation of the molecule and aligning atoms between the production and\nreactants, leading to results that are not as competitive as those of\nsemi-template models. Our proposed method, Node-Aligned Graph-to-Graph (NAG2G),\nalso serves as a transformer-based template-free model but utilizes 2D\nmolecular graphs and 3D conformation information. Furthermore, our approach\nsimplifies the incorporation of production-reactant atom mapping alignment by\nleveraging node alignment to determine a specific order for node generation and\ngenerating molecular graphs in an auto-regressive manner node-by-node. This\nmethod ensures that the node generation order coincides with the node order in\nthe input graph, overcoming the difficulty of determining a specific node\ngeneration order in an auto-regressive manner. Our extensive benchmarking\nresults demonstrate that the proposed NAG2G can outperform the previous\nstate-of-the-art baselines in various metrics.","terms":["cs.LG","physics.chem-ph","q-bio.QM"]},{"titles":"Aperture Diffraction for Compact Snapshot Spectral Imaging","summaries":"We demonstrate a compact, cost-effective snapshot spectral imaging system\nnamed Aperture Diffraction Imaging Spectrometer (ADIS), which consists only of\nan imaging lens with an ultra-thin orthogonal aperture mask and a mosaic filter\nsensor, requiring no additional physical footprint compared to common RGB\ncameras. Then we introduce a new optical design that each point in the object\nspace is multiplexed to discrete encoding locations on the mosaic filter sensor\nby diffraction-based spatial-spectral projection engineering generated from the\northogonal mask. The orthogonal projection is uniformly accepted to obtain a\nweakly calibration-dependent data form to enhance modulation robustness.\nMeanwhile, the Cascade Shift-Shuffle Spectral Transformer (CSST) with strong\nperception of the diffraction degeneration is designed to solve a\nsparsity-constrained inverse problem, realizing the volume reconstruction from\n2D measurements with Large amount of aliasing. Our system is evaluated by\nelaborating the imaging optical theory and reconstruction algorithm with\ndemonstrating the experimental imaging under a single exposure. Ultimately, we\nachieve the sub-super-pixel spatial resolution and high spectral resolution\nimaging. The code will be available at: https:\/\/github.com\/Krito-ex\/CSST.","terms":["cs.CV","eess.IV"]},{"titles":"Novel and flexible parameter estimation methods for data-consistent inversion in mechanistic modeling","summaries":"Predictions for physical systems often rely upon knowledge acquired from\nensembles of entities, e.g., ensembles of cells in biological sciences. For\nqualitative and quantitative analysis, these ensembles are simulated with\nparametric families of mechanistic models (MM). Two classes of methodologies,\nbased on Bayesian inference and Population of Models, currently prevail in\nparameter estimation for physical systems. However, in Bayesian analysis,\nuninformative priors for MM parameters introduce undesirable bias. Here, we\npropose how to infer parameters within the framework of stochastic inverse\nproblems (SIP), also termed data-consistent inversion, wherein the prior\ntargets only uncertainties that arise due to MM non-invertibility. To\ndemonstrate, we introduce new methods to solve SIP based on rejection sampling,\nMarkov chain Monte Carlo, and generative adversarial networks (GANs). In\naddition, to overcome limitations of SIP, we reformulate SIP based on\nconstrained optimization and present a novel GAN to solve the constrained\noptimization problem.","terms":["stat.ML","cs.LG"]},{"titles":"Metalearning generalizable dynamics from trajectories","summaries":"We present the interpretable meta neural ordinary differential equation\n(iMODE) method to rapidly learn generalizable (i.e., not parameter-specific)\ndynamics from trajectories of multiple dynamical systems that vary in their\nphysical parameters. The iMODE method learns meta-knowledge, the functional\nvariations of the force field of dynamical system instances without knowing the\nphysical parameters, by adopting a bi-level optimization framework: an outer\nlevel capturing the common force field form among studied dynamical system\ninstances and an inner level adapting to individual system instances. A priori\nphysical knowledge can be conveniently embedded in the neural network\narchitecture as inductive bias, such as conservative force field and Euclidean\nsymmetry. With the learned meta-knowledge, iMODE can model an unseen system\nwithin seconds, and inversely reveal knowledge on the physical parameters of a\nsystem, or as a Neural Gauge to \"measure\" the physical parameters of an unseen\nsystem with observed trajectories. We test the validity of the iMODE method on\nbistable, double pendulum, Van der Pol, Slinky, and reaction-diffusion systems.","terms":["cs.LG","physics.comp-ph"]},{"titles":"InvKA: Gait Recognition via Invertible Koopman Autoencoder","summaries":"Most current gait recognition methods suffer from poor interpretability and\nhigh computational cost. To improve interpretability, we investigate gait\nfeatures in the embedding space based on Koopman operator theory. The\ntransition matrix in this space captures complex kinematic features of gait\ncycles, namely the Koopman operator. The diagonal elements of the operator\nmatrix can represent the overall motion trend, providing a physically\nmeaningful descriptor. To reduce the computational cost of our algorithm, we\nuse a reversible autoencoder to reduce the model size and eliminate\nconvolutional layers to compress its depth, resulting in fewer floating-point\noperations. Experimental results on multiple datasets show that our method\nreduces computational cost to 1% compared to state-of-the-art methods while\nachieving competitive recognition accuracy 98% on non-occlusion datasets.","terms":["cs.CV"]},{"titles":"Physics-Based Rigid Body Object Tracking and Friction Filtering From RGB-D Videos","summaries":"Physics-based understanding of object interactions from sensory observations\nis an essential capability in augmented reality and robotics. It enables\ncapturing the properties of a scene for simulation and control. In this paper,\nwe propose a novel approach for real-to-sim which tracks rigid objects in 3D\nfrom RGB-D images and infers physical properties of the objects. We use a\ndifferentiable physics simulation as state-transition model in an Extended\nKalman Filter which can model contact and friction for arbitrary mesh-based\nshapes and in this way estimate physically plausible trajectories. We\ndemonstrate that our approach can filter position, orientation, velocities, and\nconcurrently can estimate the coefficient of friction of the objects. We\nanalyse our approach on various sliding scenarios in synthetic image sequences\nof single objects and colliding objects. We also demonstrate and evaluate our\napproach on a real-world dataset. We will make our novel benchmark datasets\npublicly available to foster future research in this novel problem setting and\ncomparison with our method.","terms":["cs.CV"]},{"titles":"Physics Inspired Hybrid Attention for SAR Target Recognition","summaries":"There has been a recent emphasis on integrating physical models and deep\nneural networks (DNNs) for SAR target recognition, to improve performance and\nachieve a higher level of physical interpretability. The attributed scattering\ncenter (ASC) parameters garnered the most interest, being considered as\nadditional input data or features for fusion in most methods. However, the\nperformance greatly depends on the ASC optimization result, and the fusion\nstrategy is not adaptable to different types of physical information.\nMeanwhile, the current evaluation scheme is inadequate to assess the model's\nrobustness and generalizability. Thus, we propose a physics inspired hybrid\nattention (PIHA) mechanism and the once-for-all (OFA) evaluation protocol to\naddress the above issues. PIHA leverages the high-level semantics of physical\ninformation to activate and guide the feature group aware of local semantics of\ntarget, so as to re-weight the feature importance based on knowledge prior. It\nis flexible and generally applicable to various physical models, and can be\nintegrated into arbitrary DNNs without modifying the original architecture. The\nexperiments involve a rigorous assessment using the proposed OFA, which entails\ntraining and validating a model on either sufficient or limited data and\nevaluating on multiple test sets with different data distributions. Our method\noutperforms other state-of-the-art approaches in 12 test scenarios with same\nASC parameters. Moreover, we analyze the working mechanism of PIHA and evaluate\nvarious PIHA enabled DNNs. The experiments also show PIHA is effective for\ndifferent physical information. The source code together with the adopted\nphysical information is available at https:\/\/github.com\/XAI4SAR.","terms":["cs.CV","eess.IV"]},{"titles":"OceanBench: The Sea Surface Height Edition","summaries":"The ocean profoundly influences human activities and plays a critical role in\nclimate regulation. Our understanding has improved over the last decades with\nthe advent of satellite remote sensing data, allowing us to capture essential\nquantities over the globe, e.g., sea surface height (SSH). However, ocean\nsatellite data presents challenges for information extraction due to their\nsparsity and irregular sampling, signal complexity, and noise. Machine learning\n(ML) techniques have demonstrated their capabilities in dealing with\nlarge-scale, complex signals. Therefore we see an opportunity for ML models to\nharness the information contained in ocean satellite data. However, data\nrepresentation and relevant evaluation metrics can be the defining factors when\ndetermining the success of applied ML. The processing steps from the raw\nobservation data to a ML-ready state and from model outputs to interpretable\nquantities require domain expertise, which can be a significant barrier to\nentry for ML researchers. OceanBench is a unifying framework that provides\nstandardized processing steps that comply with domain-expert standards. It\nprovides plug-and-play data and pre-configured pipelines for ML researchers to\nbenchmark their models and a transparent configurable framework for researchers\nto customize and extend the pipeline for their tasks. In this work, we\ndemonstrate the OceanBench framework through a first edition dedicated to SSH\ninterpolation challenges. We provide datasets and ML-ready benchmarking\npipelines for the long-standing problem of interpolating observations from\nsimulated ocean satellite data, multi-modal and multi-sensor fusion issues, and\ntransfer-learning to real ocean satellite observations. The OceanBench\nframework is available at github.com\/jejjohnson\/oceanbench and the dataset\nregistry is available at github.com\/quentinf00\/oceanbench-data-registry.","terms":["cs.LG","physics.ao-ph"]},{"titles":"NAI$_2$: Learning Noise-Aware Illuminance-Interpolator for Unsupervised Low-Light Image Enhancement","summaries":"Low-light situations severely restrict the pursuit of aesthetic quality in\nconsumer photography. Although many efforts are devoted to designing\nheuristics, it is generally mired in a shallow spiral of tedium, such as piling\nup complex network architectures and empirical strategies. How to delve into\nthe essential physical principles of illumination compensation has been\nneglected. Following the way of simplifying the complexity, this paper\ninnovatively proposes a simple and efficient Noise-Aware Illumination\nInterpolator (NAI$_2$). According to the constraint principle of illuminance\nand reflectance within a limited dynamic range, as a prior knowledge in the\nrecovery process, we construct a learnable illuminance interpolator and thereby\ncompensating for non-uniform lighting. With the intention of adapting denoising\nwithout annotated data, we design a self-calibrated denoiser with the intrinsic\nimage properties to acquire noiseless low-light images. Starting from the\nproperties of natural image manifolds, a self-regularized recovery loss is\nintroduced as a way to encourage more natural and realistic reflectance map.\n  The model architecture and training losses, guided by prior knowledge,\ncomplement and benefit each other, forming a powerful unsupervised leaning\nframework. Comprehensive experiments demonstrate that the proposed algorithm\nproduces competitive qualitative and quantitative results while maintaining\nfavorable generalization capability in unknown real-world scenarios.\n  The code will be available online upon publication of the paper.","terms":["cs.CV","cs.MM"]},{"titles":"HPL-ViT: A Unified Perception Framework for Heterogeneous Parallel LiDARs in V2V","summaries":"To develop the next generation of intelligent LiDARs, we propose a novel\nframework of parallel LiDARs and construct a hardware prototype in our\nexperimental platform, DAWN (Digital Artificial World for Natural). It\nemphasizes the tight integration of physical and digital space in LiDAR\nsystems, with networking being one of its supported core features. In the\ncontext of autonomous driving, V2V (Vehicle-to-Vehicle) technology enables\nefficient information sharing between different agents which significantly\npromotes the development of LiDAR networks. However, current research operates\nunder an ideal situation where all vehicles are equipped with identical LiDAR,\nignoring the diversity of LiDAR categories and operating frequencies. In this\npaper, we first utilize OpenCDA and RLS (Realistic LiDAR Simulation) to\nconstruct a novel heterogeneous LiDAR dataset named OPV2V-HPL. Additionally, we\npresent HPL-ViT, a pioneering architecture designed for robust feature fusion\nin heterogeneous and dynamic scenarios. It uses a graph-attention Transformer\nto extract domain-specific features for each agent, coupled with a\ncross-attention mechanism for the final fusion. Extensive experiments on\nOPV2V-HPL demonstrate that HPL-ViT achieves SOTA (state-of-the-art) performance\nin all settings and exhibits outstanding generalization capabilities.","terms":["cs.CV"]},{"titles":"Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation","summaries":"Automatic 3D content creation has achieved rapid progress recently due to the\navailability of pre-trained, large language models and image diffusion models,\nforming the emerging topic of text-to-3D content creation. Existing text-to-3D\nmethods commonly use implicit scene representations, which couple the geometry\nand appearance via volume rendering and are suboptimal in terms of recovering\nfiner geometries and achieving photorealistic rendering; consequently, they are\nless effective for generating high-quality 3D assets. In this work, we propose\na new method of Fantasia3D for high-quality text-to-3D content creation. Key to\nFantasia3D is the disentangled modeling and learning of geometry and\nappearance. For geometry learning, we rely on a hybrid scene representation,\nand propose to encode surface normal extracted from the representation as the\ninput of the image diffusion model. For appearance modeling, we introduce the\nspatially varying bidirectional reflectance distribution function (BRDF) into\nthe text-to-3D task, and learn the surface material for photorealistic\nrendering of the generated surface. Our disentangled framework is more\ncompatible with popular graphics engines, supporting relighting, editing, and\nphysical simulation of the generated 3D assets. We conduct thorough experiments\nthat show the advantages of our method over existing ones under different\ntext-to-3D task settings. Project page and source codes:\nhttps:\/\/fantasia3d.github.io\/.","terms":["cs.CV","cs.AI"]},{"titles":"Defending Against Physical Adversarial Patch Attacks on Infrared Human Detection","summaries":"Infrared detection is an emerging technique for safety-critical tasks owing\nto its remarkable anti-interference capability. However, recent studies have\nrevealed that it is vulnerable to physically-realizable adversarial patches,\nposing risks in its real-world applications. To address this problem, we are\nthe first to investigate defense strategies against adversarial patch attacks\non infrared detection, especially human detection. We have devised a\nstraightforward defense strategy, patch-based occlusion-aware detection (POD),\nwhich efficiently augments training samples with random patches and\nsubsequently detects them. POD not only robustly detects people but also\nidentifies adversarial patch locations. Surprisingly, while being extremely\ncomputationally efficient, POD easily generalizes to state-of-the-art\nadversarial patch attacks that are unseen during training. Furthermore, POD\nimproves detection precision even in a clean (i.e., no-patch) situation due to\nthe data augmentation effect. Evaluation demonstrated that POD is robust to\nadversarial patches of various shapes and sizes. The effectiveness of our\nbaseline approach is shown to be a viable defense mechanism for real-world\ninfrared human detection systems, paving the way for exploring future research\ndirections.","terms":["cs.CV"]},{"titles":"Accelerating Training Time with Feature Enforcing- Physics Informed Neural Network (FE-PINN): Utilizing Boundary Conditions as Prior Knowledge for Faster Convergence","summaries":"In this study, Feature Enforcing Physics Informed Neural Network FEPINN is\nintroduced, which is a data free framework that enables a neural network to\nlearn boundary conditions before solving continuity and momentum partial\ndifferential equations PDEs. A new sampling approach is represented, which\nselects domain points as a function of different initial weight states of\nneural networks initialized with the Xavier scheme. The study hypothesizes that\nusing an initial weight state with lower variance is beneficial when learning\nsimple patterns, such as boundary conditions. After training the neural network\nto learn boundary conditions, the complexity of the model is increased by\nadding new layers on top of the neural network. FE-PINN is used to solve two\nbenchmarks, 2D flow over a cylinder and an inverse problem of determining the\ninlet velocity for a 2D flow over a cylinder. It is found that FE-PINN trains\nabout two times faster than vanilla PINN for both benchmarks, even when the\nvanilla PINN uses optimal weight values in the loss function obtained by random\nsearch. Moreover, FE-PINN's loss function is balanced due to its prior\nknowledge of boundary conditions, eliminating the need for loss weighting. For\ninstance, learning no slip boundary conditions on side walls with FE-PINN\nbefore the main training loop is 574 times faster than loss weighting process\nin vanilla PINN. In conclusion, FE-PINN offers a fast and accurate tool for\nsolving various PDEs across different fields.","terms":["cs.LG"]},{"titles":"SimPINNs: Simulation-Driven Physics-Informed Neural Networks for Enhanced Performance in Nonlinear Inverse Problems","summaries":"This paper introduces a novel approach to solve inverse problems by\nleveraging deep learning techniques. The objective is to infer unknown\nparameters that govern a physical system based on observed data. We focus on\nscenarios where the underlying forward model demonstrates pronounced nonlinear\nbehaviour, and where the dimensionality of the unknown parameter space is\nsubstantially smaller than that of the observations. Our proposed method builds\nupon physics-informed neural networks (PINNs) trained with a hybrid loss\nfunction that combines observed data with simulated data generated by a known\n(approximate) physical model. Experimental results on an orbit restitution\nproblem demonstrate that our approach surpasses the performance of standard\nPINNs, providing improved accuracy and robustness.","terms":["cs.LG","cs.AI"]},{"titles":"Text-to-Image Generation for Abstract Concepts","summaries":"Recent years have witnessed the substantial progress of large-scale models\nacross various domains, such as natural language processing and computer\nvision, facilitating the expression of concrete concepts. Unlike concrete\nconcepts that are usually directly associated with physical objects, expressing\nabstract concepts through natural language requires considerable effort, which\nresults from their intricate semantics and connotations. An alternative\napproach is to leverage images to convey rich visual information as a\nsupplement. Nevertheless, existing Text-to-Image (T2I) models are primarily\ntrained on concrete physical objects and tend to fail to visualize abstract\nconcepts. Inspired by the three-layer artwork theory that identifies critical\nfactors, intent, object and form during artistic creation, we propose a\nframework of Text-to-Image generation for Abstract Concepts (TIAC). The\nabstract concept is clarified into a clear intent with a detailed definition to\navoid ambiguity. LLMs then transform it into semantic-related physical objects,\nand the concept-dependent form is retrieved from an LLM-extracted form pattern\nset. Information from these three aspects will be integrated to generate\nprompts for T2I models via LLM. Evaluation results from human assessments and\nour newly designed metric concept score demonstrate the effectiveness of our\nframework in creating images that can sufficiently express abstract concepts.","terms":["cs.CV"]},{"titles":"Subjective Face Transform using Human First Impressions","summaries":"Humans tend to form quick subjective first impressions of non-physical\nattributes when seeing someone's face, such as perceived trustworthiness or\nattractiveness. To understand what variations in a face lead to different\nsubjective impressions, this work uses generative models to find semantically\nmeaningful edits to a face image that change perceived attributes. Unlike prior\nwork that relied on statistical manipulation in feature space, our end-to-end\nframework considers trade-offs between preserving identity and changing\nperceptual attributes. It maps identity-preserving latent space directions to\nchanges in attribute scores, enabling transformation of any input face along an\nattribute axis according to a target change. We train on real and synthetic\nfaces, evaluate for in-domain and out-of-domain images using predictive models\nand human ratings, demonstrating the generalizability of our approach.\nUltimately, such a framework can be used to understand and explain biases in\nsubjective interpretation of faces that are not dependent on the identity.","terms":["cs.CV"]},{"titles":"C3Net: interatomic potential neural network for prediction of physicochemical properties in heterogenous systems","summaries":"Understanding the interactions of a solute with its environment is of\nfundamental importance in chemistry and biology. In this work, we propose a\ndeep neural network architecture for atom type embeddings in its molecular\ncontext and interatomic potential that follows fundamental physical laws. The\narchitecture is applied to predict physicochemical properties in heterogeneous\nsystems including solvation in diverse solvents, 1-octanol-water partitioning,\nand PAMPA with a single set of network weights. We show that our architecture\nis generalized well to the physicochemical properties and outperforms\nstate-of-the-art approaches based on quantum mechanics and neural networks in\nthe task of solvation free energy prediction. The interatomic potentials at\neach atom in a solute obtained from the model allow quantitative analysis of\nthe physicochemical properties at atomic resolution consistent with chemical\nand physical reasoning. The software is available at\nhttps:\/\/github.com\/SehanLee\/C3Net.","terms":["cs.LG","cs.AI"]},{"titles":"A Physics Enhanced Residual Learning (PERL) Framework for Traffic State Prediction","summaries":"In vehicle trajectory prediction, physics models and data-driven models are\ntwo predominant methodologies. However, each approach presents its own set of\nchallenges: physics models fall short in predictability, while data-driven\nmodels lack interpretability. Addressing these identified shortcomings, this\npaper proposes a novel framework, the Physics-Enhanced Residual Learning (PERL)\nmodel. PERL integrates the strengths of physics-based and data-driven methods\nfor traffic state prediction. PERL contains a physics model and a residual\nlearning model. Its prediction is the sum of the physics model result and a\npredicted residual as a correction to it. It preserves the interpretability\ninherent to physics-based models and has reduced data requirements compared to\ndata-driven methods. Experiments were conducted using a real-world vehicle\ntrajectory dataset. We proposed a PERL model, with the Intelligent Driver Model\n(IDM) as its physics car-following model and Long Short-Term Memory (LSTM) as\nits residual learning model. We compare this PERL model with the physics\ncar-following model, data-driven model, and other physics-informed neural\nnetwork (PINN) models. The result reveals that PERL achieves better prediction\nwith a small dataset, compared to the physics model, data-driven model, and\nPINN model. Second, the PERL model showed faster convergence during training,\noffering comparable performance with fewer training samples than the\ndata-driven model and PINN model. Sensitivity analysis also proves comparable\nperformance of PERL using another residual learning model and a physics\ncar-following model.","terms":["cs.LG"]},{"titles":"DECO: Dense Estimation of 3D Human-Scene Contact In The Wild","summaries":"Understanding how humans use physical contact to interact with the world is\nkey to enabling human-centric artificial intelligence. While inferring 3D\ncontact is crucial for modeling realistic and physically-plausible human-object\ninteractions, existing methods either focus on 2D, consider body joints rather\nthan the surface, use coarse 3D body regions, or do not generalize to\nin-the-wild images. In contrast, we focus on inferring dense, 3D contact\nbetween the full body surface and objects in arbitrary images. To achieve this,\nwe first collect DAMON, a new dataset containing dense vertex-level contact\nannotations paired with RGB images containing complex human-object and\nhuman-scene contact. Second, we train DECO, a novel 3D contact detector that\nuses both body-part-driven and scene-context-driven attention to estimate\nvertex-level contact on the SMPL body. DECO builds on the insight that human\nobservers recognize contact by reasoning about the contacting body parts, their\nproximity to scene objects, and the surrounding scene context. We perform\nextensive evaluations of our detector on DAMON as well as on the RICH and\nBEHAVE datasets. We significantly outperform existing SOTA methods across all\nbenchmarks. We also show qualitatively that DECO generalizes well to diverse\nand challenging real-world human interactions in natural images. The code,\ndata, and models are available at https:\/\/deco.is.tue.mpg.de.","terms":["cs.CV"]},{"titles":"Identifying Simulation Model Through Alternative Techniques for a Medical Device Assembly Process","summaries":"This scientific paper explores two distinct approaches for identifying and\napproximating the simulation model, particularly in the context of the snap\nprocess crucial to medical device assembly. Simulation models play a pivotal\nrole in providing engineers with insights into industrial processes, enabling\nexperimentation and troubleshooting before physical assembly. However, their\ncomplexity often results in time-consuming computations.\n  To mitigate this complexity, we present two distinct methods for identifying\nsimulation models: one utilizing Spline functions and the other harnessing\nMachine Learning (ML) models. Our goal is to create adaptable models that\naccurately represent the snap process and can accommodate diverse scenarios.\nSuch models hold promise for enhancing process understanding and aiding in\ndecision-making, especially when data availability is limited.","terms":["cs.LG"]},{"titles":"PINF: Continuous Normalizing Flows for Physics-Constrained Deep Learning","summaries":"The normalization constraint on probability density poses a significant\nchallenge for solving the Fokker-Planck equation. Normalizing Flow, an\ninvertible generative model leverages the change of variables formula to ensure\nprobability density conservation and enable the learning of complex data\ndistributions. In this paper, we introduce Physics-Informed Normalizing Flows\n(PINF), a novel extension of continuous normalizing flows, incorporating\ndiffusion through the method of characteristics. Our method, which is mesh-free\nand causality-free, can efficiently solve high dimensional time-dependent and\nsteady-state Fokker-Planck equations.","terms":["cs.LG","cs.AI"]},{"titles":"Scene-aware Egocentric 3D Human Pose Estimation","summaries":"Egocentric 3D human pose estimation with a single head-mounted fisheye camera\nhas recently attracted attention due to its numerous applications in virtual\nand augmented reality. Existing methods still struggle in challenging poses\nwhere the human body is highly occluded or is closely interacting with the\nscene. To address this issue, we propose a scene-aware egocentric pose\nestimation method that guides the prediction of the egocentric pose with scene\nconstraints. To this end, we propose an egocentric depth estimation network to\npredict the scene depth map from a wide-view egocentric fisheye camera while\nmitigating the occlusion of the human body with a depth-inpainting network.\nNext, we propose a scene-aware pose estimation network that projects the 2D\nimage features and estimated depth map of the scene into a voxel space and\nregresses the 3D pose with a V2V network. The voxel-based feature\nrepresentation provides the direct geometric connection between 2D image\nfeatures and scene geometry, and further facilitates the V2V network to\nconstrain the predicted pose based on the estimated scene geometry. To enable\nthe training of the aforementioned networks, we also generated a synthetic\ndataset, called EgoGTA, and an in-the-wild dataset based on EgoPW, called\nEgoPW-Scene. The experimental results of our new evaluation sequences show that\nthe predicted 3D egocentric poses are accurate and physically plausible in\nterms of human-scene interaction, demonstrating that our method outperforms the\nstate-of-the-art methods both quantitatively and qualitatively.","terms":["cs.CV"]},{"titles":"Uncertainty Aware Deep Learning for Particle Accelerators","summaries":"Standard deep learning models for classification and regression applications\nare ideal for capturing complex system dynamics. However, their predictions can\nbe arbitrarily inaccurate when the input samples are not similar to the\ntraining data. Implementation of distance aware uncertainty estimation can be\nused to detect these scenarios and provide a level of confidence associated\nwith their predictions. In this paper, we present results from using Deep\nGaussian Process Approximation (DGPA) methods for errant beam prediction at\nSpallation Neutron Source (SNS) accelerator (classification) and we provide an\nuncertainty aware surrogate model for the Fermi National Accelerator Lab (FNAL)\nBooster Accelerator Complex (regression).","terms":["cs.LG","physics.acc-ph"]},{"titles":"ControlMat: A Controlled Generative Approach to Material Capture","summaries":"Material reconstruction from a photograph is a key component of 3D content\ncreation democratization. We propose to formulate this ill-posed problem as a\ncontrolled synthesis one, leveraging the recent progress in generative deep\nnetworks. We present ControlMat, a method which, given a single photograph with\nuncontrolled illumination as input, conditions a diffusion model to generate\nplausible, tileable, high-resolution physically-based digital materials. We\ncarefully analyze the behavior of diffusion models for multi-channel outputs,\nadapt the sampling process to fuse multi-scale information and introduce rolled\ndiffusion to enable both tileability and patched diffusion for high-resolution\noutputs. Our generative approach further permits exploration of a variety of\nmaterials which could correspond to the input image, mitigating the unknown\nlighting conditions. We show that our approach outperforms recent inference and\nlatent-space-optimization methods, and carefully validate our diffusion process\ndesign choices. Supplemental materials and additional details are available at:\nhttps:\/\/gvecchio.com\/controlmat\/.","terms":["cs.CV","cs.GR"]},{"titles":"Efficient Finite Initialization for Tensorized Neural Networks","summaries":"We present a novel method for initializing layers of tensorized neural\nnetworks in a way that avoids the explosion of the parameters of the matrix it\nemulates. The method is intended for layers with a high number of nodes in\nwhich there is a connection to the input or output of all or most of the nodes.\nThe core of this method is the use of the Frobenius norm of this layer in an\niterative partial form, so that it has to be finite and within a certain range.\nThis norm is efficient to compute, fully or partially for most cases of\ninterest. We apply the method to different layers and check its performance. We\ncreate a Python function to run it on an arbitrary layer, available in a\nJupyter Notebook in the i3BQuantum repository:\nhttps:\/\/github.com\/i3BQuantumTeam\/Q4Real\/blob\/e07c827651ef16bcf74590ab965ea3985143f891\/Quantum-Inspired%20Variational%20Methods\/Normalization_process.ipynb","terms":["cs.LG","quant-ph","68Q12, 15A69, 68T07"]},{"titles":"p$^3$VAE: a physics-integrated generative model. Application to the pixel-wise classification of airborne hyperspectral images","summaries":"The combination of machine learning models with physical models is a recent\nresearch path to learn robust data representations. In this paper, we introduce\np$^3$VAE, a generative model that integrates a physical model which\ndeterministically models some of the true underlying factors of variation in\nthe data. To fully leverage our hybrid design, we enhance an existing\nsemi-supervised optimization technique and introduce a new inference scheme\nthat comes along meaningful uncertainty estimates. We apply p$^3$VAE to the\npixel-wise classification of airborne hyperspectral images. Our experiments on\nsimulated and real data demonstrate the benefits of our hybrid model against\nconventional machine learning models in terms of extrapolation capabilities and\ninterpretability. In particular, we show that p$^3$VAE naturally has high\ndisentanglement capabilities. Our code and data have been made publicly\navailable at https:\/\/github.com\/Romain3Ch216\/p3VAE.","terms":["cs.CV","stat.ML","68T45","I.2.6; I.2.10"]},{"titles":"Geometric structure of Deep Learning networks and construction of global ${\\mathcal L}^2$ minimizers","summaries":"In this paper, we provide a geometric interpretation of the structure of Deep\nLearning (DL) networks, characterized by $L$ hidden layers, a ramp activation\nfunction, an ${\\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost\nfunction, and input and output spaces ${\\mathbb R}^Q$ with equal dimension\n$Q\\geq1$. The hidden layers are defined on spaces ${\\mathbb R}^{Q}$, as well.\nWe apply our recent results on shallow neural networks to construct an explicit\nfamily of minimizers for the global minimum of the cost function in the case\n$L\\geq Q$, which we show to be degenerate. In the context presented here, the\nhidden layers of the DL network \"curate\" the training inputs by recursive\napplication of a truncation map that minimizes the noise to signal ratio of the\ntraining inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate\nlocal minima of the cost function.","terms":["cs.LG","cs.AI","math-ph","math.MP","math.OC","stat.ML","57R70, 62M45"]},{"titles":"SuPerPM: A Large Deformation-Robust Surgical Perception Framework Based on Deep Point Matching Learned from Physical Constrained Simulation Data","summaries":"Manipulation of tissue with surgical tools often results in large\ndeformations that current methods in tracking and reconstructing algorithms\nhave not effectively addressed. A major source of tracking errors during large\ndeformations stems from wrong data association between observed sensor\nmeasurements with previously tracked scene. To mitigate this issue, we present\na surgical perception framework, SuPerPM, that leverages learning-based\nnon-rigid point cloud matching for data association, thus accommodating larger\ndeformations. The learning models typically require training data with ground\ntruth point cloud correspondences, which is challenging or even impractical to\ncollect in surgical environments. Thus, for tuning the learning model, we\ngather endoscopic data of soft tissue being manipulated by a surgical robot and\nthen establish correspondences between point clouds at different time points to\nserve as ground truth. This was achieved by employing a position-based dynamics\n(PBD) simulation to ensure that the correspondences adhered to physical\nconstraints. The proposed framework is demonstrated on several challenging\nsurgical datasets that are characterized by large deformations, achieving\nsuperior performance over state-of-the-art surgical scene tracking algorithms.","terms":["cs.CV"]},{"titles":"LineMarkNet: Line Landmark Detection for Valet Parking","summaries":"We aim for accurate and efficient line landmark detection for valet parking,\nwhich is a long-standing yet unsolved problem in autonomous driving. To this\nend, we present a deep line landmark detection system where we carefully design\nthe modules to be lightweight. Specifically, we first empirically design four\ngeneral line landmarks including three physical lines and one novel mental\nline. The four line landmarks are effective for valet parking. We then develop\na deep network (LineMarkNet) to detect line landmarks from surround-view\ncameras where we, via the pre-calibrated homography, fuse context from four\nseparate cameras into the unified bird-eye-view (BEV) space, specifically we\nfuse the surroundview features and BEV features, then employ the multi-task\ndecoder to detect multiple line landmarks where we apply the center-based\nstrategy for object detection task, and design our graph transformer to enhance\nthe vision transformer with hierarchical level graph reasoning for semantic\nsegmentation task. At last, we further parameterize the detected line landmarks\n(e.g., intercept-slope form) whereby a novel filtering backend incorporates\ntemporal and multi-view consistency to achieve smooth and stable detection.\nMoreover, we annotate a large-scale dataset to validate our method.\nExperimental results show that our framework achieves the enhanced performance\ncompared with several line detection methods and validate the multi-task\nnetwork's efficiency about the real-time line landmark detection on the\nQualcomm 820A platform while meantime keeps superior accuracy, with our deep\nline landmark detection system.","terms":["cs.CV"]},{"titles":"NDDepth: Normal-Distance Assisted Monocular Depth Estimation","summaries":"Monocular depth estimation has drawn widespread attention from the vision\ncommunity due to its broad applications. In this paper, we propose a novel\nphysics (geometry)-driven deep learning framework for monocular depth\nestimation by assuming that 3D scenes are constituted by piece-wise planes.\nParticularly, we introduce a new normal-distance head that outputs pixel-level\nsurface normal and plane-to-origin distance for deriving depth at each\nposition. Meanwhile, the normal and distance are regularized by a developed\nplane-aware consistency constraint. We further integrate an additional depth\nhead to improve the robustness of the proposed framework. To fully exploit the\nstrengths of these two heads, we develop an effective contrastive iterative\nrefinement module that refines depth in a complementary manner according to the\ndepth uncertainty. Extensive experiments indicate that the proposed method\nexceeds previous state-of-the-art competitors on the NYU-Depth-v2, KITTI and\nSUN RGB-D datasets. Notably, it ranks 1st among all submissions on the KITTI\ndepth prediction online benchmark at the submission time.","terms":["cs.CV"]},{"titles":"Identifying Systematic Errors in Object Detectors with the SCROD Pipeline","summaries":"The identification and removal of systematic errors in object detectors can\nbe a prerequisite for their deployment in safety-critical applications like\nautomated driving and robotics. Such systematic errors can for instance occur\nunder very specific object poses (location, scale, orientation), object\ncolors\/textures, and backgrounds. Real images alone are unlikely to cover all\nrelevant combinations. We overcome this limitation by generating synthetic\nimages with fine-granular control. While generating synthetic images with\nphysical simulators and hand-designed 3D assets allows fine-grained control\nover generated images, this approach is resource-intensive and has limited\nscalability. In contrast, using generative models is more scalable but less\nreliable in terms of fine-grained control. In this paper, we propose a novel\nframework that combines the strengths of both approaches. Our meticulously\ndesigned pipeline along with custom models enables us to generate street scenes\nwith fine-grained control in a fully automated and scalable manner. Moreover,\nour framework introduces an evaluation setting that can serve as a benchmark\nfor similar pipelines. This evaluation setting will contribute to advancing the\nfield and promoting standardized testing procedures.","terms":["cs.CV"]},{"titles":"A Graph-Based Modeling Framework for Tracing Hydrological Pollutant Transport in Surface Waters","summaries":"Anthropogenic pollution of hydrological systems affects diverse communities\nand ecosystems around the world. Data analytics and modeling tools play a key\nrole in fighting this challenge, as they can help identify key sources as well\nas trace transport and quantify impact within complex hydrological systems.\nSeveral tools exist for simulating and tracing pollutant transport throughout\nsurface waters using detailed physical models; these tools are powerful, but\ncan be computationally intensive, require significant amounts of data to be\ndeveloped, and require expert knowledge for their use (ultimately limiting\napplication scope). In this work, we present a graph modeling framework --\nwhich we call ${\\tt HydroGraphs}$ -- for understanding pollutant transport and\nfate across waterbodies, rivers, and watersheds. This framework uses a\nsimplified representation of hydrological systems that can be constructed based\npurely on open-source data (National Hydrography Dataset and Watershed Boundary\nDataset). The graph representation provides an flexible intuitive approach for\ncapturing connectivity and for identifying upstream pollutant sources and for\ntracing downstream impacts within small and large hydrological systems.\nMoreover, the graph representation can facilitate the use of advanced\nalgorithms and tools of graph theory, topology, optimization, and machine\nlearning to aid data analytics and decision-making. We demonstrate the\ncapabilities of our framework by using case studies in the State of Wisconsin;\nhere, we aim to identify upstream nutrient pollutant sources that arise from\nagricultural practices and trace downstream impacts to waterbodies, rivers, and\nstreams. Our tool ultimately seeks to help stakeholders design effective\npollution prevention\/mitigation practices and evaluate how surface waters\nrespond to such practices.","terms":["cs.LG"]},{"titles":"Evidential Deep Learning: Enhancing Predictive Uncertainty Estimation for Earth System Science Applications","summaries":"Robust quantification of predictive uncertainty is critical for understanding\nfactors that drive weather and climate outcomes. Ensembles provide predictive\nuncertainty estimates and can be decomposed physically, but both physics and\nmachine learning ensembles are computationally expensive. Parametric deep\nlearning can estimate uncertainty with one model by predicting the parameters\nof a probability distribution but do not account for epistemic uncertainty..\nEvidential deep learning, a technique that extends parametric deep learning to\nhigher-order distributions, can account for both aleatoric and epistemic\nuncertainty with one model. This study compares the uncertainty derived from\nevidential neural networks to those obtained from ensembles. Through\napplications of classification of winter precipitation type and regression of\nsurface layer fluxes, we show evidential deep learning models attaining\npredictive accuracy rivaling standard methods, while robustly quantifying both\nsources of uncertainty. We evaluate the uncertainty in terms of how well the\npredictions are calibrated and how well the uncertainty correlates with\nprediction error. Analyses of uncertainty in the context of the inputs reveal\nsensitivities to underlying meteorological processes, facilitating\ninterpretation of the models. The conceptual simplicity, interpretability, and\ncomputational efficiency of evidential neural networks make them highly\nextensible, offering a promising approach for reliable and practical\nuncertainty quantification in Earth system science modeling. In order to\nencourage broader adoption of evidential deep learning in Earth System Science,\nwe have developed a new Python package, MILES-GUESS\n(https:\/\/github.com\/ai2es\/miles-guess), that enables users to train and\nevaluate both evidential and ensemble deep learning.","terms":["cs.LG"]},{"titles":"Enhancing Multi-Objective Optimization through Machine Learning-Supported Multiphysics Simulation","summaries":"Multiphysics simulations that involve multiple coupled physical phenomena\nquickly become computationally expensive. This imposes challenges for\npractitioners aiming to find optimal configurations for these problems\nsatisfying multiple objectives, as optimization algorithms often require\nquerying the simulation many times. This paper presents a methodological\nframework for training, self-optimizing, and self-organizing surrogate models\nto approximate and speed up Multiphysics simulations. We generate two\nreal-world tabular datasets, which we make publicly available, and show that\nsurrogate models can be trained on relatively small amounts of data to\napproximate the underlying simulations accurately. We conduct extensive\nexperiments combining four machine learning and deep learning algorithms with\ntwo optimization algorithms and a comprehensive evaluation strategy. Finally,\nwe evaluate the performance of our combined training and optimization pipeline\nby verifying the generated Pareto-optimal results using the ground truth\nsimulations. We also employ explainable AI techniques to analyse our surrogates\nand conduct a preselection strategy to determine the most relevant features in\nour real-world examples. This approach lets us understand the underlying\nproblem and identify critical partial dependencies.","terms":["cs.LG","math.OC"]},{"titles":"Pixel-wise Smoothing for Certified Robustness against Camera Motion Perturbations","summaries":"In recent years, computer vision has made remarkable advancements in\nautonomous driving and robotics. However, it has been observed that deep\nlearning-based visual perception models lack robustness when faced with camera\nmotion perturbations. The current certification process for assessing\nrobustness is costly and time-consuming due to the extensive number of image\nprojections required for Monte Carlo sampling in the 3D camera motion space. To\naddress these challenges, we present a novel, efficient, and practical\nframework for certifying the robustness of 3D-2D projective transformations\nagainst camera motion perturbations. Our approach leverages a smoothing\ndistribution over the 2D pixel space instead of in the 3D physical space,\neliminating the need for costly camera motion sampling and significantly\nenhancing the efficiency of robustness certifications. With the pixel-wise\nsmoothed classifier, we are able to fully upper bound the projection errors\nusing a technique of uniform partitioning in camera motion space. Additionally,\nwe extend our certification framework to a more general scenario where only a\nsingle-frame point cloud is required in the projection oracle. This is achieved\nby deriving Lipschitz-based approximated partition intervals. Through extensive\nexperimentation, we validate the trade-off between effectiveness and efficiency\nenabled by our proposed method. Remarkably, our approach achieves approximately\n80% certified accuracy while utilizing only 30% of the projected image frames.","terms":["cs.LG","cs.CV","cs.RO"]},{"titles":"Higher-order Graph Convolutional Network with Flower-Petals Laplacians on Simplicial Complexes","summaries":"Despite the recent successes of vanilla Graph Neural Networks (GNNs) on many\ntasks, their foundation on pairwise interaction networks inherently limits\ntheir capacity to discern latent higher-order interactions in complex systems.\nTo bridge this capability gap, we propose a novel approach exploiting the rich\nmathematical theory of simplicial complexes (SCs) - a robust tool for modeling\nhigher-order interactions. Current SC-based GNNs are burdened by high\ncomplexity and rigidity, and quantifying higher-order interaction strengths\nremains challenging. Innovatively, we present a higher-order Flower-Petals (FP)\nmodel, incorporating FP Laplacians into SCs. Further, we introduce a\nHigher-order Graph Convolutional Network (HiGCN) grounded in FP Laplacians,\ncapable of discerning intrinsic features across varying topological scales. By\nemploying learnable graph filters, a parameter group within each FP Laplacian\ndomain, we can identify diverse patterns where the filters' weights serve as a\nquantifiable measure of higher-order interaction strengths. The theoretical\nunderpinnings of HiGCN's advanced expressiveness are rigorously demonstrated.\nAdditionally, our empirical investigations reveal that the proposed model\naccomplishes state-of-the-art (SOTA) performance on a range of graph tasks and\nprovides a scalable and flexible solution to explore higher-order interactions\nin graphs.","terms":["cs.LG","cond-mat.stat-mech","cs.AI","cs.SI","physics.soc-ph"]},{"titles":"Temporal Subsampling Diminishes Small Spatial Scales in Recurrent Neural Network Emulators of Geophysical Turbulence","summaries":"The immense computational cost of traditional numerical weather and climate\nmodels has sparked the development of machine learning (ML) based emulators.\nBecause ML methods benefit from long records of training data, it is common to\nuse datasets that are temporally subsampled relative to the time steps required\nfor the numerical integration of differential equations. Here, we investigate\nhow this often overlooked processing step affects the quality of an emulator's\npredictions. We implement two ML architectures from a class of methods called\nreservoir computing: (1) a form of Nonlinear Vector Autoregression (NVAR), and\n(2) an Echo State Network (ESN). Despite their simplicity, it is well\ndocumented that these architectures excel at predicting low dimensional chaotic\ndynamics. We are therefore motivated to test these architectures in an\nidealized setting of predicting high dimensional geophysical turbulence as\nrepresented by Surface Quasi-Geostrophic dynamics. In all cases, subsampling\nthe training data consistently leads to an increased bias at small spatial\nscales that resembles numerical diffusion. Interestingly, the NVAR architecture\nbecomes unstable when the temporal resolution is increased, indicating that the\npolynomial based interactions are insufficient at capturing the detailed\nnonlinearities of the turbulent flow. The ESN architecture is found to be more\nrobust, suggesting a benefit to the more expensive but more general structure.\nSpectral errors are reduced by including a penalty on the kinetic energy\ndensity spectrum during training, although the subsampling related errors\npersist. Future work is warranted to understand how the temporal resolution of\ntraining data affects other ML architectures.","terms":["cs.LG","physics.ao-ph","physics.flu-dyn"]},{"titles":"Active Stereo Without Pattern Projector","summaries":"This paper proposes a novel framework integrating the principles of active\nstereo in standard passive camera systems without a physical pattern projector.\nWe virtually project a pattern over the left and right images according to the\nsparse measurements obtained from a depth sensor. Any such devices can be\nseamlessly plugged into our framework, allowing for the deployment of a virtual\nactive stereo setup in any possible environment, overcoming the limitation of\npattern projectors, such as limited working range or environmental conditions.\nExperiments on indoor\/outdoor datasets, featuring both long and close-range,\nsupport the seamless effectiveness of our approach, boosting the accuracy of\nboth stereo algorithms and deep networks.","terms":["cs.CV"]},{"titles":"Physics-informed State-space Neural Networks for Transport Phenomena","summaries":"This work introduces Physics-informed State-space neural network Models\n(PSMs), a novel solution to achieving real-time optimization, flexibility, and\nfault tolerance in autonomous systems, particularly in transport-dominated\nsystems such as chemical, biomedical, and power plants. Traditional data-driven\nmethods fall short due to a lack of physical constraints like mass\nconservation; PSMs address this issue by training deep neural networks with\nsensor data and physics-informing using components' Partial Differential\nEquations (PDEs), resulting in a physics-constrained, end-to-end differentiable\nforward dynamics model. Through two in silico experiments - a heated channel\nand a cooling system loop - we demonstrate that PSMs offer a more accurate\napproach than purely data-driven models.\n  Beyond accuracy, there are several compelling use cases for PSMs. In this\nwork, we showcase two: the creation of a nonlinear supervisory controller\nthrough a sequentially updated state-space representation and the proposal of a\ndiagnostic algorithm using residuals from each of the PDEs. The former\ndemonstrates the ability of PSMs to handle both constant and time-dependent\nconstraints, while the latter illustrates their value in system diagnostics and\nfault detection. We further posit that PSMs could serve as a foundation for\nDigital Twins, constantly updated digital representations of physical systems.","terms":["cs.LG","cs.SY","eess.SY","physics.comp-ph","physics.flu-dyn"]},{"titles":"Semantic-aware Transmission Scheduling: a Monotonicity-driven Deep Reinforcement Learning Approach","summaries":"For cyber-physical systems in the 6G era, semantic communications connecting\ndistributed devices for dynamic control and remote state estimation are\nrequired to guarantee application-level performance, not merely focus on\ncommunication-centric performance. Semantics here is a measure of the\nusefulness of information transmissions. Semantic-aware transmission scheduling\nof a large system often involves a large decision-making space, and the optimal\npolicy cannot be obtained by existing algorithms effectively. In this paper, we\nfirst investigate the fundamental properties of the optimal semantic-aware\nscheduling policy and then develop advanced deep reinforcement learning (DRL)\nalgorithms by leveraging the theoretical guidelines. Our numerical results show\nthat the proposed algorithms can substantially reduce training time and enhance\ntraining performance compared to benchmark algorithms.","terms":["cs.LG","cs.AI","cs.IT","cs.SY","eess.SP","eess.SY","math.IT"]},{"titles":"Online Self-Concordant and Relatively Smooth Minimization, With Applications to Online Portfolio Selection and Learning Quantum States","summaries":"Consider an online convex optimization problem where the loss functions are\nself-concordant barriers, smooth relative to a convex function $h$, and\npossibly non-Lipschitz. We analyze the regret of online mirror descent with\n$h$. Then, based on the result, we prove the following in a unified manner.\nDenote by $T$ the time horizon and $d$ the parameter dimension. 1. For online\nportfolio selection, the regret of $\\widetilde{\\text{EG}}$, a variant of\nexponentiated gradient due to Helmbold et al., is $\\tilde{O} ( T^{2\/3} d^{1\/3}\n)$ when $T > 4 d \/ \\log d$. This improves on the original $\\tilde{O} ( T^{3\/4}\nd^{1\/2} )$ regret bound for $\\widetilde{\\text{EG}}$. 2. For online portfolio\nselection, the regret of online mirror descent with the logarithmic barrier is\n$\\tilde{O}(\\sqrt{T d})$. The regret bound is the same as that of Soft-Bayes due\nto Orseau et al. up to logarithmic terms. 3. For online learning quantum states\nwith the logarithmic loss, the regret of online mirror descent with the\nlog-determinant function is also $\\tilde{O} ( \\sqrt{T d} )$. Its per-iteration\ntime is shorter than all existing algorithms we know.","terms":["stat.ML","cs.LG","math.OC","q-fin.PM","quant-ph"]},{"titles":"Stochastic stiffness identification and response estimation of Timoshenko beams via physics-informed Gaussian processes","summaries":"Machine learning models trained with structural health monitoring data have\nbecome a powerful tool for system identification. This paper presents a\nphysics-informed Gaussian process (GP) model for Timoshenko beam elements. The\nmodel is constructed as a multi-output GP with covariance and cross-covariance\nkernels analytically derived based on the differential equations for\ndeflections, rotations, strains, bending moments, shear forces and applied\nloads. Stiffness identification is performed in a Bayesian format by maximising\na posterior model through a Markov chain Monte Carlo method, yielding a\nstochastic model for the structural parameters. The optimised GP model is\nfurther employed for probabilistic predictions of unobserved responses.\nAdditionally, an entropy-based method for physics-informed sensor placement\noptimisation is presented, exploiting heterogeneous sensor position information\nand structural boundary conditions built into the GP model. Results demonstrate\nthat the proposed approach is effective at identifying structural parameters\nand is capable of fusing data from heterogeneous and multi-fidelity sensors.\nProbabilistic predictions of structural responses and internal forces are in\ncloser agreement with measured data. We validate our model with an experimental\nsetup and discuss the quality and uncertainty of the obtained results. The\nproposed approach has potential applications in the field of structural health\nmonitoring (SHM) for both mechanical and structural systems.","terms":["cs.LG","cs.AI"]},{"titles":"ExBluRF: Efficient Radiance Fields for Extreme Motion Blurred Images","summaries":"We present ExBluRF, a novel view synthesis method for extreme motion blurred\nimages based on efficient radiance fields optimization. Our approach consists\nof two main components: 6-DOF camera trajectory-based motion blur formulation\nand voxel-based radiance fields. From extremely blurred images, we optimize the\nsharp radiance fields by jointly estimating the camera trajectories that\ngenerate the blurry images. In training, multiple rays along the camera\ntrajectory are accumulated to reconstruct single blurry color, which is\nequivalent to the physical motion blur operation. We minimize the\nphoto-consistency loss on blurred image space and obtain the sharp radiance\nfields with camera trajectories that explain the blur of all images. The joint\noptimization on the blurred image space demands painfully increasing\ncomputation and resources proportional to the blur size. Our method solves this\nproblem by replacing the MLP-based framework to low-dimensional 6-DOF camera\nposes and voxel-based radiance fields. Compared with the existing works, our\napproach restores much sharper 3D scenes from challenging motion blurred views\nwith the order of 10 times less training time and GPU memory consumption.","terms":["cs.CV"]},{"titles":"Orbital AI-based Autonomous Refuelling Solution","summaries":"Cameras are rapidly becoming the choice for on-board sensors towards space\nrendezvous due to their small form factor and inexpensive power, mass, and\nvolume costs. When it comes to docking, however, they typically serve a\nsecondary role, whereas the main work is done by active sensors such as lidar.\nThis paper documents the development of a proposed AI-based (artificial\nintelligence) navigation algorithm intending to mature the use of on-board\nvisible wavelength cameras as a main sensor for docking and on-orbit servicing\n(OOS), reducing the dependency on lidar and greatly reducing costs.\nSpecifically, the use of AI enables the expansion of the relative navigation\nsolution towards multiple classes of scenarios, e.g., in terms of targets or\nillumination conditions, which would otherwise have to be crafted on a\ncase-by-case manner using classical image processing methods. Multiple\nconvolutional neural network (CNN) backbone architectures are benchmarked on\nsynthetically generated data of docking manoeuvres with the International Space\nStation (ISS), achieving position and attitude estimates close to 1%\nrange-normalised and 1 deg, respectively. The integration of the solution with\na physical prototype of the refuelling mechanism is validated in laboratory\nusing a robotic arm to simulate a berthing procedure.","terms":["cs.CV","cs.AI","cs.LG","cs.RO"]},{"titles":"AI Foundation Models for Weather and Climate: Applications, Design, and Implementation","summaries":"Machine learning and deep learning methods have been widely explored in\nunderstanding the chaotic behavior of the atmosphere and furthering weather\nforecasting. There has been increasing interest from technology companies,\ngovernment institutions, and meteorological agencies in building digital twins\nof the Earth. Recent approaches using transformers, physics-informed machine\nlearning, and graph neural networks have demonstrated state-of-the-art\nperformance on relatively narrow spatiotemporal scales and specific tasks. With\nthe recent success of generative artificial intelligence (AI) using pre-trained\ntransformers for language modeling and vision with prompt engineering and\nfine-tuning, we are now moving towards generalizable AI. In particular, we are\nwitnessing the rise of AI foundation models that can perform competitively on\nmultiple domain-specific downstream tasks. Despite this progress, we are still\nin the nascent stages of a generalizable AI model for global Earth system\nmodels, regional climate models, and mesoscale weather models. Here, we review\ncurrent state-of-the-art AI approaches, primarily from transformer and operator\nlearning literature in the context of meteorology. We provide our perspective\non criteria for success towards a family of foundation models for nowcasting\nand forecasting weather and climate predictions. We also discuss how such\nmodels can perform competitively on downstream tasks such as downscaling\n(super-resolution), identifying conditions conducive to the occurrence of\nwildfires, and predicting consequential meteorological phenomena across various\nspatiotemporal scales such as hurricanes and atmospheric rivers. In particular,\nwe examine current AI methodologies and contend they have matured enough to\ndesign and implement a weather foundation model.","terms":["cs.LG","cs.AI","physics.ao-ph","68T07 (Primary), 68T01, 86A08","I.2.0; I.4.0; J.2.5"]},{"titles":"ModelGiF: Gradient Fields for Model Functional Distance","summaries":"The last decade has witnessed the success of deep learning and the surge of\npublicly released trained models, which necessitates the quantification of the\nmodel functional distance for various purposes. However, quantifying the model\nfunctional distance is always challenging due to the opacity in inner workings\nand the heterogeneity in architectures or tasks. Inspired by the concept of\n\"field\" in physics, in this work we introduce Model Gradient Field (abbr.\nModelGiF) to extract homogeneous representations from the heterogeneous\npre-trained models. Our main assumption underlying ModelGiF is that each\npre-trained deep model uniquely determines a ModelGiF over the input space. The\ndistance between models can thus be measured by the similarity between their\nModelGiFs. We validate the effectiveness of the proposed ModelGiF with a suite\nof testbeds, including task relatedness estimation, intellectual property\nprotection, and model unlearning verification. Experimental results demonstrate\nthe versatility of the proposed ModelGiF on these tasks, with significantly\nsuperiority performance to state-of-the-art competitors. Codes are available at\nhttps:\/\/github.com\/zju-vipa\/modelgif.","terms":["cs.LG","cs.AI"]},{"titles":"A spectrum of physics-informed Gaussian processes for regression in engineering","summaries":"Despite the growing availability of sensing and data in general, we remain\nunable to fully characterise many in-service engineering systems and structures\nfrom a purely data-driven approach. The vast data and resources available to\ncapture human activity are unmatched in our engineered world, and, even in\ncases where data could be referred to as ``big,'' they will rarely hold\ninformation across operational windows or life spans. This paper pursues the\ncombination of machine learning technology and physics-based reasoning to\nenhance our ability to make predictive models with limited data. By explicitly\nlinking the physics-based view of stochastic processes with a data-based\nregression approach, a spectrum of possible Gaussian process models are\nintroduced that enable the incorporation of different levels of expert\nknowledge of a system. Examples illustrate how these approaches can\nsignificantly reduce reliance on data collection whilst also increasing the\ninterpretability of the model, another important consideration in this context.","terms":["cs.LG"]},{"titles":"Deep Kernel Methods Learn Better: From Cards to Process Optimization","summaries":"The ability of deep learning methods to perform classification and regression\ntasks relies heavily on their capacity to uncover manifolds in high-dimensional\ndata spaces and project them into low-dimensional representation spaces. In\nthis study, we investigate the structure and character of the manifolds\ngenerated by classical variational autoencoder (VAE) approaches and deep kernel\nlearning (DKL). In the former case, the structure of the latent space is\ndetermined by the properties of the input data alone, while in the latter, the\nlatent manifold forms as a result of an active learning process that balances\nthe data distribution and target functionalities. We show that DKL with active\nlearning can produce a more compact and smooth latent space which is more\nconducive to optimization compared to previously reported methods, such as the\nVAE. We demonstrate this behavior using a simple cards data set and extend it\nto the optimization of domain-generated trajectories in physical systems. Our\nfindings suggest that latent manifolds constructed through active learning have\na more beneficial structure for optimization problems, especially in\nfeature-rich target-poor scenarios that are common in domain sciences, such as\nmaterials synthesis, energy storage, and molecular discovery. The jupyter\nnotebooks that encapsulate the complete analysis accompany the article.","terms":["cs.LG","cond-mat.dis-nn","cond-mat.mes-hall","cond-mat.mtrl-sci"]},{"titles":"Visible and NIR Image Fusion Algorithm Based on Information Complementarity","summaries":"Visible and near-infrared(NIR) band sensors provide images that capture\ncomplementary spectral radiations from a scene. And the fusion of the visible\nand NIR image aims at utilizing their spectrum properties to enhance image\nquality. However, currently visible and NIR fusion algorithms cannot well take\nadvantage of spectrum properties, as well as lack information complementarity,\nwhich results in color distortion and artifacts. Therefore, this paper designs\na complementary fusion model from the level of physical signals. First, in\norder to distinguish between noise and useful information, we use two layers of\nthe weight-guided filter and guided filter to obtain texture and edge layers,\nrespectively. Second, to generate the initial visible-NIR complementarity\nweight map, the difference maps of visible and NIR are filtered by the\nextend-DoG filter. After that, the significant region of NIR night-time\ncompensation guides the initial complementarity weight map by the arctanI\nfunction. Finally, the fusion images can be generated by the complementarity\nweight maps of visible and NIR images, respectively. The experimental results\ndemonstrate that the proposed algorithm can not only well take advantage of the\nspectrum properties and the information complementarity, but also avoid color\nunnatural while maintaining naturalness, which outperforms the\nstate-of-the-art.","terms":["cs.CV","cs.AI"]},{"titles":"Elliptic PDE learning is provably data-efficient","summaries":"PDE learning is an emerging field that combines physics and machine learning\nto recover unknown physical systems from experimental data. While deep learning\nmodels traditionally require copious amounts of training data, recent PDE\nlearning techniques achieve spectacular results with limited data availability.\nStill, these results are empirical. Our work provides theoretical guarantees on\nthe number of input-output training pairs required in PDE learning.\nSpecifically, we exploit randomized numerical linear algebra and PDE theory to\nderive a provably data-efficient algorithm that recovers solution operators of\n3D uniformly elliptic PDEs from input-output data and achieves an exponential\nconvergence rate of the error with respect to the size of the training dataset\nwith an exceptionally high probability of success.","terms":["cs.LG","cs.AI","cs.NA","math.NA"]},{"titles":"Geometric structure of shallow neural networks and constructive ${\\mathcal L}^2$ cost minimization","summaries":"In this paper, we provide a geometric interpretation of the structure of\nshallow neural networks characterized by one hidden layer, a ramp activation\nfunction, an ${\\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost\nfunction, input space ${\\mathbb R}^M$, output space ${\\mathbb R}^Q$ with $Q\\leq\nM$, and training input sample size $N>QM$. We prove an upper bound on the\nminimum of the cost function of order $O(\\delta_P$ where $\\delta_P$ measures\nthe signal to noise ratio of training inputs. We obtain an approximate\noptimizer using projections adapted to the averages $\\overline{x_{0,j}}$ of\ntraining input vectors belonging to the same output vector $y_j$,\n$j=1,\\dots,Q$. In the special case $M=Q$, we explicitly determine an exact\ndegenerate local minimum of the cost function; the sharp value differs from the\nupper bound obtained for $Q\\leq M$ by a relative error $O(\\delta_P^2)$. The\nproof of the upper bound yields a constructively trained network; we show that\nit metrizes the $Q$-dimensional subspace in the input space ${\\mathbb R}^M$\nspanned by $\\overline{x_{0,j}}$, $j=1,\\dots,Q$. We comment on the\ncharacterization of the global minimum of the cost function in the given\ncontext.","terms":["cs.LG","cs.AI","math-ph","math.MP","math.OC","stat.ML","57R70, 62M45"]},{"titles":"Multi-dimension Queried and Interacting Network for Stereo Image Deraining","summaries":"Eliminating the rain degradation in stereo images poses a formidable\nchallenge, which necessitates the efficient exploitation of mutual information\npresent between the dual views. To this end, we devise MQINet, which employs\nmulti-dimension queries and interactions for stereo image deraining. More\nspecifically, our approach incorporates a context-aware dimension-wise queried\nblock (CDQB). This module leverages dimension-wise queries that are independent\nof the input features and employs global context-aware attention (GCA) to\ncapture essential features while avoiding the entanglement of redundant or\nirrelevant information. Meanwhile, we introduce an intra-view physics-aware\nattention (IPA) based on the inverse physical model of rainy images. IPA\nextracts shallow features that are sensitive to the physics of rain\ndegradation, facilitating the reduction of rain-related artifacts during the\nearly learning period. Furthermore, we integrate a cross-view multi-dimension\ninteracting attention mechanism (CMIA) to foster comprehensive feature\ninteraction between the two views across multiple dimensions. Extensive\nexperimental evaluations demonstrate the superiority of our model over EPRRNet\nand StereoIRR, achieving respective improvements of 4.18 dB and 0.45 dB in\nPSNR. Code and models are available at \\url{https:\/\/github.com\/chdwyb\/MQINet}.","terms":["cs.CV"]},{"titles":"Calibrating multi-dimensional complex ODE from noisy data via deep neural networks","summaries":"Ordinary differential equations (ODEs) are widely used to model complex\ndynamics that arises in biology, chemistry, engineering, finance, physics, etc.\nCalibration of a complicated ODE system using noisy data is generally very\ndifficult. In this work, we propose a two-stage nonparametric approach to\naddress this problem. We first extract the de-noised data and their higher\norder derivatives using boundary kernel method, and then feed them into a\nsparsely connected deep neural network with ReLU activation function. Our\nmethod is able to recover the ODE system without being subject to the curse of\ndimensionality and complicated ODE structure. When the ODE possesses a general\nmodular structure, with each modular component involving only a few input\nvariables, and the network architecture is properly chosen, our method is\nproven to be consistent. Theoretical properties are corroborated by an\nextensive simulation study that demonstrates the validity and effectiveness of\nthe proposed method. Finally, we use our method to simultaneously characterize\nthe growth rate of Covid-19 infection cases from 50 states of the USA.","terms":["stat.ML","cs.LG","stat.ME"]},{"titles":"A new method of modeling the multi-stage decision-making process of CRT using machine learning with uncertainty quantification","summaries":"Aims. The purpose of this study is to create a multi-stage machine learning\nmodel to predict cardiac resynchronization therapy (CRT) response for heart\nfailure (HF) patients. This model exploits uncertainty quantification to\nrecommend additional collection of single-photon emission computed tomography\nmyocardial perfusion imaging (SPECT MPI) variables if baseline clinical\nvariables and features from electrocardiogram (ECG) are not sufficient.\nMethods. 218 patients who underwent rest-gated SPECT MPI were enrolled in this\nstudy. CRT response was defined as an increase in left ventricular ejection\nfraction (LVEF) > 5% at a 6 month follow-up. A multi-stage ML model was created\nby combining two ensemble models. Results. The response rate for CRT was 55.5%\n(n = 121) with overall male gender 61.0% (n = 133), an average age of 62.0, and\nLVEF of 27.7. The multi-stage model performed similarly to Ensemble 2 (which\nutilized the additional SPECT data) with AUC of 0.75 vs. 0.77, accuracy of 0.71\nvs. 0.69, sensitivity of 0.70 vs. 0.72, and specificity 0.72 vs. 0.65,\nrespectively. However, the multi-stage model only required SPECT MPI data for\n52.7% of the patients across all folds. Conclusions. By using rule-based logic\nstemming from uncertainty quantification, the multi-stage model was able to\nreduce the need for additional SPECT MPI data acquisition without sacrificing\nperformance.","terms":["cs.LG","eess.SP","physics.med-ph"]},{"titles":"Multi-fidelity climate model parameterization for better generalization and extrapolation","summaries":"Machine-learning-based parameterizations (i.e. representation of sub-grid\nprocesses) of global climate models or turbulent simulations have recently been\nproposed as a powerful alternative to physical, but empirical, representations,\noffering a lower computational cost and higher accuracy. Yet, those approaches\nstill suffer from a lack of generalization and extrapolation beyond the\ntraining data, which is however critical to projecting climate change or\nunobserved regimes of turbulence. Here we show that a multi-fidelity approach,\nwhich integrates datasets of different accuracy and abundance, can provide the\nbest of both worlds: the capacity to extrapolate leveraging the\nphysically-based parameterization and a higher accuracy using the\nmachine-learning-based parameterizations. In an application to climate\nmodeling, the multi-fidelity framework yields more accurate climate projections\nwithout requiring major increase in computational resources. Our multi-fidelity\nrandomized prior networks (MF-RPNs) combine physical parameterization data as\nlow-fidelity and storm-resolving historical run's data as high-fidelity. To\nextrapolate beyond the training data, the MF-RPNs are tested on high-fidelity\nwarming scenarios, $+4K$, data. We show the MF-RPN's capacity to return much\nmore skillful predictions compared to either low- or high-fidelity (historical\ndata) simulations trained only on one regime while providing trustworthy\nuncertainty quantification across a wide range of scenarios. Our approach paves\nthe way for the use of machine-learning based methods that can optimally\nleverage historical observations or high-fidelity simulations and extrapolate\nto unseen regimes such as climate change.","terms":["cs.LG","math.DS","physics.ao-ph","physics.comp-ph"]},{"titles":"Stochastic Deep Koopman Model for Quality Propagation Analysis in Multistage Manufacturing Systems","summaries":"The modeling of multistage manufacturing systems (MMSs) has attracted\nincreased attention from both academia and industry. Recent advancements in\ndeep learning methods provide an opportunity to accomplish this task with\nreduced cost and expertise. This study introduces a stochastic deep Koopman\n(SDK) framework to model the complex behavior of MMSs. Specifically, we present\na novel application of Koopman operators to propagate critical quality\ninformation extracted by variational autoencoders. Through this framework, we\ncan effectively capture the general nonlinear evolution of product quality\nusing a transferred linear representation, thus enhancing the interpretability\nof the data-driven model. To evaluate the performance of the SDK framework, we\ncarried out a comparative study on an open-source dataset. The main findings of\nthis paper are as follows. Our results indicate that SDK surpasses other\npopular data-driven models in accuracy when predicting stagewise product\nquality within the MMS. Furthermore, the unique linear propagation property in\nthe stochastic latent space of SDK enables traceability for quality evolution\nthroughout the process, thereby facilitating the design of root cause analysis\nschemes. Notably, the proposed framework requires minimal knowledge of the\nunderlying physics of production lines. It serves as a virtual metrology tool\nthat can be applied to various MMSs, contributing to the ultimate goal of Zero\nDefect Manufacturing.","terms":["cs.LG","cs.SY","eess.SY"]},{"titles":"Integration of Swin UNETR and statistical shape modeling for a semi-automated segmentation of the knee and biomechanical modeling of articular cartilage","summaries":"Simulation studies like finite element (FE) modeling provide insight into\nknee joint mechanics without patient experimentation. Generic FE models\nrepresent biomechanical behavior of the tissue by overlooking variations in\ngeometry, loading, and material properties of a population. On the other hand,\nsubject-specific models include these specifics, resulting in enhanced\npredictive precision. However, creating such models is laborious and\ntime-intensive. The present study aimed to enhance subject-specific knee joint\nFE modeling by incorporating a semi-automated segmentation algorithm. This\nsegmentation was a 3D Swin UNETR for an initial segmentation of the femur and\ntibia, followed by a statistical shape model (SSM) adjustment to improve\nsurface roughness and continuity. Five hundred and seven magnetic resonance\nimages (MRIs) from the Osteoarthritis Initiative (OAI) database were used to\nbuild and validate the segmentation model. A semi-automated FE model was\ndeveloped using this semi-automated segmentation. On the other hand, a manual\nFE model was developed through manual segmentation (i.e., the gold standard\napproach). Both FE models were subjected to gait loading. The predicted\nmechanical response of manual and semi-automated FE models were compared. In\nthe result, our semi-automated segmentation achieved Dice similarity\ncoefficient (DSC) over 98% for both femur and tibia. The mechanical results\n(max principal stress, max principal strain, fluid pressure, fibril strain, and\ncontact area) showed no significant differences between the manual and\nsemi-automated FE models, indicating the effectiveness of the proposed\nsemi-automated segmentation in creating accurate knee joint FE models. (\nhttps:\/\/data.mendeley.com\/datasets\/k5hdc9cz7w\/1 ).","terms":["cs.CV","physics.med-ph"]},{"titles":"A Semi-Supervised Approach for Power System Event Identification","summaries":"Event identification is increasingly recognized as crucial for enhancing the\nreliability, security, and stability of the electric power system. With the\ngrowing deployment of Phasor Measurement Units (PMUs) and advancements in data\nscience, there are promising opportunities to explore data-driven event\nidentification via machine learning classification techniques. However,\nobtaining accurately-labeled eventful PMU data samples remains challenging due\nto its labor-intensive nature and uncertainty about the event type (class) in\nreal-time. Thus, it is natural to use semi-supervised learning techniques,\nwhich make use of both labeled and unlabeled samples. %We propose a novel\nsemi-supervised framework to assess the effectiveness of incorporating\nunlabeled eventful samples to enhance existing event identification\nmethodologies. We evaluate three categories of classical semi-supervised\napproaches: (i) self-training, (ii) transductive support vector machines\n(TSVM), and (iii) graph-based label spreading (LS) method. Our approach\ncharacterizes events using physically interpretable features extracted from\nmodal analysis of synthetic eventful PMU data. In particular, we focus on the\nidentification of four event classes whose identification is crucial for grid\noperations. We have developed and publicly shared a comprehensive Event\nIdentification package which consists of three aspects: data generation,\nfeature extraction, and event identification with limited labels using\nsemi-supervised methodologies. Using this package, we generate and evaluate\neventful PMU data for the South Carolina synthetic network. Our evaluation\nconsistently demonstrates that graph-based LS outperforms the other two\nsemi-supervised methods that we consider, and can noticeably improve event\nidentification performance relative to the setting with only a small number of\nlabeled samples.","terms":["cs.LG","cs.SY","eess.SY"]},{"titles":"Physics-informed PointNet: On how many irregular geometries can it solve an inverse problem simultaneously? Application to linear elasticity","summaries":"Regular physics-informed neural networks (PINNs) predict the solution of\npartial differential equations using sparse labeled data but only over a single\ndomain. On the other hand, fully supervised learning models are first trained\nusually over a few thousand domains with known solutions (i.e., labeled data)\nand then predict the solution over a few hundred unseen domains.\nPhysics-informed PointNet (PIPN) is primarily designed to fill this gap between\nPINNs (as weakly supervised learning models) and fully supervised learning\nmodels. In this article, we demonstrate that PIPN predicts the solution of\ndesired partial differential equations over a few hundred domains\nsimultaneously, while it only uses sparse labeled data. This framework benefits\nfast geometric designs in the industry when only sparse labeled data are\navailable. Particularly, we show that PIPN predicts the solution of a plane\nstress problem over more than 500 domains with different geometries,\nsimultaneously. Moreover, we pioneer implementing the concept of remarkable\nbatch size (i.e., the number of geometries fed into PIPN at each sub-epoch)\ninto PIPN. Specifically, we try batch sizes of 7, 14, 19, 38, 76, and 133.\nAdditionally, the effect of the PIPN size, symmetric function in the PIPN\narchitecture, and static and dynamic weights for the component of the sparse\nlabeled data in the loss function are investigated.","terms":["cs.LG","cs.CE"]},{"titles":"Finite Expression Methods for Discovering Physical Laws from Data","summaries":"Nonlinear dynamics is a pervasive phenomenon observed in scientific and\nengineering disciplines. However, the task of deriving analytical expressions\nto describe nonlinear dynamics from limited data remains challenging. In this\npaper, we shall present a novel deep symbolic learning method called the\n\"finite expression method\" (FEX) to discover governing equations within a\nfunction space containing a finite set of analytic expressions, based on\nobserved dynamic data. The key concept is to employ FEX to generate analytical\nexpressions of the governing equations by learning the derivatives of partial\ndifferential equation (PDE) solutions through convolutions. Our numerical\nresults demonstrate that our FEX surpasses other existing methods (such as\nPDE-Net, SINDy, GP, and SPL) in terms of numerical performance across a range\nof problems, including time-dependent PDE problems and nonlinear dynamical\nsystems with time-varying coefficients. Moreover, the results highlight FEX's\nflexibility and expressive power in accurately approximating symbolic governing\nequations.","terms":["cs.LG","cs.NA","math.NA"]},{"titles":"Investigation of Compressor Cascade Flow Using Physics- Informed Neural Networks with Adaptive Learning Strategy","summaries":"In this study, we utilize the emerging Physics Informed Neural Networks\n(PINNs) approach for the first time to predict the flow field of a compressor\ncascade. Different from conventional training methods, a new adaptive learning\nstrategy that mitigates gradient imbalance through incorporating adaptive\nweights in conjunction with dynamically adjusting learning rate is used during\nthe training process to improve the convergence of PINNs. The performance of\nPINNs is assessed here by solving both the forward and inverse problems. In the\nforward problem, by encapsulating the physical relations among relevant\nvariables, PINNs demonstrate their effectiveness in accurately forecasting the\ncompressor's flow field. PINNs also show obvious advantages over the\ntraditional CFD approaches, particularly in scenarios lacking complete boundary\nconditions, as is often the case in inverse engineering problems. PINNs\nsuccessfully reconstruct the flow field of the compressor cascade solely based\non partial velocity vectors and near-wall pressure information. Furthermore,\nPINNs show robust performance in the environment of various levels of aleatory\nuncertainties stemming from labeled data. This research provides evidence that\nPINNs can offer turbomachinery designers an additional and promising option\nalongside the current dominant CFD methods.","terms":["cs.LG","physics.comp-ph","physics.flu-dyn"]},{"titles":"A Study of Data-driven Methods for Adaptive Forecasting of COVID-19 Cases","summaries":"Severe acute respiratory disease SARS-CoV-2 has had a found impact on public\nhealth systems and healthcare emergency response especially with respect to\nmaking decisions on the most effective measures to be taken at any given time.\nAs demonstrated throughout the last three years with COVID-19, the prediction\nof the number of positive cases can be an effective way to facilitate\ndecision-making. However, the limited availability of data and the highly\ndynamic and uncertain nature of the virus transmissibility makes this task very\nchallenging. Aiming at investigating these challenges and in order to address\nthis problem, this work studies data-driven (learning, statistical) methods for\nincrementally training models to adapt to these nonstationary conditions. An\nextensive empirical study is conducted to examine various characteristics, such\nas, performance analysis on a per virus wave basis, feature extraction,\n\"lookback\" window size, memory size, all for next-, 7-, and 14-day forecasting\ntasks. We demonstrate that the incremental learning framework can successfully\naddress the aforementioned challenges and perform well during outbreaks,\nproviding accurate predictions.","terms":["cs.LG","physics.soc-ph","q-bio.PE"]},{"titles":"Latent assimilation with implicit neural representations for unknown dynamics","summaries":"Data assimilation is crucial in a wide range of applications, but it often\nfaces challenges such as high computational costs due to data dimensionality\nand incomplete understanding of underlying mechanisms. To address these\nchallenges, this study presents a novel assimilation framework, termed Latent\nAssimilation with Implicit Neural Representations (LAINR). By introducing\nSpherical Implicit Neural Representations (SINR) along with a data-driven\nuncertainty estimator of the trained neural networks, LAINR enhances efficiency\nin assimilation process. Experimental results indicate that LAINR holds certain\nadvantage over existing methods based on AutoEncoders, both in terms of\naccuracy and efficiency.","terms":["cs.LG","math-ph","math.MP","math.OC","physics.ao-ph","68T07, 49N45, 33C55"]}]