[{"titles":"Scaling Laws of Synthetic Images for Model Training ... for Now","summaries":"Recent significant advances in text-to-image models unlock the possibility of\ntraining vision systems using synthetic images, potentially overcoming the\ndifficulty of collecting curated data at scale. It is unclear, however, how\nthese models behave at scale, as more synthetic data is added to the training\nset. In this paper we study the scaling laws of synthetic images generated by\nstate of the art text-to-image models, for the training of supervised models:\nimage classifiers with label supervision, and CLIP with language supervision.\nWe identify several factors, including text prompts, classifier-free guidance\nscale, and types of text-to-image models, that significantly affect scaling\nbehavior. After tuning these factors, we observe that synthetic images\ndemonstrate a scaling trend similar to, but slightly less effective than, real\nimages in CLIP training, while they significantly underperform in scaling when\ntraining supervised image classifiers. Our analysis indicates that the main\nreason for this underperformance is the inability of off-the-shelf\ntext-to-image models to generate certain concepts, a limitation that\nsignificantly impairs the training of image classifiers. Our findings also\nsuggest that scaling synthetic data can be particularly effective in scenarios\nsuch as: (1) when there is a limited supply of real images for a supervised\nproblem (e.g., fewer than 0.5 million images in ImageNet), (2) when the\nevaluation dataset diverges significantly from the training data, indicating\nthe out-of-distribution scenario, or (3) when synthetic data is used in\nconjunction with real images, as demonstrated in the training of CLIP models.","terms":["cs.CV"]},{"titles":"Gen2Det: Generate to Detect","summaries":"Recently diffusion models have shown improvement in synthetic image quality\nas well as better control in generation. We motivate and present Gen2Det, a\nsimple modular pipeline to create synthetic training data for object detection\nfor free by leveraging state-of-the-art grounded image generation methods.\nUnlike existing works which generate individual object instances, require\nidentifying foreground followed by pasting on other images, we simplify to\ndirectly generating scene-centric images. In addition to the synthetic data,\nGen2Det also proposes a suite of techniques to best utilize the generated data,\nincluding image-level filtering, instance-level filtering, and better training\nrecipe to account for imperfections in the generation. Using Gen2Det, we show\nhealthy improvements on object detection and segmentation tasks under various\nsettings and agnostic to detection methods. In the long-tailed detection\nsetting on LVIS, Gen2Det improves the performance on rare categories by a large\nmargin while also significantly improving the performance on other categories,\ne.g. we see an improvement of 2.13 Box AP and 1.84 Mask AP over just training\non real data on LVIS with Mask R-CNN. In the low-data regime setting on COCO,\nGen2Det consistently improves both Box and Mask AP by 2.27 and 1.85 points. In\nthe most general detection setting, Gen2Det still demonstrates robust\nperformance gains, e.g. it improves the Box and Mask AP on COCO by 0.45 and\n0.32 points.","terms":["cs.CV"]},{"titles":"MuRF: Multi-Baseline Radiance Fields","summaries":"We present Multi-Baseline Radiance Fields (MuRF), a general feed-forward\napproach to solving sparse view synthesis under multiple different baseline\nsettings (small and large baselines, and different number of input views). To\nrender a target novel view, we discretize the 3D space into planes parallel to\nthe target image plane, and accordingly construct a target view frustum volume.\nSuch a target volume representation is spatially aligned with the target view,\nwhich effectively aggregates relevant information from the input views for\nhigh-quality rendering. It also facilitates subsequent radiance field\nregression with a convolutional network thanks to its axis-aligned nature. The\n3D context modeled by the convolutional network enables our method to synthesis\nsharper scene structures than prior works. Our MuRF achieves state-of-the-art\nperformance across multiple different baseline settings and diverse scenarios\nranging from simple objects (DTU) to complex indoor and outdoor scenes\n(RealEstate10K and LLFF). We also show promising zero-shot generalization\nabilities on the Mip-NeRF 360 dataset, demonstrating the general applicability\nof MuRF.","terms":["cs.CV"]},{"titles":"EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS","summaries":"Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view\nscene synthesis. It addresses the challenges of lengthy training times and slow\nrendering speeds associated with Neural Radiance Fields (NeRFs). Through rapid,\ndifferentiable rasterization of 3D Gaussians, 3D-GS achieves real-time\nrendering and accelerated training. They, however, demand substantial memory\nresources for both training and storage, as they require millions of Gaussians\nin their point cloud representation for each scene. We present a technique\nutilizing quantized embeddings to significantly reduce memory storage\nrequirements and a coarse-to-fine training strategy for a faster and more\nstable optimization of the Gaussian point clouds. Our approach results in scene\nrepresentations with fewer Gaussians and quantized representations, leading to\nfaster training times and rendering speeds for real-time rendering of high\nresolution scenes. We reduce memory by more than an order of magnitude all\nwhile maintaining the reconstruction quality. We validate the effectiveness of\nour approach on a variety of datasets and scenes preserving the visual quality\nwhile consuming 10-20x less memory and faster training\/inference speed. Project\npage and code is available https:\/\/efficientgaussian.github.io","terms":["cs.CV","cs.GR"]},{"titles":"Visual Geometry Grounded Deep Structure From Motion","summaries":"Structure-from-motion (SfM) is a long-standing problem in the computer vision\ncommunity, which aims to reconstruct the camera poses and 3D structure of a\nscene from a set of unconstrained 2D images. Classical frameworks solve this\nproblem in an incremental manner by detecting and matching keypoints,\nregistering images, triangulating 3D points, and conducting bundle adjustment.\nRecent research efforts have predominantly revolved around harnessing the power\nof deep learning techniques to enhance specific elements (e.g., keypoint\nmatching), but are still based on the original, non-differentiable pipeline.\nInstead, we propose a new deep pipeline VGGSfM, where each component is fully\ndifferentiable and thus can be trained in an end-to-end manner. To this end, we\nintroduce new mechanisms and simplifications. First, we build on recent\nadvances in deep 2D point tracking to extract reliable pixel-accurate tracks,\nwhich eliminates the need for chaining pairwise matches. Furthermore, we\nrecover all cameras simultaneously based on the image and track features\ninstead of gradually registering cameras. Finally, we optimise the cameras and\ntriangulate 3D points via a differentiable bundle adjustment layer. We attain\nstate-of-the-art performance on three popular datasets, CO3D, IMC Phototourism,\nand ETH3D.","terms":["cs.CV","cs.RO"]},{"titles":"NeRFiller: Completing Scenes via Generative 3D Inpainting","summaries":"We propose NeRFiller, an approach that completes missing portions of a 3D\ncapture via generative 3D inpainting using off-the-shelf 2D visual generative\nmodels. Often parts of a captured 3D scene or object are missing due to mesh\nreconstruction failures or a lack of observations (e.g., contact regions, such\nas the bottom of objects, or hard-to-reach areas). We approach this challenging\n3D inpainting problem by leveraging a 2D inpainting diffusion model. We\nidentify a surprising behavior of these models, where they generate more 3D\nconsistent inpaints when images form a 2$\\times$2 grid, and show how to\ngeneralize this behavior to more than four images. We then present an iterative\nframework to distill these inpainted regions into a single consistent 3D scene.\nIn contrast to related works, we focus on completing scenes rather than\ndeleting foreground objects, and our approach does not require tight 2D object\nmasks or text. We compare our approach to relevant baselines adapted to our\nsetting on a variety of scenes, where NeRFiller creates the most 3D consistent\nand plausible scene completions. Our project page is at\nhttps:\/\/ethanweber.me\/nerfiller.","terms":["cs.CV","cs.AI","cs.GR"]},{"titles":"GenDeF: Learning Generative Deformation Field for Video Generation","summaries":"We offer a new perspective on approaching the task of video generation.\nInstead of directly synthesizing a sequence of frames, we propose to render a\nvideo by warping one static image with a generative deformation field (GenDeF).\nSuch a pipeline enjoys three appealing advantages. First, we can sufficiently\nreuse a well-trained image generator to synthesize the static image (also\ncalled canonical image), alleviating the difficulty in producing a video and\nthereby resulting in better visual quality. Second, we can easily convert a\ndeformation field to optical flows, making it possible to apply explicit\nstructural regularizations for motion modeling, leading to temporally\nconsistent results. Third, the disentanglement between content and motion\nallows users to process a synthesized video through processing its\ncorresponding static image without any tuning, facilitating many applications\nlike video editing, keypoint tracking, and video segmentation. Both qualitative\nand quantitative results on three common video generation benchmarks\ndemonstrate the superiority of our GenDeF method.","terms":["cs.CV"]},{"titles":"PrimDiffusion: Volumetric Primitives Diffusion for 3D Human Generation","summaries":"We present PrimDiffusion, the first diffusion-based framework for 3D human\ngeneration. Devising diffusion models for 3D human generation is difficult due\nto the intensive computational cost of 3D representations and the articulated\ntopology of 3D humans. To tackle these challenges, our key insight is operating\nthe denoising diffusion process directly on a set of volumetric primitives,\nwhich models the human body as a number of small volumes with radiance and\nkinematic information. This volumetric primitives representation marries the\ncapacity of volumetric representations with the efficiency of primitive-based\nrendering. Our PrimDiffusion framework has three appealing properties: 1)\ncompact and expressive parameter space for the diffusion model, 2) flexible 3D\nrepresentation that incorporates human prior, and 3) decoder-free rendering for\nefficient novel-view and novel-pose synthesis. Extensive experiments validate\nthat PrimDiffusion outperforms state-of-the-art methods in 3D human generation.\nNotably, compared to GAN-based methods, our PrimDiffusion supports real-time\nrendering of high-quality 3D humans at a resolution of $512\\times512$ once the\ndenoising process is done. We also demonstrate the flexibility of our framework\non training-free conditional generation such as texture transfer and 3D\ninpainting.","terms":["cs.CV","cs.GR"]},{"titles":"MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar","summaries":"The ability to animate photo-realistic head avatars reconstructed from\nmonocular portrait video sequences represents a crucial step in bridging the\ngap between the virtual and real worlds. Recent advancements in head avatar\ntechniques, including explicit 3D morphable meshes (3DMM), point clouds, and\nneural implicit representation have been exploited for this ongoing research.\nHowever, 3DMM-based methods are constrained by their fixed topologies,\npoint-based approaches suffer from a heavy training burden due to the extensive\nquantity of points involved, and the last ones suffer from limitations in\ndeformation flexibility and rendering efficiency. In response to these\nchallenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head\nAvatar), a novel approach that harnesses 3D Gaussian point representation\ncoupled with a Gaussian deformation field to learn explicit head avatars from\nmonocular portrait videos. We define our head avatars with Gaussian points\ncharacterized by adaptable shapes, enabling flexible topology. These points\nexhibit movement with a Gaussian deformation field in alignment with the target\npose and expression of a person, facilitating efficient deformation.\nAdditionally, the Gaussian points have controllable shape, size, color, and\nopacity combined with Gaussian splatting, allowing for efficient training and\nrendering. Experiments demonstrate the superior performance of our method,\nwhich achieves state-of-the-art results among previous methods.","terms":["cs.CV"]},{"titles":"GenTron: Delving Deep into Diffusion Transformers for Image and Video Generation","summaries":"In this study, we explore Transformer-based diffusion models for image and\nvideo generation. Despite the dominance of Transformer architectures in various\nfields due to their flexibility and scalability, the visual generative domain\nprimarily utilizes CNN-based U-Net architectures, particularly in\ndiffusion-based models. We introduce GenTron, a family of Generative models\nemploying Transformer-based diffusion, to address this gap. Our initial step\nwas to adapt Diffusion Transformers (DiTs) from class to text conditioning, a\nprocess involving thorough empirical exploration of the conditioning mechanism.\nWe then scale GenTron from approximately 900M to over 3B parameters, observing\nsignificant improvements in visual quality. Furthermore, we extend GenTron to\ntext-to-video generation, incorporating novel motion-free guidance to enhance\nvideo quality. In human evaluations against SDXL, GenTron achieves a 51.1% win\nrate in visual quality (with a 19.8% draw rate), and a 42.3% win rate in text\nalignment (with a 42.9% draw rate). GenTron also excels in the T2I-CompBench,\nunderscoring its strengths in compositional generation. We believe this work\nwill provide meaningful insights and serve as a valuable reference for future\nresearch.","terms":["cs.CV"]},{"titles":"Improved Visual Grounding through Self-Consistent Explanations","summaries":"Vision-and-language models trained to match images with text can be combined\nwith visual explanation methods to point to the locations of specific objects\nin an image. Our work shows that the localization --\"grounding\"-- abilities of\nthese models can be further improved by finetuning for self-consistent visual\nexplanations. We propose a strategy for augmenting existing text-image datasets\nwith paraphrases using a large language model, and SelfEQ, a weakly-supervised\nstrategy on visual explanation maps for paraphrases that encourages\nself-consistency. Specifically, for an input textual phrase, we attempt to\ngenerate a paraphrase and finetune the model so that the phrase and paraphrase\nmap to the same region in the image. We posit that this both expands the\nvocabulary that the model is able to handle, and improves the quality of the\nobject locations highlighted by gradient-based visual explanation methods (e.g.\nGradCAM). We demonstrate that SelfEQ improves performance on Flickr30k,\nReferIt, and RefCOCO+ over a strong baseline method and several prior works.\nParticularly, comparing to other methods that do not use any type of box\nannotations, we obtain 84.07% on Flickr30k (an absolute improvement of 4.69%),\n67.40% on ReferIt (an absolute improvement of 7.68%), and 75.10%, 55.49% on\nRefCOCO+ test sets A and B respectively (an absolute improvement of 3.74% on\naverage).","terms":["cs.CV","cs.CL","cs.LG"]},{"titles":"SPIDeRS: Structured Polarization for Invisible Depth and Reflectance Sensing","summaries":"Can we capture shape and reflectance in stealth? Such capability would be\nvaluable for many application domains in vision, xR, robotics, and HCI. We\nintroduce Structured Polarization, the first depth and reflectance sensing\nmethod using patterns of polarized light (SPIDeRS). The key idea is to modulate\nthe angle of linear polarization (AoLP) of projected light at each pixel. The\nuse of polarization makes it invisible and lets us recover not only depth but\nalso directly surface normals and even reflectance. We implement SPIDeRS with a\nliquid crystal spatial light modulator (SLM) and a polarimetric camera. We\nderive a novel method for robustly extracting the projected structured\npolarization pattern from the polarimetric object appearance. We evaluate the\neffectiveness of SPIDeRS by applying it to a number of real-world objects. The\nresults show that our method successfully reconstructs object shapes of various\nmaterials and is robust to diffuse reflection and ambient light. We also\ndemonstrate relighting using recovered surface normals and reflectance. We\nbelieve SPIDeRS opens a new avenue of polarization use in visual sensing.","terms":["cs.CV","eess.IV"]},{"titles":"Generating Illustrated Instructions","summaries":"We introduce the new task of generating Illustrated Instructions, i.e.,\nvisual instructions customized to a user's needs. We identify desiderata unique\nto this task, and formalize it through a suite of automatic and human\nevaluation metrics, designed to measure the validity, consistency, and efficacy\nof the generations. We combine the power of large language models (LLMs)\ntogether with strong text-to-image generation diffusion models to propose a\nsimple approach called StackedDiffusion, which generates such illustrated\ninstructions given text as input. The resulting model strongly outperforms\nbaseline approaches and state-of-the-art multimodal LLMs; and in 30% of cases,\nusers even prefer it to human-generated articles. Most notably, it enables\nvarious new and exciting applications far beyond what static articles on the\nweb can provide, such as personalized instructions complete with intermediate\nsteps and pictures in response to a user's individual situation.","terms":["cs.CV","cs.AI","cs.LG","cs.MM"]},{"titles":"Free3D: Consistent Novel View Synthesis without 3D Representation","summaries":"We introduce Free3D, a simple approach designed for open-set novel view\nsynthesis (NVS) from a single image. Similar to Zero-1-to-3, we start from a\npre-trained 2D image generator for generalization, and fine-tune it for NVS.\nCompared to recent and concurrent works, we obtain significant improvements\nwithout resorting to an explicit 3D representation, which is slow and\nmemory-consuming or training an additional 3D network. We do so by encoding\nbetter the target camera pose via a new per-pixel ray conditioning\nnormalization (RCN) layer. The latter injects pose information in the\nunderlying 2D image generator by telling each pixel its specific viewing\ndirection. We also improve multi-view consistency via a light-weight multi-view\nattention layer and multi-view noise sharing. We train Free3D on the Objaverse\ndataset and demonstrate excellent generalization to various new categories in\nseveral new datasets, including OminiObject3D and GSO. We hope our simple and\neffective approach will serve as a solid baseline and help future research in\nNVS with more accuracy pose. The project page is available at\nhttps:\/\/chuanxiaz.com\/free3d\/.","terms":["cs.CV"]},{"titles":"Multiview Aerial Visual Recognition (MAVREC): Can Multi-view Improve Aerial Visual Perception?","summaries":"Despite the commercial abundance of UAVs, aerial data acquisition remains\nchallenging, and the existing Asia and North America-centric open-source UAV\ndatasets are small-scale or low-resolution and lack diversity in scene\ncontextuality. Additionally, the color content of the scenes, solar-zenith\nangle, and population density of different geographies influence the data\ndiversity. These two factors conjointly render suboptimal aerial-visual\nperception of the deep neural network (DNN) models trained primarily on the\nground-view data, including the open-world foundational models.\n  To pave the way for a transformative era of aerial detection, we present\nMultiview Aerial Visual RECognition or MAVREC, a video dataset where we record\nsynchronized scenes from different perspectives -- ground camera and\ndrone-mounted camera. MAVREC consists of around 2.5 hours of industry-standard\n2.7K resolution video sequences, more than 0.5 million frames, and 1.1 million\nannotated bounding boxes. This makes MAVREC the largest ground and aerial-view\ndataset, and the fourth largest among all drone-based datasets across all\nmodalities and tasks. Through our extensive benchmarking on MAVREC, we\nrecognize that augmenting object detectors with ground-view images from the\ncorresponding geographical location is a superior pre-training strategy for\naerial detection. Building on this strategy, we benchmark MAVREC with a\ncurriculum-based semi-supervised object detection approach that leverages\nlabeled (ground and aerial) and unlabeled (only aerial) images to enhance the\naerial detection. We publicly release the MAVREC dataset:\nhttps:\/\/mavrec.github.io.","terms":["cs.CV","cs.AI","cs.LG","I.4.0; I.4.8; I.5.1; I.5.4; I.2.10"]},{"titles":"Digital Life Project: Autonomous 3D Characters with Social Intelligence","summaries":"In this work, we present Digital Life Project, a framework utilizing language\nas the universal medium to build autonomous 3D characters, who are capable of\nengaging in social interactions and expressing with articulated body motions,\nthereby simulating life in a digital environment. Our framework comprises two\nprimary components: 1) SocioMind: a meticulously crafted digital brain that\nmodels personalities with systematic few-shot exemplars, incorporates a\nreflection process based on psychology principles, and emulates autonomy by\ninitiating dialogue topics; 2) MoMat-MoGen: a text-driven motion synthesis\nparadigm for controlling the character's digital body. It integrates motion\nmatching, a proven industry technique to ensure motion quality, with\ncutting-edge advancements in motion generation for diversity. Extensive\nexperiments demonstrate that each module achieves state-of-the-art performance\nin its respective domain. Collectively, they enable virtual characters to\ninitiate and sustain dialogues autonomously, while evolving their\nsocio-psychological states. Concurrently, these characters can perform\ncontextually relevant bodily movements. Additionally, a motion captioning\nmodule further allows the virtual character to recognize and appropriately\nrespond to human players' actions. Homepage: https:\/\/digital-life-project.com\/","terms":["cs.CV"]},{"titles":"Adversarial Learning for Feature Shift Detection and Correction","summaries":"Data shift is a phenomenon present in many real-world applications, and while\nthere are multiple methods attempting to detect shifts, the task of localizing\nand correcting the features originating such shifts has not been studied in\ndepth. Feature shifts can occur in many datasets, including in multi-sensor\ndata, where some sensors are malfunctioning, or in tabular and structured data,\nincluding biomedical, financial, and survey data, where faulty standardization\nand data processing pipelines can lead to erroneous features. In this work, we\nexplore using the principles of adversarial learning, where the information\nfrom several discriminators trained to distinguish between two distributions is\nused to both detect the corrupted features and fix them in order to remove the\ndistribution shift between datasets. We show that mainstream supervised\nclassifiers, such as random forest or gradient boosting trees, combined with\nsimple iterative heuristics, can localize and correct feature shifts,\noutperforming current statistical and neural network-based techniques. The code\nis available at https:\/\/github.com\/AI-sandbox\/DataFix.","terms":["cs.LG","cs.AI","stat.AP","stat.ML"]},{"titles":"SceneDreamer: Unbounded 3D Scene Generation from 2D Image Collections","summaries":"In this work, we present SceneDreamer, an unconditional generative model for\nunbounded 3D scenes, which synthesizes large-scale 3D landscapes from random\nnoise. Our framework is learned from in-the-wild 2D image collections only,\nwithout any 3D annotations. At the core of SceneDreamer is a principled\nlearning paradigm comprising 1) an efficient yet expressive 3D scene\nrepresentation, 2) a generative scene parameterization, and 3) an effective\nrenderer that can leverage the knowledge from 2D images. Our approach begins\nwith an efficient bird's-eye-view (BEV) representation generated from simplex\nnoise, which includes a height field for surface elevation and a semantic field\nfor detailed scene semantics. This BEV scene representation enables 1)\nrepresenting a 3D scene with quadratic complexity, 2) disentangled geometry and\nsemantics, and 3) efficient training. Moreover, we propose a novel generative\nneural hash grid to parameterize the latent space based on 3D positions and\nscene semantics, aiming to encode generalizable features across various scenes.\nLastly, a neural volumetric renderer, learned from 2D image collections through\nadversarial training, is employed to produce photorealistic images. Extensive\nexperiments demonstrate the effectiveness of SceneDreamer and superiority over\nstate-of-the-art methods in generating vivid yet diverse unbounded 3D worlds.","terms":["cs.CV","cs.GR"]},{"titles":"HyperDreamer: Hyper-Realistic 3D Content Generation and Editing from a Single Image","summaries":"3D content creation from a single image is a long-standing yet highly\ndesirable task. Recent advances introduce 2D diffusion priors, yielding\nreasonable results. However, existing methods are not hyper-realistic enough\nfor post-generation usage, as users cannot view, render and edit the resulting\n3D content from a full range. To address these challenges, we introduce\nHyperDreamer with several key designs and appealing properties: 1) Viewable:\n360 degree mesh modeling with high-resolution textures enables the creation of\nvisually compelling 3D models from a full range of observation points. 2)\nRenderable: Fine-grained semantic segmentation and data-driven priors are\nincorporated as guidance to learn reasonable albedo, roughness, and specular\nproperties of the materials, enabling semantic-aware arbitrary material\nestimation. 3) Editable: For a generated model or their own data, users can\ninteractively select any region via a few clicks and efficiently edit the\ntexture with text-based guidance. Extensive experiments demonstrate the\neffectiveness of HyperDreamer in modeling region-aware materials with\nhigh-resolution textures and enabling user-friendly editing. We believe that\nHyperDreamer holds promise for advancing 3D content creation and finding\napplications in various domains.","terms":["cs.CV"]},{"titles":"Sim-to-Real Causal Transfer: A Metric Learning Approach to Causally-Aware Interaction Representations","summaries":"Modeling spatial-temporal interactions among neighboring agents is at the\nheart of multi-agent problems such as motion forecasting and crowd navigation.\nDespite notable progress, it remains unclear to which extent modern\nrepresentations can capture the causal relationships behind agent interactions.\nIn this work, we take an in-depth look at the causal awareness of these\nrepresentations, from computational formalism to real-world practice. First, we\ncast doubt on the notion of non-causal robustness studied in the recent\nCausalAgents benchmark. We show that recent representations are already\npartially resilient to perturbations of non-causal agents, and yet modeling\nindirect causal effects involving mediator agents remains challenging. To\naddress this challenge, we introduce a metric learning approach that\nregularizes latent representations with causal annotations. Our controlled\nexperiments show that this approach not only leads to higher degrees of causal\nawareness but also yields stronger out-of-distribution robustness. To further\noperationalize it in practice, we propose a sim-to-real causal transfer method\nvia cross-domain multi-task learning. Experiments on pedestrian datasets show\nthat our method can substantially boost generalization, even in the absence of\nreal-world causal annotations. We hope our work provides a new perspective on\nthe challenges and potential pathways towards causally-aware representations of\nmulti-agent interactions. Our code is available at\nhttps:\/\/github.com\/socialcausality.","terms":["cs.LG","cs.AI","cs.CV","cs.MA","cs.RO"]},{"titles":"Self-Guided Open-Vocabulary Semantic Segmentation","summaries":"Vision-Language Models (VLMs) have emerged as promising tools for open-ended\nimage understanding tasks, including open vocabulary segmentation. Yet, direct\napplication of such VLMs to segmentation is non-trivial, since VLMs are trained\nwith image-text pairs and naturally lack pixel-level granularity. Recent works\nhave made advancements in bridging this gap, often by leveraging the shared\nimage-text space in which the image and a provided text prompt are represented.\nIn this paper, we challenge the capabilities of VLMs further and tackle\nopen-vocabulary segmentation without the need for any textual input. To this\nend, we propose a novel Self-Guided Semantic Segmentation (Self-Seg) framework.\nSelf-Seg is capable of automatically detecting relevant class names from\nclustered BLIP embeddings and using these for accurate semantic segmentation.\nIn addition, we propose an LLM-based Open-Vocabulary Evaluator (LOVE) to\neffectively assess predicted open-vocabulary class names. We achieve\nstate-of-the-art results on Pascal VOC, ADE20K and CityScapes for\nopen-vocabulary segmentation without given class names, as well as competitive\nperformance with methods where class names are given. All code and data will be\nreleased.","terms":["cs.CV"]},{"titles":"Trajeglish: Learning the Language of Driving Scenarios","summaries":"A longstanding challenge for self-driving development is simulating dynamic\ndriving scenarios seeded from recorded driving logs. In pursuit of this\nfunctionality, we apply tools from discrete sequence modeling to model how\nvehicles, pedestrians and cyclists interact in driving scenarios. Using a\nsimple data-driven tokenization scheme, we discretize trajectories to\ncentimeter-level resolution using a small vocabulary. We then model the\nmulti-agent sequence of motion tokens with a GPT-like encoder-decoder that is\nautoregressive in time and takes into account intra-timestep interaction\nbetween agents. Scenarios sampled from our model exhibit state-of-the-art\nrealism; our model tops the Waymo Sim Agents Benchmark, surpassing prior work\nalong the realism meta metric by 3.3% and along the interaction metric by 9.9%.\nWe ablate our modeling choices in full autonomy and partial autonomy settings,\nand show that the representations learned by our model can quickly be adapted\nto improve performance on nuScenes. We additionally evaluate the scalability of\nour model with respect to parameter count and dataset size, and use density\nestimates from our model to quantify the saliency of context length and\nintra-timestep interaction for the traffic modeling task.","terms":["cs.LG","cs.RO"]},{"titles":"PICTURE: PhotorealistIC virtual Try-on from UnconstRained dEsigns","summaries":"In this paper, we propose a novel virtual try-on from unconstrained designs\n(ucVTON) task to enable photorealistic synthesis of personalized composite\nclothing on input human images. Unlike prior arts constrained by specific input\ntypes, our method allows flexible specification of style (text or image) and\ntexture (full garment, cropped sections, or texture patches) conditions. To\naddress the entanglement challenge when using full garment images as\nconditions, we develop a two-stage pipeline with explicit disentanglement of\nstyle and texture. In the first stage, we generate a human parsing map\nreflecting the desired style conditioned on the input. In the second stage, we\ncomposite textures onto the parsing map areas based on the texture input. To\nrepresent complex and non-stationary textures that have never been achieved in\nprevious fashion editing works, we first propose extracting hierarchical and\nbalanced CLIP features and applying position encoding in VTON. Experiments\ndemonstrate superior synthesis quality and personalization enabled by our\nmethod. The flexible control over style and texture mixing brings virtual\ntry-on to a new level of user experience for online shopping and fashion\ndesign.","terms":["cs.CV"]},{"titles":"Plotting Behind the Scenes: Towards Learnable Game Engines","summaries":"Neural video game simulators emerged as powerful tools to generate and edit\nvideos. Their idea is to represent games as the evolution of an environment's\nstate driven by the actions of its agents. While such a paradigm enables users\nto play a game action-by-action, its rigidity precludes more semantic forms of\ncontrol. To overcome this limitation, we augment game models with prompts\nspecified as a set of natural language actions and desired states. The result-a\nPromptable Game Model (PGM)-makes it possible for a user to play the game by\nprompting it with high- and low-level action sequences. Most captivatingly, our\nPGM unlocks the director's mode, where the game is played by specifying goals\nfor the agents in the form of a prompt. This requires learning \"game AI\",\nencapsulated by our animation model, to navigate the scene using high-level\nconstraints, play against an adversary, and devise a strategy to win a point.\nTo render the resulting state, we use a compositional NeRF representation\nencapsulated in our synthesis model. To foster future research, we present\nnewly collected, annotated and calibrated Tennis and Minecraft datasets. Our\nmethod significantly outperforms existing neural video game simulators in terms\nof rendering quality and unlocks applications beyond the capabilities of the\ncurrent state of the art. Our framework, data, and models are available at\nhttps:\/\/snap-research.github.io\/promptable-game-models\/.","terms":["cs.CV","cs.AI"]},{"titles":"Camera Height Doesn't Change: Unsupervised Monocular Scale-Aware Road-Scene Depth Estimation","summaries":"Monocular depth estimators either require explicit scale supervision through\nauxiliary sensors or suffer from scale ambiguity, which renders them difficult\nto deploy in downstream applications. A possible source of scale is the sizes\nof objects found in the scene, but inaccurate localization makes them difficult\nto exploit. In this paper, we introduce a novel scale-aware monocular depth\nestimation method called StableCamH that does not require any auxiliary sensor\nor supervision. The key idea is to exploit prior knowledge of object heights in\nthe scene but aggregate the height cues into a single invariant measure common\nto all frames in a road video sequence, namely the camera height. By\nformulating monocular depth estimation as camera height optimization, we\nachieve robust and accurate unsupervised end-to-end training. To realize\nStableCamH, we devise a novel learning-based size prior that can directly\nconvert car appearance into its dimensions. Extensive experiments on KITTI and\nCityscapes show the effectiveness of StableCamH, its state-of-the-art accuracy\ncompared with related methods, and its generalizability. The training framework\nof StableCamH can be used for any monocular depth estimation method and will\nhopefully become a fundamental building block for further work.","terms":["cs.CV","cs.RO"]},{"titles":"Diffusion Reflectance Map: Single-Image Stochastic Inverse Rendering of Illumination and Reflectance","summaries":"Reflectance bounds the frequency spectrum of illumination in the object\nappearance. In this paper, we introduce the first stochastic inverse rendering\nmethod, which recovers the full frequency spectrum of an illumination jointly\nwith the object reflectance from a single image. Our key idea is to solve this\nblind inverse problem in the reflectance map, an appearance representation\ninvariant to the underlying geometry, by learning to reverse the image\nformation with a novel diffusion model which we refer to as the Diffusion\nReflectance Map Network (DRMNet). Given an observed reflectance map converted\nand completed from the single input image, DRMNet generates a reflectance map\ncorresponding to a perfect mirror sphere while jointly estimating the\nreflectance. The forward process can be understood as gradually filtering a\nnatural illumination with lower and lower frequency reflectance and additive\nGaussian noise. DRMNet learns to invert this process with two subnetworks,\nIllNet and RefNet, which work in concert towards this joint estimation. The\nnetwork is trained on an extensive synthetic dataset and is demonstrated to\ngeneralize to real images, showing state-of-the-art accuracy on established\ndatasets.","terms":["cs.CV"]},{"titles":"Evaluation of Active Feature Acquisition Methods for Time-varying Feature Settings","summaries":"Machine learning methods often assume input features are available at no\ncost. However, in domains like healthcare, where acquiring features could be\nexpensive or harmful, it is necessary to balance a feature's acquisition cost\nagainst its predictive value. The task of training an AI agent to decide which\nfeatures to acquire is called active feature acquisition (AFA). By deploying an\nAFA agent, we effectively alter the acquisition strategy and trigger a\ndistribution shift. To safely deploy AFA agents under this distribution shift,\nwe present the problem of active feature acquisition performance evaluation\n(AFAPE). We examine AFAPE under i) a no direct effect (NDE) assumption, stating\nthat acquisitions don't affect the underlying feature values; and ii) a no\nunobserved confounding (NUC) assumption, stating that retrospective feature\nacquisition decisions were only based on observed features. We show that one\ncan apply offline reinforcement learning under the NUC assumption and missing\ndata methods under the NDE assumption. When NUC and NDE hold, we propose a\nnovel semi-offline reinforcement learning framework, which requires a weaker\npositivity assumption and yields more data-efficient estimators. We introduce\nthree novel estimators: a direct method (DM), an inverse probability weighting\n(IPW), and a double reinforcement learning (DRL) estimator.","terms":["stat.ML","cs.LG"]},{"titles":"Using Large Language Models for Hyperparameter Optimization","summaries":"This paper studies using foundational large language models (LLMs) to make\ndecisions during hyperparameter optimization (HPO). Empirical evaluations\ndemonstrate that in settings with constrained search budgets, LLMs can perform\ncomparably or better than traditional HPO methods like random search and\nBayesian optimization on standard benchmarks. Furthermore, we propose to treat\nthe code specifying our model as a hyperparameter, which the LLM outputs, going\nbeyond the capabilities of existing HPO approaches. Our findings suggest that\nLLMs are a promising tool for improving efficiency in the traditional\ndecision-making problem of hyperparameter optimization.","terms":["cs.LG","cs.AI"]},{"titles":"Correspondences of the Third Kind: Camera Pose Estimation from Object Reflection","summaries":"Computer vision has long relied on two kinds of correspondences: pixel\ncorrespondences in images and 3D correspondences on object surfaces. Is there\nanother kind, and if there is, what can they do for us? In this paper, we\nintroduce correspondences of the third kind we call reflection correspondences\nand show that they can help estimate camera pose by just looking at objects\nwithout relying on the background. Reflection correspondences are point\ncorrespondences in the reflected world, i.e., the scene reflected by the object\nsurface. The object geometry and reflectance alters the scene geometrically and\nradiometrically, respectively, causing incorrect pixel correspondences.\nGeometry recovered from each image is also hampered by distortions, namely\ngeneralized bas-relief ambiguity, leading to erroneous 3D correspondences. We\nshow that reflection correspondences can resolve the ambiguities arising from\nthese distortions. We introduce a neural correspondence estimator and a RANSAC\nalgorithm that fully leverages all three kinds of correspondences for robust\nand accurate joint camera pose and object shape estimation just from the object\nappearance. The method expands the horizon of numerous downstream tasks,\nincluding camera pose estimation for appearance modeling (e.g., NeRF) and\nmotion estimation of reflective objects (e.g., cars on the road), to name a\nfew, as it relieves the requirement of overlapping background.","terms":["cs.CV"]},{"titles":"Evaluation of Active Feature Acquisition Methods for Static Feature Settings","summaries":"Active feature acquisition (AFA) agents, crucial in domains like healthcare\nwhere acquiring features is often costly or harmful, determine the optimal set\nof features for a subsequent classification task. As deploying an AFA agent\nintroduces a shift in missingness distribution, it's vital to assess its\nexpected performance at deployment using retrospective data. In a companion\npaper, we introduce a semi-offline reinforcement learning (RL) framework for\nactive feature acquisition performance evaluation (AFAPE) where features are\nassumed to be time-dependent. Here, we study and extend the AFAPE problem to\ncover static feature settings, where features are time-invariant, and hence\nprovide more flexibility to the AFA agents in deciding the order of the\nacquisitions. In this static feature setting, we derive and adapt new inverse\nprobability weighting (IPW), direct method (DM), and double reinforcement\nlearning (DRL) estimators within the semi-offline RL framework. These\nestimators can be applied when the missingness in the retrospective dataset\nfollows a missing-at-random (MAR) pattern. They also can be applied to\nmissing-not-at-random (MNAR) patterns in conjunction with appropriate existing\nmissing data techniques. We illustrate the improved data efficiency offered by\nthe semi-offline RL estimators in synthetic and real-world data experiments\nunder synthetic MAR and MNAR missingness.","terms":["stat.ML","cs.LG"]},{"titles":"RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models","summaries":"Recent advancements in diffusion-based models have demonstrated significant\nsuccess in generating images from text. However, video editing models have not\nyet reached the same level of visual quality and user control. To address this,\nwe introduce RAVE, a zero-shot video editing method that leverages pre-trained\ntext-to-image diffusion models without additional training. RAVE takes an input\nvideo and a text prompt to produce high-quality videos while preserving the\noriginal motion and semantic structure. It employs a novel noise shuffling\nstrategy, leveraging spatio-temporal interactions between frames, to produce\ntemporally consistent videos faster than existing methods. It is also efficient\nin terms of memory requirements, allowing it to handle longer videos. RAVE is\ncapable of a wide range of edits, from local attribute modifications to shape\ntransformations. In order to demonstrate the versatility of RAVE, we create a\ncomprehensive video evaluation dataset ranging from object-focused scenes to\ncomplex human activities like dancing and typing, and dynamic scenes featuring\nswimming fish and boats. Our qualitative and quantitative experiments highlight\nthe effectiveness of RAVE in diverse video editing scenarios compared to\nexisting methods. Our code, dataset and videos can be found in\nhttps:\/\/rave-video.github.io.","terms":["cs.CV"]},{"titles":"Multimodal Industrial Anomaly Detection by Crossmodal Feature Mapping","summaries":"The paper explores the industrial multimodal Anomaly Detection (AD) task,\nwhich exploits point clouds and RGB images to localize anomalies. We introduce\na novel light and fast framework that learns to map features from one modality\nto the other on nominal samples. At test time, anomalies are detected by\npinpointing inconsistencies between observed and mapped features. Extensive\nexperiments show that our approach achieves state-of-the-art detection and\nsegmentation performance in both the standard and few-shot settings on the\nMVTec 3D-AD dataset while achieving faster inference and occupying less memory\nthan previous multimodal AD methods. Moreover, we propose a layer-pruning\ntechnique to improve memory and time efficiency with a marginal sacrifice in\nperformance.","terms":["cs.CV"]},{"titles":"Bootstrapping Autonomous Radars with Self-Supervised Learning","summaries":"The perception of autonomous vehicles using radars has attracted increased\nresearch interest due its ability to operate in fog and bad weather. However,\ntraining radar models is hindered by the cost and difficulty of annotating\nlarge-scale radar data. To overcome this bottleneck, we propose a\nself-supervised learning framework to leverage the large amount of unlabeled\nradar data to pre-train radar-only embeddings for self-driving perception\ntasks. The proposed method combines radar-to-radar and radar-to-vision\ncontrastive losses to learn a general representation from unlabeled radar\nheatmaps paired with their corresponding camera images. When used for\ndownstream object detection, we demonstrate that the proposed self-supervision\nframework can improve the accuracy of state-of-the-art supervised baselines by\n5.8% in mAP.","terms":["cs.CV"]},{"titles":"Preserving privacy in domain transfer of medical AI models comes at no performance costs: The integral role of differential privacy","summaries":"Developing robust and effective artificial intelligence (AI) models in\nmedicine requires access to large amounts of patient data. The use of AI models\nsolely trained on large multi-institutional datasets can help with this, yet\nthe imperative to ensure data privacy remains, particularly as membership\ninference risks breaching patient confidentiality. As a proposed remedy, we\nadvocate for the integration of differential privacy (DP). We specifically\ninvestigate the performance of models trained with DP as compared to models\ntrained without DP on data from institutions that the model had not seen during\nits training (i.e., external validation) - the situation that is reflective of\nthe clinical use of AI models. By leveraging more than 590,000 chest\nradiographs from five institutions, we evaluated the efficacy of DP-enhanced\ndomain transfer (DP-DT) in diagnosing cardiomegaly, pleural effusion,\npneumonia, atelectasis, and in identifying healthy subjects. We juxtaposed\nDP-DT with non-DP-DT and examined diagnostic accuracy and demographic fairness\nusing the area under the receiver operating characteristic curve (AUC) as the\nmain metric, as well as accuracy, sensitivity, and specificity. Our results\nshow that DP-DT, even with exceptionally high privacy levels (epsilon around\n1), performs comparably to non-DP-DT (P>0.119 across all domains). Furthermore,\nDP-DT led to marginal AUC differences - less than 1% - for nearly all\nsubgroups, relative to non-DP-DT. Despite consistent evidence suggesting that\nDP models induce significant performance degradation for on-domain\napplications, we show that off-domain performance is almost not affected.\nTherefore, we ardently advocate for the adoption of DP in training diagnostic\nmedical AI models, given its minimal impact on performance.","terms":["cs.LG","cs.AI","cs.CR","eess.IV"]},{"titles":"LAVA: Data Valuation without Pre-Specified Learning Algorithms","summaries":"Traditionally, data valuation (DV) is posed as a problem of equitably\nsplitting the validation performance of a learning algorithm among the training\ndata. As a result, the calculated data values depend on many design choices of\nthe underlying learning algorithm. However, this dependence is undesirable for\nmany DV use cases, such as setting priorities over different data sources in a\ndata acquisition process and informing pricing mechanisms in a data\nmarketplace. In these scenarios, data needs to be valued before the actual\nanalysis and the choice of the learning algorithm is still undetermined then.\nAnother side-effect of the dependence is that to assess the value of individual\npoints, one needs to re-run the learning algorithm with and without a point,\nwhich incurs a large computation burden. This work leapfrogs over the current\nlimits of data valuation methods by introducing a new framework that can value\ntraining data in a way that is oblivious to the downstream learning algorithm.\nOur main results are as follows. (1) We develop a proxy for the validation\nperformance associated with a training set based on a non-conventional\nclass-wise Wasserstein distance between training and validation sets. We show\nthat the distance characterizes the upper bound of the validation performance\nfor any given model under certain Lipschitz conditions. (2) We develop a novel\nmethod to value individual data based on the sensitivity analysis of the\nclass-wise Wasserstein distance. Importantly, these values can be directly\nobtained for free from the output of off-the-shelf optimization solvers when\ncomputing the distance. (3) We evaluate our new data valuation framework over\nvarious use cases related to detecting low-quality data and show that,\nsurprisingly, the learning-agnostic feature of our framework enables a\nsignificant improvement over SOTA performance while being orders of magnitude\nfaster.","terms":["cs.LG","cs.AI","stat.ML"]},{"titles":"Coordination-free Decentralised Federated Learning on Complex Networks: Overcoming Heterogeneity","summaries":"Federated Learning (FL) is a well-known framework for successfully performing\na learning task in an edge computing scenario where the devices involved have\nlimited resources and incomplete data representation. The basic assumption of\nFL is that the devices communicate directly or indirectly with a parameter\nserver that centrally coordinates the whole process, overcoming several\nchallenges associated with it. However, in highly pervasive edge scenarios, the\npresence of a central controller that oversees the process cannot always be\nguaranteed, and the interactions (i.e., the connectivity graph) between devices\nmight not be predetermined, resulting in a complex network structure. Moreover,\nthe heterogeneity of data and devices further complicates the learning process.\nThis poses new challenges from a learning standpoint that we address by\nproposing a communication-efficient Decentralised Federated Learning (DFL)\nalgorithm able to cope with them. Our solution allows devices communicating\nonly with their direct neighbours to train an accurate model, overcoming the\nheterogeneity induced by data and different training histories. Our results\nshow that the resulting local models generalise better than those trained with\ncompeting approaches, and do so in a more communication-efficient way.","terms":["cs.LG","cs.AI","cs.DC","cs.MA","cs.SI"]},{"titles":"Graph Metanetworks for Processing Diverse Neural Architectures","summaries":"Neural networks efficiently encode learned information within their\nparameters. Consequently, many tasks can be unified by treating neural networks\nthemselves as input data. When doing so, recent studies demonstrated the\nimportance of accounting for the symmetries and geometry of parameter spaces.\nHowever, those works developed architectures tailored to specific networks such\nas MLPs and CNNs without normalization layers, and generalizing such\narchitectures to other types of networks can be challenging. In this work, we\novercome these challenges by building new metanetworks - neural networks that\ntake weights from other neural networks as input. Put simply, we carefully\nbuild graphs representing the input neural networks and process the graphs\nusing graph neural networks. Our approach, Graph Metanetworks (GMNs),\ngeneralizes to neural architectures where competing methods struggle, such as\nmulti-head attention layers, normalization layers, convolutional layers, ResNet\nblocks, and group-equivariant linear layers. We prove that GMNs are expressive\nand equivariant to parameter permutation symmetries that leave the input neural\nnetwork functions unchanged. We validate the effectiveness of our method on\nseveral metanetwork tasks over diverse neural network architectures.","terms":["cs.LG","cs.AI","stat.ML"]},{"titles":"A Stability Analysis of Fine-Tuning a Pre-Trained Model","summaries":"Fine-tuning a pre-trained model (such as BERT, ALBERT, RoBERTa, T5, GPT,\netc.) has proven to be one of the most promising paradigms in recent NLP\nresearch. However, numerous recent works indicate that fine-tuning suffers from\nthe instability problem, i.e., tuning the same model under the same setting\nresults in significantly different performance. Many recent works have proposed\ndifferent methods to solve this problem, but there is no theoretical\nunderstanding of why and how these methods work. In this paper, we propose a\nnovel theoretical stability analysis of fine-tuning that focuses on two\ncommonly used settings, namely, full fine-tuning and head tuning. We define the\nstability under each setting and prove the corresponding stability bounds. The\ntheoretical bounds explain why and how several existing methods can stabilize\nthe fine-tuning procedure. In addition to being able to explain most of the\nobserved empirical discoveries, our proposed theoretical analysis framework can\nalso help in the design of effective and provable methods. Based on our theory,\nwe propose three novel strategies to stabilize the fine-tuning procedure,\nnamely, Maximal Margin Regularizer (MMR), Multi-Head Loss (MHLoss), and Self\nUnsupervised Re-Training (SURT). We extensively evaluate our proposed\napproaches on 11 widely used real-world benchmark datasets, as well as hundreds\nof synthetic classification datasets. The experiment results show that our\nproposed methods significantly stabilize the fine-tuning procedure and also\ncorroborate our theoretical analysis.","terms":["cs.LG","cs.AI","cs.CL"]},{"titles":"Listen to Look into the Future: Audio-Visual Egocentric Gaze Anticipation","summaries":"Egocentric gaze anticipation serves as a key building block for the emerging\ncapability of Augmented Reality. Notably, gaze behavior is driven by both\nvisual cues and audio signals during daily activities. Motivated by this\nobservation, we introduce the first model that leverages both the video and\naudio modalities for egocentric gaze anticipation. Specifically, we propose a\nContrastive Spatial-Temporal Separable (CSTS) fusion approach that adopts two\nmodules to separately capture audio-visual correlations in spatial and temporal\ndimensions, and applies a contrastive loss on the re-weighted audio-visual\nfeatures from fusion modules for representation learning. We conduct extensive\nablation studies and thorough analysis using two egocentric video datasets:\nEgo4D and Aria, to validate our model design. We also demonstrate our model\noutperforms prior state-of-the-art methods by at least +1.9% and +1.6%.\nMoreover, we provide visualizations to show the gaze anticipation results and\nprovide additional insights into audio-visual representation learning.","terms":["cs.CV"]},{"titles":"FRNet: Frustum-Range Networks for Scalable LiDAR Segmentation","summaries":"LiDAR segmentation is crucial for autonomous driving systems. The recent\nrange-view approaches are promising for real-time processing. However, they\nsuffer inevitably from corrupted contextual information and rely heavily on\npost-processing techniques for prediction refinement. In this work, we propose\na simple yet powerful FRNet that restores the contextual information of the\nrange image pixels with corresponding frustum LiDAR points. Firstly, a frustum\nfeature encoder module is used to extract per-point features within the frustum\nregion, which preserves scene consistency and is crucial for point-level\npredictions. Next, a frustum-point fusion module is introduced to update\nper-point features hierarchically, which enables each point to extract more\nsurrounding information via the frustum features. Finally, a head fusion module\nis used to fuse features at different levels for final semantic prediction.\nExtensive experiments on four popular LiDAR segmentation benchmarks under\nvarious task setups demonstrate our superiority. FRNet achieves competitive\nperformance while maintaining high efficiency. The code is publicly available.","terms":["cs.CV","cs.RO"]},{"titles":"Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation","summaries":"Despite diffusion models having shown powerful abilities to generate\nphotorealistic images, generating videos that are realistic and diverse still\nremains in its infancy. One of the key reasons is that current methods\nintertwine spatial content and temporal dynamics together, leading to a notably\nincreased complexity of text-to-video generation (T2V). In this work, we\npropose HiGen, a diffusion model-based method that improves performance by\ndecoupling the spatial and temporal factors of videos from two perspectives,\ni.e., structure level and content level. At the structure level, we decompose\nthe T2V task into two steps, including spatial reasoning and temporal\nreasoning, using a unified denoiser. Specifically, we generate spatially\ncoherent priors using text during spatial reasoning and then generate\ntemporally coherent motions from these priors during temporal reasoning. At the\ncontent level, we extract two subtle cues from the content of the input video\nthat can express motion and appearance changes, respectively. These two cues\nthen guide the model's training for generating videos, enabling flexible\ncontent variations and enhancing temporal stability. Through the decoupled\nparadigm, HiGen can effectively reduce the complexity of this task and generate\nrealistic videos with semantics accuracy and motion stability. Extensive\nexperiments demonstrate the superior performance of HiGen over the\nstate-of-the-art T2V methods.","terms":["cs.CV"]},{"titles":"If your data distribution shifts, use self-learning","summaries":"We demonstrate that self-learning techniques like entropy minimization and\npseudo-labeling are simple and effective at improving performance of a deployed\ncomputer vision model under systematic domain shifts. We conduct a wide range\nof large-scale experiments and show consistent improvements irrespective of the\nmodel architecture, the pre-training technique or the type of distribution\nshift. At the same time, self-learning is simple to use in practice because it\ndoes not require knowledge or access to the original training data or scheme,\nis robust to hyperparameter choices, is straight-forward to implement and\nrequires only a few adaptation epochs. This makes self-learning techniques\nhighly attractive for any practitioner who applies machine learning algorithms\nin the real world. We present state-of-the-art adaptation results on CIFAR10-C\n(8.5% error), ImageNet-C (22.0% mCE), ImageNet-R (17.4% error) and ImageNet-A\n(14.8% error), theoretically study the dynamics of self-supervised adaptation\nmethods and propose a new classification dataset (ImageNet-D) which is\nchallenging even with adaptation.","terms":["cs.CV","cs.LG"]},{"titles":"Distributed Bayesian Estimation in Sensor Networks: Consensus on Marginal Densities","summaries":"In this paper, we aim to design and analyze distributed Bayesian estimation\nalgorithms for sensor networks. The challenges we address are to (i) derive a\ndistributed provably-correct algorithm in the functional space of probability\ndistributions over continuous variables, and (ii) leverage these results to\nobtain new distributed estimators restricted to subsets of variables observed\nby individual agents. This relates to applications such as cooperative\nlocalization and federated learning, where the data collected at any agent\ndepends on a subset of all variables of interest. We present Bayesian density\nestimation algorithms using data from non-linear likelihoods at agents in\ncentralized, distributed, and marginal distributed settings. After setting up a\ndistributed estimation objective, we prove almost-sure convergence to the\noptimal set of pdfs at each agent. Then, we prove the same for a storage-aware\nalgorithm estimating densities only over relevant variables at each agent.\nFinally, we present a Gaussian version of these algorithms and implement it in\na mapping problem using variational inference to handle non-linear likelihood\nmodels associated with LiDAR sensing.","terms":["cs.LG","cs.MA","cs.RO","eess.SP"]},{"titles":"GSGFormer: Generative Social Graph Transformer for Multimodal Pedestrian Trajectory Prediction","summaries":"Pedestrian trajectory prediction, vital for selfdriving cars and\nsocially-aware robots, is complicated due to intricate interactions between\npedestrians, their environment, and other Vulnerable Road Users. This paper\npresents GSGFormer, an innovative generative model adept at predicting\npedestrian trajectories by considering these complex interactions and offering\na plethora of potential modal behaviors. We incorporate a heterogeneous graph\nneural network to capture interactions between pedestrians, semantic maps, and\npotential destinations. The Transformer module extracts temporal features,\nwhile our novel CVAE-Residual-GMM module promotes diverse behavioral modality\ngeneration. Through evaluations on multiple public datasets, GSGFormer not only\noutperforms leading methods with ample data but also remains competitive when\ndata is limited.","terms":["cs.CV","cs.AI"]},{"titles":"ViCo: Plug-and-play Visual Condition for Personalized Text-to-image Generation","summaries":"Personalized text-to-image generation using diffusion models has recently\nemerged and garnered significant interest. This task learns a novel concept\n(e.g., a unique toy), illustrated in a handful of images, into a generative\nmodel that captures fine visual details and generates photorealistic images\nbased on textual embeddings. In this paper, we present ViCo, a novel\nlightweight plug-and-play method that seamlessly integrates visual condition\ninto personalized text-to-image generation. ViCo stands out for its unique\nfeature of not requiring any fine-tuning of the original diffusion model\nparameters, thereby facilitating more flexible and scalable model deployment.\nThis key advantage distinguishes ViCo from most existing models that\nnecessitate partial or full diffusion fine-tuning. ViCo incorporates an image\nattention module that conditions the diffusion process on patch-wise visual\nsemantics, and an attention-based object mask that comes at no extra cost from\nthe attention module. Despite only requiring light parameter training (~6%\ncompared to the diffusion U-Net), ViCo delivers performance that is on par\nwith, or even surpasses, all state-of-the-art models, both qualitatively and\nquantitatively. This underscores the efficacy of ViCo, making it a highly\npromising solution for personalized text-to-image generation without the need\nfor diffusion model fine-tuning. Code: https:\/\/github.com\/haoosz\/ViCo","terms":["cs.CV","cs.AI"]},{"titles":"On the Learnability of Watermarks for Language Models","summaries":"Watermarking of language model outputs enables statistical detection of\nmodel-generated text, which has many applications in the responsible deployment\nof language models. Existing watermarking strategies operate by altering the\ndecoder of an existing language model, and the ability for a language model to\ndirectly learn to generate the watermark would have significant implications\nfor the real-world deployment of watermarks. First, learned watermarks could be\nused to build open models that naturally generate watermarked text, allowing\nfor open models to benefit from watermarking. Second, if watermarking is used\nto determine the provenance of generated text, an adversary can hurt the\nreputation of a victim model by spoofing its watermark and generating damaging\nwatermarked text. To investigate the learnability of watermarks, we propose\nwatermark distillation, which trains a student model to behave like a teacher\nmodel that uses decoding-based watermarking. We test our approach on three\ndistinct decoding-based watermarking strategies and various hyperparameter\nsettings, finding that models can learn to generate watermarked text with high\ndetectability. We also find limitations to learnability, including the loss of\nwatermarking capabilities under fine-tuning on normal text and high sample\ncomplexity when learning low-distortion watermarks.","terms":["cs.LG","cs.CL","cs.CR"]},{"titles":"Emotional Speech-driven 3D Body Animation via Disentangled Latent Diffusion","summaries":"Existing methods for synthesizing 3D human gestures from speech have shown\npromising results, but they do not explicitly model the impact of emotions on\nthe generated gestures. Instead, these methods directly output animations from\nspeech without control over the expressed emotion. To address this limitation,\nwe present AMUSE, an emotional speech-driven body animation model based on\nlatent diffusion. Our observation is that content (i.e., gestures related to\nspeech rhythm and word utterances), emotion, and personal style are separable.\nTo account for this, AMUSE maps the driving audio to three disentangled latent\nvectors: one for content, one for emotion, and one for personal style. A latent\ndiffusion model, trained to generate gesture motion sequences, is then\nconditioned on these latent vectors. Once trained, AMUSE synthesizes 3D human\ngestures directly from speech with control over the expressed emotions and\nstyle by combining the content from the driving speech with the emotion and\nstyle of another speech sequence. Randomly sampling the noise of the diffusion\nmodel further generates variations of the gesture with the same emotional\nexpressivity. Qualitative, quantitative, and perceptual evaluations demonstrate\nthat AMUSE outputs realistic gesture sequences. Compared to the state of the\nart, the generated gestures are better synchronized with the speech content and\nbetter represent the emotion expressed by the input speech. Our project website\nis amuse.is.tue.mpg.de.","terms":["cs.CV"]},{"titles":"FitDiff: Robust monocular 3D facial shape and reflectance estimation using Diffusion Models","summaries":"The remarkable progress in 3D face reconstruction has resulted in high-detail\nand photorealistic facial representations. Recently, Diffusion Models have\nrevolutionized the capabilities of generative methods by achieving far better\nperformance than GANs. In this work, we present FitDiff, a diffusion-based 3D\nfacial avatar generative model. This model accurately generates relightable\nfacial avatars, utilizing an identity embedding extracted from an \"in-the-wild\"\n2D facial image. Our multi-modal diffusion model concurrently outputs facial\nreflectance maps (diffuse and specular albedo and normals) and shapes,\nshowcasing great generalization capabilities. It is solely trained on an\nannotated subset of a public facial dataset, paired with 3D reconstructions. We\nrevisit the typical 3D facial fitting approach by guiding a reverse diffusion\nprocess using perceptual and face recognition losses. Being the first LDM\nconditioned on face recognition embeddings, FitDiff reconstructs relightable\nhuman avatars, that can be used as-is in common rendering engines, starting\nonly from an unconstrained facial image, and achieving state-of-the-art\nperformance.","terms":["cs.CV"]},{"titles":"Horizon-Free and Instance-Dependent Regret Bounds for Reinforcement Learning with General Function Approximation","summaries":"To tackle long planning horizon problems in reinforcement learning with\ngeneral function approximation, we propose the first algorithm, termed as\nUCRL-WVTR, that achieves both \\emph{horizon-free} and\n\\emph{instance-dependent}, since it eliminates the polynomial dependency on the\nplanning horizon. The derived regret bound is deemed \\emph{sharp}, as it\nmatches the minimax lower bound when specialized to linear mixture MDPs up to\nlogarithmic factors. Furthermore, UCRL-WVTR is \\emph{computationally efficient}\nwith access to a regression oracle. The achievement of such a horizon-free,\ninstance-dependent, and sharp regret bound hinges upon (i) novel algorithm\ndesigns: weighted value-targeted regression and a high-order moment estimator\nin the context of general function approximation; and (ii) fine-grained\nanalyses: a novel concentration bound of weighted non-linear least squares and\na refined analysis which leads to the tight instance-dependent bound. We also\nconduct comprehensive experiments to corroborate our theoretical findings.","terms":["cs.LG","stat.ML"]},{"titles":"PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding","summaries":"Recent advances in text-to-image generation have made remarkable progress in\nsynthesizing realistic human photos conditioned on given text prompts. However,\nexisting personalized generation methods cannot simultaneously satisfy the\nrequirements of high efficiency, promising identity (ID) fidelity, and flexible\ntext controllability. In this work, we introduce PhotoMaker, an efficient\npersonalized text-to-image generation method, which mainly encodes an arbitrary\nnumber of input ID images into a stack ID embedding for preserving ID\ninformation. Such an embedding, serving as a unified ID representation, can not\nonly encapsulate the characteristics of the same input ID comprehensively, but\nalso accommodate the characteristics of different IDs for subsequent\nintegration. This paves the way for more intriguing and practically valuable\napplications. Besides, to drive the training of our PhotoMaker, we propose an\nID-oriented data construction pipeline to assemble the training data. Under the\nnourishment of the dataset constructed through the proposed pipeline, our\nPhotoMaker demonstrates better ID preservation ability than test-time\nfine-tuning based methods, yet provides significant speed improvements,\nhigh-quality generation results, strong generalization capabilities, and a wide\nrange of applications. Our project page is available at\nhttps:\/\/photo-maker.github.io\/","terms":["cs.CV","cs.AI","cs.LG","cs.MM"]},{"titles":"DreamVideo: Composing Your Dream Videos with Customized Subject and Motion","summaries":"Customized generation using diffusion models has made impressive progress in\nimage generation, but remains unsatisfactory in the challenging video\ngeneration task, as it requires the controllability of both subjects and\nmotions. To that end, we present DreamVideo, a novel approach to generating\npersonalized videos from a few static images of the desired subject and a few\nvideos of target motion. DreamVideo decouples this task into two stages,\nsubject learning and motion learning, by leveraging a pre-trained video\ndiffusion model. The subject learning aims to accurately capture the fine\nappearance of the subject from provided images, which is achieved by combining\ntextual inversion and fine-tuning of our carefully designed identity adapter.\nIn motion learning, we architect a motion adapter and fine-tune it on the given\nvideos to effectively model the target motion pattern. Combining these two\nlightweight and efficient adapters allows for flexible customization of any\nsubject with any motion. Extensive experimental results demonstrate the\nsuperior performance of our DreamVideo over the state-of-the-art methods for\ncustomized video generation. Our project page is at\nhttps:\/\/dreamvideo-t2v.github.io.","terms":["cs.CV"]},{"titles":"Approximate Caching for Efficiently Serving Diffusion Models","summaries":"Text-to-image generation using diffusion models has seen explosive popularity\nowing to their ability in producing high quality images adhering to text\nprompts. However, production-grade diffusion model serving is a resource\nintensive task that not only require high-end GPUs which are expensive but also\nincurs considerable latency. In this paper, we introduce a technique called\napproximate-caching that can reduce such iterative denoising steps for an image\ngeneration based on a prompt by reusing intermediate noise states created\nduring a prior image generation for similar prompts. Based on this idea, we\npresent an end to end text-to-image system, Nirvana, that uses the\napproximate-caching with a novel cache management-policy Least Computationally\nBeneficial and Frequently Used (LCBFU) to provide % GPU compute savings, 19.8%\nend-to-end latency reduction and 19% dollar savings, on average, on two real\nproduction workloads. We further present an extensive characterization of real\nproduction text-to-image prompts from the perspective of caching, popularity\nand reuse of intermediate states in a large production environment.","terms":["cs.CV"]},{"titles":"Cascade-Zero123: One Image to Highly Consistent 3D with Self-Prompted Nearby Views","summaries":"Synthesizing multi-view 3D from one single image is a significant and\nchallenging task. For this goal, Zero-1-to-3 methods aim to extend a 2D latent\ndiffusion model to the 3D scope. These approaches generate the target-view\nimage with a single-view source image and the camera pose as condition\ninformation. However, the one-to-one manner adopted in Zero-1-to-3 incurs\nchallenges for building geometric and visual consistency across views,\nespecially for complex objects. We propose a cascade generation framework\nconstructed with two Zero-1-to-3 models, named Cascade-Zero123, to tackle this\nissue, which progressively extracts 3D information from the source image.\nSpecifically, a self-prompting mechanism is designed to generate several nearby\nviews at first. These views are then fed into the second-stage model along with\nthe source image as generation conditions. With self-prompted multiple views as\nthe supplementary information, our Cascade-Zero123 generates more highly\nconsistent novel-view images than Zero-1-to-3. The promotion is significant for\nvarious complex and challenging scenes, involving insects, humans, transparent\nobjects, and stacked multiple objects etc. The project page is at\nhttps:\/\/cascadezero123.github.io\/.","terms":["cs.CV","cs.GR"]},{"titles":"MapFormer: Boosting Change Detection by Using Pre-change Information","summaries":"Change detection in remote sensing imagery is essential for a variety of\napplications such as urban planning, disaster management, and climate research.\nHowever, existing methods for identifying semantically changed areas overlook\nthe availability of semantic information in the form of existing maps\ndescribing features of the earth's surface. In this paper, we leverage this\ninformation for change detection in bi-temporal images. We show that the simple\nintegration of the additional information via concatenation of latent\nrepresentations suffices to significantly outperform state-of-the-art change\ndetection methods. Motivated by this observation, we propose the new task of\n*Conditional Change Detection*, where pre-change semantic information is used\nas input next to bi-temporal images. To fully exploit the extra information, we\npropose *MapFormer*, a novel architecture based on a multi-modal feature fusion\nmodule that allows for feature processing conditioned on the available semantic\ninformation. We further employ a supervised, cross-modal contrastive loss to\nguide the learning of visual representations. Our approach outperforms existing\nchange detection methods by an absolute 11.7\\% and 18.4\\% in terms of binary\nchange IoU on DynamicEarthNet and HRSCD, respectively. Furthermore, we\ndemonstrate the robustness of our approach to the quality of the pre-change\nsemantic information and the absence pre-change imagery. The code is available\nat https:\/\/github.com\/mxbh\/mapformer.","terms":["cs.CV"]},{"titles":"Monitoring Sustainable Global Development Along Shared Socioeconomic Pathways","summaries":"Sustainable global development is one of the most prevalent challenges facing\nthe world today, hinging on the equilibrium between socioeconomic growth and\nenvironmental sustainability. We propose approaches to monitor and quantify\nsustainable development along the Shared Socioeconomic Pathways (SSPs),\nincluding mathematically derived scoring algorithms, and machine learning\nmethods. These integrate socioeconomic and environmental datasets, to produce\nan interpretable metric for SSP alignment. An initial study demonstrates\npromising results, laying the groundwork for the application of different\nmethods to the monitoring of sustainable global development.","terms":["cs.LG","cs.CY"]},{"titles":"MAELi: Masked Autoencoder for Large-Scale LiDAR Point Clouds","summaries":"The sensing process of large-scale LiDAR point clouds inevitably causes large\nblind spots, i.e. regions not visible to the sensor. We demonstrate how these\ninherent sampling properties can be effectively utilized for self-supervised\nrepresentation learning by designing a highly effective pre-training framework\nthat considerably reduces the need for tedious 3D annotations to train\nstate-of-the-art object detectors. Our Masked AutoEncoder for LiDAR point\nclouds (MAELi) intuitively leverages the sparsity of LiDAR point clouds in both\nthe encoder and decoder during reconstruction. This results in more expressive\nand useful initialization, which can be directly applied to downstream\nperception tasks, such as 3D object detection or semantic segmentation for\nautonomous driving. In a novel reconstruction approach, MAELi distinguishes\nbetween empty and occluded space and employs a new masking strategy that\ntargets the LiDAR's inherent spherical projection. Thereby, without any ground\ntruth whatsoever and trained on single frames only, MAELi obtains an\nunderstanding of the underlying 3D scene geometry and semantics. To demonstrate\nthe potential of MAELi, we pre-train backbones in an end-to-end manner and show\nthe effectiveness of our unsupervised pre-trained weights on the tasks of 3D\nobject detection and semantic segmentation.","terms":["cs.CV"]},{"titles":"Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models","summaries":"Recently, diffusion models have made remarkable progress in text-to-image\n(T2I) generation, synthesizing images with high fidelity and diverse contents.\nDespite this advancement, latent space smoothness within diffusion models\nremains largely unexplored. Smooth latent spaces ensure that a perturbation on\nan input latent corresponds to a steady change in the output image. This\nproperty proves beneficial in downstream tasks, including image interpolation,\ninversion, and editing. In this work, we expose the non-smoothness of diffusion\nlatent spaces by observing noticeable visual fluctuations resulting from minor\nlatent variations. To tackle this issue, we propose Smooth Diffusion, a new\ncategory of diffusion models that can be simultaneously high-performing and\nsmooth. Specifically, we introduce Step-wise Variation Regularization to\nenforce the proportion between the variations of an arbitrary input latent and\nthat of the output image is a constant at any diffusion training step. In\naddition, we devise an interpolation standard deviation (ISTD) metric to\neffectively assess the latent space smoothness of a diffusion model. Extensive\nquantitative and qualitative experiments demonstrate that Smooth Diffusion\nstands out as a more desirable solution not only in T2I generation but also\nacross various downstream tasks. Smooth Diffusion is implemented as a\nplug-and-play Smooth-LoRA to work with various community models. Code is\navailable at https:\/\/github.com\/SHI-Labs\/Smooth-Diffusion.","terms":["cs.CV"]},{"titles":"On the Impact of Multi-dimensional Local Differential Privacy on Fairness","summaries":"Automated decision systems are increasingly used to make consequential\ndecisions in people's lives. Due to the sensitivity of the manipulated data as\nwell as the resulting decisions, several ethical concerns need to be addressed\nfor the appropriate use of such technologies, in particular, fairness and\nprivacy. Unlike previous work, which focused on centralized differential\nprivacy (DP) or local DP (LDP) for a single sensitive attribute, in this paper,\nwe examine the impact of LDP in the presence of several sensitive attributes\n(i.e., multi-dimensional data) on fairness. Detailed empirical analysis on\nsynthetic and benchmark datasets revealed very relevant observations. In\nparticular, (1) multi-dimensional LDP is an efficient approach to reduce\ndisparity, (2) the multi-dimensional approach of LDP (independent vs. combined)\nmatters only at low privacy guarantees, and (3) the outcome Y distribution has\nan important effect on which group is more sensitive to the obfuscation. Last,\nwe summarize our findings in the form of recommendations to guide practitioners\nin adopting effective privacy-preserving practices while maintaining fairness\nand utility in ML applications.","terms":["cs.LG","cs.CR","cs.CY"]},{"titles":"OT-Attack: Enhancing Adversarial Transferability of Vision-Language Models via Optimal Transport Optimization","summaries":"Vision-language pre-training (VLP) models demonstrate impressive abilities in\nprocessing both images and text. However, they are vulnerable to multi-modal\nadversarial examples (AEs). Investigating the generation of\nhigh-transferability adversarial examples is crucial for uncovering VLP models'\nvulnerabilities in practical scenarios. Recent works have indicated that\nleveraging data augmentation and image-text modal interactions can enhance the\ntransferability of adversarial examples for VLP models significantly. However,\nthey do not consider the optimal alignment problem between dataaugmented\nimage-text pairs. This oversight leads to adversarial examples that are overly\ntailored to the source model, thus limiting improvements in transferability. In\nour research, we first explore the interplay between image sets produced\nthrough data augmentation and their corresponding text sets. We find that\naugmented image samples can align optimally with certain texts while exhibiting\nless relevance to others. Motivated by this, we propose an Optimal\nTransport-based Adversarial Attack, dubbed OT-Attack. The proposed method\nformulates the features of image and text sets as two distinct distributions\nand employs optimal transport theory to determine the most efficient mapping\nbetween them. This optimal mapping informs our generation of adversarial\nexamples to effectively counteract the overfitting issues. Extensive\nexperiments across various network architectures and datasets in image-text\nmatching tasks reveal that our OT-Attack outperforms existing state-of-the-art\nmethods in terms of adversarial transferability.","terms":["cs.CV"]},{"titles":"Sem@$K$: Is my knowledge graph embedding model semantic-aware?","summaries":"Using knowledge graph embedding models (KGEMs) is a popular approach for\npredicting links in knowledge graphs (KGs). Traditionally, the performance of\nKGEMs for link prediction is assessed using rank-based metrics, which evaluate\ntheir ability to give high scores to ground-truth entities. However, the\nliterature claims that the KGEM evaluation procedure would benefit from adding\nsupplementary dimensions to assess. That is why, in this paper, we extend our\npreviously introduced metric Sem@K that measures the capability of models to\npredict valid entities w.r.t. domain and range constraints. In particular, we\nconsider a broad range of KGs and take their respective characteristics into\naccount to propose different versions of Sem@K. We also perform an extensive\nstudy to qualify the abilities of KGEMs as measured by our metric. Our\nexperiments show that Sem@K provides a new perspective on KGEM quality. Its\njoint analysis with rank-based metrics offers different conclusions on the\npredictive power of models. Regarding Sem@K, some KGEMs are inherently better\nthan others, but this semantic superiority is not indicative of their\nperformance w.r.t. rank-based metrics. In this work, we generalize conclusions\nabout the relative performance of KGEMs w.r.t. rank-based and semantic-oriented\nmetrics at the level of families of models. The joint analysis of the\naforementioned metrics gives more insight into the peculiarities of each model.\nThis work paves the way for a more comprehensive evaluation of KGEM adequacy\nfor specific downstream tasks.","terms":["cs.LG","cs.AI"]},{"titles":"Intelligent Anomaly Detection for Lane Rendering Using Transformer with Self-Supervised Pre-Training and Customized Fine-Tuning","summaries":"The burgeoning navigation services using digital maps provide great\nconvenience to drivers. Nevertheless, the presence of anomalies in lane\nrendering map images occasionally introduces potential hazards, as such\nanomalies can be misleading to human drivers and consequently contribute to\nunsafe driving conditions. In response to this concern and to accurately and\neffectively detect the anomalies, this paper transforms lane rendering image\nanomaly detection into a classification problem and proposes a four-phase\npipeline consisting of data pre-processing, self-supervised pre-training with\nthe masked image modeling (MiM) method, customized fine-tuning using\ncross-entropy based loss with label smoothing, and post-processing to tackle it\nleveraging state-of-the-art deep learning techniques, especially those\ninvolving Transformer models. Various experiments verify the effectiveness of\nthe proposed pipeline. Results indicate that the proposed pipeline exhibits\nsuperior performance in lane rendering image anomaly detection, and notably,\nthe self-supervised pre-training with MiM can greatly enhance the detection\naccuracy while significantly reducing the total training time. For instance,\nemploying the Swin Transformer with Uniform Masking as self-supervised\npretraining (Swin-Trans-UM) yielded a heightened accuracy at 94.77% and an\nimproved Area Under The Curve (AUC) score of 0.9743 compared with the pure Swin\nTransformer without pre-training (Swin-Trans) with an accuracy of 94.01% and an\nAUC of 0.9498. The fine-tuning epochs were dramatically reduced to 41 from the\noriginal 280. In conclusion, the proposed pipeline, with its incorporation of\nself-supervised pre-training using MiM and other advanced deep learning\ntechniques, emerges as a robust solution for enhancing the accuracy and\nefficiency of lane rendering image anomaly detection in digital navigation\nsystems.","terms":["cs.CV","cs.AI","cs.LG","eess.IV","stat.ML"]},{"titles":"PhysHOI: Physics-Based Imitation of Dynamic Human-Object Interaction","summaries":"Humans interact with objects all the time. Enabling a humanoid to learn\nhuman-object interaction (HOI) is a key step for future smart animation and\nintelligent robotics systems. However, recent progress in physics-based HOI\nrequires carefully designed task-specific rewards, making the system unscalable\nand labor-intensive. This work focuses on dynamic HOI imitation: teaching\nhumanoid dynamic interaction skills through imitating kinematic HOI\ndemonstrations. It is quite challenging because of the complexity of the\ninteraction between body parts and objects and the lack of dynamic HOI data. To\nhandle the above issues, we present PhysHOI, the first physics-based whole-body\nHOI imitation approach without task-specific reward designs. Except for the\nkinematic HOI representations of humans and objects, we introduce the contact\ngraph to model the contact relations between body parts and objects explicitly.\nA contact graph reward is also designed, which proved to be critical for\nprecise HOI imitation. Based on the key designs, PhysHOI can imitate diverse\nHOI tasks simply yet effectively without prior knowledge. To make up for the\nlack of dynamic HOI scenarios in this area, we introduce the BallPlay dataset\nthat contains eight whole-body basketball skills. We validate PhysHOI on\ndiverse HOI tasks, including whole-body grasping and basketball skills.","terms":["cs.CV","cs.GR","cs.RO"]},{"titles":"Loss-Optimal Classification Trees: A Generalized Framework and the Logistic Case","summaries":"The Classification Tree (CT) is one of the most common models in\ninterpretable machine learning. Although such models are usually built with\ngreedy strategies, in recent years, thanks to remarkable advances in\nMixer-Integer Programming (MIP) solvers, several exact formulations of the\nlearning problem have been developed. In this paper, we argue that some of the\nmost relevant ones among these training models can be encapsulated within a\ngeneral framework, whose instances are shaped by the specification of loss\nfunctions and regularizers. Next, we introduce a novel realization of this\nframework: specifically, we consider the logistic loss, handled in the MIP\nsetting by a linear piece-wise approximation, and couple it with\n$\\ell_1$-regularization terms. The resulting Optimal Logistic Tree model\nnumerically proves to be able to induce trees with enhanced interpretability\nfeatures and competitive generalization capabilities, compared to the\nstate-of-the-art MIP-based approaches.","terms":["stat.ML","cs.LG"]},{"titles":"Model-Based Epistemic Variance of Values for Risk-Aware Policy Optimization","summaries":"We consider the problem of quantifying uncertainty over expected cumulative\nrewards in model-based reinforcement learning. In particular, we focus on\ncharacterizing the variance over values induced by a distribution over MDPs.\nPrevious work upper bounds the posterior variance over values by solving a\nso-called uncertainty Bellman equation (UBE), but the over-approximation may\nresult in inefficient exploration. We propose a new UBE whose solution\nconverges to the true posterior variance over values and leads to lower regret\nin tabular exploration problems. We identify challenges to apply the UBE theory\nbeyond tabular problems and propose a suitable approximation. Based on this\napproximation, we introduce a general-purpose policy optimization algorithm,\nQ-Uncertainty Soft Actor-Critic (QU-SAC), that can be applied for either\nrisk-seeking or risk-averse policy optimization with minimal changes.\nExperiments in both online and offline RL demonstrate improved performance\ncompared to other uncertainty estimation methods.","terms":["cs.LG","cs.AI"]},{"titles":"SingingHead: A Large-scale 4D Dataset for Singing Head Animation","summaries":"Singing, as a common facial movement second only to talking, can be regarded\nas a universal language across ethnicities and cultures, plays an important\nrole in emotional communication, art, and entertainment. However, it is often\noverlooked in the field of audio-driven facial animation due to the lack of\nsinging head datasets and the domain gap between singing and talking in rhythm\nand amplitude. To this end, we collect a high-quality large-scale singing head\ndataset, SingingHead, which consists of more than 27 hours of synchronized\nsinging video, 3D facial motion, singing audio, and background music from 76\nindividuals and 8 types of music. Along with the SingingHead dataset, we argue\nthat 3D and 2D facial animation tasks can be solved together, and propose a\nunified singing facial animation framework named UniSinger to achieve both\nsinging audio-driven 3D singing head animation and 2D singing portrait video\nsynthesis. Extensive comparative experiments with both SOTA 3D facial animation\nand 2D portrait animation methods demonstrate the necessity of singing-specific\ndatasets in singing head animation tasks and the promising performance of our\nunified facial animation framework.","terms":["cs.CV"]},{"titles":"DemoCaricature: Democratising Caricature Generation with a Rough Sketch","summaries":"In this paper, we democratise caricature generation, empowering individuals\nto effortlessly craft personalised caricatures with just a photo and a\nconceptual sketch. Our objective is to strike a delicate balance between\nabstraction and identity, while preserving the creativity and subjectivity\ninherent in a sketch. To achieve this, we present Explicit Rank-1 Model Editing\nalongside single-image personalisation, selectively applying nuanced edits to\ncross-attention layers for a seamless merge of identity and style.\nAdditionally, we propose Random Mask Reconstruction to enhance robustness,\ndirecting the model to focus on distinctive identity and style features.\nCrucially, our aim is not to replace artists but to eliminate accessibility\nbarriers, allowing enthusiasts to engage in the artistry.","terms":["cs.CV"]},{"titles":"A Machine Learning Approach to Two-Stage Adaptive Robust Optimization","summaries":"We propose an approach based on machine learning to solve two-stage linear\nadaptive robust optimization (ARO) problems with binary here-and-now variables\nand polyhedral uncertainty sets. We encode the optimal here-and-now decisions,\nthe worst-case scenarios associated with the optimal here-and-now decisions,\nand the optimal wait-and-see decisions into what we denote as the strategy. We\nsolve multiple similar ARO instances in advance using the column and constraint\ngeneration algorithm and extract the optimal strategies to generate a training\nset. We train a machine learning model that predicts high-quality strategies\nfor the here-and-now decisions, the worst-case scenarios associated with the\noptimal here-and-now decisions, and the wait-and-see decisions. We also\nintroduce an algorithm to reduce the number of different target classes the\nmachine learning algorithm needs to be trained on. We apply the proposed\napproach to the facility location, the multi-item inventory control and the\nunit commitment problems. Our approach solves ARO problems drastically faster\nthan the state-of-the-art algorithms with high accuracy.","terms":["cs.LG","math.OC"]},{"titles":"CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models","summaries":"Incorporating a customized object into image generation presents an\nattractive feature in text-to-image generation. However, existing\noptimization-based and encoder-based methods are hindered by drawbacks such as\ntime-consuming optimization, insufficient identity preservation, and a\nprevalent copy-pasting effect. To overcome these limitations, we introduce\nCustomNet, a novel object customization approach that explicitly incorporates\n3D novel view synthesis capabilities into the object customization process.\nThis integration facilitates the adjustment of spatial position relationships\nand viewpoints, yielding diverse outputs while effectively preserving object\nidentity. Moreover, we introduce delicate designs to enable location control\nand flexible background control through textual descriptions or specific\nuser-defined images, overcoming the limitations of existing 3D novel view\nsynthesis methods. We further leverage a dataset construction pipeline that can\nbetter handle real-world objects and complex backgrounds. Equipped with these\ndesigns, our method facilitates zero-shot object customization without\ntest-time optimization, offering simultaneous control over the viewpoints,\nlocation, and background. As a result, our CustomNet ensures enhanced identity\npreservation and generates diverse, harmonious outputs.","terms":["cs.CV","cs.AI"]},{"titles":"Improved Efficient Two-Stage Denoising Diffusion Power System Measurement Recovery Against False Data Injection Attacks and Data Losses","summaries":"Measurement uncertainties, represented by cyber-attacks and data losses,\nseriously degrade the quality of power system measurements. Fortunately, the\npowerful generation ability of the denoising diffusion models can enable more\nprecise measurement generation for power system data recovery. However, the\ncontrollable data generation and efficient computing methods of denoising\ndiffusion models for deterministic trajectory still need further investigation.\nTo this end, this paper proposes an improved two-stage denoising diffusion\nmodel (TSDM) to identify and reconstruct the measurements with various\nmeasurement uncertainties. The first stage of the model comprises a\nclassifier-guided conditional anomaly detection component, while the second\nstage involves diffusion-based measurement imputation component. Moreover, the\nproposed TSDM adopts precise means and optimal variances to accelerate the\ndiffusion generation process with subsequence sampling. Extensive numerical\ncase studies demonstrate that the proposed TSDM can accurately recover power\nsystem measurements despite strong randomness under renewable energy\nintegration and highly nonlinear dynamics under complex cyber-physical\ncontingencies. Additionally, the proposed TSDM has stronger robustness compared\nto existing reconstruction networks and exhibits lower computational complexity\nthan general denoising diffusion models.","terms":["cs.LG","cs.CR"]},{"titles":"Causality and Explainability for Trustworthy Integrated Pest Management","summaries":"Pesticides serve as a common tool in agricultural pest control but\nsignificantly contribute to the climate crisis. To combat this, Integrated Pest\nManagement (IPM) stands as a climate-smart alternative. Despite its potential,\nIPM faces low adoption rates due to farmers' skepticism about its\neffectiveness. To address this challenge, we introduce an advanced data\nanalysis framework tailored to enhance IPM adoption. Our framework provides i)\nrobust pest population predictions across diverse environments with invariant\nand causal learning, ii) interpretable pest presence predictions using\ntransparent models, iii) actionable advice through counterfactual explanations\nfor in-season IPM interventions, iv) field-specific treatment effect\nestimations, and v) assessments of the effectiveness of our advice using causal\ninference. By incorporating these features, our framework aims to alleviate\nskepticism and encourage wider adoption of IPM practices among farmers.","terms":["cs.LG","cs.AI"]},{"titles":"Merging by Matching Models in Task Subspaces","summaries":"Model merging aims to cheaply combine individual task-specific models into a\nsingle multitask model. In this work, we view past merging methods as\nleveraging different notions of a ''task subspace'' in which models are matched\nbefore being merged. We connect the task subspace of a given model to its loss\nlandscape and formalize how this approach to model merging can be seen as\nsolving a linear system of equations. While past work has generally been\nlimited to linear systems that have a closed-form solution, we consider using\nthe conjugate gradient method to find a solution. We show that using the\nconjugate gradient method can outperform closed-form solutions, enables merging\nvia linear systems that are otherwise intractable to solve, and flexibly allows\nchoosing from a wide variety of initializations and estimates for the ''task\nsubspace''. We ultimately demonstrate that our merging framework called\n''Matching Models in their Task Subspace'' (MaTS) achieves state-of-the-art\nresults in multitask and intermediate-task model merging. We release all of the\ncode and checkpoints used in our work at https:\/\/github.com\/r-three\/mats.","terms":["cs.LG","cs.CL"]},{"titles":"Multi-View Unsupervised Image Generation with Cross Attention Guidance","summaries":"The growing interest in novel view synthesis, driven by Neural Radiance Field\n(NeRF) models, is hindered by scalability issues due to their reliance on\nprecisely annotated multi-view images. Recent models address this by\nfine-tuning large text2image diffusion models on synthetic multi-view data.\nDespite robust zero-shot generalization, they may need post-processing and can\nface quality issues due to the synthetic-real domain gap. This paper introduces\na novel pipeline for unsupervised training of a pose-conditioned diffusion\nmodel on single-category datasets. With the help of pretrained self-supervised\nVision Transformers (DINOv2), we identify object poses by clustering the\ndataset through comparing visibility and locations of specific object parts.\nThe pose-conditioned diffusion model, trained on pose labels, and equipped with\ncross-frame attention at inference time ensures cross-view consistency, that is\nfurther aided by our novel hard-attention guidance. Our model, MIRAGE,\nsurpasses prior work in novel view synthesis on real images. Furthermore,\nMIRAGE is robust to diverse textures and geometries, as demonstrated with our\nexperiments on synthetic images generated with pretrained Stable Diffusion.","terms":["cs.CV"]},{"titles":"Towards a Perceptual Evaluation Framework for Lighting Estimation","summaries":"Progress in lighting estimation is tracked by computing existing image\nquality assessment (IQA) metrics on images from standard datasets. While this\nmay appear to be a reasonable approach, we demonstrate that doing so does not\ncorrelate to human preference when the estimated lighting is used to relight a\nvirtual scene into a real photograph. To study this, we design a controlled\npsychophysical experiment where human observers must choose their preference\namongst rendered scenes lit using a set of lighting estimation algorithms\nselected from the recent literature, and use it to analyse how these algorithms\nperform according to human perception. Then, we demonstrate that none of the\nmost popular IQA metrics from the literature, taken individually, correctly\nrepresent human perception. Finally, we show that by learning a combination of\nexisting IQA metrics, we can more accurately represent human preference. This\nprovides a new perceptual framework to help evaluate future lighting estimation\nalgorithms.","terms":["cs.CV"]},{"titles":"Surrogate Modelling for Sea Ice Concentration using Lightweight Neural Ensemble","summaries":"The modeling and forecasting of sea ice conditions in the Arctic region are\nimportant tasks for ship routing, offshore oil production, and environmental\nmonitoring. We propose the adaptive surrogate modeling approach named LANE-SI\n(Lightweight Automated Neural Ensembling for Sea Ice) that uses ensemble of\nrelatively simple deep learning models with different loss functions for\nforecasting of spatial distribution for sea ice concentration in the specified\nwater area. Experimental studies confirm the quality of a long-term forecast\nbased on a deep learning model fitted to the specific water area is comparable\nto resource-intensive physical modeling, and for some periods of the year, it\nis superior. We achieved a 20% improvement against the state-of-the-art\nphysics-based forecast system SEAS5 for the Kara Sea.","terms":["cs.LG","cs.AI","physics.ao-ph"]},{"titles":"A Multi-scale Information Integration Framework for Infrared and Visible Image Fusion","summaries":"Infrared and visible image fusion aims at generating a fused image containing\nthe intensity and detail information of source images, and the key issue is\neffectively measuring and integrating the complementary information of\nmulti-modality images from the same scene. Existing methods mostly adopt a\nsimple weight in the loss function to decide the information retention of each\nmodality rather than adaptively measuring complementary information for\ndifferent image pairs. In this study, we propose a multi-scale dual attention\n(MDA) framework for infrared and visible image fusion, which is designed to\nmeasure and integrate complementary information in both structure and loss\nfunction at the image and patch level. In our method, the residual downsample\nblock decomposes source images into three scales first. Then, dual attention\nfusion block integrates complementary information and generates a spatial and\nchannel attention map at each scale for feature fusion. Finally, the output\nimage is reconstructed by the residual reconstruction block. Loss function\nconsists of image-level, feature-level and patch-level three parts, of which\nthe calculation of the image-level and patch-level two parts are based on the\nweights generated by the complementary information measurement. Indeed, to\nconstrain the pixel intensity distribution between the output and infrared\nimage, a style loss is added. Our fusion results perform robust and informative\nacross different scenarios. Qualitative and quantitative results on two\ndatasets illustrate that our method is able to preserve both thermal radiation\nand detailed information from two modalities and achieve comparable results\ncompared with the other state-of-the-art methods. Ablation experiments show the\neffectiveness of our information integration architecture and adaptively\nmeasure complementary information retention in the loss function.","terms":["cs.CV"]},{"titles":"Learning to sample in Cartesian MRI","summaries":"Despite its exceptional soft tissue contrast, Magnetic Resonance Imaging\n(MRI) faces the challenge of long scanning times compared to other modalities\nlike X-ray radiography. Shortening scanning times is crucial in clinical\nsettings, as it increases patient comfort, decreases examination costs and\nimproves throughput. Recent advances in compressed sensing (CS) and deep\nlearning allow accelerated MRI acquisition by reconstructing high-quality\nimages from undersampled data. While reconstruction algorithms have received\nmost of the focus, designing acquisition trajectories to optimize\nreconstruction quality remains an open question. This thesis explores two\napproaches to address this gap in the context of Cartesian MRI. First, we\npropose two algorithms, lazy LBCS and stochastic LBCS, that significantly\nimprove upon G\\\"ozc\\\"u et al.'s greedy learning-based CS (LBCS) approach. These\nalgorithms scale to large, clinically relevant scenarios like multi-coil 3D MR\nand dynamic MRI, previously inaccessible to LBCS. Additionally, we demonstrate\nthat generative adversarial networks (GANs) can serve as a natural criterion\nfor adaptive sampling by leveraging variance in the measurement domain to guide\nacquisition. Second, we delve into the underlying structures or assumptions\nthat enable mask design algorithms to perform well in practice. Our experiments\nreveal that state-of-the-art deep reinforcement learning (RL) approaches, while\ncapable of adaptation and long-horizon planning, offer only marginal\nimprovements over stochastic LBCS, which is neither adaptive nor does long-term\nplanning. Altogether, our findings suggest that stochastic LBCS and similar\nmethods represent promising alternatives to deep RL. They shine in particular\nby their scalability and computational efficiency and could be key in the\ndeployment of optimized acquisition trajectories in Cartesian MRI.","terms":["cs.LG","eess.IV"]},{"titles":"iDesigner: A High-Resolution and Complex-Prompt Following Text-to-Image Diffusion Model for Interior Design","summaries":"With the open-sourcing of text-to-image models (T2I) such as stable diffusion\n(SD) and stable diffusion XL (SD-XL), there is an influx of models fine-tuned\nin specific domains based on the open-source SD model, such as in anime,\ncharacter portraits, etc. However, there are few specialized models in certain\ndomains, such as interior design, which is attributed to the complex textual\ndescriptions and detailed visual elements inherent in design, alongside the\nnecessity for adaptable resolution. Therefore, text-to-image models for\ninterior design are required to have outstanding prompt-following capabilities,\nas well as iterative collaboration with design professionals to achieve the\ndesired outcome. In this paper, we collect and optimize text-image data in the\ndesign field and continue training in both English and Chinese on the basis of\nthe open-source CLIP model. We also proposed a fine-tuning strategy with\ncurriculum learning and reinforcement learning from CLIP feedback to enhance\nthe prompt-following capabilities of our approach so as to improve the quality\nof image generation. The experimental results on the collected dataset\ndemonstrate the effectiveness of the proposed approach, which achieves\nimpressive results and outperforms strong baselines.","terms":["cs.CV"]},{"titles":"How (not) to ensemble LVLMs for VQA","summaries":"This paper studies ensembling in the era of Large Vision-Language Models\n(LVLMs). Ensembling is a classical method to combine different models to get\nincreased performance. In the recent work on Encyclopedic-VQA the authors\nexamine a wide variety of models to solve their task: from vanilla LVLMs, to\nmodels including the caption as extra context, to models augmented with\nLens-based retrieval of Wikipedia pages. Intuitively these models are highly\ncomplementary, which should make them ideal for ensembling. Indeed, an oracle\nexperiment shows potential gains from 48.8% accuracy (the best single model)\nall the way up to 67% (best possible ensemble). So it is a trivial exercise to\ncreate an ensemble with substantial real gains. Or is it?","terms":["cs.CV"]},{"titles":"GPT4SGG: Synthesizing Scene Graphs from Holistic and Region-specific Narratives","summaries":"Learning scene graphs from natural language descriptions has proven to be a\ncheap and promising scheme for Scene Graph Generation (SGG). However, such\nunstructured caption data and its processing are troubling the learning an\nacurrate and complete scene graph. This dilema can be summarized as three\npoints. First, traditional language parsers often fail to extract meaningful\nrelationship triplets from caption data. Second, grounding unlocalized objects\nin parsed triplets will meet ambiguity in visual-language alignment. Last,\ncaption data typically are sparse and exhibit bias to partial observations of\nimage content. These three issues make it hard for the model to generate\ncomprehensive and accurate scene graphs. To fill this gap, we propose a simple\nyet effective framework, GPT4SGG, to synthesize scene graphs from holistic and\nregion-specific narratives. The framework discards traditional language parser,\nand localize objects before obtaining relationship triplets. To obtain\nrelationship triplets, holistic and dense region-specific narratives are\ngenerated from the image. With such textual representation of image data and a\ntask-specific prompt, an LLM, particularly GPT-4, directly synthesizes a scene\ngraph as \"pseudo labels\". Experimental results showcase GPT4SGG significantly\nimproves the performance of SGG models trained on image-caption data. We\nbelieve this pioneering work can motivate further research into mining the\nvisual reasoning capabilities of LLMs.","terms":["cs.CV"]},{"titles":"Finding Interpretable Class-Specific Patterns through Efficient Neural Search","summaries":"Discovering patterns in data that best describe the differences between\nclasses allows to hypothesize and reason about class-specific mechanisms. In\nmolecular biology, for example, this bears promise of advancing the\nunderstanding of cellular processes differing between tissues or diseases,\nwhich could lead to novel treatments. To be useful in practice, methods that\ntackle the problem of finding such differential patterns have to be readily\ninterpretable by domain experts, and scalable to the extremely high-dimensional\ndata.\n  In this work, we propose a novel, inherently interpretable binary neural\nnetwork architecture DIFFNAPS that extracts differential patterns from data.\nDiffNaps is scalable to hundreds of thousands of features and robust to noise,\nthus overcoming the limitations of current state-of-the-art methods in\nlarge-scale applications such as in biology. We show on synthetic and real\nworld data, including three biological applications, that, unlike its\ncompetitors, DiffNaps consistently yields accurate, succinct, and interpretable\nclass descriptions","terms":["cs.LG","q-bio.QM"]},{"titles":"A Structural-Clustering Based Active Learning for Graph Neural Networks","summaries":"In active learning for graph-structured data, Graph Neural Networks (GNNs)\nhave shown effectiveness. However, a common challenge in these applications is\nthe underutilization of crucial structural information. To address this\nproblem, we propose the Structural-Clustering PageRank method for improved\nActive learning (SPA) specifically designed for graph-structured data. SPA\nintegrates community detection using the SCAN algorithm with the PageRank\nscoring method for efficient and informative sample selection. SPA prioritizes\nnodes that are not only informative but also central in structure. Through\nextensive experiments, SPA demonstrates higher accuracy and macro-F1 score over\nexisting methods across different annotation budgets and achieves significant\nreductions in query time. In addition, the proposed method only adds two\nhyperparameters, $\\epsilon$ and $\\mu$ in the algorithm to finely tune the\nbalance between structural learning and node selection. This simplicity is a\nkey advantage in active learning scenarios, where extensive hyperparameter\ntuning is often impractical.","terms":["cs.LG"]},{"titles":"Reinforcement Learning for Combining Search Methods in the Calibration of Economic ABMs","summaries":"Calibrating agent-based models (ABMs) in economics and finance typically\ninvolves a derivative-free search in a very large parameter space. In this\nwork, we benchmark a number of search methods in the calibration of a\nwell-known macroeconomic ABM on real data, and further assess the performance\nof \"mixed strategies\" made by combining different methods. We find that methods\nbased on random-forest surrogates are particularly efficient, and that\ncombining search methods generally increases performance since the biases of\nany single method are mitigated. Moving from these observations, we propose a\nreinforcement learning (RL) scheme to automatically select and combine search\nmethods on-the-fly during a calibration run. The RL agent keeps exploiting a\nspecific method only as long as this keeps performing well, but explores new\nstrategies when the specific method reaches a performance plateau. The\nresulting RL search scheme outperforms any other method or method combination\ntested, and does not rely on any prior information or trial and error\nprocedure.","terms":["cs.LG","cs.AI","cs.MA","econ.GN","q-fin.EC","J.4; I.6.3"]},{"titles":"Prompt Highlighter: Interactive Control for Multi-Modal LLMs","summaries":"This study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs)\ninference: explicit controllable text generation. Multi-modal LLMs empower\nmulti-modality understanding with the capability of semantic generation yet\nbring less explainability and heavier reliance on prompt contents due to their\nautoregressive generative nature. While manipulating prompt formats could\nimprove outputs, designing specific and precise prompts per task can be\nchallenging and ineffective. To tackle this issue, we introduce a novel\ninference method, Prompt Highlighter, which enables users to highlight specific\nprompt spans to interactively control the focus during generation. Motivated by\nthe classifier-free diffusion guidance, we form regular and unconditional\ncontext pairs based on highlighted tokens, demonstrating that the\nautoregressive generation in models can be guided in a classifier-free way.\nNotably, we find that, during inference, guiding the models with highlighted\ntokens through the attention weights leads to more desired outputs. Our\napproach is compatible with current LLMs and VLMs, achieving impressive\ncustomized generation results without training. Experiments confirm its\neffectiveness in focusing on input contexts and generating reliable content.\nWithout tuning on LLaVA-v1.5, our method secured 69.5 in the MMBench test and\n1552.5 in MME-perception. The code is available at:\nhttps:\/\/github.com\/dvlab-research\/Prompt-Highlighter\/","terms":["cs.CV","cs.CL"]},{"titles":"TSGBench: Time Series Generation Benchmark","summaries":"Synthetic Time Series Generation (TSG) is crucial in a range of applications,\nincluding data augmentation, anomaly detection, and privacy preservation.\nAlthough significant strides have been made in this field, existing methods\nexhibit three key limitations: (1) They often benchmark against similar model\ntypes, constraining a holistic view of performance capabilities. (2) The use of\nspecialized synthetic and private datasets introduces biases and hampers\ngeneralizability. (3) Ambiguous evaluation measures, often tied to custom\nnetworks or downstream tasks, hinder consistent and fair comparison.\n  To overcome these limitations, we introduce \\textsf{TSGBench}, the inaugural\nTime Series Generation Benchmark, designed for a unified and comprehensive\nassessment of TSG methods. It comprises three modules: (1) a curated collection\nof publicly available, real-world datasets tailored for TSG, together with a\nstandardized preprocessing pipeline; (2) a comprehensive evaluation measures\nsuite including vanilla measures, new distance-based assessments, and\nvisualization tools; (3) a pioneering generalization test rooted in Domain\nAdaptation (DA), compatible with all methods. We have conducted comprehensive\nexperiments using \\textsf{TSGBench} across a spectrum of ten real-world\ndatasets from diverse domains, utilizing ten advanced TSG methods and twelve\nevaluation measures. The results highlight the reliability and efficacy of\n\\textsf{TSGBench} in evaluating TSG methods. Crucially, \\textsf{TSGBench}\ndelivers a statistical analysis of the performance rankings of these methods,\nilluminating their varying performance across different datasets and measures\nand offering nuanced insights into the effectiveness of each method.","terms":["cs.LG","cs.AI","cs.DB"]},{"titles":"Cross-codex Learning for Reliable Scribe Identification in Medieval Manuscripts","summaries":"Historic scribe identification is a substantial task for obtaining\ninformation about the past. Uniform script styles, such as the Carolingian\nminuscule, make it a difficult task for classification to focus on meaningful\nfeatures. Therefore, we demonstrate in this paper the importance of cross-codex\ntraining data for CNN based text-independent off-line scribe identification, to\novercome codex dependent overfitting. We report three main findings: First, we\nfound that preprocessing with masked grayscale images instead of RGB images\nclearly increased the F1-score of the classification results. Second, we\ntrained different neural networks on our complex data, validating time and\naccuracy differences in order to define the most reliable network architecture.\nWith AlexNet, the network with the best trade-off between F1-score and time, we\nachieved for individual classes F1-scores of up to 0,96 on line level and up to\n1.0 on page level in classification. Third, we could replicate the finding that\nthe CNN output can be further improved by implementing a reject option, giving\nmore stable results. We present the results on our large scale open source\ndataset -- the Codex Claustroneoburgensis database (CCl-DB) -- containing a\nsignificant number of writings from different scribes in several codices. We\ndemonstrate for the first time on a dataset with such a variety of codices that\npaleographic decisions can be reproduced automatically and precisely with CNNs.\nThis gives manifold new and fast possibilities for paleographers to gain\ninsights into unlabeled material, but also to develop further hypotheses.","terms":["cs.CV","I.4.9; I.5.4"]},{"titles":"GPT-4V with Emotion: A Zero-shot Benchmark for Multimodal Emotion Understanding","summaries":"Recently, GPT-4 with Vision (GPT-4V) has shown remarkable performance across\nvarious multimodal tasks. However, its efficacy in emotion recognition remains\na question. This paper quantitatively evaluates GPT-4V's capabilities in\nmultimodal emotion understanding, encompassing tasks such as facial emotion\nrecognition, visual sentiment analysis, micro-expression recognition, dynamic\nfacial emotion recognition, and multimodal emotion recognition. Our experiments\nshow that GPT-4V exhibits impressive multimodal and temporal understanding\ncapabilities, even surpassing supervised systems in some tasks. Despite these\nachievements, GPT-4V is currently tailored for general domains. It performs\npoorly in micro-expression recognition that requires specialized expertise. The\nmain purpose of this paper is to present quantitative results of GPT-4V on\nemotion understanding and establish a zero-shot benchmark for future research.\nCode and evaluation results are available at:\nhttps:\/\/github.com\/zeroQiaoba\/gpt4v-emotion.","terms":["cs.CV","cs.MM"]},{"titles":"Factor-Assisted Federated Learning for Personalized Optimization with Heterogeneous Data","summaries":"Federated learning is an emerging distributed machine learning framework\naiming at protecting data privacy. Data heterogeneity is one of the core\nchallenges in federated learning, which could severely degrade the convergence\nrate and prediction performance of deep neural networks. To address this issue,\nwe develop a novel personalized federated learning framework for heterogeneous\ndata, which we refer to as FedSplit. This modeling framework is motivated by\nthe finding that, data in different clients contain both common knowledge and\npersonalized knowledge. Then the hidden elements in each neural layer can be\nsplit into the shared and personalized groups. With this decomposition, a novel\nobjective function is established and optimized. We demonstrate FedSplit\nenjoyers a faster convergence speed than the standard federated learning method\nboth theoretically and empirically. The generalization bound of the FedSplit\nmethod is also studied. To practically implement the proposed method on real\ndatasets, factor analysis is introduced to facilitate the decoupling of hidden\nelements. This leads to a practically implemented model for FedSplit and we\nfurther refer to as FedFac. We demonstrated by simulation studies that, using\nfactor analysis can well recover the underlying shared\/personalized\ndecomposition. The superior prediction performance of FedFac is further\nverified empirically by comparison with various state-of-the-art federated\nlearning methods on several real datasets.","terms":["stat.ML","cs.LG"]},{"titles":"D2S: Representing local descriptors and global scene coordinates for camera relocalization","summaries":"State-of-the-art visual localization methods mostly rely on complex\nprocedures to match local descriptors and 3D point clouds. However, these\nprocedures can incur significant cost in terms of inference, storage, and\nupdates over time. In this study, we propose a direct learning-based approach\nthat utilizes a simple network named D2S to represent local descriptors and\ntheir scene coordinates. Our method is characterized by its simplicity and\ncost-effectiveness. It solely leverages a single RGB image for localization\nduring the testing phase and only requires a lightweight model to encode a\ncomplex sparse scene. The proposed D2S employs a combination of a simple loss\nfunction and graph attention to selectively focus on robust descriptors while\ndisregarding areas such as clouds, trees, and several dynamic objects. This\nselective attention enables D2S to effectively perform a binary-semantic\nclassification for sparse descriptors. Additionally, we propose a new outdoor\ndataset to evaluate the capabilities of visual localization methods in terms of\nscene generalization and self-updating from unlabeled observations. Our\napproach outperforms the state-of-the-art CNN-based methods in scene coordinate\nregression in indoor and outdoor environments. It demonstrates the ability to\ngeneralize beyond training data, including scenarios involving transitions from\nday to night and adapting to domain shifts, even in the absence of the labeled\ndata sources. The source code, trained models, dataset, and demo videos are\navailable at the following link: https:\/\/thpjp.github.io\/d2s","terms":["cs.CV","cs.RO"]},{"titles":"Estimating Countries with Similar Maternal Mortality Rate using Cluster Analysis and Pairing Countries with Identical MMR","summaries":"In the evolving world, we require more additionally the young era to flourish\nand evolve into developed land. Most of the population all around the world are\nunaware of the complications involved in the routine they follow while they are\npregnant and how hospital facilities affect maternal health. Maternal Mortality\nis the death of a pregnant woman due to intricacies correlated to pregnancy,\nunderlying circumstances exacerbated by the pregnancy or management of these\nsituations. It is crucial to consider the Maternal Mortality Rate (MMR) in\ndiverse locations and determine which human routines and hospital facilities\ndiminish the Maternal Mortality Rate (MMR). This research aims to examine and\ndiscover the countries which are keeping more lavish threats of MMR and\ncountries alike in MMR encountered. Data is examined and collected for various\ncountries, data consists of the earlier years' observation. From the\nperspective of Machine Learning, Unsupervised Machine Learning is implemented\nto perform Cluster Analysis. Therefore the pairs of countries with similar MMR\nas well as the extreme opposite pair concerning the MMR are found.","terms":["cs.LG","cs.CY"]},{"titles":"Invariant Random Forest: Tree-Based Model Solution for OOD Generalization","summaries":"Out-Of-Distribution (OOD) generalization is an essential topic in machine\nlearning. However, recent research is only focusing on the corresponding\nmethods for neural networks. This paper introduces a novel and effective\nsolution for OOD generalization of decision tree models, named Invariant\nDecision Tree (IDT). IDT enforces a penalty term with regard to the\nunstable\/varying behavior of a split across different environments during the\ngrowth of the tree. Its ensemble version, the Invariant Random Forest (IRF), is\nconstructed. Our proposed method is motivated by a theoretical result under\nmild conditions, and validated by numerical tests with both synthetic and real\ndatasets. The superior performance compared to non-OOD tree models implies that\nconsidering OOD generalization for tree models is absolutely necessary and\nshould be given more attention.","terms":["cs.LG"]},{"titles":"HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting","summaries":"We have recently seen tremendous progress in photo-real human modeling and\nrendering. Yet, efficiently rendering realistic human performance and\nintegrating it into the rasterization pipeline remains challenging. In this\npaper, we present HiFi4G, an explicit and compact Gaussian-based approach for\nhigh-fidelity human performance rendering from dense footage. Our core\nintuition is to marry the 3D Gaussian representation with non-rigid tracking,\nachieving a compact and compression-friendly representation. We first propose a\ndual-graph mechanism to obtain motion priors, with a coarse deformation graph\nfor effective initialization and a fine-grained Gaussian graph to enforce\nsubsequent constraints. Then, we utilize a 4D Gaussian optimization scheme with\nadaptive spatial-temporal regularizers to effectively balance the non-rigid\nprior and Gaussian updating. We also present a companion compression scheme\nwith residual compensation for immersive experiences on various platforms. It\nachieves a substantial compression rate of approximately 25 times, with less\nthan 2MB of storage per frame. Extensive experiments demonstrate the\neffectiveness of our approach, which significantly outperforms existing\napproaches in terms of optimization speed, rendering quality, and storage\noverhead.","terms":["cs.CV"]},{"titles":"Activity Grammars for Temporal Action Segmentation","summaries":"Sequence prediction on temporal data requires the ability to understand\ncompositional structures of multi-level semantics beyond individual and\ncontextual properties. The task of temporal action segmentation, which aims at\ntranslating an untrimmed activity video into a sequence of action segments,\nremains challenging for this reason. This paper addresses the problem by\nintroducing an effective activity grammar to guide neural predictions for\ntemporal action segmentation. We propose a novel grammar induction algorithm\nthat extracts a powerful context-free grammar from action sequence data. We\nalso develop an efficient generalized parser that transforms frame-level\nprobability distributions into a reliable sequence of actions according to the\ninduced grammar with recursive rules. Our approach can be combined with any\nneural network for temporal action segmentation to enhance the sequence\nprediction and discover its compositional structure. Experimental results\ndemonstrate that our method significantly improves temporal action segmentation\nin terms of both performance and interpretability on two standard benchmarks,\nBreakfast and 50 Salads.","terms":["cs.CV"]},{"titles":"Stronger, Fewer, & Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation","summaries":"In this paper, we first assess and harness various Vision Foundation Models\n(VFMs) in the context of Domain Generalized Semantic Segmentation (DGSS).\nDriven by the motivation that Leveraging Stronger pre-trained models and Fewer\ntrainable parameters for Superior generalizability, we introduce a robust\nfine-tuning approach, namely Rein, to parameter-efficiently harness VFMs for\nDGSS. Built upon a set of trainable tokens, each linked to distinct instances,\nRein precisely refines and forwards the feature maps from each layer to the\nnext layer within the backbone. This process produces diverse refinements for\ndifferent categories within a single image. With fewer trainable parameters,\nRein efficiently fine-tunes VFMs for DGSS tasks, surprisingly surpassing full\nparameter fine-tuning. Extensive experiments across various settings\ndemonstrate that Rein significantly outperforms state-of-the-art methods.\nRemarkably, with just an extra 1% of trainable parameters within the frozen\nbackbone, Rein achieves a mIoU of 68.1% on the Cityscapes, without accessing\nany real urban-scene datasets.","terms":["cs.CV"]},{"titles":"Low-complexity subspace-descent over symmetric positive definite manifold","summaries":"This work puts forth low-complexity Riemannian subspace descent algorithms\nfor the minimization of functions over the symmetric positive definite (SPD)\nmanifold. Different from the existing Riemannian gradient descent variants, the\nproposed approach utilizes carefully chosen subspaces that allow the update to\nbe written as a product of the Cholesky factor of the iterate and a sparse\nmatrix. The resulting updates avoid the costly matrix operations like matrix\nexponentiation and dense matrix multiplication, which are generally required in\nalmost all other Riemannian optimization algorithms on SPD manifold. We further\nidentify a broad class of functions, arising in diverse applications, such as\nkernel matrix learning, covariance estimation of Gaussian distributions,\nmaximum likelihood parameter estimation of elliptically contoured\ndistributions, and parameter estimation in Gaussian mixture model problems,\nover which the Riemannian gradients can be calculated efficiently. The proposed\nuni-directional and multi-directional Riemannian subspace descent variants\nincur per-iteration complexities of $\\O(n)$ and $\\O(n^2)$ respectively, as\ncompared to the $\\O(n^3)$ or higher complexity incurred by all existing\nRiemannian gradient descent variants. The superior runtime and low\nper-iteration complexity of the proposed algorithms is also demonstrated via\nnumerical tests on large-scale covariance estimation and matrix square root\nproblems.","terms":["stat.ML","cs.LG","eess.SP","math.OC"]},{"titles":"Dense Optical Tracking: Connecting the Dots","summaries":"Recent approaches to point tracking are able to recover the trajectory of any\nscene point through a large portion of a video despite the presence of\nocclusions. They are, however, too slow in practice to track every point\nobserved in a single frame in a reasonable amount of time. This paper\nintroduces DOT, a novel, simple and efficient method for solving this problem.\nIt first extracts a small set of tracks from key regions at motion boundaries\nusing an off-the-shelf point tracking algorithm. Given source and target\nframes, DOT then computes rough initial estimates of a dense flow field and\nvisibility mask through nearest-neighbor interpolation, before refining them\nusing a learnable optical flow estimator that explicitly handles occlusions and\ncan be trained on synthetic data with ground-truth correspondences. We show\nthat DOT is significantly more accurate than current optical flow techniques,\noutperforms sophisticated \"universal\" trackers like OmniMotion, and is on par\nwith, or better than, the best point tracking algorithms like CoTracker while\nbeing at least two orders of magnitude faster. Quantitative and qualitative\nexperiments with synthetic and real videos validate the promise of the proposed\napproach. Code, data, and videos showcasing the capabilities of our approach\nare available in the project webpage: https:\/\/16lemoing.github.io\/dot .","terms":["cs.CV"]},{"titles":"Trajectory-User Linking via Hierarchical Spatio-Temporal Attention Networks","summaries":"Trajectory-User Linking (TUL) is crucial for human mobility modeling by\nlinking diferent trajectories to users with the exploration of complex mobility\npatterns. Existing works mainly rely on the recurrent neural framework to\nencode the temporal dependencies in trajectories, have fall short in capturing\nspatial-temporal global context for TUL prediction. To ill this gap, this work\npresents a new hierarchical spatio-temporal attention neural network, called\nAttnTUL, to jointly encode the local trajectory transitional patterns and\nglobal spatial dependencies for TUL. Speciically, our irst model component is\nbuilt over the graph neural architecture to preserve the local and global\ncontext and enhance the representation paradigm of geographical regions and\nuser trajectories. Additionally, a hierarchically structured attention network\nis designed to simultaneously encode the intra-trajectory and inter-trajectory\ndependencies, with the integration of the temporal attention mechanism and\nglobal elastic attentional encoder. Extensive experiments demonstrate the\nsuperiority of our AttnTUL method as compared to state-of-the-art baselines on\nvarious trajectory datasets. The source code of our model is available at\nhttps:\/\/github.com\/Onedean\/AttnTUL.","terms":["cs.LG","cs.AI","68-07","I.2.6"]},{"titles":"A Parameterized Generative Adversarial Network Using Cyclic Projection for Explainable Medical Image Classification","summaries":"Although current data augmentation methods are successful to alleviate the\ndata insufficiency, conventional augmentation are primarily intra-domain while\nadvanced generative adversarial networks (GANs) generate images remaining\nuncertain, particularly in small-scale datasets. In this paper, we propose a\nparameterized GAN (ParaGAN) that effectively controls the changes of synthetic\nsamples among domains and highlights the attention regions for downstream\nclassification. Specifically, ParaGAN incorporates projection distance\nparameters in cyclic projection and projects the source images to the decision\nboundary to obtain the class-difference maps. Our experiments show that ParaGAN\ncan consistently outperform the existing augmentation methods with explainable\nclassification on two small-scale medical datasets.","terms":["cs.CV","cs.LG"]},{"titles":"Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications","summaries":"Understanding how the surrounding environment changes is crucial for\nperforming downstream tasks safely and reliably in autonomous driving\napplications. Recent occupancy estimation techniques using only camera images\nas input can provide dense occupancy representations of large-scale scenes\nbased on the current observation. However, they are mostly limited to\nrepresenting the current 3D space and do not consider the future state of\nsurrounding objects along the time axis. To extend camera-only occupancy\nestimation into spatiotemporal prediction, we propose Cam4DOcc, a new benchmark\nfor camera-only 4D occupancy forecasting, evaluating the surrounding scene\nchanges in a near future. We build our benchmark based on multiple publicly\navailable datasets, including nuScenes, nuScenes-Occupancy, and Lyft-Level5,\nwhich provides sequential occupancy states of general movable and static\nobjects, as well as their 3D backward centripetal flow. To establish this\nbenchmark for future research with comprehensive comparisons, we introduce four\nbaseline types from diverse camera-based perception and prediction\nimplementations, including a static-world occupancy model, voxelization of\npoint cloud prediction, 2D-3D instance-based prediction, and our proposed novel\nend-to-end 4D occupancy forecasting network. Furthermore, the standardized\nevaluation protocol for preset multiple tasks is also provided to compare the\nperformance of all the proposed baselines on present and future occupancy\nestimation with respect to objects of interest in autonomous driving scenarios.\nThe dataset and our implementation of all four baselines in the proposed\nCam4DOcc benchmark will be released here: https:\/\/github.com\/haomo-ai\/Cam4DOcc.","terms":["cs.CV"]},{"titles":"visClust: A visual clustering algorithm based on orthogonal projections","summaries":"We present a novel clustering algorithm, visClust, that is based on lower\ndimensional data representations and visual interpretation. Thereto, we design\na transformation that allows the data to be represented by a binary integer\narray enabling the use of image processing methods to select a partition.\nQualitative and quantitative analyses measured in accuracy and an adjusted\nRand-Index show that the algorithm performs well while requiring low runtime\nand RAM. We compare the results to 6 state-of-the-art algorithms with available\ncode, confirming the quality of visClust by superior performance in most\nexperiments. Moreover, the algorithm asks for just one obligatory input\nparameter while allowing optimization via optional parameters. The code is made\navailable on GitHub and straightforward to use.","terms":["cs.CV"]},{"titles":"Efficient LLM Inference on CPUs","summaries":"Large language models (LLMs) have demonstrated remarkable performance and\ntremendous potential across a wide range of tasks. However, deploying these\nmodels has been challenging due to the astronomical amount of model parameters,\nwhich requires a demand for large memory capacity and high memory bandwidth. In\nthis paper, we propose an effective approach that can make the deployment of\nLLMs more efficiently. We support an automatic INT4 weight-only quantization\nflow and design a special LLM runtime with highly-optimized kernels to\naccelerate the LLM inference on CPUs. We demonstrate the general applicability\nof our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase\nthe extreme inference efficiency on CPUs. The code is publicly available at:\nhttps:\/\/github.com\/intel\/intel-extension-for-transformers.","terms":["cs.LG","cs.AI","cs.CL"]},{"titles":"TeMO: Towards Text-Driven 3D Stylization for Multi-Object Meshes","summaries":"Recent progress in the text-driven 3D stylization of a single object has been\nconsiderably promoted by CLIP-based methods. However, the stylization of\nmulti-object 3D scenes is still impeded in that the image-text pairs used for\npre-training CLIP mostly consist of an object. Meanwhile, the local details of\nmultiple objects may be susceptible to omission due to the existing supervision\nmanner primarily relying on coarse-grained contrast of image-text pairs. To\novercome these challenges, we present a novel framework, dubbed TeMO, to parse\nmulti-object 3D scenes and edit their styles under the contrast supervision at\nmultiple levels. We first propose a Decoupled Graph Attention (DGA) module to\ndistinguishably reinforce the features of 3D surface points. Particularly, a\ncross-modal graph is constructed to align the object points accurately and noun\nphrases decoupled from the 3D mesh and textual description. Then, we develop a\nCross-Grained Contrast (CGC) supervision system, where a fine-grained loss\nbetween the words in the textual description and the randomly rendered images\nare constructed to complement the coarse-grained loss. Extensive experiments\nshow that our method can synthesize high-quality stylized content and\noutperform the existing methods over a wide range of multi-object 3D meshes.\nOur code and results will be made publicly available","terms":["cs.CV"]},{"titles":"4D Gaussian Splatting for Real-Time Dynamic Scene Rendering","summaries":"Representing and rendering dynamic scenes has been an important but\nchallenging task. Especially, to accurately model complex motions, high\nefficiency is usually hard to guarantee. To achieve real-time dynamic scene\nrendering while also enjoying high training and storage efficiency, we propose\n4D Gaussian Splatting (4D-GS) as a holistic representation for dynamic scenes\nrather than applying 3D-GS for each individual frame. In 4D-GS, a novel\nexplicit representation containing both 3D Gaussians and 4D neural voxels is\nproposed. A decomposed neural voxel encoding algorithm inspired by HexPlane is\nproposed to efficiently build Gaussian features from 4D neural voxels and then\na lightweight MLP is applied to predict Gaussian deformations at novel\ntimestamps. Our 4D-GS method achieves real-time rendering under high\nresolutions, 82 FPS at an 800$\\times$800 resolution on an RTX 3090 GPU while\nmaintaining comparable or better quality than previous state-of-the-art\nmethods. More demos and code are available at\nhttps:\/\/guanjunwu.github.io\/4dgs\/.","terms":["cs.CV","cs.GR"]},{"titles":"Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?","summaries":"Existing analyses of the expressive capacity of Transformer models have\nrequired excessively deep layers for data memorization, leading to a\ndiscrepancy with the Transformers actually used in practice. This is primarily\ndue to the interpretation of the softmax function as an approximation of the\nhardmax function. By clarifying the connection between the softmax function and\nthe Boltzmann operator, we prove that a single layer of self-attention with\nlow-rank weight matrices possesses the capability to perfectly capture the\ncontext of an entire input sequence. As a consequence, we show that one-layer\nand single-head Transformers have a memorization capacity for finite samples,\nand that Transformers consisting of one self-attention layer with two\nfeed-forward neural networks are universal approximators for continuous\npermutation equivariant functions on a compact domain.","terms":["cs.LG","68T07","I.2.0"]},{"titles":"Detecting and Restoring Non-Standard Hands in Stable Diffusion Generated Images","summaries":"We introduce a pipeline to address anatomical inaccuracies in Stable\nDiffusion generated hand images. The initial step involves constructing a\nspecialized dataset, focusing on hand anomalies, to train our models\neffectively. A finetuned detection model is pivotal for precise identification\nof these anomalies, ensuring targeted correction. Body pose estimation aids in\nunderstanding hand orientation and positioning, crucial for accurate anomaly\ncorrection. The integration of ControlNet and InstructPix2Pix facilitates\nsophisticated inpainting and pixel-level transformation, respectively. This\ndual approach allows for high-fidelity image adjustments. This comprehensive\napproach ensures the generation of images with anatomically accurate hands,\nclosely resembling real-world appearances. Our experimental results demonstrate\nthe pipeline's efficacy in enhancing hand image realism in Stable Diffusion\noutputs. We provide an online demo at https:\/\/fixhand.yiqun.io","terms":["cs.CV","cs.AI"]},{"titles":"Graph Convolutions Enrich the Self-Attention in Transformers!","summaries":"Transformers, renowned for their self-attention mechanism, have achieved\nstate-of-the-art performance across various tasks in natural language\nprocessing, computer vision, time-series modeling, etc. However, one of the\nchallenges with deep Transformer models is the oversmoothing problem, where\nrepresentations across layers converge to indistinguishable values, leading to\nsignificant performance degradation. We interpret the original self-attention\nas a simple graph filter and redesign it from a graph signal processing (GSP)\nperspective. We propose graph-filter-based self-attention (GFSA) to learn a\ngeneral yet effective one, whose complexity, however, is slightly larger than\nthat of the original self-attention mechanism. We demonstrate that GFSA\nimproves the performance of Transformers in various fields, including computer\nvision, natural language processing, graph pattern classification, speech\nrecognition, and code classification.","terms":["cs.LG","cs.AI"]},{"titles":"Fine-tune vision foundation model for crack segmentation in civil infrastructures","summaries":"Large-scale foundation models have become the mainstream method in the field\nof deep learning, while in civil engineering, the scale of AI models is\nstrictly limited. In this work, vision foundation model is introduced for crack\nsegmentation. Two Parameter-efficient fine-tuning methods, adapter and low-rank\nadaptation, are adopted to fine-tune the foundation model in the field of\nsemantic segmentation: Segment Anything Model (SAM). The fine-tuned model\nCrackSAM is much larger than all the existing crack segmentation models, but\nshows excellent performance. To test the zero-shot performance of the proposed\nmethod, two unique datasets related to road and exterior wall cracks are\ncollected, annotated and open-sourced, in total 810 images. Comparative\nexperiments are conducted with twelve mature semantic segmentation models. On\ndatasets with artificial noise and previously unseen datasets, the performance\nof CrackSAM far exceeds that of all state-of-the-art models. CrackSAM exhibits\nremarkable superiority, particularly in challenging conditions such as dim\nlighting, shadows, road markings, construction joints, and other interference\nfactors. Such cross-scenario results demonstrate the outstanding zero-shot\ncapability of foundation models, and provide new ideas for the development of\nvision models in civil engineering.","terms":["cs.CV"]},{"titles":"Adventures of Trustworthy Vision-Language Models: A Survey","summaries":"Recently, transformers have become incredibly popular in computer vision and\nvision-language tasks. This notable rise in their usage can be primarily\nattributed to the capabilities offered by attention mechanisms and the\noutstanding ability of transformers to adapt and apply themselves to a variety\nof tasks and domains. Their versatility and state-of-the-art performance have\nestablished them as indispensable tools for a wide array of applications.\nHowever, in the constantly changing landscape of machine learning, the\nassurance of the trustworthiness of transformers holds utmost importance. This\npaper conducts a thorough examination of vision-language transformers,\nemploying three fundamental principles of responsible AI: Bias, Robustness, and\nInterpretability. The primary objective of this paper is to delve into the\nintricacies and complexities associated with the practical use of transformers,\nwith the overarching goal of advancing our comprehension of how to enhance\ntheir reliability and accountability.","terms":["cs.CV","cs.AI"]},{"titles":"Bayesian Methods for Media Mix Modelling with shape and funnel effects","summaries":"In recent years, significant progress in generative AI has highlighted the\nimportant role of physics-inspired models that utilize advanced mathematical\nconcepts based on fundamental physics principles to enhance artificial\nintelligence capabilities. Among these models, those based on diffusion\nequations have greatly improved image quality. This study aims to explore the\npotential uses of Maxwell-Boltzmann equation, which forms the basis of the\nkinetic theory of gases, and the Michaelis-Menten model in Marketing Mix\nModelling (MMM) applications. We propose incorporating these equations into\nHierarchical Bayesian models to analyse consumer behaviour in the context of\nadvertising. These equation sets excel in accurately describing the random\ndynamics in complex systems like social interactions and consumer-advertising\ninteractions.","terms":["cs.LG"]},{"titles":"Similarity of Neural Architectures using Adversarial Attack Transferability","summaries":"In recent years, many deep neural architectures have been developed for image\nclassification. Whether they are similar or dissimilar and what factors\ncontribute to their (dis)similarities remains curious. To address this\nquestion, we aim to design a quantitative and scalable similarity measure\nbetween neural architectures. We propose Similarity by Attack Transferability\n(SAT) from the observation that adversarial attack transferability contains\ninformation related to input gradients and decision boundaries widely used to\nunderstand model behaviors. We conduct a large-scale analysis on 69\nstate-of-the-art ImageNet classifiers using our proposed similarity function to\nanswer the question. Moreover, we observe neural architecture-related phenomena\nusing model similarity that model diversity can lead to better performance on\nmodel ensembles and knowledge distillation under specific conditions. Our\nresults provide insights into why developing diverse neural architectures with\ndistinct components is necessary.","terms":["cs.LG","cs.CV"]},{"titles":"Internal-Coordinate Density Modelling of Protein Structure: Covariance Matters","summaries":"After the recent ground-breaking advances in protein structure prediction,\none of the remaining challenges in protein machine learning is to reliably\npredict distributions of structural states. Parametric models of fluctuations\nare difficult to fit due to complex covariance structures between degrees of\nfreedom in the protein chain, often causing models to either violate local or\nglobal structural constraints. In this paper, we present a new strategy for\nmodelling protein densities in internal coordinates, which uses constraints in\n3D space to induce covariance structure between the internal degrees of\nfreedom. We illustrate the potential of the procedure by constructing a\nvariational autoencoder with full covariance output induced by the constraints\nimplied by the conditional mean in 3D, and demonstrate that our approach makes\nit possible to scale density models of internal coordinates to full protein\nbackbones in two settings: 1) a unimodal setting for proteins exhibiting small\nfluctuations and limited amounts of available data, and 2) a multimodal setting\nfor larger conformational changes in a high data regime.","terms":["cs.LG","q-bio.BM"]},{"titles":"TLCE: Transfer-Learning Based Classifier Ensembles for Few-Shot Class-Incremental Learning","summaries":"Few-shot class-incremental learning (FSCIL) struggles to incrementally\nrecognize novel classes from few examples without catastrophic forgetting of\nold classes or overfitting to new classes. We propose TLCE, which ensembles\nmultiple pre-trained models to improve separation of novel and old classes.\nTLCE minimizes interference between old and new classes by mapping old class\nimages to quasi-orthogonal prototypes using episodic training. It then\nensembles diverse pre-trained models to better adapt to novel classes despite\ndata imbalance. Extensive experiments on various datasets demonstrate that our\ntransfer learning ensemble approach outperforms state-of-the-art FSCIL methods.","terms":["cs.CV"]},{"titles":"Optimizing K-means for Big Data: A Comparative Study","summaries":"This paper presents a comparative analysis of different optimization\ntechniques for the K-means algorithm in the context of big data. K-means is a\nwidely used clustering algorithm, but it can suffer from scalability issues\nwhen dealing with large datasets. The paper explores different approaches to\novercome these issues, including parallelization, approximation, and sampling\nmethods. The authors evaluate the performance of these techniques on various\nbenchmark datasets and compare them in terms of speed, quality of clustering,\nand scalability according to the LIMA dominance criterion. The results show\nthat different techniques are more suitable for different types of datasets and\nprovide insights into the trade-offs between speed and accuracy in K-means\nclustering for big data. Overall, the paper offers a comprehensive guide for\npractitioners and researchers on how to optimize K-means for big data\napplications.","terms":["cs.LG","cs.AI","math.OC"]},{"titles":"CODEX: A Cluster-Based Method for Explainable Reinforcement Learning","summaries":"Despite the impressive feats demonstrated by Reinforcement Learning (RL),\nthese algorithms have seen little adoption in high-risk, real-world\napplications due to current difficulties in explaining RL agent actions and\nbuilding user trust. We present Counterfactual Demonstrations for Explanation\n(CODEX), a method that incorporates semantic clustering, which can effectively\nsummarize RL agent behavior in the state-action space. Experimentation on the\nMiniGrid and StarCraft II gaming environments reveals the semantic clusters\nretain temporal as well as entity information, which is reflected in the\nconstructed summary of agent behavior. Furthermore, clustering the\ndiscrete+continuous game-state latent representations identifies the most\ncrucial episodic events, demonstrating a relationship between the latent and\nsemantic spaces. This work contributes to the growing body of work that strives\nto unlock the power of RL for widespread use by leveraging and extending\ntechniques from Natural Language Processing.","terms":["cs.LG"]},{"titles":"Universal Segmentation at Arbitrary Granularity with Language Instruction","summaries":"This paper aims to achieve universal segmentation of arbitrary semantic\nlevel. Despite significant progress in recent years, specialist segmentation\napproaches are limited to specific tasks and data distribution. Retraining a\nnew model for adaptation to new scenarios or settings takes expensive\ncomputation and time cost, which raises the demand for versatile and universal\nsegmentation model that can cater to various granularity. Although some\nattempts have been made for unifying different segmentation tasks or\ngeneralization to various scenarios, limitations in the definition of paradigms\nand input-output spaces make it difficult for them to achieve accurate\nunderstanding of content at arbitrary granularity. To this end, we present\nUniLSeg, a universal segmentation model that can perform segmentation at any\nsemantic level with the guidance of language instructions. For training\nUniLSeg, we reorganize a group of tasks from original diverse distributions\ninto a unified data format, where images with texts describing segmentation\ntargets as input and corresponding masks are output. Combined with a automatic\nannotation engine for utilizing numerous unlabeled data, UniLSeg achieves\nexcellent performance on various tasks and settings, surpassing both specialist\nand unified segmentation models.","terms":["cs.CV"]},{"titles":"Constrained Hierarchical Clustering via Graph Coarsening and Optimal Cuts","summaries":"Motivated by extracting and summarizing relevant information in short\nsentence settings, such as satisfaction questionnaires, hotel reviews, and\nX\/Twitter, we study the problem of clustering words in a hierarchical fashion.\nIn particular, we focus on the problem of clustering with horizontal and\nvertical structural constraints. Horizontal constraints are typically\ncannot-link and must-link among words, while vertical constraints are\nprecedence constraints among cluster levels. We overcome state-of-the-art\nbottlenecks by formulating the problem in two steps: first, as a\nsoft-constrained regularized least-squares which guides the result of a\nsequential graph coarsening algorithm towards the horizontal feasible set.\nThen, flat clusters are extracted from the resulting hierarchical tree by\ncomputing optimal cut heights based on the available constraints. We show that\nthe resulting approach compares very well with respect to existing algorithms\nand is computationally light.","terms":["cs.LG","math.OC"]},{"titles":"SILC: Improving Vision Language Pretraining with Self-Distillation","summaries":"Image-Text pretraining on web-scale image caption datasets has become the\ndefault recipe for open vocabulary classification and retrieval models thanks\nto the success of CLIP and its variants. Several works have also used CLIP\nfeatures for dense prediction tasks and have shown the emergence of open-set\nabilities. However, the contrastive objective used by these models only focuses\non image-text alignment and does not incentivise image feature learning for\ndense prediction tasks. In this work, we introduce SILC, a novel framework for\nvision language pretraining. SILC improves image-text contrastive learning with\nthe simple addition of local-to-global correspondence learning by\nself-distillation. We show that distilling local image features from an\nexponential moving average (EMA) teacher model significantly improves model\nperformance on dense predictions tasks like detection and segmentation, while\nalso providing improvements on image-level tasks such as classification and\nretrieval. SILC models sets a new state of the art for zero-shot\nclassification, few shot classification, image and text retrieval, zero-shot\nsegmentation, and open vocabulary segmentation. We further show that SILC\nfeatures greatly benefit open vocabulary detection, captioning and visual\nquestion answering.","terms":["cs.CV"]},{"titles":"Towards Sobolev Pruning","summaries":"The increasing use of stochastic models for describing complex phenomena\nwarrants surrogate models that capture the reference model characteristics at a\nfraction of the computational cost, foregoing potentially expensive Monte Carlo\nsimulation. The predominant approach of fitting a large neural network and then\npruning it to a reduced size has commonly neglected shortcomings. The produced\nsurrogate models often will not capture the sensitivities and uncertainties\ninherent in the original model. In particular, (higher-order) derivative\ninformation of such surrogates could differ drastically. Given a large enough\nnetwork, we expect this derivative information to match. However, the pruned\nmodel will almost certainly not share this behavior.\n  In this paper, we propose to find surrogate models by using sensitivity\ninformation throughout the learning and pruning process. We build on work using\nInterval Adjoint Significance Analysis for pruning and combine it with the\nrecent advancements in Sobolev Training to accurately model the original\nsensitivity information in the pruned neural network based surrogate model. We\nexperimentally underpin the method on an example of pricing a multidimensional\nBasket option modelled through a stochastic differential equation with Brownian\nmotion. The proposed method is, however, not limited to the domain of\nquantitative finance, which was chosen as a case study for intuitive\ninterpretations of the sensitivities. It serves as a foundation for building\nfurther surrogate modelling techniques considering sensitivity information.","terms":["cs.LG","q-fin.CP"]},{"titles":"XAI-TRIS: Non-linear image benchmarks to quantify false positive post-hoc attribution of feature importance","summaries":"The field of 'explainable' artificial intelligence (XAI) has produced highly\ncited methods that seek to make the decisions of complex machine learning (ML)\nmethods 'understandable' to humans, for example by attributing 'importance'\nscores to input features. Yet, a lack of formal underpinning leaves it unclear\nas to what conclusions can safely be drawn from the results of a given XAI\nmethod and has also so far hindered the theoretical verification and empirical\nvalidation of XAI methods. This means that challenging non-linear problems,\ntypically solved by deep neural networks, presently lack appropriate remedies.\nHere, we craft benchmark datasets for three different non-linear classification\nscenarios, in which the important class-conditional features are known by\ndesign, serving as ground truth explanations. Using novel quantitative metrics,\nwe benchmark the explanation performance of a wide set of XAI methods across\nthree deep learning model architectures. We show that popular XAI methods are\noften unable to significantly outperform random performance baselines and edge\ndetection methods. Moreover, we demonstrate that explanations derived from\ndifferent model architectures can be vastly different; thus, prone to\nmisinterpretation even under controlled conditions.","terms":["cs.LG","cs.AI","cs.CV"]},{"titles":"SAMBA: A Trainable Segmentation Web-App with Smart Labelling","summaries":"Segmentation is the assigning of a semantic class to every pixel in an image\nand is a prerequisite for various statistical analysis tasks in materials\nscience, like phase quantification, physics simulations or morphological\ncharacterization. The wide range of length scales, imaging techniques and\nmaterials studied in materials science means any segmentation algorithm must\ngeneralise to unseen data and support abstract, user-defined semantic classes.\nTrainable segmentation is a popular interactive segmentation paradigm where a\nclassifier is trained to map from image features to user drawn labels. SAMBA is\na trainable segmentation tool that uses Meta's Segment Anything Model (SAM) for\nfast, high-quality label suggestions and a random forest classifier for robust,\ngeneralizable segmentations. It is accessible in the browser\n(https:\/\/www.sambasegment.com\/) without the need to download any external\ndependencies. The segmentation backend is run in the cloud, so does not require\nthe user to have powerful hardware.","terms":["cs.CV"]},{"titles":"Joint-Individual Fusion Structure with Fusion Attention Module for Multi-Modal Skin Cancer Classification","summaries":"Most convolutional neural network (CNN) based methods for skin cancer\nclassification obtain their results using only dermatological images. Although\ngood classification results have been shown, more accurate results can be\nachieved by considering the patient's metadata, which is valuable clinical\ninformation for dermatologists. Current methods only use the simple joint\nfusion structure (FS) and fusion modules (FMs) for the multi-modal\nclassification methods, there still is room to increase the accuracy by\nexploring more advanced FS and FM. Therefore, in this paper, we design a new\nfusion method that combines dermatological images (dermoscopy images or\nclinical images) and patient metadata for skin cancer classification from the\nperspectives of FS and FM. First, we propose a joint-individual fusion (JIF)\nstructure that learns the shared features of multi-modality data and preserves\nspecific features simultaneously. Second, we introduce a fusion attention (FA)\nmodule that enhances the most relevant image and metadata features based on\nboth the self and mutual attention mechanism to support the decision-making\npipeline. We compare the proposed JIF-MMFA method with other state-of-the-art\nfusion methods on three different public datasets. The results show that our\nJIF-MMFA method improves the classification results for all tested CNN\nbackbones and performs better than the other fusion methods on the three public\ndatasets, demonstrating our method's effectiveness and robustness","terms":["cs.CV","cs.AI"]},{"titles":"Exploring the Interactive Guidance for Unified and Effective Image Matting","summaries":"Recent image matting studies are developing towards proposing trimap-free or\ninteractive methods for complete complex image matting tasks. Although avoiding\nthe extensive labors of trimap annotation, existing methods still suffer from\ntwo limitations: (1) For the single image with multiple objects, it is\nessential to provide extra interaction information to help determining the\nmatting target; (2) For transparent objects, the accurate regression of alpha\nmatte from RGB image is much more difficult compared with the opaque ones. In\nthis work, we propose a Unified Interactive image Matting method, named UIM,\nwhich solves the limitations and achieves satisfying matting results for any\nscenario. Specifically, UIM leverages multiple types of user interaction to\navoid the ambiguity of multiple matting targets, and we compare the pros and\ncons of different annotation types in detail. To unify the matting performance\nfor transparent and opaque objects, we decouple image matting into two stages,\ni.e., foreground segmentation and transparency prediction. Moreover, we design\na multi-scale attentive fusion module to alleviate the vagueness in the\nboundary region. Experimental results demonstrate that UIM achieves\nstate-of-the-art performance on the Composition-1K test set and a synthetic\nunified dataset.","terms":["cs.CV","cs.MM"]},{"titles":"LIPEx-Locally Interpretable Probabilistic Explanations-To Look Beyond The True Class","summaries":"In this work, we instantiate a novel perturbation-based multi-class\nexplanation framework, LIPEx (Locally Interpretable Probabilistic Explanation).\nWe demonstrate that LIPEx not only locally replicates the probability\ndistributions output by the widely used complex classification models but also\nprovides insight into how every feature deemed to be important affects the\nprediction probability for each of the possible classes. We achieve this by\ndefining the explanation as a matrix obtained via regression with respect to\nthe Hellinger distance in the space of probability distributions. Ablation\ntests on text and image data, show that LIPEx-guided removal of important\nfeatures from the data causes more change in predictions for the underlying\nmodel than similar tests based on other saliency-based or feature\nimportance-based Explainable AI (XAI) methods. It is also shown that compared\nto LIME, LIPEx is more data efficient in terms of using a lesser number of\nperturbations of the data to obtain a reliable explanation. This\ndata-efficiency is seen to manifest as LIPEx being able to compute its\nexplanation matrix around 53% faster than all-class LIME, for classification\nexperiments with text data.","terms":["cs.LG"]},{"titles":"Temporal Shuffling for Defending Deep Action Recognition Models against Adversarial Attacks","summaries":"Recently, video-based action recognition methods using convolutional neural\nnetworks (CNNs) achieve remarkable recognition performance. However, there is\nstill lack of understanding about the generalization mechanism of action\nrecognition models. In this paper, we suggest that action recognition models\nrely on the motion information less than expected, and thus they are robust to\nrandomization of frame orders. Furthermore, we find that motion monotonicity\nremaining after randomization also contributes to such robustness. Based on\nthis observation, we develop a novel defense method using temporal shuffling of\ninput videos against adversarial attacks for action recognition models. Another\nobservation enabling our defense method is that adversarial perturbations on\nvideos are sensitive to temporal destruction. To the best of our knowledge,\nthis is the first attempt to design a defense method without additional\ntraining for 3D CNN-based video action recognition models.","terms":["cs.CV"]},{"titles":"Coherent energy and force uncertainty in deep learning force fields","summaries":"In machine learning energy potentials for atomic systems, forces are commonly\nobtained as the negative derivative of the energy function with respect to\natomic positions. To quantify aleatoric uncertainty in the predicted energies,\na widely used modeling approach involves predicting both a mean and variance\nfor each energy value. However, this model is not differentiable under the\nusual white noise assumption, so energy uncertainty does not naturally\ntranslate to force uncertainty. In this work we propose a machine learning\npotential energy model in which energy and force aleatoric uncertainty are\nlinked through a spatially correlated noise process. We demonstrate our\napproach on an equivariant messages passing neural network potential trained on\nenergies and forces on two out-of-equilibrium molecular datasets. Furthermore,\nwe also show how to obtain epistemic uncertainties in this setting based on a\nBayesian interpretation of deep ensemble models.","terms":["stat.ML","cs.LG","physics.comp-ph"]},{"titles":"A novel feature selection framework for incomplete data","summaries":"Feature selection on incomplete datasets is an exceptionally challenging\ntask. Existing methods address this challenge by first employing imputation\nmethods to complete the incomplete data and then conducting feature selection\nbased on the imputed data. Since imputation and feature selection are entirely\nindependent steps, the importance of features cannot be considered during\nimputation. However, in real-world scenarios or datasets, different features\nhave varying degrees of importance. To address this, we propose a novel\nincomplete data feature selection framework that considers feature importance.\nThe framework mainly consists of two alternating iterative stages: the M-stage\nand the W-stage. In the M-stage, missing values are imputed based on a given\nfeature importance vector and multiple initial imputation results. In the\nW-stage, an improved reliefF algorithm is employed to learn the feature\nimportance vector based on the imputed data. Specifically, the feature\nimportance vector obtained in the current iteration of the W-stage serves as\ninput for the next iteration of the M-stage. Experimental results on both\nartificially generated and real incomplete datasets demonstrate that the\nproposed method outperforms other approaches significantly.","terms":["cs.LG"]},{"titles":"Open-vocabulary object 6D pose estimation","summaries":"We introduce the new setting of open-vocabulary object 6D pose estimation, in\nwhich a textual prompt is used to specify the object of interest. In contrast\nto existing approaches, in our setting (i) the object of interest is specified\nsolely through the textual prompt, (ii) no object model (e.g. CAD or video\nsequence) is required at inference, (iii) the object is imaged from two\ndifferent viewpoints of two different scenes, and (iv) the object was not\nobserved during the training phase. To operate in this setting, we introduce a\nnovel approach that leverages a Vision-Language Model to segment the object of\ninterest from two distinct scenes and to estimate its relative 6D pose. The key\nof our approach is a carefully devised strategy to fuse object-level\ninformation provided by the prompt with local image features, resulting in a\nfeature space that can generalize to novel concepts. We validate our approach\non a new benchmark based on two popular datasets, REAL275 and Toyota-Light,\nwhich collectively encompass 39 object instances appearing in four thousand\nimage pairs. The results demonstrate that our approach outperforms both a\nwell-established hand-crafted method and a recent deep learning-based baseline\nin estimating the relative 6D pose of objects in different scenes. Project\npage: https:\/\/jcorsetti.github.io\/oryon\/.","terms":["cs.CV"]},{"titles":"Augmentation-Free Dense Contrastive Knowledge Distillation for Efficient Semantic Segmentation","summaries":"In recent years, knowledge distillation methods based on contrastive learning\nhave achieved promising results on image classification and object detection\ntasks. However, in this line of research, we note that less attention is paid\nto semantic segmentation. Existing methods heavily rely on data augmentation\nand memory buffer, which entail high computational resource demands when\napplying them to handle semantic segmentation that requires to preserve\nhigh-resolution feature maps for making dense pixel-wise predictions. In order\nto address this problem, we present Augmentation-free Dense Contrastive\nKnowledge Distillation (Af-DCD), a new contrastive distillation learning\nparadigm to train compact and accurate deep neural networks for semantic\nsegmentation applications. Af-DCD leverages a masked feature mimicking\nstrategy, and formulates a novel contrastive learning loss via taking advantage\nof tactful feature partitions across both channel and spatial dimensions,\nallowing to effectively transfer dense and structured local knowledge learnt by\nthe teacher model to a target student model while maintaining training\nefficiency. Extensive experiments on five mainstream benchmarks with various\nteacher-student network pairs demonstrate the effectiveness of our approach.\nFor instance, the DeepLabV3-Res18|DeepLabV3-MBV2 model trained by Af-DCD\nreaches 77.03%|76.38% mIOU on Cityscapes dataset when choosing DeepLabV3-Res101\nas the teacher, setting new performance records. Besides that, Af-DCD achieves\nan absolute mIOU improvement of 3.26%|3.04%|2.75%|2.30%|1.42% compared with\nindividually trained counterpart on Cityscapes|Pascal\nVOC|Camvid|ADE20K|COCO-Stuff-164K. Code is available at\nhttps:\/\/github.com\/OSVAI\/Af-DCD","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"Mixture of Dynamical Variational Autoencoders for Multi-Source Trajectory Modeling and Separation","summaries":"In this paper, we propose a latent-variable generative model called mixture\nof dynamical variational autoencoders (MixDVAE) to model the dynamics of a\nsystem composed of multiple moving sources. A DVAE model is pre-trained on a\nsingle-source dataset to capture the source dynamics. Then, multiple instances\nof the pre-trained DVAE model are integrated into a multi-source mixture model\nwith a discrete observation-to-source assignment latent variable. The posterior\ndistributions of both the discrete observation-to-source assignment variable\nand the continuous DVAE variables representing the sources content\/position are\nestimated using a variational expectation-maximization algorithm, leading to\nmulti-source trajectories estimation. We illustrate the versatility of the\nproposed MixDVAE model on two tasks: a computer vision task, namely\nmulti-object tracking, and an audio processing task, namely single-channel\naudio source separation. Experimental results show that the proposed method\nworks well on these two tasks, and outperforms several baseline methods.","terms":["cs.LG"]},{"titles":"Improving Communication Efficiency of Federated Distillation via Accumulating Local Updates","summaries":"As an emerging federated learning paradigm, federated distillation enables\ncommunication-efficient model training by transmitting only small-scale\nknowledge during the learning process. To further improve the communication\nefficiency of federated distillation, we propose a novel technique, ALU, which\naccumulates multiple rounds of local updates before transferring the knowledge\nto the central server. ALU drastically decreases the frequency of communication\nin federated distillation, thereby significantly reducing the communication\noverhead during the training process. Empirical experiments demonstrate the\nsubstantial effect of ALU in improving the communication efficiency of\nfederated distillation.","terms":["cs.LG","cs.DC"]},{"titles":"DualGenerator: Information Interaction-based Generative Network for Point Cloud Completion","summaries":"Point cloud completion estimates complete shapes from incomplete point clouds\nto obtain higher-quality point cloud data. Most existing methods only consider\nglobal object features, ignoring spatial and semantic information of adjacent\npoints. They cannot distinguish structural information well between different\nobject parts, and the robustness of models is poor. To tackle these challenges,\nwe propose an information interaction-based generative network for point cloud\ncompletion ($\\mathbf{DualGenerator}$). It contains an adversarial generation\npath and a variational generation path, which interact with each other and\nshare weights. DualGenerator introduces a local refinement module in generation\npaths, which captures general structures from partial inputs, and then refines\nshape details of the point cloud. It promotes completion in the unknown region\nand makes a distinction between different parts more obvious. Moreover, we\ndesign DGStyleGAN to improve the generation quality further. It promotes the\nrobustness of this network combined with fusion analysis of dual-path\ncompletion results. Qualitative and quantitative evaluations demonstrate that\nour method is superior on MVP and Completion3D datasets. The performance will\nnot degrade significantly after adding noise interference or sparse sampling.","terms":["cs.CV"]},{"titles":"Multi-scale Residual Transformer for VLF Lightning Transients Classification","summaries":"The utilization of Very Low Frequency (VLF) electromagnetic signals in\nnavigation systems is widespread. However, the non-stationary behavior of\nlightning signals can affect VLF electromagnetic signal transmission.\nAccurately classifying lightning signals is important for reducing interference\nand noise in VLF, thereby improving the reliability and overall performance of\nnavigation systems. In recent years, the evolution of deep learning,\nspecifically Convolutional Neural Network (CNNs), has sparked a transformation\nin lightning classification, surpassing traditional statistical methodologies.\nExisting CNN models have limitations as they overlook the diverse attributes of\nlightning signals across different scales and neglect the significance of\ntemporal sequencing in sequential signals. This study introduces an innovative\nmulti-scale residual transform (MRTransformer) that not only has the ability to\ndiscern intricate fine-grained patterns while also weighing the significance of\ndifferent aspects within the input lightning signal sequence. This model\nperforms the attributes of the lightning signal across different scales and the\nlevel of accuracy reached 90% in the classification. In future work, this model\nhas the potential applied to a comprehensive understanding of the localization\nand waveform characteristics of lightning signals.","terms":["stat.ML","cs.LG"]},{"titles":"Text as Image: Learning Transferable Adapter for Multi-Label Classification","summaries":"Pre-trained vision-language models have notably accelerated progress of\nopen-world concept recognition. Their impressive zero-shot ability has recently\nbeen transferred to multi-label image classification via prompt tuning,\nenabling to discover novel labels in an open-vocabulary manner. However, this\nparadigm suffers from non-trivial training costs, and becomes computationally\nprohibitive for a large number of candidate labels. To address this issue, we\nnote that vision-language pre-training aligns images and texts in a unified\nembedding space, making it potential for an adapter network to identify labels\nin visual modality while be trained in text modality. To enhance such\ncross-modal transfer ability, a simple yet effective method termed random\nperturbation is proposed, which enables the adapter to search for potential\nvisual embeddings by perturbing text embeddings with noise during training,\nresulting in better performance in visual modality. Furthermore, we introduce\nan effective approach to employ large language models for multi-label\ninstruction-following text generation. In this way, a fully automated pipeline\nfor visual label recognition is developed without relying on any manual data.\nExtensive experiments on public benchmarks show the superiority of our method\nin various multi-label classification tasks.","terms":["cs.CV"]},{"titles":"Resource Allocation of Federated Learning for the Metaverse with Mobile Augmented Reality","summaries":"The Metaverse has received much attention recently. Metaverse applications\nvia mobile augmented reality (MAR) require rapid and accurate object detection\nto mix digital data with the real world. Federated learning (FL) is an\nintriguing distributed machine learning approach due to its privacy-preserving\ncharacteristics. Due to privacy concerns and the limited computation resources\non mobile devices, we incorporate FL into MAR systems of the Metaverse to train\na model cooperatively. Besides, to balance the trade-off between energy,\nexecution latency and model accuracy, thereby accommodating different demands\nand application scenarios, we formulate an optimization problem to minimize a\nweighted combination of total energy consumption, completion time and model\naccuracy. Through decomposing the non-convex optimization problem into two\nsubproblems, we devise a resource allocation algorithm to determine the\nbandwidth allocation, transmission power, CPU frequency and video frame\nresolution for each participating device. We further present the convergence\nanalysis and computational complexity of the proposed algorithm. Numerical\nresults show that our proposed algorithm has better performance (in terms of\nenergy consumption, completion time and model accuracy) under different weight\nparameters compared to existing benchmarks.","terms":["cs.LG","cs.SI"]},{"titles":"EulerMormer: Robust Eulerian Motion Magnification via Dynamic Filtering within Transformer","summaries":"Video Motion Magnification (VMM) aims to break the resolution limit of human\nvisual perception capability and reveal the imperceptible minor motion that\ncontains valuable information in the macroscopic domain. However, challenges\narise in this task due to photon noise inevitably introduced by photographic\ndevices and spatial inconsistency in amplification, leading to flickering\nartifacts in static fields and motion blur and distortion in dynamic fields in\nthe video. Existing methods focus on explicit motion modeling without\nemphasizing prioritized denoising during the motion magnification process. This\npaper proposes a novel dynamic filtering strategy to achieve static-dynamic\nfield adaptive denoising. Specifically, based on Eulerian theory, we separate\ntexture and shape to extract motion representation through inter-frame shape\ndifferences, expecting to leverage these subdivided features to solve this task\nfinely. Then, we introduce a novel dynamic filter that eliminates noise cues\nand preserves critical features in the motion magnification and amplification\ngeneration phases. Overall, our unified framework, EulerMormer, is a pioneering\neffort to first equip with Transformer in learning-based VMM. The core of the\ndynamic filter lies in a global dynamic sparse cross-covariance attention\nmechanism that explicitly removes noise while preserving vital information,\ncoupled with a multi-scale dual-path gating mechanism that selectively\nregulates the dependence on different frequency features to reduce spatial\nattenuation and complement motion boundaries. We demonstrate extensive\nexperiments that EulerMormer achieves more robust video motion magnification\nfrom the Eulerian perspective, significantly outperforming state-of-the-art\nmethods. The source code is available at\nhttps:\/\/github.com\/VUT-HFUT\/EulerMormer.","terms":["cs.CV"]},{"titles":"Calibration in Machine Learning Uncertainty Quantification: beyond consistency to target adaptivity","summaries":"Reliable uncertainty quantification (UQ) in machine learning (ML) regression\ntasks is becoming the focus of many studies in materials and chemical science.\nIt is now well understood that average calibration is insufficient, and most\nstudies implement additional methods testing the conditional calibration with\nrespect to uncertainty, i.e. consistency. Consistency is assessed mostly by\nso-called reliability diagrams. There exists however another way beyond average\ncalibration, which is conditional calibration with respect to input features,\ni.e. adaptivity. In practice, adaptivity is the main concern of the final users\nof a ML-UQ method, seeking for the reliability of predictions and uncertainties\nfor any point in features space. This article aims to show that consistency and\nadaptivity are complementary validation targets, and that a good consistency\ndoes not imply a good adaptivity. Adapted validation methods are proposed and\nillustrated on a representative example.","terms":["stat.ML","cs.LG","physics.chem-ph","physics.data-an"]},{"titles":"Diffusing Colors: Image Colorization with Text Guided Diffusion","summaries":"The colorization of grayscale images is a complex and subjective task with\nsignificant challenges. Despite recent progress in employing large-scale\ndatasets with deep neural networks, difficulties with controllability and\nvisual quality persist. To tackle these issues, we present a novel image\ncolorization framework that utilizes image diffusion techniques with granular\ntext prompts. This integration not only produces colorization outputs that are\nsemantically appropriate but also greatly improves the level of control users\nhave over the colorization process. Our method provides a balance between\nautomation and control, outperforming existing techniques in terms of visual\nquality and semantic coherence. We leverage a pretrained generative Diffusion\nModel, and show that we can finetune it for the colorization task without\nlosing its generative power or attention to text prompts. Moreover, we present\na novel CLIP-based ranking model that evaluates color vividness, enabling\nautomatic selection of the most suitable level of vividness based on the\nspecific scene semantics. Our approach holds potential particularly for color\nenhancement and historical image colorization.","terms":["cs.CV","cs.GR","cs.LG"]},{"titles":"Towards 4D Human Video Stylization","summaries":"We present a first step towards 4D (3D and time) human video stylization,\nwhich addresses style transfer, novel view synthesis and human animation within\na unified framework. While numerous video stylization methods have been\ndeveloped, they are often restricted to rendering images in specific viewpoints\nof the input video, lacking the capability to generalize to novel views and\nnovel poses in dynamic scenes. To overcome these limitations, we leverage\nNeural Radiance Fields (NeRFs) to represent videos, conducting stylization in\nthe rendered feature space. Our innovative approach involves the simultaneous\nrepresentation of both the human subject and the surrounding scene using two\nNeRFs. This dual representation facilitates the animation of human subjects\nacross various poses and novel viewpoints. Specifically, we introduce a novel\ngeometry-guided tri-plane representation, significantly enhancing feature\nrepresentation robustness compared to direct tri-plane optimization. Following\nthe video reconstruction, stylization is performed within the NeRFs' rendered\nfeature space. Extensive experiments demonstrate that the proposed method\nstrikes a superior balance between stylized textures and temporal coherence,\nsurpassing existing approaches. Furthermore, our framework uniquely extends its\ncapabilities to accommodate novel poses and viewpoints, making it a versatile\ntool for creative human video stylization.","terms":["cs.CV"]},{"titles":"On the Interplay Between Stepsize Tuning and Progressive Sharpening","summaries":"Recent empirical work has revealed an intriguing property of deep learning\nmodels by which the sharpness (largest eigenvalue of the Hessian) increases\nthroughout optimization until it stabilizes around a critical value at which\nthe optimizer operates at the edge of stability, given a fixed stepsize (Cohen\net al, 2022). We investigate empirically how the sharpness evolves when using\nstepsize-tuners, the Armijo linesearch and Polyak stepsizes, that adapt the\nstepsize along the iterations to local quantities such as, implicitly, the\nsharpness itself. We find that the surprisingly poor performance of a classical\nArmijo linesearch may be well explained by its tendency to ever-increase the\nsharpness of the objective in the full or large batch regimes. On the other\nhand, we observe that Polyak stepsizes operate generally at the edge of\nstability or even slightly beyond, while outperforming its Armijo and constant\nstepsizes counterparts. We conclude with an analysis that suggests unlocking\nstepsize tuners requires an understanding of the joint dynamics of the step\nsize and the sharpness.","terms":["cs.LG","cs.AI","math.OC"]},{"titles":"TimeDRL: Disentangled Representation Learning for Multivariate Time-Series","summaries":"Multivariate time-series data in numerous real-world applications (e.g.,\nhealthcare and industry) are informative but challenging due to the lack of\nlabels and high dimensionality. Recent studies in self-supervised learning have\nshown their potential in learning rich representations without relying on\nlabels, yet they fall short in learning disentangled embeddings and addressing\nissues of inductive bias (e.g., transformation-invariance). To tackle these\nchallenges, we propose TimeDRL, a generic multivariate time-series\nrepresentation learning framework with disentangled dual-level embeddings.\nTimeDRL is characterized by three novel features: (i) disentangled derivation\nof timestamp-level and instance-level embeddings from patched time-series data\nusing a [CLS] token strategy; (ii) utilization of timestamp-predictive and\ninstance-contrastive tasks for disentangled representation learning, with the\nformer optimizing timestamp-level embeddings with predictive loss, and the\nlatter optimizing instance-level embeddings with contrastive loss; and (iii)\navoidance of augmentation methods to eliminate inductive biases, such as\ntransformation-invariance from cropping and masking. Comprehensive experiments\non 6 time-series forecasting datasets and 5 time-series classification datasets\nhave shown that TimeDRL consistently surpasses existing representation learning\napproaches, achieving an average improvement of forecasting by 57.98% in MSE\nand classification by 1.25% in accuracy. Furthermore, extensive ablation\nstudies confirmed the relative contribution of each component in TimeDRL's\narchitecture, and semi-supervised learning evaluations demonstrated its\neffectiveness in real-world scenarios, even with limited labeled data.","terms":["cs.LG","cs.AI"]},{"titles":"Polarimetric Light Transport Analysis for Specular Inter-reflection","summaries":"Polarization is well known for its ability to decompose diffuse and specular\nreflections. However, the existing decomposition methods only focus on direct\nreflection and overlook multiple reflections, especially specular\ninter-reflection. In this paper, we propose a novel decomposition method for\nhandling specular inter-reflection of metal objects by using a unique\npolarimetric feature: the rotation direction of linear polarization. This\nrotation direction serves as a discriminative factor between direct and\ninter-reflection on specular surfaces. To decompose the reflectance components,\nwe actively rotate the linear polarization of incident light and analyze the\nrotation direction of the reflected light. We evaluate our method using both\nsynthetic and real data, demonstrating its effectiveness in decomposing\nspecular inter-reflections of metal objects. Furthermore, we demonstrate that\nour method can be combined with other decomposition methods for a detailed\nanalysis of light transport. As a practical application, we show its\neffectiveness in improving the accuracy of 3D measurement against strong\nspecular inter-reflection.","terms":["cs.CV","eess.IV"]},{"titles":"Assessing hierarchies by their consistent segmentations","summaries":"Current approaches to generic segmentation start by creating a hierarchy of\nnested image partitions and then specifying a segmentation from it. Our first\ncontribution is to describe several ways, most of them new, for specifying\nsegmentations using the hierarchy elements. Then, we consider the best\nhierarchy-induced segmentation specified by a limited number of hierarchy\nelements. We focus on a common quality measure for binary segmentations, the\nJaccard index (also known as IoU). Optimizing the Jaccard index is highly\nnon-trivial, and yet we propose an efficient approach for doing exactly that.\nThis way we get algorithm-independent upper bounds on the quality of any\nsegmentation created from the hierarchy. We found that the obtainable\nsegmentation quality varies significantly depending on the way that the\nsegments are specified by the hierarchy elements, and that representing a\nsegmentation with only a few hierarchy elements is often possible. (Code is\navailable).","terms":["cs.CV","eess.IV"]},{"titles":"A New Fine-grained Alignment Method for Image-text Matching","summaries":"Image-text retrieval is a widely studied topic in the field of computer\nvision due to the exponential growth of multimedia data, whose core concept is\nto measure the similarity between images and text. However, most existing\nretrieval methods heavily rely on cross-attention mechanisms for cross-modal\nfine-grained alignment, which takes into account excessive irrelevant regions\nand treats prominent and non-significant words equally, thereby limiting\nretrieval accuracy. This paper aims to investigate an alignment approach that\nreduces the involvement of non-significant fragments in images and text while\nenhancing the alignment of prominent segments. For this purpose, we introduce\nthe Cross-Modal Prominent Fragments Enhancement Aligning Network(CPFEAN), which\nachieves improved retrieval accuracy by diminishing the participation of\nirrelevant regions during alignment and relatively increasing the alignment\nsimilarity of prominent words. Additionally, we incorporate prior textual\ninformation into image regions to reduce misalignment occurrences. In practice,\nwe first design a novel intra-modal fragments relationship reasoning method,\nand subsequently employ our proposed alignment mechanism to compute the\nsimilarity between images and text. Extensive quantitative comparative\nexperiments on MS-COCO and Flickr30K datasets demonstrate that our approach\noutperforms state-of-the-art methods by about 5% to 10% in the rSum metric.","terms":["cs.CV","cs.AI","cs.MM"]},{"titles":"Forensic Iris Image Synthesis","summaries":"Post-mortem iris recognition is an emerging application of iris-based human\nidentification in a forensic setup, able to correctly identify deceased\nsubjects even three weeks post-mortem. This technique thus is considered as an\nimportant component of future forensic toolkits. The current advancements in\nthis field are seriously slowed down by exceptionally difficult data\ncollection, which can happen in mortuary conditions, at crime scenes, or in\n``body farm'' facilities. This paper makes a novel contribution to facilitate\nprogress in post-mortem iris recognition by offering a conditional\nStyleGAN-based iris synthesis model, trained on the largest-available dataset\nof post-mortem iris samples acquired from more than 350 subjects, generating --\nthrough appropriate exploration of StyleGAN latent space -- multiple\nwithin-class (same identity) and between-class (different new identities)\npost-mortem iris images, compliant with ISO\/IEC 29794-6, and with decomposition\ndeformations controlled by the requested PMI (post mortem interval). Besides an\nobvious application to enhance the existing, very sparse, post-mortem iris\ndatasets to advance -- among others -- iris presentation attack endeavors, we\nanticipate it may be useful to generate samples that would expose professional\nforensic human examiners to never-seen-before deformations for various PMIs,\nincreasing their training effectiveness. The source codes and model weights are\nmade available with the paper.","terms":["cs.CV"]},{"titles":"Constrained Few-Shot Learning: Human-Like Low Sample Complexity Learning and Non-Episodic Text Classification","summaries":"Few-shot learning (FSL) is an emergent paradigm of learning that attempts to\nlearn to reason with low sample complexity to mimic the way humans learn,\ngeneralise and extrapolate from only a few seen examples. While FSL attempts to\nmimic these human characteristics, fundamentally, the task of FSL as\nconventionally formulated using meta-learning with episodic-based training does\nnot in actuality align with how humans acquire and reason with knowledge. FSL\nwith episodic training, while only requires $K$ instances of each test class,\nstill requires a large number of labelled training instances from disjoint\nclasses. In this paper, we introduce the novel task of constrained few-shot\nlearning (CFSL), a special case of FSL where $M$, the number of instances of\neach training class is constrained such that $M \\leq K$ thus applying a similar\nrestriction during FSL training and test. We propose a method for CFSL\nleveraging Cat2Vec using a novel categorical contrastive loss inspired by\ncognitive theories such as fuzzy trace theory and prototype theory.","terms":["cs.LG","cs.CL"]},{"titles":"A Multilevel Guidance-Exploration Network and Behavior-Scene Matching Method for Human Behavior Anomaly Detection","summaries":"Human behavior anomaly detection aims to identify unusual human actions,\nplaying a crucial role in intelligent surveillance and other areas. The current\nmainstream methods still adopt reconstruction or future frame prediction\ntechniques. However, reconstructing or predicting low-level pixel features\neasily enables the network to achieve overly strong generalization ability,\nallowing anomalies to be reconstructed or predicted as effectively as normal\ndata. Different from their methods, inspired by the Student-Teacher Network, we\npropose a novel framework called the Multilevel Guidance-Exploration\nNetwork(MGENet), which detects anomalies through the difference in high-level\nrepresentation between the Guidance and Exploration network. Specifically, we\nfirst utilize the pre-trained Normalizing Flow that takes skeletal keypoints as\ninput to guide an RGB encoder, which takes unmasked RGB frames as input, to\nexplore motion latent features. Then, the RGB encoder guides the mask encoder,\nwhich takes masked RGB frames as input, to explore the latent appearance\nfeature. Additionally, we design a Behavior-Scene Matching Module(BSMM) to\ndetect scene-related behavioral anomalies. Extensive experiments demonstrate\nthat our proposed method achieves state-of-the-art performance on ShanghaiTech\nand UBnormal datasets, with AUC of 86.9 % and 73.5 %, respectively. The code\nwill be available on https:\/\/github.com\/molu-ggg\/GENet.","terms":["cs.CV"]},{"titles":"Caregiver Talk Shapes Toddler Vision: A Computational Study of Dyadic Play","summaries":"Infants' ability to recognize and categorize objects develops gradually. The\nsecond year of life is marked by both the emergence of more semantic visual\nrepresentations and a better understanding of word meaning. This suggests that\nlanguage input may play an important role in shaping visual representations.\nHowever, even in suitable contexts for word learning like dyadic play sessions,\ncaregivers utterances are sparse and ambiguous, often referring to objects that\nare different from the one to which the child attends. Here, we systematically\ninvestigate to what extent caregivers' utterances can nevertheless enhance\nvisual representations. For this we propose a computational model of visual\nrepresentation learning during dyadic play. We introduce a synthetic dataset of\nego-centric images perceived by a toddler-agent that moves and rotates toy\nobjects in different parts of its home environment while hearing caregivers'\nutterances, modeled as captions. We propose to model toddlers' learning as\nsimultaneously aligning representations for 1) close-in-time images and 2)\nco-occurring images and utterances. We show that utterances with statistics\nmatching those of real caregivers give rise to representations supporting\nimproved category recognition. Our analysis reveals that a small\ndecrease\/increase in object-relevant naming frequencies can drastically impact\nthe learned representations. This affects the attention on object names within\nan utterance, which is required for efficient visuo-linguistic alignment.\nOverall, our results support the hypothesis that caregivers' naming utterances\ncan improve toddlers' visual representations.","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"Instance Tracking in 3D Scenes from Egocentric Videos","summaries":"Egocentric sensors such as AR\/VR devices capture human-object interactions\nand offer the potential to provide task-assistance by recalling 3D locations of\nobjects of interest in the surrounding environment. This capability requires\ninstance tracking in real-world 3D scenes from egocentric videos (IT3DEgo). We\nexplore this problem by first introducing a new benchmark dataset, consisting\nof RGB and depth videos, per-frame camera pose, and instance-level annotations\nin both 2D camera and 3D world coordinates. We present an evaluation protocol\nwhich evaluates tracking performance in 3D coordinates with two settings for\nenrolling instances to track: (1) single-view online enrollment where an\ninstance is specified on-the-fly based on the human wearer's interactions. and\n(2) multi-view pre-enrollment where images of an instance to be tracked are\nstored in memory ahead of time. To address IT3DEgo, we first re-purpose methods\nfrom relevant areas, e.g., single object tracking (SOT) -- running SOT methods\nto track instances in 2D frames and lifting them to 3D using camera pose and\ndepth. We also present a simple method that leverages pretrained segmentation\nand detection models to generate proposals from RGB frames and match proposals\nwith enrolled instance images. Perhaps surprisingly, our extensive experiments\nshow that our method (with no finetuning) significantly outperforms SOT-based\napproaches. We conclude by arguing that the problem of egocentric instance\ntracking is made easier by leveraging camera pose and using a 3D allocentric\n(world) coordinate representation.","terms":["cs.CV"]},{"titles":"HODN: Disentangling Human-Object Feature for HOI Detection","summaries":"The task of Human-Object Interaction (HOI) detection is to detect humans and\ntheir interactions with surrounding objects, where transformer-based methods\nshow dominant advances currently. However, these methods ignore the\nrelationship among humans, objects, and interactions: 1) human features are\nmore contributive than object ones to interaction prediction; 2) interactive\ninformation disturbs the detection of objects but helps human detection. In\nthis paper, we propose a Human and Object Disentangling Network (HODN) to model\nthe HOI relationships explicitly, where humans and objects are first detected\nby two disentangling decoders independently and then processed by an\ninteraction decoder. Considering that human features are more contributive to\ninteraction, we propose a Human-Guide Linking method to make sure the\ninteraction decoder focuses on the human-centric regions with human features as\nthe positional embeddings. To handle the opposite influences of interactions on\nhumans and objects, we propose a Stop-Gradient Mechanism to stop interaction\ngradients from optimizing the object detection but to allow them to optimize\nthe human detection. Our proposed method achieves competitive performance on\nboth the V-COCO and the HICO-Det datasets. It can be combined with existing\nmethods easily for state-of-the-art results.","terms":["cs.CV"]},{"titles":"Activity Sparsity Complements Weight Sparsity for Efficient RNN Inference","summaries":"Artificial neural networks open up unprecedented machine learning\ncapabilities at the cost of ever growing computational requirements.\nSparsifying the parameters, often achieved through weight pruning, has been\nidentified as a powerful technique to compress the number of model parameters\nand reduce the computational operations of neural networks. Yet, sparse\nactivations, while omnipresent in both biological neural networks and deep\nlearning systems, have not been fully utilized as a compression technique in\ndeep learning. Moreover, the interaction between sparse activations and weight\npruning is not fully understood. In this work, we demonstrate that activity\nsparsity can compose multiplicatively with parameter sparsity in a recurrent\nneural network model based on the GRU that is designed to be activity sparse.\nWe achieve up to $20\\times$ reduction of computation while maintaining\nperplexities below $60$ on the Penn Treebank language modeling task. This\nmagnitude of reduction has not been achieved previously with solely sparsely\nconnected LSTMs, and the language modeling performance of our model has not\nbeen achieved previously with any sparsely activated recurrent neural networks\nor spiking neural networks. Neuromorphic computing devices are especially good\nat taking advantage of the dynamic activity sparsity, and our results provide\nstrong evidence that making deep learning models activity sparse and porting\nthem to neuromorphic devices can be a viable strategy that does not compromise\non task performance. Our results also drive further convergence of methods from\ndeep learning and neuromorphic computing for efficient machine learning.","terms":["cs.LG"]},{"titles":"Multi-strategy Collaborative Optimized YOLOv5s and its Application in Distance Estimation","summaries":"The increasing accident rate brought about by the explosive growth of\nautomobiles has made the research on active safety systems of automobiles\nincreasingly important. The importance of improving the accuracy of vehicle\ntarget detection is self-evident. To achieve the goals of vehicle detection and\ndistance estimation and provide safety warnings, a Distance Estimation Safety\nWarning System (DESWS) based on a new neural network model (YOLOv5s-SE) by\nreplacing the IoU with DIoU, embedding SE attention module, and a distance\nestimation method through using the principle of similar triangles was\nproposed. In addition, a method that can give safety suggestions based on the\nestimated distance using nonparametric testing was presented in this work.\nThrough the simulation experiment, it was verified that the mAP was improved by\n5.5% and the purpose of giving safety suggestions based on the estimated\ndistance information can be achieved.","terms":["cs.CV"]},{"titles":"Breaking the Entanglement of Homophily and Heterophily in Semi-supervised Node Classification","summaries":"Recently, graph neural networks (GNNs) have shown prominent performance in\nsemi-supervised node classification by leveraging knowledge from the graph\ndatabase. However, most existing GNNs follow the homophily assumption, where\nconnected nodes are more likely to exhibit similar feature distributions and\nthe same labels, and such an assumption has proven to be vulnerable in a\ngrowing number of practical applications. As a supplement, heterophily reflects\ndissimilarity in connected nodes, which has gained significant attention in\ngraph learning. To this end, data engineers aim to develop a powerful GNN model\nthat can ensure performance under both homophily and heterophily. Despite\nnumerous attempts, most existing GNNs struggle to achieve optimal node\nrepresentations due to the constraints of undirected graphs. The neglect of\ndirected edges results in sub-optimal graph representations, thereby hindering\nthe capacity of GNNs. To address this issue, we introduce AMUD, which\nquantifies the relationship between node profiles and topology from a\nstatistical perspective, offering valuable insights for \\underline{A}daptively\n\\underline{M}odeling the natural directed graphs as the \\underline{U}ndirected\nor \\underline{D}irected graph to maximize the benefits from subsequent graph\nlearning. Furthermore, we propose \\underline{A}daptive \\underline{D}irected\n\\underline{P}attern \\underline{A}ggregation (ADPA) as a new directed graph\nlearning paradigm for AMUD. Empirical studies have demonstrated that AMUD\nguides efficient graph learning. Meanwhile, extensive experiments on 14\nbenchmark datasets substantiate the impressive performance of ADPA,\noutperforming baselines by significant margins of 3.96\\%.","terms":["cs.LG","cs.AI","cs.SI"]},{"titles":"Small Area Estimation of Case Growths for Timely COVID-19 Outbreak Detection","summaries":"The COVID-19 pandemic has exerted a profound impact on the global economy and\ncontinues to exact a significant toll on human lives. The COVID-19 case growth\nrate stands as a key epidemiological parameter to estimate and monitor for\neffective detection and containment of the resurgence of outbreaks. A\nfundamental challenge in growth rate estimation and hence outbreak detection is\nbalancing the accuracy-speed tradeoff, where accuracy typically degrades with\nshorter fitting windows. In this paper, we develop a machine learning (ML)\nalgorithm, which we call Transfer Learning Generalized Random Forest (TLGRF),\nthat balances this accuracy-speed tradeoff. Specifically, we estimate the\ninstantaneous COVID-19 exponential growth rate for each U.S. county by using\nTLGRF that chooses an adaptive fitting window size based on relevant day-level\nand county-level features affecting the disease spread. Through transfer\nlearning, TLGRF can accurately estimate case growth rates for counties with\nsmall sample sizes. Out-of-sample prediction analysis shows that TLGRF\noutperforms established growth rate estimation methods. Furthermore, we\nconducted a case study based on outbreak case data from the state of Colorado\nand showed that the timely detection of outbreaks could have been improved by\nup to 224% using TLGRF when compared to the decisions made by Colorado's\nDepartment of Health and Environment (CDPHE). To facilitate implementation, we\nhave developed a publicly available outbreak detection tool for timely\ndetection of COVID-19 outbreaks in each U.S. county, which received substantial\nattention from policymakers.","terms":["stat.ML","cs.LG","physics.soc-ph"]},{"titles":"Adv-4-Adv: Thwarting Changing Adversarial Perturbations via Adversarial Domain Adaptation","summaries":"Whereas adversarial training can be useful against specific adversarial\nperturbations, they have also proven ineffective in generalizing towards\nattacks deviating from those used for training. However, we observe that this\nineffectiveness is intrinsically connected to domain adaptability, another\ncrucial issue in deep learning for which adversarial domain adaptation appears\nto be a promising solution. Consequently, we proposed Adv-4-Adv as a novel\nadversarial training method that aims to retain robustness against unseen\nadversarial perturbations. Essentially, Adv-4-Adv treats attacks incurring\ndifferent perturbations as distinct domains, and by leveraging the power of\nadversarial domain adaptation, it aims to remove the domain\/attack-specific\nfeatures. This forces a trained model to learn a robust domain-invariant\nrepresentation, which in turn enhances its generalization ability. Extensive\nevaluations on Fashion-MNIST, SVHN, CIFAR-10, and CIFAR-100 demonstrate that a\nmodel trained by Adv-4-Adv based on samples crafted by simple attacks (e.g.,\nFGSM) can be generalized to more advanced attacks (e.g., PGD), and the\nperformance exceeds state-of-the-art proposals on these datasets.","terms":["cs.CV","cs.AI"]},{"titles":"Invariant Target Detection in Images through the Normalized 2-D Correlation Technique","summaries":"The normalized 2-D correlation technique is a robust method for detecting\ntargets in images due to its ability to remain invariant under rotation,\ntranslation, and scaling. This paper examines the impact of translation, and\nscaling on target identification in images. The results indicate a high level\nof accuracy in detecting targets, even when they are exhibit variations in\nlocation and size. The results indicate that the similarity between the image\nand the two used targets improves as the resize ratio increases. All\nstatistical estimators demonstrate a strong similarity between the original and\nextracted targets. The elapsed time for all scenarios falls within the range\n(44.75-44.85), (37.48-37.73) seconds for bird and children targets\nrespectively, and the correlation coefficient displays stable relationships\nwith values that fall within the range of (0.90-0.98) and (0.87-0.93) for bird\nand children targets respectively.","terms":["cs.CV","eess.IV"]},{"titles":"Identity-Obscured Neural Radiance Fields: Privacy-Preserving 3D Facial Reconstruction","summaries":"Neural radiance fields (NeRF) typically require a complete set of images\ntaken from multiple camera perspectives to accurately reconstruct geometric\ndetails. However, this approach raise significant privacy concerns in the\ncontext of facial reconstruction. The critical need for privacy protection\noften leads invidividuals to be reluctant in sharing their facial images, due\nto fears of potential misuse or security risks. Addressing these concerns, we\npropose a method that leverages privacy-preserving images for reconstructing 3D\nhead geometry within the NeRF framework. Our method stands apart from\ntraditional facial reconstruction techniques as it does not depend on RGB\ninformation from images containing sensitive facial data. Instead, it\neffectively generates plausible facial geometry using a series of\nidentity-obscured inputs, thereby protecting facial privacy.","terms":["cs.CV"]},{"titles":"Universal Online Learning with Gradient Variations: A Multi-layer Online Ensemble Approach","summaries":"In this paper, we propose an online convex optimization approach with two\ndifferent levels of adaptivity. On a higher level, our approach is agnostic to\nthe unknown types and curvatures of the online functions, while at a lower\nlevel, it can exploit the unknown niceness of the environments and attain\nproblem-dependent guarantees. Specifically, we obtain $\\mathcal{O}(\\log V_T)$,\n$\\mathcal{O}(d \\log V_T)$ and $\\widehat{\\mathcal{O}}(\\sqrt{V_T})$ regret bounds\nfor strongly convex, exp-concave and convex loss functions, respectively, where\n$d$ is the dimension, $V_T$ denotes problem-dependent gradient variations and\nthe $\\widehat{\\mathcal{O}}(\\cdot)$-notation omits $\\log V_T$ factors. Our\nresult not only safeguards the worst-case guarantees but also directly implies\nthe small-loss bounds in analysis. Moreover, when applied to\nadversarial\/stochastic convex optimization and game theory problems, our result\nenhances the existing universal guarantees. Our approach is based on a\nmulti-layer online ensemble framework incorporating novel ingredients,\nincluding a carefully designed optimism for unifying diverse function types and\ncascaded corrections for algorithmic stability. Notably, despite its\nmulti-layer structure, our algorithm necessitates only one gradient query per\nround, making it favorable when the gradient evaluation is time-consuming. This\nis facilitated by a novel regret decomposition with carefully designed\nsurrogate losses.","terms":["cs.LG","math.OC","stat.ML"]},{"titles":"Learn to Unlearn for Deep Neural Networks: Minimizing Unlearning Interference with Gradient Projection","summaries":"Recent data-privacy laws have sparked interest in machine unlearning, which\ninvolves removing the effect of specific training samples from a learnt model\nas if they were never present in the original training dataset. The challenge\nof machine unlearning is to discard information about the ``forget'' data in\nthe learnt model without altering the knowledge about the remaining dataset and\nto do so more efficiently than the naive retraining approach. To achieve this,\nwe adopt a projected-gradient based learning method, named as\nProjected-Gradient Unlearning (PGU), in which the model takes steps in the\northogonal direction to the gradient subspaces deemed unimportant for the\nretaining dataset, so as to its knowledge is preserved. By utilizing Stochastic\nGradient Descent (SGD) to update the model weights, our method can efficiently\nscale to any model and dataset size. We provide empirically evidence to\ndemonstrate that our unlearning method can produce models that behave similar\nto models retrained from scratch across various metrics even when the training\ndataset is no longer accessible. Our code is available at\nhttps:\/\/github.com\/hnanhtuan\/projected_gradient_unlearning.","terms":["cs.LG","cs.CV"]},{"titles":"Point Cloud Attacks in Graph Spectral Domain: When 3D Geometry Meets Graph Signal Processing","summaries":"With the increasing attention in various 3D safety-critical applications,\npoint cloud learning models have been shown to be vulnerable to adversarial\nattacks. Although existing 3D attack methods achieve high success rates, they\ndelve into the data space with point-wise perturbation, which may neglect the\ngeometric characteristics. Instead, we propose point cloud attacks from a new\nperspective -- the graph spectral domain attack, aiming to perturb graph\ntransform coefficients in the spectral domain that corresponds to varying\ncertain geometric structure. Specifically, leveraging on graph signal\nprocessing, we first adaptively transform the coordinates of points onto the\nspectral domain via graph Fourier transform (GFT) for compact representation.\nThen, we analyze the influence of different spectral bands on the geometric\nstructure, based on which we propose to perturb the GFT coefficients via a\nlearnable graph spectral filter. Considering the low-frequency components\nmainly contribute to the rough shape of the 3D object, we further introduce a\nlow-frequency constraint to limit perturbations within imperceptible\nhigh-frequency components. Finally, the adversarial point cloud is generated by\ntransforming the perturbed spectral representation back to the data domain via\nthe inverse GFT. Experimental results demonstrate the effectiveness of the\nproposed attack in terms of both the imperceptibility and attack success rates.","terms":["cs.CV","eess.IV"]},{"titles":"Open-Vocabulary Segmentation with Semantic-Assisted Calibration","summaries":"This paper studies open-vocabulary segmentation (OVS) through calibrating\nin-vocabulary and domain-biased embedding space with generalized contextual\nprior of CLIP. As the core of open-vocabulary understanding, alignment of\nvisual content with the semantics of unbounded text has become the bottleneck\nof this field. To address this challenge, recent works propose to utilize CLIP\nas an additional classifier and aggregate model predictions with CLIP\nclassification results. Despite their remarkable progress, performance of OVS\nmethods in relevant scenarios is still unsatisfactory compared with supervised\ncounterparts. We attribute this to the in-vocabulary embedding and\ndomain-biased CLIP prediction. To this end, we present a Semantic-assisted\nCAlibration Network (SCAN). In SCAN, we incorporate generalized semantic prior\nof CLIP into proposal embedding to avoid collapsing on known categories.\nBesides, a contextual shift strategy is applied to mitigate the lack of global\ncontext and unnatural background noise. With above designs, SCAN achieves\nstate-of-the-art performance on all popular open-vocabulary segmentation\nbenchmarks. Furthermore, we also focus on the problem of existing evaluation\nsystem that ignores semantic duplication across categories, and propose a new\nmetric called Semantic-Guided IoU (SG-IoU).","terms":["cs.CV"]},{"titles":"VRPTEST: Evaluating Visual Referring Prompting in Large Multimodal Models","summaries":"With recent advancements in Large Multimodal Models (LMMs) across various\ndomains, a novel prompting method called visual referring prompting has\nemerged, showing significant potential in enhancing human-computer interaction\nwithin multimodal systems. This method offers a more natural and flexible\napproach to human interaction with these systems compared to traditional text\ndescriptions or coordinates. However, the categorization of visual referring\nprompting remains undefined, and its impact on the performance of LMMs has yet\nto be formally examined. In this study, we conduct the first comprehensive\nanalysis of LMMs using a variety of visual referring prompting strategies. We\nintroduce a benchmark dataset called VRPTEST, comprising 3 different visual\ntasks and 2,275 images, spanning diverse combinations of prompt strategies.\nUsing VRPTEST, we conduct a comprehensive evaluation of eight versions of\nprominent open-source and proprietary foundation models, including two early\nversions of GPT-4V. We develop an automated assessment framework based on\nsoftware metamorphic testing techniques to evaluate the accuracy of LMMs\nwithout the need for human intervention or manual labeling. We find that the\ncurrent proprietary models generally outperform the open-source ones, showing\nan average accuracy improvement of 22.70%; however, there is still potential\nfor improvement. Moreover, our quantitative analysis shows that the choice of\nprompt strategy significantly affects the accuracy of LMMs, with variations\nranging from -17.5% to +7.3%. Further case studies indicate that an appropriate\nvisual referring prompting strategy can improve LMMs' understanding of context\nand location information, while an unsuitable one might lead to answer\nrejection. We also provide insights on minimizing the negative impact of visual\nreferring prompting on LMMs.","terms":["cs.CV","cs.AI"]},{"titles":"MTVG : Multi-text Video Generation with Text-to-Video Models","summaries":"Recently, video generation has attracted massive attention and yielded\nnoticeable outcomes. Concerning the characteristics of video, multi-text\nconditioning incorporating sequential events is necessary for next-step video\ngeneration. In this work, we propose a novel multi-text video generation~(MTVG)\nby directly utilizing a pre-trained diffusion-based text-to-video~(T2V)\ngeneration model without additional fine-tuning. To generate consecutive video\nsegments, visual consistency generated by distinct prompts is necessary with\ndiverse variations, such as motion and content-related transitions. Our\nproposed MTVG includes Dynamic Noise and Last Frame Aware Inversion which\nreinitialize the noise latent to preserve visual coherence between videos of\ndifferent prompts and prevent repetitive motion or contents. Furthermore, we\npresent Structure Guiding Sampling to maintain the global appearance across the\nframes in a single video clip, where we leverage iterative latent updates\nacross the preceding frame. Additionally, our Prompt Generator allows for\narbitrary format of text conditions consisting of diverse events. As a result,\nour extensive experiments, including diverse transitions of descriptions,\ndemonstrate that our proposed methods show superior generated outputs in terms\nof semantically coherent and temporally seamless video.Video examples are\navailable in our project page: https:\/\/kuai-lab.github.io\/mtvg-page.","terms":["cs.CV"]},{"titles":"Watching the News: Towards VideoQA Models that can Read","summaries":"Video Question Answering methods focus on commonsense reasoning and visual\ncognition of objects or persons and their interactions over time. Current\nVideoQA approaches ignore the textual information present in the video.\nInstead, we argue that textual information is complementary to the action and\nprovides essential contextualisation cues to the reasoning process. To this\nend, we propose a novel VideoQA task that requires reading and understanding\nthe text in the video. To explore this direction, we focus on news videos and\nrequire QA systems to comprehend and answer questions about the topics\npresented by combining visual and textual cues in the video. We introduce the\n``NewsVideoQA'' dataset that comprises more than $8,600$ QA pairs on $3,000+$\nnews videos obtained from diverse news channels from around the world. We\ndemonstrate the limitations of current Scene Text VQA and VideoQA methods and\npropose ways to incorporate scene text information into VideoQA methods.","terms":["cs.CV"]},{"titles":"On the adaptation of in-context learners for system identification","summaries":"In-context system identification aims at constructing meta-models to describe\nclasses of systems, differently from traditional approaches that model single\nsystems. This paradigm facilitates the leveraging of knowledge acquired from\nobserving the behaviour of different, yet related dynamics. This paper\ndiscusses the role of meta-model adaptation. Through numerical examples, we\ndemonstrate how meta-model adaptation can enhance predictive performance in\nthree realistic scenarios: tailoring the meta-model to describe a specific\nsystem rather than a class; extending the meta-model to capture the behaviour\nof systems beyond the initial training class; and recalibrating the model for\nnew prediction tasks. Results highlight the effectiveness of meta-model\nadaptation to achieve a more robust and versatile meta-learning framework for\nsystem identification.","terms":["cs.LG","cs.SY","eess.SY"]},{"titles":"Large Language Models are Good Prompt Learners for Low-Shot Image Classification","summaries":"Low-shot image classification, where training images are limited or\ninaccessible, has benefited from recent progress on pre-trained vision-language\n(VL) models with strong generalizability, e.g. CLIP. Prompt learning methods\nbuilt with VL models generate text features from the class names that only have\nconfined class-specific information. Large Language Models (LLMs), with their\nvast encyclopedic knowledge, emerge as the complement. Thus, in this paper, we\ndiscuss the integration of LLMs to enhance pre-trained VL models, specifically\non low-shot classification. However, the domain gap between language and vision\nblocks the direct application of LLMs. Thus, we propose LLaMP, Large Language\nModels as Prompt learners, that produces adaptive prompts for the CLIP text\nencoder, establishing it as the connecting bridge. Experiments show that,\ncompared with other state-of-the-art prompt learning methods, LLaMP yields\nbetter performance on both zero-shot generalization and few-shot image\nclassification, over a spectrum of 11 datasets.","terms":["cs.CV"]},{"titles":"MAUVE Scores for Generative Models: Theory and Practice","summaries":"Generative artificial intelligence has made significant strides, producing\ntext indistinguishable from human prose and remarkably photorealistic images.\nAutomatically measuring how close the generated data distribution is to the\ntarget distribution is central to diagnosing existing models and developing\nbetter ones. We present MAUVE, a family of comparison measures between pairs of\ndistributions such as those encountered in the generative modeling of text or\nimages. These scores are statistical summaries of divergence frontiers\ncapturing two types of errors in generative modeling. We explore three\napproaches to statistically estimate these scores: vector quantization,\nnon-parametric estimation, and classifier-based estimation. We provide\nstatistical bounds for the vector quantization approach.\n  Empirically, we find that the proposed scores paired with a range of\n$f$-divergences and statistical estimation methods can quantify the gaps\nbetween the distributions of human-written text and those of modern neural\nlanguage models by correlating with human judgments and identifying known\nproperties of the generated texts. We demonstrate in the vision domain that\nMAUVE can identify known properties of generated images on par with or better\nthan existing metrics. In conclusion, we present practical recommendations for\nusing MAUVE effectively with language and image modalities.","terms":["cs.LG","cs.AI","cs.CL"]},{"titles":"A Transformer Model for Symbolic Regression towards Scientific Discovery","summaries":"Symbolic Regression (SR) searches for mathematical expressions which best\ndescribe numerical datasets. This allows to circumvent interpretation issues\ninherent to artificial neural networks, but SR algorithms are often\ncomputationally expensive. This work proposes a new Transformer model aiming at\nSymbolic Regression particularly focused on its application for Scientific\nDiscovery. We propose three encoder architectures with increasing flexibility\nbut at the cost of column-permutation equivariance violation. Training results\nindicate that the most flexible architecture is required to prevent from\noverfitting. Once trained, we apply our best model to the SRSD datasets\n(Symbolic Regression for Scientific Discovery datasets) which yields\nstate-of-the-art results using the normalized tree-based edit distance, at no\nextra computational cost.","terms":["cs.LG"]},{"titles":"MeanCut: A Greedy-Optimized Graph Clustering via Path-based Similarity and Degree Descent Criterion","summaries":"As the most typical graph clustering method, spectral clustering is popular\nand attractive due to the remarkable performance, easy implementation, and\nstrong adaptability. Classical spectral clustering measures the edge weights of\ngraph using pairwise Euclidean-based metric, and solves the optimal graph\npartition by relaxing the constraints of indicator matrix and performing\nLaplacian decomposition. However, Euclidean-based similarity might cause skew\ngraph cuts when handling non-spherical data distributions, and the relaxation\nstrategy introduces information loss. Meanwhile, spectral clustering requires\nspecifying the number of clusters, which is hard to determine without enough\nprior knowledge. In this work, we leverage the path-based similarity to enhance\nintra-cluster associations, and propose MeanCut as the objective function and\ngreedily optimize it in degree descending order for a nondestructive graph\npartition. This algorithm enables the identification of arbitrary shaped\nclusters and is robust to noise. To reduce the computational complexity of\nsimilarity calculation, we transform optimal path search into generating the\nmaximum spanning tree (MST), and develop a fast MST (FastMST) algorithm to\nfurther improve its time-efficiency. Moreover, we define a density gradient\nfactor (DGF) for separating the weakly connected clusters. The validity of our\nalgorithm is demonstrated by testifying on real-world benchmarks and\napplication of face recognition. The source code of MeanCut is available at\nhttps:\/\/github.com\/ZPGuiGroupWhu\/MeanCut-Clustering.","terms":["cs.LG","I.5.3"]},{"titles":"OriCon3D: Effective 3D Object Detection using Orientation and Confidence","summaries":"We introduce a technique for detecting 3D objects and estimating their\nposition from a single image. Our method is built on top of a similar\nstate-of-the-art technique [1], but with improved accuracy. The approach\nfollowed in this research first estimates common 3D properties of an object\nusing a Deep Convolutional Neural Network (DCNN), contrary to other frameworks\nthat only leverage centre-point predictions. We then combine these estimates\nwith geometric constraints provided by a 2D bounding box to produce a complete\n3D bounding box. The first output of our network estimates the 3D object\norientation using a discrete-continuous loss [1]. The second output predicts\nthe 3D object dimensions with minimal variance. Here we also present our\nextensions by augmenting light-weight feature extractors and a customized\nmultibin architecture. By combining these estimates with the geometric\nconstraints of the 2D bounding box, we can accurately (or comparatively)\ndetermine the 3D object pose better than our baseline [1] on the KITTI 3D\ndetection benchmark [2].","terms":["cs.CV"]},{"titles":"Combining inherent knowledge of vision-language models with unsupervised domain adaptation through self-knowledge distillation","summaries":"Unsupervised domain adaptation (UDA) tries to overcome the tedious work of\nlabeling data by leveraging a labeled source dataset and transferring its\nknowledge to a similar but different target dataset. On the other hand, current\nvision-language models exhibit astonishing zero-shot prediction capabilities.\nIn this work, we combine knowledge gained through UDA with the inherent\nknowledge of vision-language models. In a first step, we generate the zero-shot\npredictions of the source and target dataset using the vision-language model.\nSince zero-shot predictions usually exhibit a large entropy, meaning that the\nclass probabilities are rather evenly distributed, we first adjust the\ndistribution to accentuate the winning probabilities. This is done using both\nsource and target data to keep the relative confidence between source and\ntarget data. We then employ a conventional DA method, to gain the knowledge\nfrom the source dataset, in combination with self-knowledge distillation, to\nmaintain the inherent knowledge of the vision-language model. We further\ncombine our method with a gradual source domain expansion strategy (GSDE) and\nshow that this strategy can also benefit by including zero-shot predictions. We\nconduct experiments and ablation studies on three benchmarks (OfficeHome,\nVisDA, and DomainNet) and outperform state-of-the-art methods. We further show\nin ablation studies the contributions of different parts of our algorithm.","terms":["cs.CV"]},{"titles":"A Robust and Efficient Boundary Point Detection Method by Measuring Local Direction Dispersion","summaries":"Boundary points pose a significant challenge for machine learning tasks,\nincluding classification, clustering, and dimensionality reduction. Due to the\nsimilarity of features, boundary areas can result in mixed-up classes or\nclusters, leading to a crowding problem in dimensionality reduction. To address\nthis challenge, numerous boundary point detection methods have been developed,\nbut they are insufficiently to accurately and efficiently identify the boundary\npoints in non-convex structures and high-dimensional manifolds. In this work,\nwe propose a robust and efficient method for detecting boundary points using\nLocal Direction Dispersion (LoDD). LoDD considers that internal points are\nsurrounded by neighboring points in all directions, while neighboring points of\na boundary point tend to be distributed only in a certain directional range.\nLoDD adopts a density-independent K-Nearest Neighbors (KNN) method to determine\nneighboring points, and defines a statistic-based metric using the eigenvalues\nof the covariance matrix of KNN coordinates to measure the centrality of a\nquery point. We demonstrated the validity of LoDD on five synthetic datasets\n(2-D and 3-D) and ten real-world benchmarks, and tested its clustering\nperformance by equipping with two typical clustering methods, K-means and Ncut.\nOur results show that LoDD achieves promising and robust detection accuracy in\na time-efficient manner.","terms":["cs.LG","I.5.2"]},{"titles":"PAPR: Proximity Attention Point Rendering","summaries":"Learning accurate and parsimonious point cloud representations of scene\nsurfaces from scratch remains a challenge in 3D representation learning.\nExisting point-based methods often suffer from the vanishing gradient problem\nor require a large number of points to accurately model scene geometry and\ntexture. To address these limitations, we propose Proximity Attention Point\nRendering (PAPR), a novel method that consists of a point-based scene\nrepresentation and a differentiable renderer. Our scene representation uses a\npoint cloud where each point is characterized by its spatial position,\ninfluence score, and view-independent feature vector. The renderer selects the\nrelevant points for each ray and produces accurate colours using their\nassociated features. PAPR effectively learns point cloud positions to represent\nthe correct scene geometry, even when the initialization drastically differs\nfrom the target geometry. Notably, our method captures fine texture details\nwhile using only a parsimonious set of points. We also demonstrate four\npractical applications of our method: zero-shot geometry editing, object\nmanipulation, texture transfer, and exposure control. More results and code are\navailable on our project website at https:\/\/zvict.github.io\/papr\/.","terms":["cs.CV","cs.AI","cs.GR","cs.LG","cs.NE"]},{"titles":"An unsupervised approach towards promptable defect segmentation in laser-based additive manufacturing by Segment Anything","summaries":"Foundation models are currently driving a paradigm shift in computer vision\ntasks for various fields including biology, astronomy, and robotics among\nothers, leveraging user-generated prompts to enhance their performance. In the\nmanufacturing domain, accurate image-based defect segmentation is imperative to\nensure product quality and facilitate real-time process control. However, such\ntasks are often characterized by multiple challenges including the absence of\nlabels and the requirement for low latency inference among others. To address\nthese issues, we construct a framework for image segmentation using a\nstate-of-the-art Vision Transformer (ViT) based Foundation model (Segment\nAnything Model) with a novel multi-point prompt generation scheme using\nunsupervised clustering. We apply our framework to perform real-time porosity\nsegmentation in a case study of laser base powder bed fusion (L-PBF) and obtain\nhigh Dice Similarity Coefficients (DSC) without the necessity for any\nsupervised fine-tuning in the model. Using such lightweight foundation model\ninference in conjunction with unsupervised prompt generation, we envision the\nconstruction of a real-time anomaly detection pipeline that has the potential\nto revolutionize the current laser-based additive manufacturing processes,\nthereby facilitating the shift towards Industry 4.0 and promoting defect-free\nproduction along with operational efficiency.","terms":["cs.CV"]},{"titles":"Video Face Re-Aging: Toward Temporally Consistent Face Re-Aging","summaries":"Video face re-aging deals with altering the apparent age of a person to the\ntarget age in videos. This problem is challenging due to the lack of paired\nvideo datasets maintaining temporal consistency in identity and age. Most\nre-aging methods process each image individually without considering the\ntemporal consistency of videos. While some existing works address the issue of\ntemporal coherence through video facial attribute manipulation in latent space,\nthey often fail to deliver satisfactory performance in age transformation. To\ntackle the issues, we propose (1) a novel synthetic video dataset that features\nsubjects across a diverse range of age groups; (2) a baseline architecture\ndesigned to validate the effectiveness of our proposed dataset, and (3) the\ndevelopment of three novel metrics tailored explicitly for evaluating the\ntemporal consistency of video re-aging techniques. Our comprehensive\nexperiments on public datasets, such as VFHQ and CelebV-HQ, show that our\nmethod outperforms the existing approaches in terms of both age transformation\nand temporal consistency.","terms":["cs.CV","cs.AI","cs.LG","cs.MM"]},{"titles":"Differentiable Registration of Images and LiDAR Point Clouds with VoxelPoint-to-Pixel Matching","summaries":"Cross-modality registration between 2D images from cameras and 3D point\nclouds from LiDARs is a crucial task in computer vision and robotic. Previous\nmethods estimate 2D-3D correspondences by matching point and pixel patterns\nlearned by neural networks, and use Perspective-n-Points (PnP) to estimate\nrigid transformation during post-processing. However, these methods struggle to\nmap points and pixels to a shared latent space robustly since points and pixels\nhave very different characteristics with patterns learned in different manners\n(MLP and CNN), and they also fail to construct supervision directly on the\ntransformation since the PnP is non-differentiable, which leads to unstable\nregistration results. To address these problems, we propose to learn a\nstructured cross-modality latent space to represent pixel features and 3D\nfeatures via a differentiable probabilistic PnP solver. Specifically, we design\na triplet network to learn VoxelPoint-to-Pixel matching, where we represent 3D\nelements using both voxels and points to learn the cross-modality latent space\nwith pixels. We design both the voxel and pixel branch based on CNNs to operate\nconvolutions on voxels\/pixels represented in grids, and integrate an additional\npoint branch to regain the information lost during voxelization. We train our\nframework end-to-end by imposing supervisions directly on the predicted pose\ndistribution with a probabilistic PnP solver. To explore distinctive patterns\nof cross-modality features, we design a novel loss with adaptive-weighted\noptimization for cross-modality feature description. The experimental results\non KITTI and nuScenes datasets show significant improvements over the\nstate-of-the-art methods. The code and models are available at\nhttps:\/\/github.com\/junshengzhou\/VP2P-Match.","terms":["cs.CV"]},{"titles":"Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds","summaries":"Recent studies have presented compelling evidence that large language models\n(LLMs) can equip embodied agents with the self-driven capability to interact\nwith the world, which marks an initial step toward versatile robotics. However,\nthese efforts tend to overlook the visual richness of open worlds, rendering\nthe entire interactive process akin to \"a blindfolded text-based game.\"\nConsequently, LLM-based agents frequently encounter challenges in intuitively\ncomprehending their surroundings and producing responses that are easy to\nunderstand. In this paper, we propose Steve-Eye, an end-to-end trained large\nmultimodal model designed to address this limitation. Steve-Eye integrates the\nLLM with a visual encoder which enables it to process visual-text inputs and\ngenerate multimodal feedback. In addition, we use a semi-automatic strategy to\ncollect an extensive dataset comprising 850K open-world instruction pairs,\nempowering our model to encompass three essential functions for an agent:\nmultimodal perception, foundational knowledge base, and skill prediction and\nplanning. Lastly, we develop three open-world evaluation benchmarks, then carry\nout extensive experiments from a wide range of perspectives to validate our\nmodel's capability to strategically act and plan. Codes and datasets will be\nreleased.","terms":["cs.CV"]},{"titles":"Jointly spatial-temporal representation learning for individual trajectories","summaries":"Individual trajectories, containing substantial information on\nhuman-environment interactions across space and time, is a crucial input for\ngeospatial foundation models (GeoFMs). However, existing attempts, leveraging\ntrajectory data for various applications have overlooked the implicit\nspatial-temporal dependency within trajectories and failed to encode and\nrepresent it in a format friendly to deep learning, posing a challenge in\nobtaining general-purpose trajectory representations. Therefore, this paper\nproposes a spatial-temporal joint representation learning method (ST-GraphRL)\nto formalize learnable spatial-temporal dependencies into trajectory\nrepresentations. The proposed ST-GraphRL consists of three compositions: (i) a\nweighted directed spatial-temporal graph to explicitly construct mobility\ninteractions over both space and time dimensions; (ii) a two-stage jointly\nencoder (i.e., decoupling and fusion) to learn entangled spatial-temporal\ndependencies by independently decomposing and jointly aggregating space and\ntime information; (iii) a decoder guides ST-GraphRL to learn explicit mobility\nregularities by simulating the spatial-temporal distributions of trajectories.\nTested on three real-world human mobility datasets, the proposed ST-GraphRL\noutperformed all the baseline models in predicting movement spatial-temporal\ndistributions and preserving trajectory similarity with high spatial-temporal\ncorrelations. We also explore how spatial-temporal features presented in latent\nspace, validating that ST-GraphRL understands spatial-temporal patterns. This\nmethod is also transferable for general-purpose geospatial data representations\nfor broad downstream tasks, as well advancing GeoFMs developing.","terms":["cs.LG"]},{"titles":"Large Language Models as Optimizers","summaries":"Optimization is ubiquitous. While derivative-based algorithms have been\npowerful tools for various problems, the absence of gradient imposes challenges\non many real-world applications. In this work, we propose Optimization by\nPROmpting (OPRO), a simple and effective approach to leverage large language\nmodels (LLMs) as optimizers, where the optimization task is described in\nnatural language. In each optimization step, the LLM generates new solutions\nfrom the prompt that contains previously generated solutions with their values,\nthen the new solutions are evaluated and added to the prompt for the next\noptimization step. We first showcase OPRO on linear regression and traveling\nsalesman problems, then move on to prompt optimization where the goal is to\nfind instructions that maximize the task accuracy. With a variety of LLMs, we\ndemonstrate that the best prompts optimized by OPRO outperform human-designed\nprompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at\nhttps:\/\/github.com\/google-deepmind\/opro.","terms":["cs.LG","cs.AI","cs.CL"]},{"titles":"Residual Graph Convolutional Network for Bird's-Eye-View Semantic Segmentation","summaries":"Retrieving spatial information and understanding the semantic information of\nthe surroundings are important for Bird's-Eye-View (BEV) semantic segmentation.\nIn the application of autonomous driving, autonomous vehicles need to be aware\nof their surroundings to drive safely. However, current BEV semantic\nsegmentation techniques, deep Convolutional Neural Networks (CNNs) and\ntransformers, have difficulties in obtaining the global semantic relationships\nof the surroundings at the early layers of the network. In this paper, we\npropose to incorporate a novel Residual Graph Convolutional (RGC) module in\ndeep CNNs to acquire both the global information and the region-level semantic\nrelationship in the multi-view image domain. Specifically, the RGC module\nemploys a non-overlapping graph space projection to efficiently project the\ncomplete BEV information into graph space. It then builds interconnected\nspatial and channel graphs to extract spatial information between each node and\nchannel information within each node (i.e., extract contextual relationships of\nthe global features). Furthermore, it uses a downsample residual process to\nenhance the coordinate feature reuse to maintain the global information. The\nsegmentation data augmentation and alignment module helps to simultaneously\naugment and align BEV features and ground truth to geometrically preserve their\nalignment to achieve better segmentation results. Our experimental results on\nthe nuScenes benchmark dataset demonstrate that the RGC network outperforms\nfour state-of-the-art networks and its four variants in terms of IoU and mIoU.\nThe proposed RGC network achieves a higher mIoU of 3.1% than the best\nstate-of-the-art network, BEVFusion. Code and models will be released.","terms":["cs.CV"]},{"titles":"Doodle Your 3D: From Abstract Freehand Sketches to Precise 3D Shapes","summaries":"In this paper, we democratise 3D content creation, enabling precise\ngeneration of 3D shapes from abstract sketches while overcoming limitations\ntied to drawing skills. We introduce a novel part-level modelling and alignment\nframework that facilitates abstraction modelling and cross-modal\ncorrespondence. Leveraging the same part-level decoder, our approach seamlessly\nextends to sketch modelling by establishing correspondence between CLIPasso\nedgemaps and projected 3D part regions, eliminating the need for a dataset\npairing human sketches and 3D shapes. Additionally, our method introduces a\nseamless in-position editing process as a byproduct of cross-modal part-aligned\nmodelling. Operating in a low-dimensional implicit space, our approach\nsignificantly reduces computational demands and processing time.","terms":["cs.CV","cs.AI"]},{"titles":"Reconstruction of dynamical systems from data without time labels","summaries":"In this paper, we study the method to reconstruct dynamical systems from data\nwithout time labels. Data without time labels appear in many applications, such\nas molecular dynamics, single-cell RNA sequencing etc. Reconstruction of\ndynamical system from time sequence data has been studied extensively. However,\nthese methods do not apply if time labels are unknown. Without time labels,\nsequence data becomes distribution data. Based on this observation, we propose\nto treat the data as samples from a probability distribution and try to\nreconstruct the underlying dynamical system by minimizing the distribution\nloss, sliced Wasserstein distance more specifically. Extensive experiment\nresults demonstrate the effectiveness of the proposed method.","terms":["cs.LG","cs.NA","math.DS","math.NA"]},{"titles":"BM2CP: Efficient Collaborative Perception with LiDAR-Camera Modalities","summaries":"Collaborative perception enables agents to share complementary perceptual\ninformation with nearby agents. This would improve the perception performance\nand alleviate the issues of single-view perception, such as occlusion and\nsparsity. Most existing approaches mainly focus on single modality (especially\nLiDAR), and not fully exploit the superiority of multi-modal perception. We\npropose a collaborative perception paradigm, BM2CP, which employs LiDAR and\ncamera to achieve efficient multi-modal perception. It utilizes LiDAR-guided\nmodal fusion, cooperative depth generation and modality-guided intermediate\nfusion to acquire deep interactions among modalities of different agents,\nMoreover, it is capable to cope with the special case where one of the sensors,\nsame or different type, of any agent is missing. Extensive experiments validate\nthat our approach outperforms the state-of-the-art methods with 50X lower\ncommunication volumes in both simulated and real-world autonomous driving\nscenarios. Our code is available at https:\/\/github.com\/byzhaoAI\/BM2CP.","terms":["cs.CV","cs.AI","cs.RO"]},{"titles":"DiffusionPhase: Motion Diffusion in Frequency Domain","summaries":"In this study, we introduce a learning-based method for generating\nhigh-quality human motion sequences from text descriptions (e.g., ``A person\nwalks forward\"). Existing techniques struggle with motion diversity and smooth\ntransitions in generating arbitrary-length motion sequences, due to limited\ntext-to-motion datasets and the pose representations used that often lack\nexpressiveness or compactness. To address these issues, we propose the first\nmethod for text-conditioned human motion generation in the frequency domain of\nmotions. We develop a network encoder that converts the motion space into a\ncompact yet expressive parameterized phase space with high-frequency details\nencoded, capturing the local periodicity of motions in time and space with high\naccuracy. We also introduce a conditional diffusion model for predicting\nperiodic motion parameters based on text descriptions and a start pose,\nefficiently achieving smooth transitions between motion sequences associated\nwith different text descriptions. Experiments demonstrate that our approach\noutperforms current methods in generating a broader variety of high-quality\nmotions, and synthesizing long sequences with natural transitions.","terms":["cs.CV","cs.LG"]},{"titles":"Enhancing Few-shot CLIP with Semantic-Aware Fine-Tuning","summaries":"Learning generalized representations from limited training samples is crucial\nfor applying deep neural networks in low-resource scenarios. Recently, methods\nbased on Contrastive Language-Image Pre-training (CLIP) have exhibited\npromising performance in few-shot adaptation tasks. To avoid catastrophic\nforgetting and overfitting caused by few-shot fine-tuning, existing works\nusually freeze the parameters of CLIP pre-trained on large-scale datasets,\noverlooking the possibility that some parameters might not be suitable for\ndownstream tasks. To this end, we revisit CLIP's visual encoder with a specific\nfocus on its distinctive attention pooling layer, which performs a spatial\nweighted-sum of the dense feature maps. Given that dense feature maps contain\nmeaningful semantic information, and different semantics hold varying\nimportance for diverse downstream tasks (such as prioritizing semantics like\nears and eyes in pet classification tasks rather than side mirrors), using the\nsame weighted-sum operation for dense features across different few-shot tasks\nmight not be appropriate. Hence, we propose fine-tuning the parameters of the\nattention pooling layer during the training process to encourage the model to\nfocus on task-specific semantics. In the inference process, we perform residual\nblending between the features pooled by the fine-tuned and the original\nattention pooling layers to incorporate both the few-shot knowledge and the\npre-trained CLIP's prior knowledge. We term this method as Semantic-Aware\nFinE-tuning (SAFE). SAFE is effective in enhancing the conventional few-shot\nCLIP and is compatible with the existing adapter approach (termed SAFE-A).","terms":["cs.CV"]},{"titles":"Class Incremental Learning for Adversarial Robustness","summaries":"Adversarial training integrates adversarial examples during model training to\nenhance robustness. However, its application in fixed dataset settings differs\nfrom real-world dynamics, where data accumulates incrementally. In this study,\nwe investigate Adversarially Robust Class Incremental Learning (ARCIL), a\nmethod that combines adversarial robustness with incremental learning. We\nobserve that combining incremental learning with naive adversarial training\neasily leads to a loss of robustness. We discover that this is attributed to\nthe disappearance of the flatness of the loss function, a characteristic of\nadversarial training. To address this issue, we propose the Flatness Preserving\nDistillation (FPD) loss that leverages the output difference between\nadversarial and clean examples. Additionally, we introduce the Logit Adjustment\nDistillation (LAD) loss, which adapts the model's knowledge to perform well on\nnew tasks. Experimental results demonstrate the superiority of our method over\napproaches that apply adversarial training to existing incremental learning\nmethods, which provides a strong baseline for incremental learning on\nadversarial robustness in the future. Our method achieves AutoAttack accuracy\nthat is 5.99\\%p, 5.27\\%p, and 3.90\\%p higher on average than the baseline on\nsplit CIFAR-10, CIFAR-100, and Tiny ImageNet, respectively. The code will be\nmade available.","terms":["cs.CV"]},{"titles":"UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs","summaries":"Text-to-image diffusion models have demonstrated remarkable capabilities in\ntransforming textual prompts into coherent images, yet the computational cost\nof their inference remains a persistent challenge. To address this issue, we\npresent UFOGen, a novel generative model designed for ultra-fast, one-step\ntext-to-image synthesis. In contrast to conventional approaches that focus on\nimproving samplers or employing distillation techniques for diffusion models,\nUFOGen adopts a hybrid methodology, integrating diffusion models with a GAN\nobjective. Leveraging a newly introduced diffusion-GAN objective and\ninitialization with pre-trained diffusion models, UFOGen excels in efficiently\ngenerating high-quality images conditioned on textual descriptions in a single\nstep. Beyond traditional text-to-image generation, UFOGen showcases versatility\nin applications. Notably, UFOGen stands among the pioneering models enabling\none-step text-to-image generation and diverse downstream tasks, presenting a\nsignificant advancement in the landscape of efficient generative models.","terms":["cs.CV"]},{"titles":"Improved Face Representation via Joint Label Classification and Supervised Contrastive Clustering","summaries":"Face clustering tasks can learn hierarchical semantic information from\nlarge-scale data, which has the potential to help facilitate face recognition.\nHowever, there are few works on this problem. This paper explores it by\nproposing a joint optimization task of label classification and supervised\ncontrastive clustering to introduce the cluster knowledge to the traditional\nface recognition task in two ways. We first extend ArcFace with a\ncluster-guided angular margin to adjust the within-class feature distribution\naccording to the hard level of face clustering. Secondly, we propose a\nsupervised contrastive clustering approach to pull the features to the cluster\ncenter and propose the cluster-aligning procedure to align the cluster center\nand the learnable class center in the classifier for joint training. Finally,\nextensive qualitative and quantitative experiments on popular facial benchmarks\ndemonstrate the effectiveness of our paradigm and its superiority over the\nexisting approaches to face recognition.","terms":["cs.CV","cs.AI"]},{"titles":"ImFace++: A Sophisticated Nonlinear 3D Morphable Face Model with Implicit Neural Representations","summaries":"Accurate representations of 3D faces are of paramount importance in various\ncomputer vision and graphics applications. However, the challenges persist due\nto the limitations imposed by data discretization and model linearity, which\nhinder the precise capture of identity and expression clues in current studies.\nThis paper presents a novel 3D morphable face model, named ImFace++, to learn a\nsophisticated and continuous space with implicit neural representations.\nImFace++ first constructs two explicitly disentangled deformation fields to\nmodel complex shapes associated with identities and expressions, respectively,\nwhich simultaneously facilitate the automatic learning of correspondences\nacross diverse facial shapes. To capture more sophisticated facial details, a\nrefinement displacement field within the template space is further\nincorporated, enabling a fine-grained learning of individual-specific facial\ndetails. Furthermore, a Neural Blend-Field is designed to reinforce the\nrepresentation capabilities through adaptive blending of an array of local\nfields. In addition to ImFace++, we have devised an improved learning strategy\nto extend expression embeddings, allowing for a broader range of expression\nvariations. Comprehensive qualitative and quantitative evaluations demonstrate\nthat ImFace++ significantly advances the state-of-the-art in terms of both face\nreconstruction fidelity and correspondence accuracy.","terms":["cs.CV"]},{"titles":"The sample complexity of multi-distribution learning","summaries":"Multi-distribution learning generalizes the classic PAC learning to handle\ndata coming from multiple distributions. Given a set of $k$ data distributions\nand a hypothesis class of VC dimension $d$, the goal is to learn a hypothesis\nthat minimizes the maximum population loss over $k$ distributions, up to\n$\\epsilon$ additive error. In this paper, we settle the sample complexity of\nmulti-distribution learning by giving an algorithm of sample complexity\n$\\widetilde{O}((d+k)\\epsilon^{-2}) \\cdot (k\/\\epsilon)^{o(1)}$. This matches the\nlower bound up to sub-polynomial factor and resolves the COLT 2023 open problem\nof Awasthi, Haghtalab and Zhao [AHZ23].","terms":["cs.LG","cs.AI","cs.DS","stat.ML"]},{"titles":"k* Distribution: Evaluating the Latent Space of Deep Neural Networks using Local Neighborhood Analysis","summaries":"Most examinations of neural networks' learned latent spaces typically employ\ndimensionality reduction techniques such as t-SNE or UMAP. While these methods\neffectively capture the overall sample distribution in the entire learned\nlatent space, they tend to distort the structure of sample distributions within\nspecific classes in the subset of the latent space. This distortion complicates\nthe task of easily distinguishing classes identifiable by neural networks. In\nresponse to this challenge, we introduce the k* Distribution methodology. This\napproach focuses on capturing the characteristics and structure of sample\ndistributions for individual classes within the subset of the learned latent\nspace using local neighborhood analysis. The key concept is to facilitate easy\ncomparison of different k* distributions, enabling analysis of how various\nclasses are processed by the same neural network. This provides a more profound\nunderstanding of existing contemporary visualizations. Our study reveals three\ndistinct distributions of samples within the learned latent space subset: a)\nFractured, b) Overlapped, and c) Clustered. We note and demonstrate that the\ndistribution of samples within the network's learned latent space significantly\nvaries depending on the class. Furthermore, we illustrate that our analysis can\nbe applied to explore the latent space of diverse neural network architectures,\nvarious layers within neural networks, transformations applied to input\nsamples, and the distribution of training and testing data for neural networks.\nWe anticipate that our approach will facilitate more targeted investigations\ninto neural networks by collectively examining the distribution of different\nsamples within the learned latent space.","terms":["cs.LG","cs.AI","cs.CV"]},{"titles":"ExpM+NF Tractable Exponential Mechanism via Normalizing Flow, A Path through the Accuracy-Privacy Ceiling Constraining Differentially Private ML","summaries":"The Exponential Mechanism (ExpM), a differentially private optimization\nmethod, promises many advantages over Differentially Private Stochastic\nGradient Descent (DPSGD), the state-of-the-art (SOTA) and de facto method for\ndifferentially private machine learning (ML). Yet, ExpM has been historically\nstymied from differentially private training of modern ML algorithms by two\nobstructions: ExpM requires a sensitivity bound for the given loss function;\nExpM requires sampling from a historically intractable density. We prove a\nsensitivity bound for $\\ell(2)$ loss, and investigate using Normalizing Flows\n(NFs), deep networks furnishing approximate sampling from the otherwise\nintractable ExpM distribution. We prove that as the NF output converges to ExpM\ndistribution, the privacy ($\\varepsilon$) of an NF sample converges to that of\nthe ExpM distribution. Under the assumption that the NF output distribution is\nthe ExpM distribution, we empirically test ExpM+NF against DPSGD using the SOTA\nimplementation (Opacus \\cite{opacus} with PRV accounting) in multiple\nclassification tasks on the Adult Dataset (census data) and MIMIC-III Dataset\n(healthcare records) using Logistic Regression and GRU-D, a deep learning\nrecurrent neural network with \\smallsim 20K-100K parameters. In all experiments\nwe find ExpM+NF achieves greater than 94\\% of the non-private training accuracy\n(AUC) with $\\varepsilon$-DP for $\\varepsilon$ a low as $1\\mathrm{e}{-3}$ --\nthree orders of magnitude stronger privacy with similar accuracy. Further,\nperformance results show ExpM+NF training time is comparable to (slightly less)\nthan DPSGD. Limitations and future directions are provided; notably, research\non NF approximation accuracy and its effect on privacy are a promising avenue\nto substantially advancing the field. Code for these experiments \\hl{will be\nprovided after review}.","terms":["stat.ML","cs.AI","cs.CR","cs.LG","math.PR"]},{"titles":"Data-Adaptive Probabilistic Likelihood Approximation for Ordinary Differential Equations","summaries":"Estimating the parameters of ordinary differential equations (ODEs) is of\nfundamental importance in many scientific applications. While ODEs are\ntypically approximated with deterministic algorithms, new research on\nprobabilistic solvers indicates that they produce more reliable parameter\nestimates by better accounting for numerical errors. However, many ODE systems\nare highly sensitive to their parameter values. This produces deep local maxima\nin the likelihood function -- a problem which existing probabilistic solvers\nhave yet to resolve. Here we present a novel probabilistic ODE likelihood\napproximation, DALTON, which can dramatically reduce parameter sensitivity by\nlearning from noisy ODE measurements in a data-adaptive manner. Our\napproximation scales linearly in both ODE variables and time discretization\npoints, and is applicable to ODEs with both partially-unobserved components and\nnon-Gaussian measurement models. Several examples demonstrate that DALTON\nproduces more accurate parameter estimates via numerical optimization than\nexisting probabilistic ODE solvers, and even in some cases than the exact ODE\nlikelihood itself.","terms":["stat.ML","cs.LG"]},{"titles":"VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior","summaries":"Audio-driven talking head generation has drawn much attention in recent\nyears, and many efforts have been made in lip-sync, expressive facial\nexpressions, natural head pose generation, and high video quality. However, no\nmodel has yet led or tied on all these metrics due to the one-to-many mapping\nbetween audio and motion. In this paper, we propose VividTalk, a two-stage\ngeneric framework that supports generating high-visual quality talking head\nvideos with all the above properties. Specifically, in the first stage, we map\nthe audio to mesh by learning two motions, including non-rigid expression\nmotion and rigid head motion. For expression motion, both blendshape and vertex\nare adopted as the intermediate representation to maximize the representation\nability of the model. For natural head motion, a novel learnable head pose\ncodebook with a two-phase training mechanism is proposed. In the second stage,\nwe proposed a dual branch motion-vae and a generator to transform the meshes\ninto dense motion and synthesize high-quality video frame-by-frame. Extensive\nexperiments show that the proposed VividTalk can generate high-visual quality\ntalking head videos with lip-sync and realistic enhanced by a large margin, and\noutperforms previous state-of-the-art works in objective and subjective\ncomparisons.","terms":["cs.CV"]},{"titles":"A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting","summaries":"Achieving high-quality versatile image inpainting, where user-specified\nregions are filled with plausible content according to user intent, presents a\nsignificant challenge. Existing methods face difficulties in simultaneously\naddressing context-aware image inpainting and text-guided object inpainting due\nto the distinct optimal training strategies required. To overcome this\nchallenge, we introduce PowerPaint, the first high-quality and versatile\ninpainting model that excels in both tasks. First, we introduce learnable task\nprompts along with tailored fine-tuning strategies to guide the model's focus\non different inpainting targets explicitly. This enables PowerPaint to\naccomplish various inpainting tasks by utilizing different task prompts,\nresulting in state-of-the-art performance. Second, we demonstrate the\nversatility of the task prompt in PowerPaint by showcasing its effectiveness as\na negative prompt for object removal. Additionally, we leverage prompt\ninterpolation techniques to enable controllable shape-guided object inpainting.\nFinally, we extensively evaluate PowerPaint on various inpainting benchmarks to\ndemonstrate its superior performance for versatile image inpainting. We release\nour codes and models on our project page: https:\/\/powerpaint.github.io\/.","terms":["cs.CV"]},{"titles":"PartDistill: 3D Shape Part Segmentation by Vision-Language Model Distillation","summaries":"This paper proposes a cross-modal distillation framework, PartDistill, which\ntransfers 2D knowledge from vision-language models (VLMs) to facilitate 3D\nshape part segmentation. PartDistill addresses three major challenges in this\ntask: the lack of 3D segmentation in invisible or undetected regions in the 2D\nprojections, inaccurate and inconsistent 2D predictions by VLMs, and the lack\nof knowledge accumulation across different 3D shapes. PartDistill consists of a\nteacher network that uses a VLM to make 2D predictions and a student network\nthat learns from the 2D predictions while extracting geometrical features from\nmultiple 3D shapes to carry out 3D part segmentation. A bi-directional\ndistillation, including forward and backward distillations, is carried out\nwithin the framework, where the former forward distills the 2D predictions to\nthe student network, and the latter improves the quality of the 2D predictions,\nwhich subsequently enhances the final 3D part segmentation. Moreover,\nPartDistill can exploit generative models that facilitate effortless 3D shape\ncreation for generating knowledge sources to be distilled. Through extensive\nexperiments, PartDistill boosts the existing methods with substantial margins\non widely used ShapeNetPart and PartE datasets, by more than 15% and 12% higher\nmIoU scores, respectively.","terms":["cs.CV"]},{"titles":"Natural-language-driven Simulation Benchmark and Copilot for Efficient Production of Object Interactions in Virtual Road Scenes","summaries":"We advocate the idea of the natural-language-driven(NLD) simulation to\nefficiently produce the object interactions between multiple objects in the\nvirtual road scenes, for teaching and testing the autonomous driving systems\nthat should take quick action to avoid collision with obstacles with\nunpredictable motions. The NLD simulation allows the brief natural-language\ndescription to control the object interactions, significantly reducing the\nhuman efforts for creating a large amount of interaction data. To facilitate\nthe research of NLD simulation, we collect the Language-to-Interaction(L2I)\nbenchmark dataset with 120,000 natural-language descriptions of object\ninteractions in 6 common types of road topologies. Each description is\nassociated with the programming code, which the graphic render can use to\nvisually reconstruct the object interactions in the virtual scenes. As a\nmethodology contribution, we design SimCopilot to translate the interaction\ndescriptions to the renderable code. We use the L2I dataset to evaluate\nSimCopilot's abilities to control the object motions, generate complex\ninteractions, and generalize interactions across road topologies. The L2I\ndataset and the evaluation results motivate the relevant research of the NLD\nsimulation.","terms":["cs.CV"]},{"titles":"Towards Clinical Prediction with Transparency: An Explainable AI Approach to Survival Modelling in Residential Aged Care","summaries":"Background: Accurate survival time estimates aid end-of-life medical\ndecision-making. Objectives: Develop an interpretable survival model for\nelderly residential aged care residents using advanced machine learning.\nSetting: A major Australasian residential aged care provider. Participants:\nResidents aged 65+ admitted for long-term care from July 2017 to August 2023.\nSample size: 11,944 residents across 40 facilities. Predictors: Factors include\nage, gender, health status, co-morbidities, cognitive function, mood,\nnutrition, mobility, smoking, sleep, skin integrity, and continence. Outcome:\nProbability of survival post-admission, specifically calibrated for 6-month\nsurvival estimates. Statistical Analysis: Tested CoxPH, EN, RR, Lasso, GB, XGB,\nand RF models in 20 experiments with a 90\/10 train\/test split. Evaluated\naccuracy using C-index, Harrell's C-index, dynamic AUROC, IBS, and calibrated\nROC. Chose XGB for its performance and calibrated it for 1, 3, 6, and 12-month\npredictions using Platt scaling. Employed SHAP values to analyze predictor\nimpacts. Results: GB, XGB, and RF models showed the highest C-Index values\n(0.714, 0.712, 0.712). The optimal XGB model demonstrated a 6-month survival\nprediction AUROC of 0.746 (95% CI 0.744-0.749). Key mortality predictors\ninclude age, male gender, mobility, health status, pressure ulcer risk, and\nappetite. Conclusions: The study successfully applies machine learning to\ncreate a survival model for aged care, aligning with clinical insights on\nmortality risk factors and enhancing model interpretability and clinical\nutility through explainable AI.","terms":["cs.LG"]},{"titles":"KOALA: Self-Attention Matters in Knowledge Distillation of Latent Diffusion Models for Memory-Efficient and Fast Image Synthesis","summaries":"Stable diffusion is the mainstay of the text-to-image (T2I) synthesis in the\ncommunity due to its generation performance and open-source nature. Recently,\nStable Diffusion XL (SDXL), the successor of stable diffusion, has received a\nlot of attention due to its significant performance improvements with a higher\nresolution of 1024x1024 and a larger model. However, its increased computation\ncost and model size require higher-end hardware(e.g., bigger VRAM GPU) for\nend-users, incurring higher costs of operation. To address this problem, in\nthis work, we propose an efficient latent diffusion model for text-to-image\nsynthesis obtained by distilling the knowledge of SDXL. To this end, we first\nperform an in-depth analysis of the denoising U-Net in SDXL, which is the main\nbottleneck of the model, and then design a more efficient U-Net based on the\nanalysis. Secondly, we explore how to effectively distill the generation\ncapability of SDXL into an efficient U-Net and eventually identify four\nessential factors, the core of which is that self-attention is the most\nimportant part. With our efficient U-Net and self-attention-based knowledge\ndistillation strategy, we build our efficient T2I models, called KOALA-1B &\n-700M, while reducing the model size up to 54% and 69% of the original SDXL\nmodel. In particular, the KOALA-700M is more than twice as fast as SDXL while\nstill retaining a decent generation quality. We hope that due to its balanced\nspeed-performance tradeoff, our KOALA models can serve as a cost-effective\nalternative to SDXL in resource-constrained environments.","terms":["cs.CV","cs.AI"]},{"titles":"FIANCEE: Faster Inference of Adversarial Networks via Conditional Early Exits","summaries":"Generative DNNs are a powerful tool for image synthesis, but they are limited\nby their computational load. On the other hand, given a trained model and a\ntask, e.g. faces generation within a range of characteristics, the output image\nquality will be unevenly distributed among images with different\ncharacteristics. It follows, that we might restrain the models complexity on\nsome instances, maintaining a high quality. We propose a method for diminishing\ncomputations by adding so-called early exit branches to the original\narchitecture, and dynamically switching the computational path depending on how\ndifficult it will be to render the output. We apply our method on two different\nSOTA models performing generative tasks: generation from a semantic map, and\ncross-reenactment of face expressions; showing it is able to output images with\ncustom lower-quality thresholds. For a threshold of LPIPS <=0.1, we diminish\ntheir computations by up to a half. This is especially relevant for real-time\napplications such as synthesis of faces, when quality loss needs to be\ncontained, but most of the inputs need fewer computations than the complex\ninstances.","terms":["cs.CV","cs.AI"]},{"titles":"LiDAR: Sensing Linear Probing Performance in Joint Embedding SSL Architectures","summaries":"Joint embedding (JE) architectures have emerged as a promising avenue for\nacquiring transferable data representations. A key obstacle to using JE\nmethods, however, is the inherent challenge of evaluating learned\nrepresentations without access to a downstream task, and an annotated dataset.\nWithout efficient and reliable evaluation, it is difficult to iterate on\narchitectural and training choices for JE methods. In this paper, we introduce\nLiDAR (Linear Discriminant Analysis Rank), a metric designed to measure the\nquality of representations within JE architectures. Our metric addresses\nseveral shortcomings of recent approaches based on feature covariance rank by\ndiscriminating between informative and uninformative features. In essence,\nLiDAR quantifies the rank of the Linear Discriminant Analysis (LDA) matrix\nassociated with the surrogate SSL task -- a measure that intuitively captures\nthe information content as it pertains to solving the SSL task. We empirically\ndemonstrate that LiDAR significantly surpasses naive rank based approaches in\nits predictive power of optimal hyperparameters. Our proposed criterion\npresents a more robust and intuitive means of assessing the quality of\nrepresentations within JE architectures, which we hope facilitates broader\nadoption of these powerful techniques in various domains.","terms":["cs.LG","cs.CV"]},{"titles":"Series2Vec: Similarity-based Self-supervised Representation Learning for Time Series Classification","summaries":"We argue that time series analysis is fundamentally different in nature to\neither vision or natural language processing with respect to the forms of\nmeaningful self-supervised learning tasks that can be defined. Motivated by\nthis insight, we introduce a novel approach called \\textit{Series2Vec} for\nself-supervised representation learning. Unlike other self-supervised methods\nin time series, which carry the risk of positive sample variants being less\nsimilar to the anchor sample than series in the negative set, Series2Vec is\ntrained to predict the similarity between two series in both temporal and\nspectral domains through a self-supervised task. Series2Vec relies primarily on\nthe consistency of the unsupervised similarity step, rather than the intrinsic\nquality of the similarity measurement, without the need for hand-crafted data\naugmentation. To further enforce the network to learn similar representations\nfor similar time series, we propose a novel approach that applies\norder-invariant attention to each representation within the batch during\ntraining. Our evaluation of Series2Vec on nine large real-world datasets, along\nwith the UCR\/UEA archive, shows enhanced performance compared to current\nstate-of-the-art self-supervised techniques for time series. Additionally, our\nextensive experiments show that Series2Vec performs comparably with fully\nsupervised training and offers high efficiency in datasets with limited-labeled\ndata. Finally, we show that the fusion of Series2Vec with other representation\nlearning models leads to enhanced performance for time series classification.\nCode and models are open-source at\n\\url{https:\/\/github.com\/Navidfoumani\/Series2Vec.}","terms":["cs.LG"]},{"titles":"Stable diffusion for Data Augmentation in COCO and Weed Datasets","summaries":"Generative models have increasingly impacted relative tasks ranging from\nimage revision and object detection in computer vision to interior design and\nidea illustration in more general fields. Stable diffusion is an outstanding\nmodel series that paves the way for producing high-resolution images with\nthorough details from text prompts or reference images. It will be an\ninteresting topic about how to leverage the capability of stable diffusion to\nelevate the image variations of certain categories (e.g., vehicles, humans, and\ndaily objects); particularly, it has the potential to gain improvements for\nsmall datasets with image-sparse categories. This study utilized seven\ncategories in the popular COCO dataset and three widespread weed species in\nMichigan to evaluate the efficiency of a recent version of stable diffusion. In\ndetail, Stable diffusion was used to generate synthetic images belonging to\nthese classes; then, YOLOv8 models were trained based on these synthetic\nimages, whose performance was compared to the models trained on original\nimages. In addition, several techniques (e.g., Image-to-image translation,\nDreambooth, ControlNet) of Stable diffusion were leveraged for image generation\nwith different focuses. In spite of the overall results being disappointing,\npromising results have been achieved in some classes, illustrating the\npotential of stable diffusion models to improve the performance of detection\nmodels, which represent more helpful information being conveyed into the models\nby the generated images. This seminal study may expedite the adaption of stable\ndiffusion models to classification and detection tasks in different fields.","terms":["cs.CV"]},{"titles":"Style Transfer to Calvin and Hobbes comics using Stable Diffusion","summaries":"This project report summarizes our journey to perform stable diffusion\nfine-tuning on a dataset containing Calvin and Hobbes comics. The purpose is to\nconvert any given input image into the comic style of Calvin and Hobbes,\nessentially performing style transfer. We train stable-diffusion-v1.5 using Low\nRank Adaptation (LoRA) to efficiently speed up the fine-tuning process. The\ndiffusion itself is handled by a Variational Autoencoder (VAE), which is a\nU-net. Our results were visually appealing for the amount of training time and\nthe quality of input data that went into training.","terms":["cs.CV","cs.AI"]},{"titles":"Intrinsic Harmonization for Illumination-Aware Compositing","summaries":"Despite significant advancements in network-based image harmonization\ntechniques, there still exists a domain disparity between typical training\npairs and real-world composites encountered during inference. Most existing\nmethods are trained to reverse global edits made on segmented image regions,\nwhich fail to accurately capture the lighting inconsistencies between the\nforeground and background found in composited images. In this work, we\nintroduce a self-supervised illumination harmonization approach formulated in\nthe intrinsic image domain. First, we estimate a simple global lighting model\nfrom mid-level vision representations to generate a rough shading for the\nforeground region. A network then refines this inferred shading to generate a\nharmonious re-shading that aligns with the background scene. In order to match\nthe color appearance of the foreground and background, we utilize ideas from\nprior harmonization approaches to perform parameterized image edits in the\nalbedo domain. To validate the effectiveness of our approach, we present\nresults from challenging real-world composites and conduct a user study to\nobjectively measure the enhanced realism achieved compared to state-of-the-art\nharmonization methods.","terms":["cs.CV","cs.AI","cs.GR"]},{"titles":"MICRO: Model-Based Offline Reinforcement Learning with a Conservative Bellman Operator","summaries":"Offline reinforcement learning (RL) faces a significant challenge of\ndistribution shift. Model-free offline RL penalizes the Q value for\nout-of-distribution (OOD) data or constrains the policy closed to the behavior\npolicy to tackle this problem, but this inhibits the exploration of the OOD\nregion. Model-based offline RL, which uses the trained environment model to\ngenerate more OOD data and performs conservative policy optimization within\nthat model, has become an effective method for this problem. However, the\ncurrent model-based algorithms rarely consider agent robustness when\nincorporating conservatism into policy. Therefore, the new model-based offline\nalgorithm with a conservative Bellman operator (MICRO) is proposed. This method\ntrades off performance and robustness via introducing the robust Bellman\noperator into the algorithm. Compared with previous model-based algorithms with\nrobust adversarial models, MICRO can significantly reduce the computation cost\nby only choosing the minimal Q value in the state uncertainty set. Extensive\nexperiments demonstrate that MICRO outperforms prior RL algorithms in offline\nRL benchmark and is considerably robust to adversarial perturbations.","terms":["cs.LG","cs.AI"]},{"titles":"Rapid detection of rare events from in situ X-ray diffraction data using machine learning","summaries":"High-energy X-ray diffraction methods can non-destructively map the 3D\nmicrostructure and associated attributes of metallic polycrystalline\nengineering materials in their bulk form. These methods are often combined with\nexternal stimuli such as thermo-mechanical loading to take snapshots over time\nof the evolving microstructure and attributes. However, the extreme data\nvolumes and the high costs of traditional data acquisition and reduction\napproaches pose a barrier to quickly extracting actionable insights and\nimproving the temporal resolution of these snapshots. Here we present a fully\nautomated technique capable of rapidly detecting the onset of plasticity in\nhigh-energy X-ray microscopy data. Our technique is computationally faster by\nat least 50 times than the traditional approaches and works for data sets that\nare up to 9 times sparser than a full data set. This new technique leverages\nself-supervised image representation learning and clustering to transform\nmassive data into compact, semantic-rich representations of visually salient\ncharacteristics (e.g., peak shapes). These characteristics can be a rapid\nindicator of anomalous events such as changes in diffraction peak shapes. We\nanticipate that this technique will provide just-in-time actionable information\nto drive smarter experiments that effectively deploy multi-modal X-ray\ndiffraction methods that span many decades of length scales.","terms":["cs.LG","cond-mat.mtrl-sci","eess.IV","physics.data-an"]},{"titles":"Convolutional layers are equivariant to discrete shifts but not continuous translations","summaries":"The purpose of this short and simple note is to clarify a common\nmisconception about convolutional neural networks (CNNs). CNNs are made up of\nconvolutional layers which are shift equivariant due to weight sharing.\nHowever, convolutional layers are not translation equivariant, even when\nboundary effects are ignored and when pooling and subsampling are absent. This\nis because shift equivariance is a discrete symmetry while translation\nequivariance is a continuous symmetry. This fact is well known among\nresearchers in equivariant machine learning, but is usually overlooked among\nnon-experts. To minimize confusion, we suggest using the term `shift\nequivariance' to refer to discrete shifts in pixels and `translation\nequivariance' to refer to continuous translations.","terms":["cs.CV","cs.LG"]},{"titles":"Domain Invariant Representation Learning and Sleep Dynamics Modeling for Automatic Sleep Staging","summaries":"Sleep staging has become a critical task in diagnosing and treating sleep\ndisorders to prevent sleep related diseases. With rapidly growing large scale\npublic sleep databases and advances in machine learning, significant progress\nhas been made toward automatic sleep staging. However, previous studies face\nsome critical problems in sleep studies; the heterogeneity of subjects'\nphysiological signals, the inability to extract meaningful information from\nunlabeled sleep signal data to improve predictive performances, the difficulty\nin modeling correlations between sleep stages, and the lack of an effective\nmechanism to quantify predictive uncertainty. In this study, we propose a\nneural network based automatic sleep staging model, named DREAM, to learn\ndomain generalized representations from physiological signals and models sleep\ndynamics. DREAM learns sleep related and subject invariant representations from\ndiverse subjects' sleep signal segments and models sleep dynamics by capturing\ninteractions between sequential signal segments and between sleep stages. In\nthe experiments, we demonstrate that DREAM outperforms the existing sleep\nstaging methods on three datasets. The case study demonstrates that our model\ncan learn the generalized decision function resulting in good prediction\nperformances for the new subjects, especially in case there are differences\nbetween testing and training subjects. The usage of unlabeled data shows the\nbenefit of leveraging unlabeled EEG data. Further, uncertainty quantification\ndemonstrates that DREAM provides prediction uncertainty, making the model\nreliable and helping sleep experts in real world applications.","terms":["cs.LG","eess.SP"]},{"titles":"SVQ: Sparse Vector Quantization for Spatiotemporal Forecasting","summaries":"Spatiotemporal forecasting tasks, such as weather forecasting and traffic\nprediction, offer significant societal benefits. These tasks can be effectively\napproached as image forecasting problems using computer vision models. Vector\nquantization (VQ) is a well-known method for discrete representation that\nimproves the latent space, leading to enhanced generalization and transfer\nlearning capabilities. One of the main challenges in using VQ for\nspatiotemporal forecasting is how to balance between keeping enough details and\nremoving noises from the original patterns for better generalization. We\naddress this challenge by developing sparse vector quantization, or {\\bf SVQ}\nfor short, that leverages sparse regression to make better trade-off between\nthe two objectives. The main innovation of this work is to approximate sparse\nregression by a two-layer MLP and a randomly fixed or learnable matrix,\ndramatically improving its computational efficiency. Through experiments\nconducted on diverse datasets in multiple fields including weather forecasting,\ntraffic flow prediction, and video forecasting, we unequivocally demonstrate\nthat our proposed method consistently enhances the performance of base models\nand achieves state-of-the-art results across all benchmarks.","terms":["cs.CV","cs.LG"]},{"titles":"Node-aware Bi-smoothing: Certified Robustness against Graph Injection Attacks","summaries":"Deep Graph Learning (DGL) has emerged as a crucial technique across various\ndomains. However, recent studies have exposed vulnerabilities in DGL models,\nsuch as susceptibility to evasion and poisoning attacks. While empirical and\nprovable robustness techniques have been developed to defend against graph\nmodification attacks (GMAs), the problem of certified robustness against graph\ninjection attacks (GIAs) remains largely unexplored. To bridge this gap, we\nintroduce the node-aware bi-smoothing framework, which is the first certifiably\nrobust approach for general node classification tasks against GIAs. Notably,\nthe proposed node-aware bi-smoothing scheme is model-agnostic and is applicable\nfor both evasion and poisoning attacks. Through rigorous theoretical analysis,\nwe establish the certifiable conditions of our smoothing scheme. We also\nexplore the practical implications of our node-aware bi-smoothing schemes in\ntwo contexts: as an empirical defense approach against real-world GIAs and in\nthe context of recommendation systems. Furthermore, we extend two\nstate-of-the-art certified robustness frameworks to address node injection\nattacks and compare our approach against them. Extensive evaluations\ndemonstrate the effectiveness of our proposed certificates.","terms":["cs.LG","cs.CR"]},{"titles":"Improving Medical Report Generation with Adapter Tuning and Knowledge Enhancement in Vision-Language Foundation Models","summaries":"Medical report generation demands automatic creation of coherent and precise\ndescriptions for medical images. However, the scarcity of labelled medical\nimage-report pairs poses formidable challenges in developing large-scale neural\nnetworks capable of harnessing the potential of artificial intelligence,\nexemplified by large language models. This study builds upon the\nstate-of-the-art vision-language pre-training and fine-tuning approach, BLIP-2,\nto customize general large-scale foundation models. Integrating adapter tuning\nand a medical knowledge enhancement loss, our model significantly improves\naccuracy and coherence. Validation on the dataset of ImageCLEFmedical 2023\ndemonstrates our model's prowess, achieving the best-averaged results against\nseveral state-of-the-art methods. Significant improvements in ROUGE and CIDEr\nunderscore our method's efficacy, highlighting promising outcomes for the rapid\nmedical-domain adaptation of the vision-language foundation models in\naddressing challenges posed by data scarcity.","terms":["cs.CV","cs.AI","cs.CE"]},{"titles":"High-resolution power equipment recognition based on improved self-attention","summaries":"The current trend of automating inspections at substations has sparked a\nsurge in interest in the field of transformer image recognition. However, due\nto restrictions in the number of parameters in existing models, high-resolution\nimages can't be directly applied, leaving significant room for enhancing\nrecognition accuracy. Addressing this challenge, the paper introduces a novel\nimprovement on deep self-attention networks tailored for this issue. The\nproposed model comprises four key components: a foundational network, a region\nproposal network, a module for extracting and segmenting target areas, and a\nfinal prediction network. The innovative approach of this paper differentiates\nitself by decoupling the processes of part localization and recognition,\ninitially using low-resolution images for localization followed by\nhigh-resolution images for recognition. Moreover, the deep self-attention\nnetwork's prediction mechanism uniquely incorporates the semantic context of\nimages, resulting in substantially improved recognition performance.\nComparative experiments validate that this method outperforms the two other\nprevalent target recognition models, offering a groundbreaking perspective for\nautomating electrical equipment inspections.","terms":["cs.CV"]},{"titles":"Divide&Classify: Fine-Grained Classification for City-Wide Visual Place Recognition","summaries":"Visual Place recognition is commonly addressed as an image retrieval problem.\nHowever, retrieval methods are impractical to scale to large datasets, densely\nsampled from city-wide maps, since their dimension impact negatively on the\ninference time. Using approximate nearest neighbour search for retrieval helps\nto mitigate this issue, at the cost of a performance drop. In this paper we\ninvestigate whether we can effectively approach this task as a classification\nproblem, thus bypassing the need for a similarity search. We find that existing\nclassification methods for coarse, planet-wide localization are not suitable\nfor the fine-grained and city-wide setting. This is largely due to how the\ndataset is split into classes, because these methods are designed to handle a\nsparse distribution of photos and as such do not consider the visual aliasing\nproblem across neighbouring classes that naturally arises in dense scenarios.\nThus, we propose a partitioning scheme that enables a fast and accurate\ninference, preserving a simple learning procedure, and a novel inference\npipeline based on an ensemble of novel classifiers that uses the prototypes\nlearned via an angular margin loss. Our method, Divide&Classify (D&C), enjoys\nthe fast inference of classification solutions and an accuracy competitive with\nretrieval methods on the fine-grained, city-wide setting. Moreover, we show\nthat D&C can be paired with existing retrieval pipelines to speed up\ncomputations by over 20 times while increasing their recall, leading to new\nstate-of-the-art results.","terms":["cs.CV"]},{"titles":"Overhead Line Defect Recognition Based on Unsupervised Semantic Segmentation","summaries":"Overhead line inspection greatly benefits from defect recognition using\nvisible light imagery. Addressing the limitations of existing feature\nextraction techniques and the heavy data dependency of deep learning\napproaches, this paper introduces a novel defect recognition framework. This is\nbuilt on the Faster RCNN network and complemented by unsupervised semantic\nsegmentation. The approach involves identifying the type and location of the\ntarget equipment, utilizing semantic segmentation to differentiate between the\ndevice and its backdrop, and finally employing similarity measures and logical\nrules to categorize the type of defect. Experimental results indicate that this\nmethodology focuses more on the equipment rather than the defects when\nidentifying issues in overhead lines. This leads to a notable enhancement in\naccuracy and exhibits impressive adaptability. Thus, offering a fresh\nperspective for automating the inspection of distribution network equipment.","terms":["cs.CV"]},{"titles":"From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces","summaries":"Much of the previous work towards digital agents for graphical user\ninterfaces (GUIs) has relied on text-based representations (derived from HTML\nor other structured data sources), which are not always readily available.\nThese input representations have been often coupled with custom, task-specific\naction spaces. This paper focuses on creating agents that interact with the\ndigital world using the same conceptual interface that humans commonly use --\nvia pixel-based screenshots and a generic action space corresponding to\nkeyboard and mouse actions. Building upon recent progress in pixel-based\npretraining, we show, for the first time, that it is possible for such agents\nto outperform human crowdworkers on the MiniWob++ benchmark of GUI-based\ninstruction following tasks.","terms":["cs.LG","cs.CL","cs.CV","cs.HC"]},{"titles":"Understanding the Role of Optimization in Double Descent","summaries":"The phenomenon of model-wise double descent, where the test error peaks and\nthen reduces as the model size increases, is an interesting topic that has\nattracted the attention of researchers due to the striking observed gap between\ntheory and practice \\citep{Belkin2018ReconcilingMM}. Additionally, while double\ndescent has been observed in various tasks and architectures, the peak of\ndouble descent can sometimes be noticeably absent or diminished, even without\nexplicit regularization, such as weight decay and early stopping. In this\npaper, we investigate this intriguing phenomenon from the optimization\nperspective and propose a simple optimization-based explanation for why double\ndescent sometimes occurs weakly or not at all. To the best of our knowledge, we\nare the first to demonstrate that many disparate factors contributing to\nmodel-wise double descent (initialization, normalization, batch size, learning\nrate, optimization algorithm) are unified from the viewpoint of optimization:\nmodel-wise double descent is observed if and only if the optimizer can find a\nsufficiently low-loss minimum. These factors directly affect the condition\nnumber of the optimization problem or the optimizer and thus affect the final\nminimum found by the optimizer, reducing or increasing the height of the double\ndescent peak. We conduct a series of controlled experiments on random feature\nmodels and two-layer neural networks under various optimization settings,\ndemonstrating this optimization-based unified view. Our results suggest the\nfollowing implication: Double descent is unlikely to be a problem for\nreal-world machine learning setups. Additionally, our results help explain the\ngap between weak double descent peaks in practice and strong peaks observable\nin carefully designed setups.","terms":["cs.LG","stat.ML"]},{"titles":"Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models","summaries":"With pre-training of vision-and-language models (VLMs) on large-scale\ndatasets of image-text pairs, several recent works showed that these\npre-trained models lack fine-grained understanding, such as the ability to\ncount and recognize verbs, attributes, or relationships. The focus of this work\nis to study the ability of these models to understand spatial relations.\nPreviously, this has been tackled using image-text matching (e.g., Visual\nSpatial Reasoning benchmark) or visual question answering (e.g., GQA or VQAv2),\nboth showing poor performance and a large gap compared to human performance. In\nthis work, we use explainability tools to understand the causes of poor\nperformance better and present an alternative fine-grained, compositional\napproach for ranking spatial clauses. We combine the evidence from grounding\nnoun phrases corresponding to objects and their locations to compute the final\nrank of the spatial clause. We demonstrate the approach on representative VLMs\n(such as LXMERT, GPV, and MDETR) and compare and highlight their abilities to\nreason about spatial relationships.","terms":["cs.CV","cs.CL","cs.LG"]},{"titles":"A Layer-Wise Tokens-to-Token Transformer Network for Improved Historical Document Image Enhancement","summaries":"Document image enhancement is a fundamental and important stage for attaining\nthe best performance in any document analysis assignment because there are many\ndegradation situations that could harm document images, making it more\ndifficult to recognize and analyze them. In this paper, we propose\n\\textbf{T2T-BinFormer} which is a novel document binarization encoder-decoder\narchitecture based on a Tokens-to-token vision transformer. Each image is\ndivided into a set of tokens with a defined length using the ViT model, which\nis then applied several times to model the global relationship between the\ntokens. However, the conventional tokenization of input data does not\nadequately reflect the crucial local structure between adjacent pixels of the\ninput image, which results in low efficiency. Instead of using a simple ViT and\nhard splitting of images for the document image enhancement task, we employed a\nprogressive tokenization technique to capture this local information from an\nimage to achieve more effective results. Experiments on various DIBCO and\nH-DIBCO benchmarks demonstrate that the proposed model outperforms the existing\nCNN and ViT-based state-of-the-art methods. In this research, the primary area\nof examination is the application of the proposed architecture to the task of\ndocument binarization. The source code will be made available at\nhttps:\/\/github.com\/RisabBiswas\/T2T-BinFormer.","terms":["cs.CV"]},{"titles":"How Many Validation Labels Do You Need? Exploring the Design Space of Label-Efficient Model Ranking","summaries":"The paper introduces LEMR, a framework that reduces annotation costs for\nmodel selection tasks. Our approach leverages ensemble methods to generate\npseudo-labels, employs uncertainty sampling for target acquisition, and\nutilizes a Z-score mechanism for iterative committee reelection to refine model\nranks. We present a systematic study across various selection metrics,\ndemonstrating that LEMR achieves comparable results to fully labeled datasets\nwith a fraction of the labeling budget. Our findings indicate that LEMR not\nonly economizes the labeling effort in weak supervision and semi-supervised\nlearning settings but also effectively guides prompt selection for large\nlanguage models. With extensive experiments across 23 tasks, we reveal that our\nframework can dramatically decrease the labeling cost without compromising the\naccuracy of model selection, thereby offering a cost-effective alternative to\ntraditional practices.","terms":["cs.LG"]},{"titles":"Adapting HouseDiffusion for conditional Floor Plan generation on Modified Swiss Dwellings dataset","summaries":"Automated floor plan generation has recently gained momentum with several\nmethods that have been proposed. The CVAAD Floor Plan Auto-Completion workshop\nchallenge introduced MSD, a new dataset that includes existing structural walls\nof the building as an additional input constraint. This technical report\npresents an approach for extending a recent work, HouseDiffusion\n(arXiv:2211.13287 [cs.CV]), to the MSD dataset. The adaption involves modifying\nthe model's transformer layers to condition on a set of wall lines. The report\nintroduces a pre-processing pipeline to extract wall lines from the binary mask\nof the building structure provided as input. Additionally, it was found that a\ndata processing procedure that simplifies all room polygons to rectangles leads\nto better performance. This indicates that future work should explore better\nrepresentations of variable-length polygons in diffusion models. The code will\nbe made available at a later date.","terms":["cs.CV"]},{"titles":"The Potential of Vision-Language Models for Content Moderation of Children's Videos","summaries":"Natural language supervision has been shown to be effective for zero-shot\nlearning in many computer vision tasks, such as object detection and activity\nrecognition. However, generating informative prompts can be challenging for\nmore subtle tasks, such as video content moderation. This can be difficult, as\nthere are many reasons why a video might be inappropriate, beyond violence and\nobscenity. For example, scammers may attempt to create junk content that is\nsimilar to popular educational videos but with no meaningful information. This\npaper evaluates the performance of several CLIP variations for content\nmoderation of children's cartoons in both the supervised and zero-shot setting.\nWe show that our proposed model (Vanilla CLIP with Projection Layer)\noutperforms previous work conducted on the Malicious or Benign (MOB) benchmark\nfor video content moderation. This paper presents an in depth analysis of how\ncontext-specific language prompts affect content moderation performance. Our\nresults indicate that it is important to include more context in content\nmoderation prompts, particularly for cartoon videos as they are not well\nrepresented in the CLIP training data.","terms":["cs.CV","cs.CY","cs.LG","cs.SI"]},{"titles":"Adaptive Weighted Co-Learning for Cross-Domain Few-Shot Learning","summaries":"Due to the availability of only a few labeled instances for the novel target\nprediction task and the significant domain shift between the well annotated\nsource domain and the target domain, cross-domain few-shot learning (CDFSL)\ninduces a very challenging adaptation problem. In this paper, we propose a\nsimple Adaptive Weighted Co-Learning (AWCoL) method to address the CDFSL\nchallenge by adapting two independently trained source prototypical\nclassification models to the target task in a weighted co-learning manner. The\nproposed method deploys a weighted moving average prediction strategy to\ngenerate probabilistic predictions from each model, and then conducts adaptive\nco-learning by jointly fine-tuning the two models in an alternating manner\nbased on the pseudo-labels and instance weights produced from the predictions.\nMoreover, a negative pseudo-labeling regularizer is further deployed to improve\nthe fine-tuning process by penalizing false predictions. Comprehensive\nexperiments are conducted on multiple benchmark datasets and the empirical\nresults demonstrate that the proposed method produces state-of-the-art CDFSL\nperformance.","terms":["cs.LG"]},{"titles":"Emergence of Latent Binary Encoding in Deep Neural Network Classifiers","summaries":"We observe the emergence of binary encoding within the latent space of\ndeep-neural-network classifiers. Such binary encoding is induced by introducing\na linear penultimate layer, which is equipped during training with a loss\nfunction that grows as $\\exp(\\vec{x}^2)$, where $\\vec{x}$ are the coordinates\nin the latent space. The phenomenon we describe represents a specific instance\nof a well-documented occurrence known as \\textit{neural collapse}, which arises\nin the terminal phase of training and entails the collapse of latent class\nmeans to the vertices of a simplex equiangular tight frame (ETF). We show that\nbinary encoding accelerates convergence toward the simplex ETF and enhances\nclassification accuracy.","terms":["cs.LG"]},{"titles":"Gradient play in stochastic games: stationary points, convergence, and sample complexity","summaries":"We study the performance of the gradient play algorithm for stochastic games\n(SGs), where each agent tries to maximize its own total discounted reward by\nmaking decisions independently based on current state information which is\nshared between agents. Policies are directly parameterized by the probability\nof choosing a certain action at a given state. We show that Nash equilibria\n(NEs) and first-order stationary policies are equivalent in this setting, and\ngive a local convergence rate around strict NEs. Further, for a subclass of SGs\ncalled Markov potential games (which includes the setting with identical\nrewards as an important special case), we design a sample-based reinforcement\nlearning algorithm and give a non-asymptotic global convergence rate analysis\nfor both exact gradient play and our sample-based learning algorithm. Our\nresult shows that the number of iterations to reach an $\\epsilon$-NE scales\nlinearly, instead of exponentially, with the number of agents. Local geometry\nand local stability are also considered, where we prove that strict NEs are\nlocal maxima of the total potential function and fully-mixed NEs are saddle\npoints.","terms":["cs.LG","cs.GT","cs.MA","math.OC"]},{"titles":"Controllable Human-Object Interaction Synthesis","summaries":"Synthesizing semantic-aware, long-horizon, human-object interaction is\ncritical to simulate realistic human behaviors. In this work, we address the\nchallenging problem of generating synchronized object motion and human motion\nguided by language descriptions in 3D scenes. We propose Controllable\nHuman-Object Interaction Synthesis (CHOIS), an approach that generates object\nmotion and human motion simultaneously using a conditional diffusion model\ngiven a language description, initial object and human states, and sparse\nobject waypoints. While language descriptions inform style and intent,\nwaypoints ground the motion in the scene and can be effectively extracted using\nhigh-level planning methods. Naively applying a diffusion model fails to\npredict object motion aligned with the input waypoints and cannot ensure the\nrealism of interactions that require precise hand-object contact and\nappropriate contact grounded by the floor. To overcome these problems, we\nintroduce an object geometry loss as additional supervision to improve the\nmatching between generated object motion and input object waypoints. In\naddition, we design guidance terms to enforce contact constraints during the\nsampling process of the trained diffusion model.","terms":["cs.CV"]},{"titles":"Improving Gradient-guided Nested Sampling for Posterior Inference","summaries":"We present a performant, general-purpose gradient-guided nested sampling\nalgorithm, ${\\tt GGNS}$, combining the state of the art in differentiable\nprogramming, Hamiltonian slice sampling, clustering, mode separation, dynamic\nnested sampling, and parallelization. This unique combination allows ${\\tt\nGGNS}$ to scale well with dimensionality and perform competitively on a variety\nof synthetic and real-world problems. We also show the potential of combining\nnested sampling with generative flow networks to obtain large amounts of\nhigh-quality samples from the posterior distribution. This combination leads to\nfaster mode discovery and more accurate estimates of the partition function.","terms":["cs.LG","stat.CO","stat.ME","stat.ML"]},{"titles":"Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation","summaries":"In this paper, we devise a mechanism for the addition of multi-modal\ninformation with an existing pipeline for continuous sign language recognition\nand translation. In our procedure, we have incorporated optical flow\ninformation with RGB images to enrich the features with movement-related\ninformation. This work studies the feasibility of such modality inclusion using\na cross-modal encoder. The plugin we have used is very lightweight and doesn't\nneed to include a separate feature extractor for the new modality in an\nend-to-end manner. We have applied the changes in both sign language\nrecognition and translation, improving the result in each case. We have\nevaluated the performance on the RWTH-PHOENIX-2014 dataset for sign language\nrecognition and the RWTH-PHOENIX-2014T dataset for translation. On the\nrecognition task, our approach reduced the WER by 0.9, and on the translation\ntask, our approach increased most of the BLEU scores by ~0.6 on the test set.","terms":["cs.CV","cs.CL","cs.LG"]},{"titles":"Inverse distance weighting attention","summaries":"We report the effects of replacing the scaled dot-product (within softmax)\nattention with the negative-log of Euclidean distance. This form of attention\nsimplifies to inverse distance weighting interpolation. Used in simple one\nhidden layer networks and trained with vanilla cross-entropy loss on\nclassification problems, it tends to produce a key matrix containing prototypes\nand a value matrix with corresponding logits. We also show that the resulting\ninterpretable networks can be augmented with manually-constructed prototypes to\nperform low-impact handling of special cases.","terms":["cs.LG"]},{"titles":"A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints","summaries":"Neuro-symbolic AI bridges the gap between purely symbolic and neural\napproaches to learning. This often requires maximizing the likelihood of a\nsymbolic constraint w.r.t the neural network's output distribution. Such output\ndistributions are typically assumed to be fully-factorized. This limits the\napplicability of neuro-symbolic learning to the more expressive autoregressive\ndistributions, e.g., transformers. Under such distributions, computing the\nlikelihood of even simple constraints is #P-hard. Instead of attempting to\nenforce the constraint on the entire output distribution, we propose to do so\non a random, local approximation thereof. More precisely, we optimize the\nlikelihood of the constraint under a pseudolikelihood-based approximation\ncentered around a model sample. Our approximation is factorized, allowing the\nreuse of solutions to sub-problems, a main tenet for efficiently computing\nneuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of\nthe likelihood, exhibiting low entropy and KL-divergence around the model\nsample. We evaluate our approach on Sudoku and shortest-path prediction cast as\nautoregressive generation, and observe that we greatly improve upon the base\nmodel's ability to predict logically-consistent outputs. We also evaluate on\nthe task of detoxifying large language models. Using a simple constraint\ndisallowing a list of toxic words, we are able to steer the model's outputs\naway from toxic generations, achieving SoTA detoxification compared to previous\napproaches.","terms":["cs.LG","cs.AI","cs.CL"]},{"titles":"Detecting algorithmic bias in medical AI-models","summaries":"With the growing prevalence of machine learning and artificial\nintelligence-based medical decision support systems, it is equally important to\nensure that these systems provide patient outcomes in a fair and equitable\nfashion. This paper presents an innovative framework for detecting areas of\nalgorithmic bias in medical-AI decision support systems. Our approach\nefficiently identifies potential biases in medical-AI models, specifically in\nthe context of sepsis prediction, by employing the Classification and\nRegression Trees (CART) algorithm. We verify our methodology by conducting a\nseries of synthetic data experiments, showcasing its ability to estimate areas\nof bias in controlled settings precisely. The effectiveness of the concept is\nfurther validated by experiments using electronic medical records from Grady\nMemorial Hospital in Atlanta, Georgia. These tests demonstrate the practical\nimplementation of our strategy in a clinical environment, where it can function\nas a vital instrument for guaranteeing fairness and equity in AI-based medical\ndecisions.","terms":["stat.ML","cs.CY","cs.LG","stat.AP"]},{"titles":"Adaptive Dependency Learning Graph Neural Networks","summaries":"Graph Neural Networks (GNN) have recently gained popularity in the\nforecasting domain due to their ability to model complex spatial and temporal\npatterns in tasks such as traffic forecasting and region-based demand\nforecasting. Most of these methods require a predefined graph as input, whereas\nin real-life multivariate time series problems, a well-predefined dependency\ngraph rarely exists. This requirement makes it harder for GNNs to be utilised\nwidely for multivariate forecasting problems in other domains such as retail or\nenergy. In this paper, we propose a hybrid approach combining neural networks\nand statistical structure learning models to self-learn the dependencies and\nconstruct a dynamically changing dependency graph from multivariate data aiming\nto enable the use of GNNs for multivariate forecasting even when a well-defined\ngraph does not exist. The statistical structure modeling in conjunction with\nneural networks provides a well-principled and efficient approach by bringing\nin causal semantics to determine dependencies among the series. Finally, we\ndemonstrate significantly improved performance using our proposed approach on\nreal-world benchmark datasets without a pre-defined dependency graph.","terms":["cs.LG"]},{"titles":"HLoOP -- Hyperbolic 2-space Local Outlier Probabilities","summaries":"Hyperbolic geometry has recently garnered considerable attention in machine\nlearning due to its capacity to embed hierarchical graph structures with low\ndistortions for further downstream processing. This paper introduces a simple\nframework to detect local outliers for datasets grounded in hyperbolic 2-space\nreferred to as HLoOP (Hyperbolic Local Outlier Probability). Within a Euclidean\nspace, well-known techniques for local outlier detection are based on the Local\nOutlier Factor (LOF) and its variant, the LoOP (Local Outlier Probability),\nwhich incorporates probabilistic concepts to model the outlier level of a data\nvector. The developed HLoOP combines the idea of finding nearest neighbors,\ndensity-based outlier scoring with a probabilistic, statistically oriented\napproach. Therefore, the method consists in computing the Riemmanian distance\nof a data point to its nearest neighbors following a Gaussian probability\ndensity function expressed in a hyperbolic space. This is achieved by defining\na Gaussian cumulative distribution in this space. The HLoOP algorithm is tested\non the WordNet dataset yielding promising results. Code and data will be made\navailable on request for reproductibility.","terms":["stat.ML","cs.LG"]},{"titles":"A Masked Pruning Approach for Dimensionality Reduction in Communication-Efficient Federated Learning Systems","summaries":"Federated Learning (FL) represents a growing machine learning (ML) paradigm\ndesigned for training models across numerous nodes that retain local datasets,\nall without directly exchanging the underlying private data with the parameter\nserver (PS). Its increasing popularity is attributed to notable advantages in\nterms of training deep neural network (DNN) models under privacy aspects and\nefficient utilization of communication resources. Unfortunately, DNNs suffer\nfrom high computational and communication costs, as well as memory consumption\nin intricate tasks. These factors restrict the applicability of FL algorithms\nin communication-constrained systems with limited hardware resources.\n  In this paper, we develop a novel algorithm that overcomes these limitations\nby synergistically combining a pruning-based method with the FL process,\nresulting in low-dimensional representations of the model with minimal\ncommunication cost, dubbed Masked Pruning over FL (MPFL). The algorithm\noperates by initially distributing weights to the nodes through the PS.\nSubsequently, each node locally trains its model and computes pruning masks.\nThese low-dimensional masks are then transmitted back to the PS, which\ngenerates a consensus pruning mask, broadcasted back to the nodes. This\niterative process enhances the robustness and stability of the masked pruning\nmodel. The generated mask is used to train the FL model, achieving significant\nbandwidth savings. We present an extensive experimental study demonstrating the\nsuperior performance of MPFL compared to existing methods. Additionally, we\nhave developed an open-source software package for the benefit of researchers\nand developers in related fields.","terms":["cs.LG","cs.AI","cs.DC"]},{"titles":"On The Fairness Impacts of Hardware Selection in Machine Learning","summaries":"In the machine learning ecosystem, hardware selection is often regarded as a\nmere utility, overshadowed by the spotlight on algorithms and data. This\noversight is particularly problematic in contexts like ML-as-a-service\nplatforms, where users often lack control over the hardware used for model\ndeployment. How does the choice of hardware impact generalization properties?\nThis paper investigates the influence of hardware on the delicate balance\nbetween model performance and fairness. We demonstrate that hardware choices\ncan exacerbate existing disparities, attributing these discrepancies to\nvariations in gradient flows and loss surfaces across different demographic\ngroups. Through both theoretical and empirical analysis, the paper not only\nidentifies the underlying factors but also proposes an effective strategy for\nmitigating hardware-induced performance imbalances.","terms":["cs.LG","cs.AI","cs.CY"]},{"titles":"Adapting Newton's Method to Neural Networks through a Summary of Higher-Order Derivatives","summaries":"We consider a gradient-based optimization method applied to a function\n$\\mathcal{L}$ of a vector of variables $\\boldsymbol{\\theta}$, in the case where\n$\\boldsymbol{\\theta}$ is represented as a tuple of tensors $(\\mathbf{T}_1,\n\\cdots, \\mathbf{T}_S)$. This framework encompasses many common use-cases, such\nas training neural networks by gradient descent. First, we propose a\ncomputationally inexpensive technique providing higher-order information on\n$\\mathcal{L}$, especially about the interactions between the tensors\n$\\mathbf{T}_s$, based on automatic differentiation and computational tricks.\nSecond, we use this technique at order 2 to build a second-order optimization\nmethod which is suitable, among other things, for training deep neural networks\nof various architectures. This second-order method leverages the partition\nstructure of $\\boldsymbol{\\theta}$ into tensors $(\\mathbf{T}_1, \\cdots,\n\\mathbf{T}_S)$, in such a way that it requires neither the computation of the\nHessian of $\\mathcal{L}$ according to $\\boldsymbol{\\theta}$, nor any\napproximation of it. The key part consists in computing a smaller matrix\ninterpretable as a \"Hessian according to the partition\", which can be computed\nexactly and efficiently. In contrast to many existing practical second-order\nmethods used in neural networks, which perform a diagonal or block-diagonal\napproximation of the Hessian or its inverse, the method we propose does not\nneglect interactions between layers. Finally, we can tune the coarseness of the\npartition to recover well-known optimization methods: the coarsest case\ncorresponds to Cauchy's steepest descent method, the finest case corresponds to\nthe usual Newton's method.","terms":["cs.LG","math.OC"]},{"titles":"WonderJourney: Going from Anywhere to Everywhere","summaries":"We introduce WonderJourney, a modularized framework for perpetual 3D scene\ngeneration. Unlike prior work on view generation that focuses on a single type\nof scenes, we start at any user-provided location (by a text description or an\nimage) and generate a journey through a long sequence of diverse yet coherently\nconnected 3D scenes. We leverage an LLM to generate textual descriptions of the\nscenes in this journey, a text-driven point cloud generation pipeline to make a\ncompelling and coherent sequence of 3D scenes, and a large VLM to verify the\ngenerated scenes. We show compelling, diverse visual results across various\nscene types and styles, forming imaginary \"wonderjourneys\". Project website:\nhttps:\/\/kovenyu.com\/WonderJourney\/","terms":["cs.CV","cs.GR"]},{"titles":"FoMo Rewards: Can we cast foundation models as reward functions?","summaries":"We explore the viability of casting foundation models as generic reward\nfunctions for reinforcement learning. To this end, we propose a simple pipeline\nthat interfaces an off-the-shelf vision model with a large language model.\nSpecifically, given a trajectory of observations, we infer the likelihood of an\ninstruction describing the task that the user wants an agent to perform. We\nshow that this generic likelihood function exhibits the characteristics ideally\nexpected from a reward function: it associates high values with the desired\nbehaviour and lower values for several similar, but incorrect policies.\nOverall, our work opens the possibility of designing open-ended agents for\ninteractive tasks via foundation models.","terms":["cs.LG","cs.AI"]},{"titles":"Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition","summaries":"Face recognition systems are widely deployed in safety-critical applications,\nincluding law enforcement, yet they exhibit bias across a range of\nsocio-demographic dimensions, such as gender and race. Conventional wisdom\ndictates that model biases arise from biased training data. As a consequence,\nprevious works on bias mitigation largely focused on pre-processing the\ntraining data, adding penalties to prevent bias from effecting the model during\ntraining, or post-processing predictions to debias them, yet these approaches\nhave shown limited success on hard problems such as face recognition. In our\nwork, we discover that biases are actually inherent to neural network\narchitectures themselves. Following this reframing, we conduct the first neural\narchitecture search for fairness, jointly with a search for hyperparameters.\nOur search outputs a suite of models which Pareto-dominate all other\nhigh-performance architectures and existing bias mitigation methods in terms of\naccuracy and fairness, often by large margins, on the two most widely used\ndatasets for face identification, CelebA and VGGFace2. Furthermore, these\nmodels generalize to other datasets and sensitive attributes. We release our\ncode, models and raw data files at https:\/\/github.com\/dooleys\/FR-NAS.","terms":["cs.CV","cs.AI","cs.CY","cs.LG"]},{"titles":"Domain constraints improve risk prediction when outcome data is missing","summaries":"Machine learning models are often trained to predict the outcome resulting\nfrom a human decision. For example, if a doctor decides to test a patient for\ndisease, will the patient test positive? A challenge is that the human decision\ncensors the outcome data: we only observe test outcomes for patients doctors\nhistorically tested. Untested patients, for whom outcomes are unobserved, may\ndiffer from tested patients along observed and unobserved dimensions. We\npropose a Bayesian model class which captures this setting. The purpose of the\nmodel is to accurately estimate risk for both tested and untested patients.\nEstimating this model is challenging due to the wide range of possibilities for\nuntested patients. To address this, we propose two domain constraints which are\nplausible in health settings: a prevalence constraint, where the overall\ndisease prevalence is known, and an expertise constraint, where the human\ndecision-maker deviates from purely risk-based decision-making only along a\nconstrained feature set. We show theoretically and on synthetic data that\ndomain constraints improve parameter inference. We apply our model to a case\nstudy of cancer risk prediction, showing that the model's inferred risk\npredicts cancer diagnoses, its inferred testing policy captures known public\nhealth policies, and it can identify suboptimalities in test allocation. Though\nour case study is in healthcare, our analysis reveals a general class of domain\nconstraints which can improve model estimation in many settings.","terms":["cs.LG"]},{"titles":"Facial recognition technology and human raters can predict political orientation from images of expressionless faces even when controlling for demographics and self-presentation","summaries":"Carefully standardized facial images of 591 participants were taken in the\nlaboratory, while controlling for self-presentation, facial expression, head\norientation, and image properties. They were presented to human raters and a\nfacial recognition algorithm: both humans (r=.21) and the algorithm (r=.22)\ncould predict participants' scores on a political orientation scale (Cronbach's\nalpha=.94) decorrelated with age, gender, and ethnicity. These effects are on\npar with how well job interviews predict job success, or alcohol drives\naggressiveness. Algorithm's predictive accuracy was even higher (r=.31) when it\nleveraged information on participants' age, gender, and ethnicity. Moreover,\nthe associations between facial appearance and political orientation seem to\ngeneralize beyond our sample: The predictive model derived from standardized\nimages (while controlling for age, gender, and ethnicity) could predict\npolitical orientation (r=.13) from naturalistic images of 3,401 politicians\nfrom the U.S., UK, and Canada. The analysis of facial features associated with\npolitical orientation revealed that conservatives tended to have larger lower\nfaces. The predictability of political orientation from standardized images has\ncritical implications for privacy, the regulation of facial recognition\ntechnology, and understanding the origins and consequences of political\norientation.","terms":["cs.CV","cs.CY","cs.HC","cs.LG"]},{"titles":"Hidden yet quantifiable: A lower bound for confounding strength using randomized trials","summaries":"In the era of fast-paced precision medicine, observational studies play a\nmajor role in properly evaluating new treatments in clinical practice. Yet,\nunobserved confounding can significantly compromise causal conclusions drawn\nfrom non-randomized data. We propose a novel strategy that leverages randomized\ntrials to quantify unobserved confounding. First, we design a statistical test\nto detect unobserved confounding with strength above a given threshold. Then,\nwe use the test to estimate an asymptotically valid lower bound on the\nunobserved confounding strength. We evaluate the power and validity of our\nstatistical test on several synthetic and semi-synthetic datasets. Further, we\nshow how our lower bound can correctly identify the absence and presence of\nunobserved confounding in a real-world setting.","terms":["stat.ML","cs.LG"]},{"titles":"Characterizing the Optimal 0-1 Loss for Multi-class Classification with a Test-time Attacker","summaries":"Finding classifiers robust to adversarial examples is critical for their safe\ndeployment. Determining the robustness of the best possible classifier under a\ngiven threat model for a given data distribution and comparing it to that\nachieved by state-of-the-art training methods is thus an important diagnostic\ntool. In this paper, we find achievable information-theoretic lower bounds on\nloss in the presence of a test-time attacker for multi-class classifiers on any\ndiscrete dataset. We provide a general framework for finding the optimal 0-1\nloss that revolves around the construction of a conflict hypergraph from the\ndata and adversarial constraints. We further define other variants of the\nattacker-classifier game that determine the range of the optimal loss more\nefficiently than the full-fledged hypergraph construction. Our evaluation\nshows, for the first time, an analysis of the gap to optimal robustness for\nclassifiers in the multi-class setting on benchmark datasets.","terms":["cs.LG","cs.CR"]},{"titles":"Inpaint3D: 3D Scene Content Generation using 2D Inpainting Diffusion","summaries":"This paper presents a novel approach to inpainting 3D regions of a scene,\ngiven masked multi-view images, by distilling a 2D diffusion model into a\nlearned 3D scene representation (e.g. a NeRF). Unlike 3D generative methods\nthat explicitly condition the diffusion model on camera pose or multi-view\ninformation, our diffusion model is conditioned only on a single masked 2D\nimage. Nevertheless, we show that this 2D diffusion model can still serve as a\ngenerative prior in a 3D multi-view reconstruction problem where we optimize a\nNeRF using a combination of score distillation sampling and NeRF reconstruction\nlosses. Predicted depth is used as additional supervision to encourage accurate\ngeometry. We compare our approach to 3D inpainting methods that focus on object\nremoval. Because our method can generate content to fill any 3D masked region,\nwe additionally demonstrate 3D object completion, 3D object replacement, and 3D\nscene completion.","terms":["cs.CV"]},{"titles":"Multi-Group Fairness Evaluation via Conditional Value-at-Risk Testing","summaries":"Machine learning (ML) models used in prediction and classification tasks may\ndisplay performance disparities across population groups determined by\nsensitive attributes (e.g., race, sex, age). We consider the problem of\nevaluating the performance of a fixed ML model across population groups defined\nby multiple sensitive attributes (e.g., race and sex and age). Here, the sample\ncomplexity for estimating the worst-case performance gap across groups (e.g.,\nthe largest difference in error rates) increases exponentially with the number\nof group-denoting sensitive attributes. To address this issue, we propose an\napproach to test for performance disparities based on Conditional Value-at-Risk\n(CVaR). By allowing a small probabilistic slack on the groups over which a\nmodel has approximately equal performance, we show that the sample complexity\nrequired for discovering performance violations is reduced exponentially to be\nat most upper bounded by the square root of the number of groups. As a\nbyproduct of our analysis, when the groups are weighted by a specific prior\ndistribution, we show that R\\'enyi entropy of order $2\/3$ of the prior\ndistribution captures the sample complexity of the proposed CVaR test\nalgorithm. Finally, we also show that there exists a non-i.i.d. data collection\nstrategy that results in a sample complexity independent of the number of\ngroups.","terms":["cs.LG","cs.CY","cs.IT","math.IT","stat.ML"]},{"titles":"Learning Genomic Sequence Representations using Graph Neural Networks over De Bruijn Graphs","summaries":"The rapid expansion of genomic sequence data calls for new methods to achieve\nrobust sequence representations. Existing techniques often neglect intricate\nstructural details, emphasizing mainly contextual information. To address this,\nwe developed k-mer embeddings that merge contextual and structural string\ninformation by enhancing De Bruijn graphs with structural similarity\nconnections. Subsequently, we crafted a self-supervised method based on\nContrastive Learning that employs a heterogeneous Graph Convolutional Network\nencoder and constructs positive pairs based on node similarities. Our\nembeddings consistently outperform prior techniques for Edit Distance\nApproximation and Closest String Retrieval tasks.","terms":["cs.LG","q-bio.GN"]},{"titles":"Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training","summaries":"In this work, we tackle the problem of unsupervised domain adaptation (UDA)\nfor video action recognition. Our approach, which we call UNITE, uses an image\nteacher model to adapt a video student model to the target domain. UNITE first\nemploys self-supervised pre-training to promote discriminative feature learning\non target domain videos using a teacher-guided masked distillation objective.\nWe then perform self-training on masked target data, using the video student\nmodel and image teacher model together to generate improved pseudolabels for\nunlabeled target videos. Our self-training process successfully leverages the\nstrengths of both models to achieve strong transfer performance across domains.\nWe evaluate our approach on multiple video domain adaptation benchmarks and\nobserve significant improvements upon previously reported results.","terms":["cs.CV","cs.LG"]},{"titles":"LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning","summaries":"Generating instructional images of human daily actions from an egocentric\nviewpoint serves a key step towards efficient skill transfer. In this paper, we\nintroduce a novel problem -- egocentric action frame generation. The goal is to\nsynthesize the action frame conditioning on the user prompt question and an\ninput egocentric image that captures user's environment. Notably, existing\negocentric datasets lack the detailed annotations that describe the execution\nof actions. Additionally, the diffusion-based image manipulation models fail to\ncontrol the state change of an action within the corresponding egocentric image\npixel space. To this end, we finetune a visual large language model (VLLM) via\nvisual instruction tuning for curating the enriched action descriptions to\naddress our proposed problem. Moreover, we propose to Learn EGOcentric (LEGO)\naction frame generation using image and text embeddings from VLLM as additional\nconditioning. We validate our proposed model on two egocentric datasets --\nEgo4D and Epic-Kitchens. Our experiments show prominent improvement over prior\nimage manipulation models in both quantitative and qualitative evaluation. We\nalso conduct detailed ablation studies and analysis to provide insights on our\nmethod.","terms":["cs.CV"]},{"titles":"In-N-Out: Faithful 3D GAN Inversion with Volumetric Decomposition for Face Editing","summaries":"3D-aware GANs offer new capabilities for view synthesis while preserving the\nediting functionalities of their 2D counterparts. GAN inversion is a crucial\nstep that seeks the latent code to reconstruct input images or videos,\nsubsequently enabling diverse editing tasks through manipulation of this latent\ncode. However, a model pre-trained on a particular dataset (e.g., FFHQ) often\nhas difficulty reconstructing images with out-of-distribution (OOD) objects\nsuch as faces with heavy make-up or occluding objects. We address this issue by\nexplicitly modeling OOD objects from the input in 3D-aware GANs. Our core idea\nis to represent the image using two individual neural radiance fields: one for\nthe in-distribution content and the other for the out-of-distribution object.\nThe final reconstruction is achieved by optimizing the composition of these two\nradiance fields with carefully designed regularization. We demonstrate that our\nexplicit decomposition alleviates the inherent trade-off between reconstruction\nfidelity and editability. We evaluate reconstruction accuracy and editability\nof our method on challenging real face images and videos and showcase favorable\nresults against other baselines.","terms":["cs.CV"]},{"titles":"Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning","summaries":"In-context learning provides a new perspective for multi-task modeling for\nvision and NLP. Under this setting, the model can perceive tasks from prompts\nand accomplish them without any extra task-specific head predictions or model\nfine-tuning. However, Skeleton sequence modeling via in-context learning\nremains unexplored. Directly applying existing in-context models from other\nareas onto skeleton sequences fails due to the inter-frame and cross-task pose\nsimilarity that makes it outstandingly hard to perceive the task correctly from\na subtle context. To address this challenge, we propose Skeleton-in-Context\n(SiC), an effective framework for in-context skeleton sequence modeling. Our\nSiC is able to handle multiple skeleton-based tasks simultaneously after a\nsingle training process and accomplish each task from context according to the\ngiven prompt. It can further generalize to new, unseen tasks according to\ncustomized prompts. To facilitate context perception, we additionally propose a\ntask-unified prompt, which adaptively learns tasks of different natures, such\nas partial joint-level generation, sequence-level prediction, or 2D-to-3D\nmotion prediction. We conduct extensive experiments to evaluate the\neffectiveness of our SiC on multiple tasks, including motion prediction, pose\nestimation, joint completion, and future pose estimation. We also evaluate its\ngeneralization capability on unseen tasks such as motion-in-between. These\nexperiments show that our model achieves state-of-the-art multi-task\nperformance and even outperforms single-task methods on certain tasks.","terms":["cs.CV"]},{"titles":"Self-conditioned Image Generation via Generating Representations","summaries":"This paper presents $\\textbf{R}$epresentation-$\\textbf{C}$onditioned image\n$\\textbf{G}$eneration (RCG), a simple yet effective image generation framework\nwhich sets a new benchmark in class-unconditional image generation. RCG does\nnot condition on any human annotations. Instead, it conditions on a\nself-supervised representation distribution which is mapped from the image\ndistribution using a pre-trained encoder. During generation, RCG samples from\nsuch representation distribution using a representation diffusion model (RDM),\nand employs a pixel generator to craft image pixels conditioned on the sampled\nrepresentation. Such a design provides substantial guidance during the\ngenerative process, resulting in high-quality image generation. Tested on\nImageNet 256$\\times$256, RCG achieves a Frechet Inception Distance (FID) of\n3.31 and an Inception Score (IS) of 253.4. These results not only significantly\nimprove the state-of-the-art of class-unconditional image generation but also\nrival the current leading methods in class-conditional image generation,\nbridging the long-standing performance gap between these two tasks. Code is\navailable at https:\/\/github.com\/LTH14\/rcg.","terms":["cs.CV"]},{"titles":"Alpha-CLIP: A CLIP Model Focusing on Wherever You Want","summaries":"Contrastive Language-Image Pre-training (CLIP) plays an essential role in\nextracting valuable content information from images across diverse tasks. It\naligns textual and visual modalities to comprehend the entire image, including\nall the details, even those irrelevant to specific tasks. However, for a finer\nunderstanding and controlled editing of images, it becomes crucial to focus on\nspecific regions of interest, which can be indicated as points, masks, or boxes\nby humans or perception models. To fulfill the requirements, we introduce\nAlpha-CLIP, an enhanced version of CLIP with an auxiliary alpha channel to\nsuggest attentive regions and fine-tuned with constructed millions of RGBA\nregion-text pairs. Alpha-CLIP not only preserves the visual recognition ability\nof CLIP but also enables precise control over the emphasis of image contents.\nIt demonstrates effectiveness in various tasks, including but not limited to\nopen-world recognition, multimodal large language models, and conditional 2D \/\n3D generation. It has a strong potential to serve as a versatile tool for\nimage-related tasks.","terms":["cs.CV","cs.AI","cs.CL","cs.LG"]},{"titles":"OneLLM: One Framework to Align All Modalities with Language","summaries":"Multimodal large language models (MLLMs) have gained significant attention\ndue to their strong multimodal understanding capability. However, existing\nworks rely heavily on modality-specific encoders, which usually differ in\narchitecture and are limited to common modalities. In this paper, we present\nOneLLM, an MLLM that aligns eight modalities to language using a unified\nframework. We achieve this through a unified multimodal encoder and a\nprogressive multimodal alignment pipeline. In detail, we first train an image\nprojection module to connect a vision encoder with LLM. Then, we build a\nuniversal projection module (UPM) by mixing multiple image projection modules\nand dynamic routing. Finally, we progressively align more modalities to LLM\nwith the UPM. To fully leverage the potential of OneLLM in following\ninstructions, we also curated a comprehensive multimodal instruction dataset,\nincluding 2M items from image, audio, video, point cloud, depth\/normal map, IMU\nand fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks,\nencompassing tasks such as multimodal captioning, question answering and\nreasoning, where it delivers excellent performance. Code, data, model and\nonline demo are available at https:\/\/github.com\/csuhan\/OneLLM","terms":["cs.CV","cs.AI","cs.CL","cs.LG","cs.MM"]},{"titles":"Diffusion Illusions: Hiding Images in Plain Sight","summaries":"We explore the problem of computationally generating special `prime' images\nthat produce optical illusions when physically arranged and viewed in a certain\nway. First, we propose a formal definition for this problem. Next, we introduce\nDiffusion Illusions, the first comprehensive pipeline designed to automatically\ngenerate a wide range of these illusions. Specifically, we both adapt the\nexisting `score distillation loss' and propose a new `dream target loss' to\noptimize a group of differentially parametrized prime images, using a frozen\ntext-to-image diffusion model. We study three types of illusions, each where\nthe prime images are arranged in different ways and optimized using the\naforementioned losses such that images derived from them align with user-chosen\ntext prompts or images. We conduct comprehensive experiments on these illusions\nand verify the effectiveness of our proposed method qualitatively and\nquantitatively. Additionally, we showcase the successful physical fabrication\nof our illusions -- as they are all designed to work in the real world. Our\ncode and examples are publicly available at our interactive project website:\nhttps:\/\/diffusionillusions.com","terms":["cs.CV"]},{"titles":"A unified framework for information-theoretic generalization bounds","summaries":"This paper presents a general methodology for deriving information-theoretic\ngeneralization bounds for learning algorithms. The main technical tool is a\nprobabilistic decorrelation lemma based on a change of measure and a relaxation\nof Young's inequality in $L_{\\psi_p}$ Orlicz spaces. Using the decorrelation\nlemma in combination with other techniques, such as symmetrization, couplings,\nand chaining in the space of probability measures, we obtain new upper bounds\non the generalization error, both in expectation and in high probability, and\nrecover as special cases many of the existing generalization bounds, including\nthe ones based on mutual information, conditional mutual information,\nstochastic chaining, and PAC-Bayes inequalities. In addition, the\nFernique-Talagrand upper bound on the expected supremum of a subgaussian\nprocess emerges as a special case.","terms":["cs.LG","cs.IT","math.IT","stat.ML"]},{"titles":"AVID: Any-Length Video Inpainting with Diffusion Model","summaries":"Recent advances in diffusion models have successfully enabled text-guided\nimage inpainting. While it seems straightforward to extend such editing\ncapability into video domain, there has been fewer works regarding text-guided\nvideo inpainting. Given a video, a masked region at its initial frame, and an\nediting prompt, it requires a model to do infilling at each frame following the\nediting guidance while keeping the out-of-mask region intact. There are three\nmain challenges in text-guided video inpainting: ($i$) temporal consistency of\nthe edited video, ($ii$) supporting different inpainting types at different\nstructural fidelity level, and ($iii$) dealing with variable video length. To\naddress these challenges, we introduce Any-Length Video Inpainting with\nDiffusion Model, dubbed as AVID. At its core, our model is equipped with\neffective motion modules and adjustable structure guidance, for fixed-length\nvideo inpainting. Building on top of that, we propose a novel Temporal\nMultiDiffusion sampling pipeline with an middle-frame attention guidance\nmechanism, facilitating the generation of videos with any desired duration. Our\ncomprehensive experiments show our model can robustly deal with various\ninpainting types at different video duration range, with high quality. More\nvisualization results is made publicly available at\nhttps:\/\/zhang-zx.github.io\/AVID\/ .","terms":["cs.CV"]},{"titles":"Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US","summaries":"In policy research, one of the most critical analytic tasks is to estimate\nthe causal effect of a policy-relevant shift to the distribution of a\ncontinuous exposure\/treatment on an outcome of interest. We call this problem\nshift-response function (SRF) estimation. Existing neural network methods\ninvolving robust causal-effect estimators lack theoretical guarantees and\npractical implementations for SRF estimation. Motivated by a key\npolicy-relevant question in public health, we develop a neural network method\nand its theoretical underpinnings to estimate SRFs with robustness and\nefficiency guarantees. We then apply our method to data consisting of 68\nmillion individuals and 27 million deaths across the U.S. to estimate the\ncausal effect from revising the US National Ambient Air Quality Standards\n(NAAQS) for PM 2.5 from 12 $\\mu g\/m^3$ to 9 $\\mu g\/m^3$. This change has been\nrecently proposed by the US Environmental Protection Agency (EPA). Our goal is\nto estimate, for the first time, the reduction in deaths that would result from\nthis anticipated revision using causal methods for SRFs. Our proposed method,\ncalled {T}argeted {R}egularization for {E}xposure {S}hifts with Neural\n{Net}works (TRESNET), contributes to the neural network literature for causal\ninference in two ways: first, it proposes a targeted regularization loss with\ntheoretical properties that ensure double robustness and achieves asymptotic\nefficiency specific for SRF estimation; second, it enables loss functions from\nthe exponential family of distributions to accommodate non-continuous outcome\ndistributions (such as hospitalization or mortality counts). We complement our\napplication with benchmark experiments that demonstrate TRESNET's broad\napplicability and competitiveness.","terms":["cs.LG","stat.ME","stat.ML"]},{"titles":"On the Role of Edge Dependency in Graph Generative Models","summaries":"In this work, we introduce a novel evaluation framework for generative models\nof graphs, emphasizing the importance of model-generated graph overlap\n(Chanpuriya et al., 2021) to ensure both accuracy and edge-diversity. We\ndelineate a hierarchy of graph generative models categorized into three levels\nof complexity: edge independent, node independent, and fully dependent models.\nThis hierarchy encapsulates a wide range of prevalent methods. We derive\ntheoretical bounds on the number of triangles and other short-length cycles\nproducible by each level of the hierarchy, contingent on the model overlap. We\nprovide instances demonstrating the asymptotic optimality of our bounds.\nFurthermore, we introduce new generative models for each of the three\nhierarchical levels, leveraging dense subgraph discovery (Gionis & Tsourakakis,\n2015). Our evaluation, conducted on real-world datasets, focuses on assessing\nthe output quality and overlap of our proposed models in comparison to other\npopular models. Our results indicate that our simple, interpretable models\nprovide competitive baselines to popular generative models. Through this\ninvestigation, we aim to propel the advancement of graph generative models by\noffering a structured framework and robust evaluation metrics, thereby\nfacilitating the development of models capable of generating accurate and\nedge-diverse graphs.","terms":["cs.LG","cs.SI"]},{"titles":"What Planning Problems Can A Relational Neural Network Solve?","summaries":"Goal-conditioned policies are generally understood to be \"feed-forward\"\ncircuits, in the form of neural networks that map from the current state and\nthe goal specification to the next action to take. However, under what\ncircumstances such a policy can be learned and how efficient the policy will be\nare not well understood. In this paper, we present a circuit complexity\nanalysis for relational neural networks (such as graph neural networks and\ntransformers) representing policies for planning problems, by drawing\nconnections with serialized goal regression search (S-GRS). We show that there\nare three general classes of planning problems, in terms of the growth of\ncircuit width and depth as a function of the number of objects and planning\nhorizon, providing constructive proofs. We also illustrate the utility of this\nanalysis for designing neural networks for policy learning.","terms":["cs.LG","cs.AI","cs.NE","stat.ML"]},{"titles":"Temporal Robustness against Data Poisoning","summaries":"Data poisoning considers cases when an adversary manipulates the behavior of\nmachine learning algorithms through malicious training data. Existing threat\nmodels of data poisoning center around a single metric, the number of poisoned\nsamples. In consequence, if attackers can poison more samples than expected\nwith affordable overhead, as in many practical scenarios, they may be able to\nrender existing defenses ineffective in a short time. To address this issue, we\nleverage timestamps denoting the birth dates of data, which are often available\nbut neglected in the past. Benefiting from these timestamps, we propose a\ntemporal threat model of data poisoning with two novel metrics, earliness and\nduration, which respectively measure how long an attack started in advance and\nhow long an attack lasted. Using these metrics, we define the notions of\ntemporal robustness against data poisoning, providing a meaningful sense of\nprotection even with unbounded amounts of poisoned samples when the attacks are\ntemporally bounded. We present a benchmark with an evaluation protocol\nsimulating continuous data collection and periodic deployments of updated\nmodels, thus enabling empirical evaluation of temporal robustness. Lastly, we\ndevelop and also empirically verify a baseline defense, namely temporal\naggregation, offering provable temporal robustness and highlighting the\npotential of our temporal threat model for data poisoning.","terms":["cs.LG","cs.AI","cs.CR","stat.ML"]},{"titles":"WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation","summaries":"Generating multi-camera street-view videos is critical for augmenting\nautonomous driving datasets, addressing the urgent demand for extensive and\nvaried data. Due to the limitations in diversity and challenges in handling\nlighting conditions, traditional rendering-based methods are increasingly being\nsupplanted by diffusion-based methods. However, a significant challenge in\ndiffusion-based methods is ensuring that the generated sensor data preserve\nboth intra-world consistency and inter-sensor coherence. To address these\nchallenges, we combine an additional explicit world volume and propose the\nWorld Volume-aware Multi-camera Driving Scene Generator (WoVoGen). This system\nis specifically designed to leverage 4D world volume as a foundational element\nfor video generation. Our model operates in two distinct phases: (i)\nenvisioning the future 4D temporal world volume based on vehicle control\nsequences, and (ii) generating multi-camera videos, informed by this envisioned\n4D temporal world volume and sensor interconnectivity. The incorporation of the\n4D world volume empowers WoVoGen not only to generate high-quality street-view\nvideos in response to vehicle control inputs but also to facilitate scene\nediting tasks.","terms":["cs.CV"]},{"titles":"Hybrid Functional Maps for Crease-Aware Non-Isometric Shape Matching","summaries":"Non-isometric shape correspondence remains a fundamental challenge in\ncomputer vision. Traditional methods using Laplace-Beltrami operator (LBO)\neigenmodes face limitations in characterizing high-frequency extrinsic shape\nchanges like bending and creases. We propose a novel approach of combining the\nnon-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shell\nhessian with the intrinsic ones of the LBO, creating a hybrid spectral space in\nwhich we construct functional maps. To this end, we present a theoretical\nframework to effectively integrate non-orthogonal basis functions into\ndescriptor- and learning-based functional map methods. Our approach can be\nincorporated easily into existing functional map pipelines across varying\napplications and is able to handle complex deformations beyond isometries. We\nshow extensive evaluations across various supervised and unsupervised settings\nand demonstrate significant improvements. Notably, our approach achieves up to\n15% better mean geodesic error for non-isometric correspondence settings and up\nto 45% improvement in scenarios with topological noise.","terms":["cs.CV"]},{"titles":"GeoShapley: A Game Theory Approach to Measuring Spatial Effects in Machine Learning Models","summaries":"This paper introduces GeoShapley, a game theory approach to measuring spatial\neffects in machine learning models. GeoShapley extends the Nobel Prize-winning\nShapley value framework in game theory by conceptualizing location as a player\nin a model prediction game, which enables the quantification of the importance\nof location and the synergies between location and other features in a model.\nGeoShapley is a model-agnostic approach and can be applied to statistical or\nblack-box machine learning models in various structures. The interpretation of\nGeoShapley is directly linked with spatially varying coefficient models for\nexplaining spatial effects and additive models for explaining non-spatial\neffects. Using simulated data, GeoShapley values are validated against known\ndata-generating processes and are used for cross-comparison of seven\nstatistical and machine learning models. An empirical example of house price\nmodeling is used to illustrate GeoShapley's utility and interpretation with\nreal world data. The method is available as an open-source Python package named\ngeoshapley.","terms":["cs.LG","stat.ML"]},{"titles":"WarpDiffusion: Efficient Diffusion Model for High-Fidelity Virtual Try-on","summaries":"Image-based Virtual Try-On (VITON) aims to transfer an in-shop garment image\nonto a target person. While existing methods focus on warping the garment to\nfit the body pose, they often overlook the synthesis quality around the\ngarment-skin boundary and realistic effects like wrinkles and shadows on the\nwarped garments. These limitations greatly reduce the realism of the generated\nresults and hinder the practical application of VITON techniques. Leveraging\nthe notable success of diffusion-based models in cross-modal image synthesis,\nsome recent diffusion-based methods have ventured to tackle this issue.\nHowever, they tend to either consume a significant amount of training resources\nor struggle to achieve realistic try-on effects and retain garment details. For\nefficient and high-fidelity VITON, we propose WarpDiffusion, which bridges the\nwarping-based and diffusion-based paradigms via a novel informative and local\ngarment feature attention mechanism. Specifically, WarpDiffusion incorporates\nlocal texture attention to reduce resource consumption and uses a novel\nauto-mask module that effectively retains only the critical areas of the warped\ngarment while disregarding unrealistic or erroneous portions. Notably,\nWarpDiffusion can be integrated as a plug-and-play component into existing\nVITON methodologies, elevating their synthesis quality. Extensive experiments\non high-resolution VITON benchmarks and an in-the-wild test set demonstrate the\nsuperiority of WarpDiffusion, surpassing state-of-the-art methods both\nqualitatively and quantitatively.","terms":["cs.CV"]},{"titles":"Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving","summaries":"Large vision-language models (VLMs) have garnered increasing interest in\nautonomous driving areas, due to their advanced capabilities in complex\nreasoning tasks essential for highly autonomous vehicle behavior. Despite their\npotential, research in autonomous systems is hindered by the lack of datasets\nwith annotated reasoning chains that explain the decision-making processes in\ndriving. To bridge this gap, we present Reason2Drive, a benchmark dataset with\nover 600K video-text pairs, aimed at facilitating the study of interpretable\nreasoning in complex driving environments. We distinctly characterize the\nautonomous driving process as a sequential combination of perception,\nprediction, and reasoning steps, and the question-answer pairs are\nautomatically collected from a diverse range of open-source outdoor driving\ndatasets, including nuScenes, Waymo and ONCE. Moreover, we introduce a novel\naggregated evaluation metric to assess chain-based reasoning performance in\nautonomous systems, addressing the semantic ambiguities of existing metrics\nsuch as BLEU and CIDEr. Based on the proposed benchmark, we conduct experiments\nto assess various existing VLMs, revealing insights into their reasoning\ncapabilities. Additionally, we develop an efficient approach to empower VLMs to\nleverage object-level perceptual elements in both feature extraction and\nprediction, further enhancing their reasoning accuracy. The code and dataset\nwill be released.","terms":["cs.CV"]},{"titles":"Pearl: A Production-ready Reinforcement Learning Agent","summaries":"Reinforcement Learning (RL) offers a versatile framework for achieving\nlong-term goals. Its generality allows us to formalize a wide range of problems\nthat real-world intelligent systems encounter, such as dealing with delayed\nrewards, handling partial observability, addressing the exploration and\nexploitation dilemma, utilizing offline data to improve online performance, and\nensuring safety constraints are met. Despite considerable progress made by the\nRL research community in addressing these issues, existing open-source RL\nlibraries tend to focus on a narrow portion of the RL solution pipeline,\nleaving other aspects largely unattended. This paper introduces Pearl, a\nProduction-ready RL agent software package explicitly designed to embrace these\nchallenges in a modular fashion. In addition to presenting preliminary\nbenchmark results, this paper highlights Pearl's industry adoptions to\ndemonstrate its readiness for production usage. Pearl is open sourced on Github\nat github.com\/facebookresearch\/pearl and its official website is located at\npearlagent.github.io.","terms":["cs.LG","cs.AI"]},{"titles":"Interpretability Illusions in the Generalization of Simplified Models","summaries":"A common method to study deep learning systems is to use simplified model\nrepresentations -- for example, using singular value decomposition to visualize\nthe model's hidden states in a lower dimensional space. This approach assumes\nthat the results of these simplified are faithful to the original model. Here,\nwe illustrate an important caveat to this assumption: even if the simplified\nrepresentations can accurately approximate the full model on the training set,\nthey may fail to accurately capture the model's behavior out of distribution --\nthe understanding developed from simplified representations may be an illusion.\nWe illustrate this by training Transformer models on controlled datasets with\nsystematic generalization splits. First, we train models on the Dyck\nbalanced-parenthesis languages. We simplify these models using tools like\ndimensionality reduction and clustering, and then explicitly test how these\nsimplified proxies match the behavior of the original model on various\nout-of-distribution test sets. We find that the simplified proxies are\ngenerally less faithful out of distribution. In cases where the original model\ngeneralizes to novel structures or deeper depths, the simplified versions may\nfail, or generalize better. This finding holds even if the simplified\nrepresentations do not directly depend on the training distribution. Next, we\nstudy a more naturalistic task: predicting the next character in a dataset of\ncomputer code. We find similar generalization gaps between the original model\nand simplified proxies, and conduct further analysis to investigate which\naspects of the code completion task are associated with the largest gaps.\nTogether, our results raise questions about the extent to which mechanistic\ninterpretations derived using tools like SVD can reliably predict what a model\nwill do in novel situations.","terms":["cs.LG","cs.CL"]},{"titles":"Seeing the random forest through the decision trees. Supporting learning health systems from histopathology with machine learning models: Challenges and opportunities","summaries":"This paper discusses some overlooked challenges faced when working with\nmachine learning models for histopathology and presents a novel opportunity to\nsupport \"Learning Health Systems\" with them. Initially, the authors elaborate\non these challenges after separating them according to their mitigation\nstrategies: those that need innovative approaches, time, or future\ntechnological capabilities and those that require a conceptual reappraisal from\na critical perspective. Then, a novel opportunity to support \"Learning Health\nSystems\" by integrating hidden information extracted by ML models from\ndigitalized histopathology slides with other healthcare big data is presented.","terms":["cs.CV"]},{"titles":"Source-Free Domain Adaptation for RGB-D Semantic Segmentation with Vision Transformers","summaries":"With the increasing availability of depth sensors, multimodal frameworks that\ncombine color information with depth data are gaining interest. However, ground\ntruth data for semantic segmentation is burdensome to provide, thus making\ndomain adaptation a significant research area. Yet most domain adaptation\nmethods are not able to effectively handle multimodal data. Specifically, we\naddress the challenging source-free domain adaptation setting where the\nadaptation is performed without reusing source data. We propose MISFIT:\nMultImodal Source-Free Information fusion Transformer, a depth-aware framework\nwhich injects depth data into a segmentation module based on vision\ntransformers at multiple stages, namely at the input, feature and output\nlevels. Color and depth style transfer helps early-stage domain alignment while\nre-wiring self-attention between modalities creates mixed features, allowing\nthe extraction of better semantic content. Furthermore, a depth-based entropy\nminimization strategy is also proposed to adaptively weight regions at\ndifferent distances. Our framework, which is also the first approach using\nRGB-D vision transformers for source-free semantic segmentation, shows\nnoticeable performance improvements with respect to standard strategies.","terms":["cs.CV","cs.MM"]},{"titles":"Diffusion Handles: Enabling 3D Edits for Diffusion Models by Lifting Activations to 3D","summaries":"Diffusion Handles is a novel approach to enabling 3D object edits on\ndiffusion images. We accomplish these edits using existing pre-trained\ndiffusion models, and 2D image depth estimation, without any fine-tuning or 3D\nobject retrieval. The edited results remain plausible, photo-real, and preserve\nobject identity. Diffusion Handles address a critically missing facet of\ngenerative image based creative design, and significantly advance the\nstate-of-the-art in generative image editing. Our key insight is to lift\ndiffusion activations for an object to 3D using a proxy depth, 3D-transform the\ndepth and associated activations, and project them back to image space. The\ndiffusion process applied to the manipulated activations with identity control,\nproduces plausible edited images showing complex 3D occlusion and lighting\neffects. We evaluate Diffusion Handles: quantitatively, on a large synthetic\ndata benchmark; and qualitatively by a user study, showing our output to be\nmore plausible, and better than prior art at both, 3D editing and identity\ncontrol. Project Webpage: https:\/\/diffusionhandles.github.io\/","terms":["cs.CV","cs.GR"]},{"titles":"Targeted Separation and Convergence with Kernel Discrepancies","summaries":"Maximum mean discrepancies (MMDs) like the kernel Stein discrepancy (KSD)\nhave grown central to a wide range of applications, including hypothesis\ntesting, sampler selection, distribution approximation, and variational\ninference. In each setting, these kernel-based discrepancy measures are\nrequired to (i) separate a target P from other probability measures or even\n(ii) control weak convergence to P. In this article we derive new sufficient\nand necessary conditions to ensure (i) and (ii). For MMDs on separable metric\nspaces, we characterize those kernels that separate Bochner embeddable measures\nand introduce simple conditions for separating all measures with unbounded\nkernels and for controlling convergence with bounded kernels. We use these\nresults on $\\mathbb{R}^d$ to substantially broaden the known conditions for KSD\nseparation and convergence control and to develop the first KSDs known to\nexactly metrize weak convergence to P. Along the way, we highlight the\nimplications of our results for hypothesis testing, measuring and improving\nsample quality, and sampling with Stein variational gradient descent.","terms":["stat.ML","cs.LG","math.ST","stat.TH"]},{"titles":"Incorporating Crowdsourced Annotator Distributions into Ensemble Modeling to Improve Classification Trustworthiness for Ancient Greek Papyri","summaries":"Performing classification on noisy, crowdsourced image datasets can prove\nchallenging even for the best neural networks. Two issues which complicate the\nproblem on such datasets are class imbalance and ground-truth uncertainty in\nlabeling. The AL-ALL and AL-PUB datasets - consisting of tightly cropped,\nindividual characters from images of ancient Greek papyri - are strongly\naffected by both issues. The application of ensemble modeling to such datasets\ncan help identify images where the ground-truth is questionable and quantify\nthe trustworthiness of those samples. As such, we apply stacked generalization\nconsisting of nearly identical ResNets with different loss functions: one\nutilizing sparse cross-entropy (CXE) and the other Kullback-Liebler Divergence\n(KLD). Both networks use labels drawn from a crowd-sourced consensus. This\nconsensus is derived from a Normalized Distribution of Annotations (NDA) based\non all annotations for a given character in the dataset. For the second\nnetwork, the KLD is calculated with respect to the NDA. For our ensemble model,\nwe apply a k-nearest neighbors model to the outputs of the CXE and KLD\nnetworks. Individually, the ResNet models have approximately 93% accuracy,\nwhile the ensemble model achieves an accuracy of > 95%, increasing the\nclassification trustworthiness. We also perform an analysis of the Shannon\nentropy of the various models' output distributions to measure classification\nuncertainty. Our results suggest that entropy is useful for predicting model\nmisclassifications.","terms":["cs.CV","cs.LG"]},{"titles":"MACCA: Offline Multi-agent Reinforcement Learning with Causal Credit Assignment","summaries":"Offline Multi-agent Reinforcement Learning (MARL) is valuable in scenarios\nwhere online interaction is impractical or risky. While independent learning in\nMARL offers flexibility and scalability, accurately assigning credit to\nindividual agents in offline settings poses challenges due to partial\nobservability and emergent behavior. Directly transferring the online credit\nassignment method to offline settings results in suboptimal outcomes due to the\nabsence of real-time feedback and intricate agent interactions. Our approach,\nMACCA, characterizing the generative process as a Dynamic Bayesian Network,\ncaptures relationships between environmental variables, states, actions, and\nrewards. Estimating this model on offline data, MACCA can learn each agent's\ncontribution by analyzing the causal relationship of their individual rewards,\nensuring accurate and interpretable credit assignment. Additionally, the\nmodularity of our approach allows it to seamlessly integrate with various\noffline MARL methods. Theoretically, we proved that under the setting of the\noffline dataset, the underlying causal structure and the function for\ngenerating the individual rewards of agents are identifiable, which laid the\nfoundation for the correctness of our modeling. Experimentally, we tested MACCA\nin two environments, including discrete and continuous action settings. The\nresults show that MACCA outperforms SOTA methods and improves performance upon\ntheir backbones.","terms":["cs.LG","cs.MA"]},{"titles":"Emergent Correspondence from Image Diffusion","summaries":"Finding correspondences between images is a fundamental problem in computer\nvision. In this paper, we show that correspondence emerges in image diffusion\nmodels without any explicit supervision. We propose a simple strategy to\nextract this implicit knowledge out of diffusion networks as image features,\nnamely DIffusion FeaTures (DIFT), and use them to establish correspondences\nbetween real images. Without any additional fine-tuning or supervision on the\ntask-specific data or annotations, DIFT is able to outperform both\nweakly-supervised methods and competitive off-the-shelf features in identifying\nsemantic, geometric, and temporal correspondences. Particularly for semantic\ncorrespondence, DIFT from Stable Diffusion is able to outperform DINO and\nOpenCLIP by 19 and 14 accuracy points respectively on the challenging SPair-71k\nbenchmark. It even outperforms the state-of-the-art supervised methods on 9 out\nof 18 categories while remaining on par for the overall performance. Project\npage: https:\/\/diffusionfeatures.github.io","terms":["cs.CV"]},{"titles":"Transformer-Powered Surrogates Close the ICF Simulation-Experiment Gap with Extremely Limited Data","summaries":"Recent advances in machine learning, specifically transformer architecture,\nhave led to significant advancements in commercial domains. These powerful\nmodels have demonstrated superior capability to learn complex relationships and\noften generalize better to new data and problems. This paper presents a novel\ntransformer-powered approach for enhancing prediction accuracy in multi-modal\noutput scenarios, where sparse experimental data is supplemented with\nsimulation data. The proposed approach integrates transformer-based\narchitecture with a novel graph-based hyper-parameter optimization technique.\nThe resulting system not only effectively reduces simulation bias, but also\nachieves superior prediction accuracy compared to the prior method. We\ndemonstrate the efficacy of our approach on inertial confinement fusion\nexperiments, where only 10 shots of real-world data are available, as well as\nsynthetic versions of these experiments.","terms":["cs.LG"]},{"titles":"Improved Convergence of Score-Based Diffusion Models via Prediction-Correction","summaries":"Score-based generative models (SGMs) are powerful tools to sample from\ncomplex data distributions. Their underlying idea is to (i) run a forward\nprocess for time $T_1$ by adding noise to the data, (ii) estimate its score\nfunction, and (iii) use such estimate to run a reverse process. As the reverse\nprocess is initialized with the stationary distribution of the forward one, the\nexisting analysis paradigm requires $T_1\\to\\infty$. This is however\nproblematic: from a theoretical viewpoint, for a given precision of the score\napproximation, the convergence guarantee fails as $T_1$ diverges; from a\npractical viewpoint, a large $T_1$ increases computational costs and leads to\nerror propagation. This paper addresses the issue by considering a version of\nthe popular predictor-corrector scheme: after running the forward process, we\nfirst estimate the final distribution via an inexact Langevin dynamics and then\nrevert the process. Our key technical contribution is to provide convergence\nguarantees which require to run the forward process only for a fixed finite\ntime $T_1$. Our bounds exhibit a mild logarithmic dependence on the input\ndimension and the subgaussian norm of the target distribution, have minimal\nassumptions on the data, and require only to control the $L^2$ loss on the\nscore approximation, which is the quantity minimized in practice.","terms":["cs.LG","math.ST","stat.ML","stat.TH"]},{"titles":"MotionCtrl: A Unified and Flexible Motion Controller for Video Generation","summaries":"Motions in a video primarily consist of camera motion, induced by camera\nmovement, and object motion, resulting from object movement. Accurate control\nof both camera and object motion is essential for video generation. However,\nexisting works either mainly focus on one type of motion or do not clearly\ndistinguish between the two, limiting their control capabilities and diversity.\nTherefore, this paper presents MotionCtrl, a unified and flexible motion\ncontroller for video generation designed to effectively and independently\ncontrol camera and object motion. The architecture and training strategy of\nMotionCtrl are carefully devised, taking into account the inherent properties\nof camera motion, object motion, and imperfect training data. Compared to\nprevious methods, MotionCtrl offers three main advantages: 1) It effectively\nand independently controls camera motion and object motion, enabling more\nfine-grained motion control and facilitating flexible and diverse combinations\nof both types of motion. 2) Its motion conditions are determined by camera\nposes and trajectories, which are appearance-free and minimally impact the\nappearance or shape of objects in generated videos. 3) It is a relatively\ngeneralizable model that can adapt to a wide array of camera poses and\ntrajectories once trained. Extensive qualitative and quantitative experiments\nhave been conducted to demonstrate the superiority of MotionCtrl over existing\nmethods.","terms":["cs.CV","cs.AI","cs.LG","cs.MM"]},{"titles":"Internal Representations of Vision Models Through the Lens of Frames on Data Manifolds","summaries":"While the last five years have seen considerable progress in understanding\nthe internal representations of deep learning models, many questions remain.\nThis is especially true when trying to understand the impact of model design\nchoices, such as model architecture or training algorithm, on hidden\nrepresentation geometry and dynamics. In this work we present a new approach to\nstudying such representations inspired by the idea of a frame on the tangent\nbundle of a manifold. Our construction, which we call a neural frame, is formed\nby assembling a set of vectors representing specific types of perturbations of\na data point, for example infinitesimal augmentations, noise perturbations, or\nperturbations produced by a generative model, and studying how these change as\nthey pass through a network. Using neural frames, we make observations about\nthe way that models process, layer-by-layer, specific modes of variation within\na small neighborhood of a datapoint. Our results provide new perspectives on a\nnumber of phenomena, such as the manner in which training with augmentation\nproduces model invariance or the proposed trade-off between adversarial\ntraining and model generalization.","terms":["cs.LG","cs.CV"]},{"titles":"MOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations","summaries":"While recent years have seen rapid progress in image-conditioned text\ngeneration, image captioning still suffers from the fundamental issue of\nhallucinations, the generation of spurious details that cannot be inferred from\nthe given image. Dedicated methods for reducing hallucinations in image\ncaptioning largely focus on closed-vocabulary object tokens, ignoring most\ntypes of hallucinations that occur in practice. In this work, we propose MOCHa,\nan approach that harnesses advancements in reinforcement learning (RL) to\naddress the sequence-level nature of hallucinations in an open-world setup. To\noptimize for caption fidelity to the input image, we leverage ground-truth\nreference captions as proxies to measure the logical consistency of generated\ncaptions. However, optimizing for caption fidelity alone fails to preserve the\nsemantic adequacy of generations; therefore, we propose a multi-objective\nreward function that jointly targets these qualities, without requiring any\nstrong supervision. We demonstrate that these goals can be simultaneously\noptimized with our framework, enhancing performance for various captioning\nmodels of different scales. Our qualitative and quantitative results\ndemonstrate MOCHa's superior performance across various established metrics. We\nalso demonstrate the benefit of our method in the open-vocabulary setting. To\nthis end, we contribute OpenCHAIR, a new benchmark for quantifying\nopen-vocabulary hallucinations in image captioning models, constructed using\ngenerative foundation models. We will release our code, benchmark, and trained\nmodels.","terms":["cs.CV","cs.AI"]},{"titles":"Boosting Segment Anything Model Towards Open-Vocabulary Learning","summaries":"The recent Segment Anything Model (SAM) has emerged as a new paradigmatic\nvision foundation model, showcasing potent zero-shot generalization and\nflexible prompting. Despite SAM finding applications and adaptations in various\ndomains, its primary limitation lies in the inability to grasp object\nsemantics. In this paper, we present Sambor to seamlessly integrate SAM with\nthe open-vocabulary object detector in an end-to-end framework. While retaining\nall the remarkable capabilities inherent to SAM, we enhance it with the\ncapacity to detect arbitrary objects based on human inputs like category names\nor reference expressions. To accomplish this, we introduce a novel SideFormer\nmodule that extracts SAM features to facilitate zero-shot object localization\nand inject comprehensive semantic information for open-vocabulary recognition.\nIn addition, we devise an open-set region proposal network (Open-set RPN),\nenabling the detector to acquire the open-set proposals generated by SAM.\nSambor demonstrates superior zero-shot performance across benchmarks, including\nCOCO and LVIS, proving highly competitive against previous SoTA methods. We\naspire for this work to serve as a meaningful endeavor in endowing SAM to\nrecognize diverse object categories and advancing open-vocabulary learning with\nthe support of vision foundation models.","terms":["cs.CV"]},{"titles":"TokenCompose: Grounding Diffusion with Token-level Supervision","summaries":"We present TokenCompose, a Latent Diffusion Model for text-to-image\ngeneration that achieves enhanced consistency between user-specified text\nprompts and model-generated images. Despite its tremendous success, the\nstandard denoising process in the Latent Diffusion Model takes text prompts as\nconditions only, absent explicit constraint for the consistency between the\ntext prompts and the image contents, leading to unsatisfactory results for\ncomposing multiple object categories. TokenCompose aims to improve\nmulti-category instance composition by introducing the token-wise consistency\nterms between the image content and object segmentation maps in the finetuning\nstage. TokenCompose can be applied directly to the existing training pipeline\nof text-conditioned diffusion models without extra human labeling information.\nBy finetuning Stable Diffusion, the model exhibits significant improvements in\nmulti-category instance composition and enhanced photorealism for its generated\nimages.","terms":["cs.CV"]},{"titles":"Physical Symbolic Optimization","summaries":"We present a framework for constraining the automatic sequential generation\nof equations to obey the rules of dimensional analysis by construction.\nCombining this approach with reinforcement learning, we built $\\Phi$-SO, a\nPhysical Symbolic Optimization method for recovering analytical functions from\nphysical data leveraging units constraints. Our symbolic regression algorithm\nachieves state-of-the-art results in contexts in which variables and constants\nhave known physical units, outperforming all other methods on SRBench's Feynman\nbenchmark in the presence of noise (exceeding 0.1%) and showing resilience even\nin the presence of significant (10%) levels of noise.","terms":["cs.LG","astro-ph.IM","cs.SC","physics.comp-ph","physics.data-an"]},{"titles":"DreamComposer: Controllable 3D Object Generation via Multi-View Conditions","summaries":"Utilizing pre-trained 2D large-scale generative models, recent works are\ncapable of generating high-quality novel views from a single in-the-wild image.\nHowever, due to the lack of information from multiple views, these works\nencounter difficulties in generating controllable novel views. In this paper,\nwe present DreamComposer, a flexible and scalable framework that can enhance\nexisting view-aware diffusion models by injecting multi-view conditions.\nSpecifically, DreamComposer first uses a view-aware 3D lifting module to obtain\n3D representations of an object from multiple views. Then, it renders the\nlatent features of the target view from 3D representations with the multi-view\nfeature fusion module. Finally the target view features extracted from\nmulti-view inputs are injected into a pre-trained diffusion model. Experiments\nshow that DreamComposer is compatible with state-of-the-art diffusion models\nfor zero-shot novel view synthesis, further enhancing them to generate\nhigh-fidelity novel view images with multi-view conditions, ready for\ncontrollable 3D object reconstruction and various other applications.","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"Automated Multimodal Data Annotation via Calibration With Indoor Positioning System","summaries":"Learned object detection methods based on fusion of LiDAR and camera data\nrequire labeled training samples, but niche applications, such as warehouse\nrobotics or automated infrastructure, require semantic classes not available in\nlarge existing datasets. Therefore, to facilitate the rapid creation of\nmultimodal object detection datasets and alleviate the burden of human\nlabeling, we propose a novel automated annotation pipeline. Our method uses an\nindoor positioning system (IPS) to produce accurate detection labels for both\npoint clouds and images and eliminates manual annotation entirely. In an\nexperiment, the system annotates objects of interest 261.8 times faster than a\nhuman baseline and speeds up end-to-end dataset creation by 61.5%.","terms":["cs.CV","I.4.m"]},{"titles":"DiffusionSat: A Generative Foundation Model for Satellite Imagery","summaries":"Diffusion models have achieved state-of-the-art results on many modalities\nincluding images, speech, and video. However, existing models are not tailored\nto support remote sensing data, which is widely used in important applications\nincluding environmental monitoring and crop-yield prediction. Satellite images\nare significantly different from natural images -- they can be multi-spectral,\nirregularly sampled across time -- and existing diffusion models trained on\nimages from the Web do not support them. Furthermore, remote sensing data is\ninherently spatio-temporal, requiring conditional generation tasks not\nsupported by traditional methods based on captions or images. In this paper, we\npresent DiffusionSat, to date the largest generative foundation model trained\non a collection of publicly available large, high-resolution remote sensing\ndatasets. As text-based captions are sparsely available for satellite images,\nwe incorporate the associated metadata such as geolocation as conditioning\ninformation. Our method produces realistic samples and can be used to solve\nmultiple generative tasks including temporal generation, superresolution given\nmulti-spectral inputs and in-painting. Our method outperforms previous\nstate-of-the-art methods for satellite image generation and is the first\nlarge-scale $\\textit{generative}$ foundation model for satellite imagery.","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"SurfaceAug: Closing the Gap in Multimodal Ground Truth Sampling","summaries":"Despite recent advances in both model architectures and data augmentation,\nmultimodal object detectors still barely outperform their LiDAR-only\ncounterparts. This shortcoming has been attributed to a lack of sufficiently\npowerful multimodal data augmentation. To address this, we present SurfaceAug,\na novel ground truth sampling algorithm. SurfaceAug pastes objects by\nresampling both images and point clouds, enabling object-level transformations\nin both modalities. We evaluate our algorithm by training a multimodal detector\non KITTI and compare its performance to previous works. We show experimentally\nthat SurfaceAug outperforms existing methods on car detection tasks and\nestablishes a new state of the art for multimodal ground truth sampling.","terms":["cs.CV"]},{"titles":"Dimensionless Anomaly Detection on Multivariate Streams with Variance Norm and Path Signature","summaries":"In this paper, we propose a dimensionless anomaly detection method for\nmultivariate streams. Our method is independent of the unit of measurement for\nthe different stream channels, therefore dimensionless. We first propose the\nvariance norm, a generalisation of Mahalanobis distance to handle\ninfinite-dimensional feature space and singular empirical covariance matrix\nrigorously. We then combine the variance norm with the path signature, an\ninfinite collection of iterated integrals that provide global features of\nstreams, to propose SigMahaKNN, a method for anomaly detection on\n(multivariate) streams. We show that SigMahaKNN is invariant to stream\nreparametrisation, stream concatenation and has a graded discrimination power\ndepending on the truncation level of the path signature. We implement\nSigMahaKNN as an open-source software, and perform extensive numerical\nexperiments, showing significantly improved anomaly detection on streams\ncompared to isolation forest and local outlier factors in applications ranging\nfrom language analysis, hand-writing analysis, ship movement paths analysis and\nunivariate time-series analysis.","terms":["cs.LG","stat.ML"]},{"titles":"MMM: Generative Masked Motion Model","summaries":"Recent advances in text-to-motion generation using diffusion and\nautoregressive models have shown promising results. However, these models often\nsuffer from a trade-off between real-time performance, high fidelity, and\nmotion editability. To address this gap, we introduce MMM, a novel yet simple\nmotion generation paradigm based on Masked Motion Model. MMM consists of two\nkey components: (1) a motion tokenizer that transforms 3D human motion into a\nsequence of discrete tokens in latent space, and (2) a conditional masked\nmotion transformer that learns to predict randomly masked motion tokens,\nconditioned on the pre-computed text tokens. By attending to motion and text\ntokens in all directions, MMM explicitly captures inherent dependency among\nmotion tokens and semantic mapping between motion and text tokens. During\ninference, this allows parallel and iterative decoding of multiple motion\ntokens that are highly consistent with fine-grained text descriptions,\ntherefore simultaneously achieving high-fidelity and high-speed motion\ngeneration. In addition, MMM has innate motion editability. By simply placing\nmask tokens in the place that needs editing, MMM automatically fills the gaps\nwhile guaranteeing smooth transitions between editing and non-editing parts.\nExtensive experiments on the HumanML3D and KIT-ML datasets demonstrate that MMM\nsurpasses current leading methods in generating high-quality motion (evidenced\nby superior FID scores of 0.08 and 0.429), while offering advanced editing\nfeatures such as body-part modification, motion in-betweening, and the\nsynthesis of long motion sequences. In addition, MMM is two orders of magnitude\nfaster on a single mid-range GPU than editable motion diffusion models. Our\nproject page is available at \\url{https:\/\/exitudio.github.io\/MMM-page}.","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"Language-Informed Visual Concept Learning","summaries":"Our understanding of the visual world is centered around various concept\naxes, characterizing different aspects of visual entities. While different\nconcept axes can be easily specified by language, e.g. color, the exact visual\nnuances along each axis often exceed the limitations of linguistic\narticulations, e.g. a particular style of painting. In this work, our goal is\nto learn a language-informed visual concept representation, by simply\ndistilling large pre-trained vision-language models. Specifically, we train a\nset of concept encoders to encode the information pertinent to a set of\nlanguage-informed concept axes, with an objective of reproducing the input\nimage through a pre-trained Text-to-Image (T2I) model. To encourage better\ndisentanglement of different concept encoders, we anchor the concept embeddings\nto a set of text embeddings obtained from a pre-trained Visual Question\nAnswering (VQA) model. At inference time, the model extracts concept embeddings\nalong various axes from new test images, which can be remixed to generate\nimages with novel compositions of visual concepts. With a lightweight test-time\nfinetuning procedure, it can also generalize to novel concepts unseen at\ntraining.","terms":["cs.CV"]},{"titles":"Autoencoders for discovering manifold dimension and coordinates in data from complex dynamical systems","summaries":"While many phenomena in physics and engineering are formally\nhigh-dimensional, their long-time dynamics often live on a lower-dimensional\nmanifold. The present work introduces an autoencoder framework that combines\nimplicit regularization with internal linear layers and $L_2$ regularization\n(weight decay) to automatically estimate the underlying dimensionality of a\ndata set, produce an orthogonal manifold coordinate system, and provide the\nmapping functions between the ambient space and manifold space, allowing for\nout-of-sample projections. We validate our framework's ability to estimate the\nmanifold dimension for a series of datasets from dynamical systems of varying\ncomplexities and compare to other state-of-the-art estimators. We analyze the\ntraining dynamics of the network to glean insight into the mechanism of\nlow-rank learning and find that collectively each of the implicit regularizing\nlayers compound the low-rank representation and even self-correct during\ntraining. Analysis of gradient descent dynamics for this architecture in the\nlinear case reveals the role of the internal linear layers in leading to faster\ndecay of a \"collective weight variable\" incorporating all layers, and the role\nof weight decay in breaking degeneracies and thus driving convergence along\ndirections in which no decay would occur in its absence. We show that this\nframework can be naturally extended for applications of state-space modeling\nand forecasting by generating a data-driven dynamic model of a spatiotemporally\nchaotic partial differential equation using only the manifold coordinates.\nFinally, we demonstrate that our framework is robust to hyperparameter choices.","terms":["cs.LG","nlin.CD"]},{"titles":"XCube ($\\mathcal{X}^3$): Large-Scale 3D Generative Modeling using Sparse Voxel Hierarchies","summaries":"We present $\\mathcal{X}^3$ (pronounced XCube), a novel generative model for\nhigh-resolution sparse 3D voxel grids with arbitrary attributes. Our model can\ngenerate millions of voxels with a finest effective resolution of up to\n$1024^3$ in a feed-forward fashion without time-consuming test-time\noptimization. To achieve this, we employ a hierarchical voxel latent diffusion\nmodel which generates progressively higher resolution grids in a coarse-to-fine\nmanner using a custom framework built on the highly efficient VDB data\nstructure. Apart from generating high-resolution objects, we demonstrate the\neffectiveness of XCube on large outdoor scenes at scales of 100m$\\times$100m\nwith a voxel size as small as 10cm. We observe clear qualitative and\nquantitative improvements over past approaches. In addition to unconditional\ngeneration, we show that our model can be used to solve a variety of tasks such\nas user-guided editing, scene completion from a single scan, and text-to-3D.\nMore results and details can be found at\nhttps:\/\/research.nvidia.com\/labs\/toronto-ai\/xcube\/.","terms":["cs.CV","cs.GR","cs.LG"]},{"titles":"Foundation Model Assisted Weakly Supervised Semantic Segmentation","summaries":"This work aims to leverage pre-trained foundation models, such as contrastive\nlanguage-image pre-training (CLIP) and segment anything model (SAM), to address\nweakly supervised semantic segmentation (WSSS) using image-level labels. To\nthis end, we propose a coarse-to-fine framework based on CLIP and SAM for\ngenerating high-quality segmentation seeds. Specifically, we construct an image\nclassification task and a seed segmentation task, which are jointly performed\nby CLIP with frozen weights and two sets of learnable task-specific prompts. A\nSAM-based seeding (SAMS) module is designed and applied to each task to produce\neither coarse or fine seed maps. Moreover, we design a multi-label contrastive\nloss supervised by image-level labels and a CAM activation loss supervised by\nthe generated coarse seed map. These losses are used to learn the prompts,\nwhich are the only parts need to be learned in our framework. Once the prompts\nare learned, we input each image along with the learned segmentation-specific\nprompts into CLIP and the SAMS module to produce high-quality segmentation\nseeds. These seeds serve as pseudo labels to train an off-the-shelf\nsegmentation network like other two-stage WSSS methods. Experiments show that\nour method achieves the state-of-the-art performance on PASCAL VOC 2012 and\ncompetitive results on MS COCO 2014.","terms":["cs.CV","cs.AI"]},{"titles":"Context Diffusion: In-Context Aware Image Generation","summaries":"We propose Context Diffusion, a diffusion-based framework that enables image\ngeneration models to learn from visual examples presented in context. Recent\nwork tackles such in-context learning for image generation, where a query image\nis provided alongside context examples and text prompts. However, the quality\nand fidelity of the generated images deteriorate when the prompt is not\npresent, demonstrating that these models are unable to truly learn from the\nvisual context. To address this, we propose a novel framework that separates\nthe encoding of the visual context and preserving the structure of the query\nimages. This results in the ability to learn from the visual context and text\nprompts, but also from either one of them. Furthermore, we enable our model to\nhandle few-shot settings, to effectively address diverse in-context learning\nscenarios. Our experiments and user study demonstrate that Context Diffusion\nexcels in both in-domain and out-of-domain tasks, resulting in an overall\nenhancement in image quality and fidelity compared to counterpart models.","terms":["cs.CV"]},{"titles":"Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models","summaries":"The popularity of pre-trained large models has revolutionized downstream\ntasks across diverse fields, such as language, vision, and multi-modality. To\nminimize the adaption cost for downstream tasks, many Parameter-Efficient\nFine-Tuning (PEFT) techniques are proposed for language and 2D image\npre-trained models. However, the specialized PEFT method for 3D pre-trained\nmodels is still under-explored. To this end, we introduce Point-PEFT, a novel\nframework for adapting point cloud pre-trained models with minimal learnable\nparameters. Specifically, for a pre-trained 3D model, we freeze most of its\nparameters, and only tune the newly added PEFT modules on downstream tasks,\nwhich consist of a Point-prior Prompt and a Geometry-aware Adapter. The\nPoint-prior Prompt adopts a set of learnable prompt tokens, for which we\npropose to construct a memory bank with domain-specific knowledge, and utilize\na parameter-free attention to enhance the prompt tokens. The Geometry-aware\nAdapter aims to aggregate point cloud features within spatial neighborhoods to\ncapture fine-grained geometric information through local interactions.\nExtensive experiments indicate that our Point-PEFT can achieve better\nperformance than the full fine-tuning on various downstream tasks, while using\nonly 5% of the trainable parameters, demonstrating the efficiency and\neffectiveness of our approach. Code will be released at\nhttps:\/\/github.com\/Even-JK\/PEFT-3D.","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"Invariance & Causal Representation Learning: Prospects and Limitations","summaries":"In causal models, a given mechanism is assumed to be invariant to changes of\nother mechanisms. While this principle has been utilized for inference in\nsettings where the causal variables are observed, theoretical insights when the\nvariables of interest are latent are largely missing. We assay the connection\nbetween invariance and causal representation learning by establishing\nimpossibility results which show that invariance alone is insufficient to\nidentify latent causal variables. Together with practical considerations, we\nuse these theoretical findings to highlight the need for additional constraints\nin order to identify representations by exploiting invariance.","terms":["stat.ML","cs.AI","cs.LG"]},{"titles":"DocBinFormer: A Two-Level Transformer Network for Effective Document Image Binarization","summaries":"In real life, various degradation scenarios exist that might damage document\nimages, making it harder to recognize and analyze them, thus binarization is a\nfundamental and crucial step for achieving the most optimal performance in any\ndocument analysis task. We propose DocBinFormer (Document Binarization\nTransformer), a novel two-level vision transformer (TL-ViT) architecture based\non vision transformers for effective document image binarization. The presented\narchitecture employs a two-level transformer encoder to effectively capture\nboth global and local feature representation from the input images. These\ncomplimentary bi-level features are exploited for efficient document image\nbinarization, resulting in improved results for system-generated as well as\nhandwritten document images in a comprehensive approach. With the absence of\nconvolutional layers, the transformer encoder uses the pixel patches and\nsub-patches along with their positional information to operate directly on\nthem, while the decoder generates a clean (binarized) output image from the\nlatent representation of the patches. Instead of using a simple vision\ntransformer block to extract information from the image patches, the proposed\narchitecture uses two transformer blocks for greater coverage of the extracted\nfeature space on a global and local scale. The encoded feature representation\nis used by the decoder block to generate the corresponding binarized output.\nExtensive experiments on a variety of DIBCO and H-DIBCO benchmarks show that\nthe proposed model outperforms state-of-the-art techniques on four metrics. The\nsource code will be made available at\nhttps:\/\/github.com\/RisabBiswas\/DocBinFormer.","terms":["cs.CV"]},{"titles":"SYNC-CLIP: Synthetic Data Make CLIP Generalize Better in Data-Limited Scenarios","summaries":"Prompt learning is a powerful technique for transferring Vision-Language\nModels (VLMs) such as CLIP to downstream tasks. However, the prompt-based\nmethods that are fine-tuned solely with base classes may struggle to generalize\nto novel classes in open-vocabulary scenarios, especially when data are\nlimited. To address this issue, we propose an innovative approach called\nSYNC-CLIP that leverages SYNthetiC data for enhancing the generalization\ncapability of CLIP. Based on the observation of the distribution shift between\nthe real and synthetic samples, we treat real and synthetic samples as distinct\ndomains and propose to optimize separate domain prompts to capture\ndomain-specific information, along with the shared visual prompts to preserve\nthe semantic consistency between two domains. By aligning the cross-domain\nfeatures, the synthetic data from novel classes can provide implicit guidance\nto rebalance the decision boundaries. Experimental results on three model\ngeneralization tasks demonstrate that our method performs very competitively\nacross various benchmarks. Notably, SYNC-CLIP outperforms the state-of-the-art\ncompetitor PromptSRC by an average improvement of 3.0% on novel classes across\n11 datasets in open-vocabulary scenarios.","terms":["cs.CV"]},{"titles":"Enhancing Kinship Verification through Multiscale Retinex and Combined Deep-Shallow features","summaries":"The challenge of kinship verification from facial images represents a\ncutting-edge and formidable frontier in the realms of pattern recognition and\ncomputer vision. This area of study holds a myriad of potential applications,\nspanning from image annotation and forensic analysis to social media research.\nOur research stands out by integrating a preprocessing method named Multiscale\nRetinex (MSR), which elevates image quality and amplifies contrast, ultimately\nbolstering the end results. Strategically, our methodology capitalizes on the\nharmonious blend of deep and shallow texture descriptors, merging them\nproficiently at the score level through the Logistic Regression (LR) method. To\nelucidate, we employ the Local Phase Quantization (LPQ) descriptor to extract\nshallow texture characteristics. For deep feature extraction, we turn to the\nprowess of the VGG16 model, which is pre-trained on a convolutional neural\nnetwork (CNN). The robustness and efficacy of our method have been put to the\ntest through meticulous experiments on three rigorous kinship datasets, namely:\nCornell Kin Face, UB Kin Face, and TS Kin Face.","terms":["cs.CV"]},{"titles":"Towards Causal Representations of Climate Model Data","summaries":"Climate models, such as Earth system models (ESMs), are crucial for\nsimulating future climate change based on projected Shared Socioeconomic\nPathways (SSP) greenhouse gas emissions scenarios. While ESMs are sophisticated\nand invaluable, machine learning-based emulators trained on existing simulation\ndata can project additional climate scenarios much faster and are\ncomputationally efficient. However, they often lack generalizability and\ninterpretability. This work delves into the potential of causal representation\nlearning, specifically the \\emph{Causal Discovery with Single-parent Decoding}\n(CDSD) method, which could render climate model emulation efficient\n\\textit{and} interpretable. We evaluate CDSD on multiple climate datasets,\nfocusing on emissions, temperature, and precipitation. Our findings shed light\non the challenges, limitations, and promise of using CDSD as a stepping stone\ntowards more interpretable and robust climate model emulation.","terms":["cs.LG","cs.AI","physics.ao-ph","stat.ME"]},{"titles":"Deeply Coupled Cross-Modal Prompt Learning","summaries":"Recent advancements in multimodal foundation models (e.g., CLIP) have\nexcelled in zero-shot generalization. Prompt tuning involved in the knowledge\ntransfer from foundation models to downstream tasks has gained significant\nattention recently. Existing prompt-tuning methods in cross-modal learning,\nhowever, either solely focus on language branch, or learn vision-language\ninteraction in a shallow mechanism. In this context, we propose a Deeply\ncoupled Cross-modal Prompt learning (DCP) method based on CLIP. DCP flexibly\naccommodates the interplay between vision and language with a Cross-Modal\nPrompt Attention (CMPA) mechanism, which enables the mutual exchange of\nrespective representation through a well-connected multi-head attention module\nprogressively and strongly. We then conduct comprehensive few-shot learning\nexperiments on 11 image classification datasets and analyze the robustness to\ndomain shift as well. Thorough experimental analysis evidently demonstrates the\nsuperb few-shot generalization and compelling domain adaption capacity of a\nwell-executed DCP. The code can be found at https:\/\/github.com\/GingL\/CMPA.","terms":["cs.CV"]},{"titles":"When an Image is Worth 1,024 x 1,024 Words: A Case Study in Computational Pathology","summaries":"This technical report presents LongViT, a vision Transformer that can process\ngigapixel images in an end-to-end manner. Specifically, we split the gigapixel\nimage into a sequence of millions of patches and project them linearly into\nembeddings. LongNet is then employed to model the extremely long sequence,\ngenerating representations that capture both short-range and long-range\ndependencies. The linear computation complexity of LongNet, along with its\ndistributed algorithm, enables us to overcome the constraints of both\ncomputation and memory. We apply LongViT in the field of computational\npathology, aiming for cancer diagnosis and prognosis within gigapixel\nwhole-slide images. Experimental results demonstrate that LongViT effectively\nencodes gigapixel images and outperforms previous state-of-the-art methods on\ncancer subtyping and survival prediction. Code and models will be available at\nhttps:\/\/aka.ms\/LongViT.","terms":["cs.CV"]},{"titles":"Personalized Face Inpainting with Diffusion Models by Parallel Visual Attention","summaries":"Face inpainting is important in various applications, such as photo\nrestoration, image editing, and virtual reality. Despite the significant\nadvances in face generative models, ensuring that a person's unique facial\nidentity is maintained during the inpainting process is still an elusive goal.\nCurrent state-of-the-art techniques, exemplified by MyStyle, necessitate\nresource-intensive fine-tuning and a substantial number of images for each new\nidentity. Furthermore, existing methods often fall short in accommodating\nuser-specified semantic attributes, such as beard or expression. To improve\ninpainting results, and reduce the computational complexity during inference,\nthis paper proposes the use of Parallel Visual Attention (PVA) in conjunction\nwith diffusion models. Specifically, we insert parallel attention matrices to\neach cross-attention module in the denoising network, which attends to features\nextracted from reference images by an identity encoder. We train the added\nattention modules and identity encoder on CelebAHQ-IDI, a dataset proposed for\nidentity-preserving face inpainting. Experiments demonstrate that PVA attains\nunparalleled identity resemblance in both face inpainting and face inpainting\nwith language guidance tasks, in comparison to various benchmarks, including\nMyStyle, Paint by Example, and Custom Diffusion. Our findings reveal that PVA\nensures good identity preservation while offering effective\nlanguage-controllability. Additionally, in contrast to Custom Diffusion, PVA\nrequires just 40 fine-tuning steps for each new identity, which translates to a\nsignificant speed increase of over 20 times.","terms":["cs.CV","cs.LG"]},{"titles":"How Low Can You Go? Surfacing Prototypical In-Distribution Samples for Unsupervised Anomaly Detection","summaries":"Unsupervised anomaly detection (UAD) alleviates large labeling efforts by\ntraining exclusively on unlabeled in-distribution data and detecting outliers\nas anomalies. Generally, the assumption prevails that large training datasets\nallow the training of higher-performing UAD models. However, in this work, we\nshow that using only very few training samples can already match - and in some\ncases even improve - anomaly detection compared to training with the whole\ntraining dataset. We propose three methods to identify prototypical samples\nfrom a large dataset of in-distribution samples. We demonstrate that by\ntraining with a subset of just ten such samples, we achieve an area under the\nreceiver operating characteristics curve (AUROC) of $96.37 \\%$ on CIFAR10,\n$92.59 \\%$ on CIFAR100, $95.37 \\%$ on MNIST, $95.38 \\%$ on Fashion-MNIST,\n$96.37 \\%$ on MVTec-AD, $98.81 \\%$ on BraTS, and $81.95 \\%$ on RSNA pneumonia\ndetection, even exceeding the performance of full training in $25\/67$ classes\nwe tested. Additionally, we show that the prototypical in-distribution samples\nidentified by our proposed methods translate well to different models and other\ndatasets and that using their characteristics as guidance allows for successful\nmanual selection of small subsets of high-performing samples. Our code is\navailable at https:\/\/anonymous.4open.science\/r\/uad_prototypical_samples\/","terms":["cs.CV"]},{"titles":"Texture-Semantic Collaboration Network for ORSI Salient Object Detection","summaries":"Salient object detection (SOD) in optical remote sensing images (ORSIs) has\nbecome increasingly popular recently. Due to the characteristics of ORSIs,\nORSI-SOD is full of challenges, such as multiple objects, small objects, low\nilluminations, and irregular shapes. To address these challenges, we propose a\nconcise yet effective Texture-Semantic Collaboration Network (TSCNet) to\nexplore the collaboration of texture cues and semantic cues for ORSI-SOD.\nSpecifically, TSCNet is based on the generic encoder-decoder structure. In\naddition to the encoder and decoder, TSCNet includes a vital Texture-Semantic\nCollaboration Module (TSCM), which performs valuable feature modulation and\ninteraction on basic features extracted from the encoder. The main idea of our\nTSCM is to make full use of the texture features at the lowest level and the\nsemantic features at the highest level to achieve the expression enhancement of\nsalient regions on features. In the TSCM, we first enhance the position of\npotential salient regions using semantic features. Then, we render and restore\nthe object details using the texture features. Meanwhile, we also perceive\nregions of various scales, and construct interactions between different\nregions. Thanks to the perfect combination of TSCM and generic structure, our\nTSCNet can take care of both the position and details of salient objects,\neffectively handling various scenes. Extensive experiments on three datasets\ndemonstrate that our TSCNet achieves competitive performance compared to 14\nstate-of-the-art methods. The code and results of our method are available at\nhttps:\/\/github.com\/MathLee\/TSCNet.","terms":["cs.CV"]},{"titles":"Multiple Instance Learning for Digital Pathology: A Review on the State-of-the-Art, Limitations & Future Potential","summaries":"Digital whole slides images contain an enormous amount of information\nproviding a strong motivation for the development of automated image analysis\ntools. Particularly deep neural networks show high potential with respect to\nvarious tasks in the field of digital pathology. However, a limitation is given\nby the fact that typical deep learning algorithms require (manual) annotations\nin addition to the large amounts of image data, to enable effective training.\nMultiple instance learning exhibits a powerful tool for learning deep neural\nnetworks in a scenario without fully annotated data. These methods are\nparticularly effective in this domain, due to the fact that labels for a\ncomplete whole slide image are often captured routinely, whereas labels for\npatches, regions or pixels are not. This potential already resulted in a\nconsiderable number of publications, with the majority published in the last\nthree years. Besides the availability of data and a high motivation from the\nmedical perspective, the availability of powerful graphics processing units\nexhibits an accelerator in this field. In this paper, we provide an overview of\nwidely and effectively used concepts of used deep multiple instance learning\napproaches, recent advances and also critically discuss remaining challenges\nand future potential.","terms":["cs.CV"]},{"titles":"Generalization to New Sequential Decision Making Tasks with In-Context Learning","summaries":"Training autonomous agents that can learn new tasks from only a handful of\ndemonstrations is a long-standing problem in machine learning. Recently,\ntransformers have been shown to learn new language or vision tasks without any\nweight updates from only a few examples, also referred to as in-context\nlearning. However, the sequential decision making setting poses additional\nchallenges having a lower tolerance for errors since the environment's\nstochasticity or the agent's actions can lead to unseen, and sometimes\nunrecoverable, states. In this paper, we use an illustrative example to show\nthat naively applying transformers to sequential decision making problems does\nnot enable in-context learning of new tasks. We then demonstrate how training\non sequences of trajectories with certain distributional properties leads to\nin-context learning of new sequential decision making tasks. We investigate\ndifferent design choices and find that larger model and dataset sizes, as well\nas more task diversity, environment stochasticity, and trajectory burstiness,\nall result in better in-context learning of new out-of-distribution tasks. By\ntraining on large diverse offline datasets, our model is able to learn new\nMiniHack and Procgen tasks without any weight updates from just a handful of\ndemonstrations.","terms":["cs.LG","cs.AI"]},{"titles":"GPT-4 Enhanced Multimodal Grounding for Autonomous Driving: Leveraging Cross-Modal Attention with Large Language Models","summaries":"In the field of autonomous vehicles (AVs), accurately discerning commander\nintent and executing linguistic commands within a visual context presents a\nsignificant challenge. This paper introduces a sophisticated encoder-decoder\nframework, developed to address visual grounding in AVs.Our Context-Aware\nVisual Grounding (CAVG) model is an advanced system that integrates five core\nencoders-Text, Image, Context, and Cross-Modal-with a Multimodal decoder. This\nintegration enables the CAVG model to adeptly capture contextual semantics and\nto learn human emotional features, augmented by state-of-the-art Large Language\nModels (LLMs) including GPT-4. The architecture of CAVG is reinforced by the\nimplementation of multi-head cross-modal attention mechanisms and a\nRegion-Specific Dynamic (RSD) layer for attention modulation. This\narchitectural design enables the model to efficiently process and interpret a\nrange of cross-modal inputs, yielding a comprehensive understanding of the\ncorrelation between verbal commands and corresponding visual scenes. Empirical\nevaluations on the Talk2Car dataset, a real-world benchmark, demonstrate that\nCAVG establishes new standards in prediction accuracy and operational\nefficiency. Notably, the model exhibits exceptional performance even with\nlimited training data, ranging from 50% to 75% of the full dataset. This\nfeature highlights its effectiveness and potential for deployment in practical\nAV applications. Moreover, CAVG has shown remarkable robustness and\nadaptability in challenging scenarios, including long-text command\ninterpretation, low-light conditions, ambiguous command contexts, inclement\nweather conditions, and densely populated urban environments. The code for the\nproposed model is available at our Github.","terms":["cs.CV","cs.AI"]},{"titles":"FoodFusion: A Latent Diffusion Model for Realistic Food Image Generation","summaries":"Current state-of-the-art image generation models such as Latent Diffusion\nModels (LDMs) have demonstrated the capacity to produce visually striking\nfood-related images. However, these generated images often exhibit an artistic\nor surreal quality that diverges from the authenticity of real-world food\nrepresentations. This inadequacy renders them impractical for applications\nrequiring realistic food imagery, such as training models for image-based\ndietary assessment. To address these limitations, we introduce FoodFusion, a\nLatent Diffusion model engineered specifically for the faithful synthesis of\nrealistic food images from textual descriptions. The development of the\nFoodFusion model involves harnessing an extensive array of open-source food\ndatasets, resulting in over 300,000 curated image-caption pairs. Additionally,\nwe propose and employ two distinct data cleaning methodologies to ensure that\nthe resulting image-text pairs maintain both realism and accuracy. The\nFoodFusion model, thus trained, demonstrates a remarkable ability to generate\nfood images that exhibit a significant improvement in terms of both realism and\ndiversity over the publicly available image generation models. We openly share\nthe dataset and fine-tuned models to support advancements in this critical\nfield of food image synthesis at https:\/\/bit.ly\/genai4good.","terms":["cs.CV"]},{"titles":"Low-power, Continuous Remote Behavioral Localization with Event Cameras","summaries":"Researchers in natural science need reliable methods for quantifying animal\nbehavior. Recently, numerous computer vision methods emerged to automate the\nprocess. However, observing wild species at remote locations remains a\nchallenging task due to difficult lighting conditions and constraints on power\nsupply and data storage. Event cameras offer unique advantages for\nbattery-dependent remote monitoring due to their low power consumption and high\ndynamic range capabilities. We use this novel sensor to quantify a behavior in\nChinstrap penguins called ecstatic display. We formulate the problem as a\ntemporal action detection task, determining the start and end times of the\nbehavior. For this purpose, we recorded a colony of breeding penguins in\nAntarctica during several weeks and labeled event data on 16 nests. The\ndeveloped method consists of a generator of candidate time intervals\n(proposals) and a classifier of the actions within them. The experiments show\nthat the event cameras' natural response to motion is effective for continuous\nbehavior monitoring and detection, reaching a mean average precision (mAP) of\n58% (which increases to 63% in good weather conditions). The results also\ndemonstrate the robustness against various lighting conditions contained in the\nchallenging dataset. The low-power capabilities of the event camera allows to\nrecord three times longer than with a conventional camera. This work pioneers\nthe use of event cameras for remote wildlife observation, opening new\ninterdisciplinary opportunities. https:\/\/tub-rip.github.io\/eventpenguins\/","terms":["cs.CV","cs.AI"]},{"titles":"Low-shot Object Learning with Mutual Exclusivity Bias","summaries":"This paper introduces Low-shot Object Learning with Mutual Exclusivity Bias\n(LSME), the first computational framing of mutual exclusivity bias, a\nphenomenon commonly observed in infants during word learning. We provide a\nnovel dataset, comprehensive baselines, and a state-of-the-art method to enable\nthe ML community to tackle this challenging learning task. The goal of LSME is\nto analyze an RGB image of a scene containing multiple objects and correctly\nassociate a previously-unknown object instance with a provided category label.\nThis association is then used to perform low-shot learning to test category\ngeneralization. We provide a data generation pipeline for the LSME problem and\nconduct a thorough analysis of the factors that contribute to its difficulty.\nAdditionally, we evaluate the performance of multiple baselines, including\nstate-of-the-art foundation models. Finally, we present a baseline approach\nthat outperforms state-of-the-art models in terms of low-shot accuracy.","terms":["cs.CV"]},{"titles":"Single Image Reflection Removal with Reflection Intensity Prior Knowledge","summaries":"Single Image Reflection Removal (SIRR) in real-world images is a challenging\ntask due to diverse image degradations occurring on the glass surface during\nlight transmission and reflection. Many existing methods rely on specific prior\nassumptions to resolve the problem. In this paper, we propose a general\nreflection intensity prior that captures the intensity of the reflection\nphenomenon and demonstrate its effectiveness. To learn the reflection intensity\nprior, we introduce the Reflection Prior Extraction Network (RPEN). By\nsegmenting images into regional patches, RPEN learns non-uniform reflection\nprior in an image. We propose Prior-based Reflection Removal Network (PRRN)\nusing a simple transformer U-Net architecture that adapts reflection prior fed\nfrom RPEN. Experimental results on real-world benchmarks demonstrate the\neffectiveness of our approach achieving state-of-the-art accuracy in SIRR.","terms":["cs.CV"]},{"titles":"Personalized Pose Forecasting","summaries":"Human pose forecasting is the task of predicting articulated human motion\ngiven past human motion. There exists a number of popular benchmarks that\nevaluate an array of different models performing human pose forecasting. These\nbenchmarks do not reflect that a human interacting system, such as a delivery\nrobot, observes and plans for the motion of the same individual over an\nextended period of time. Every individual has unique and distinct movement\npatterns. This is however not reflected in existing benchmarks that evaluate a\nmodel's ability to predict an average human's motion rather than a particular\nindividual's. We reformulate the human motion forecasting problem and present a\nmodel-agnostic personalization method. Motion forecasting personalization can\nbe performed efficiently online by utilizing a low-parametric time-series\nanalysis model that personalizes neural network pose predictions.","terms":["cs.CV"]},{"titles":"On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm","summaries":"Contemporary machine learning requires training large neural networks on\nmassive datasets and thus faces the challenges of high computational demands.\nDataset distillation, as a recent emerging strategy, aims to compress\nreal-world datasets for efficient training. However, this line of research\ncurrently struggle with large-scale and high-resolution datasets, hindering its\npracticality and feasibility. To this end, we re-examine the existing dataset\ndistillation methods and identify three properties required for large-scale\nreal-world applications, namely, realism, diversity, and efficiency. As a\nremedy, we propose RDED, a novel computationally-efficient yet effective data\ndistillation paradigm, to enable both diversity and realism of the distilled\ndata. Extensive empirical results over various neural architectures and\ndatasets demonstrate the advancement of RDED: we can distill the full\nImageNet-1K to a small dataset comprising 10 images per class within 7 minutes,\nachieving a notable 42% top-1 accuracy with ResNet-18 on a single RTX-4090 GPU\n(while the SOTA only achieves 21% but requires 6 hours).","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"Multi-Scale and Multi-Modal Contrastive Learning Network for Biomedical Time Series","summaries":"Multi-modal biomedical time series (MBTS) data offers a holistic view of the\nphysiological state, holding significant importance in various bio-medical\napplications. Owing to inherent noise and distribution gaps across different\nmodalities, MBTS can be complex to model. Various deep learning models have\nbeen developed to learn representations of MBTS but still fall short in\nrobustness due to the ignorance of modal-to-modal variations. This paper\npresents a multi-scale and multi-modal biomedical time series representation\nlearning (MBSL) network with contrastive learning to migrate these variations.\nFirstly, MBTS is grouped based on inter-modal distances, then each group with\nminimum intra-modal variations can be effectively modeled by individual\nencoders. Besides, to enhance the multi-scale feature extraction (encoder),\nvarious patch lengths and mask ratios are designed to generate tokens with\nsemantic information at different scales and diverse contextual perspectives\nrespectively. Finally, cross-modal contrastive learning is proposed to maximize\nconsistency among inter-modal groups, maintaining useful information and\neliminating noises. Experiments against four bio-medical applications show that\nMBSL outperforms state-of-the-art models by 33.9% mean average errors (MAE) in\nrespiration rate, by 13.8% MAE in exercise heart rate, by 1.41% accuracy in\nhuman activity recognition, and by 1.14% F1-score in obstructive sleep\napnea-hypopnea syndrome.","terms":["cs.LG","cs.AI"]},{"titles":"Defense Against Adversarial Attacks using Convolutional Auto-Encoders","summaries":"Deep learning models, while achieving state-of-the-art performance on many\ntasks, are susceptible to adversarial attacks that exploit inherent\nvulnerabilities in their architectures. Adversarial attacks manipulate the\ninput data with imperceptible perturbations, causing the model to misclassify\nthe data or produce erroneous outputs. This work is based on enhancing the\nrobustness of targeted classifier models against adversarial attacks. To\nachieve this, an convolutional autoencoder-based approach is employed that\neffectively counters adversarial perturbations introduced to the input images.\nBy generating images closely resembling the input images, the proposed\nmethodology aims to restore the model's accuracy.","terms":["cs.CV","cs.AI","I.4.5; I.5.1; I.5.4"]},{"titles":"Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching","summaries":"Mechanistic interpretability aims to understand model behaviors in terms of\nspecific, interpretable features, often hypothesized to manifest as\nlow-dimensional subspaces of activations. Specifically, recent studies have\nexplored subspace interventions (such as activation patching) as a way to\nsimultaneously manipulate model behavior and attribute the features behind it\nto given subspaces.\n  In this work, we demonstrate that these two aims diverge, potentially leading\nto an illusory sense of interpretability. Counterintuitively, even if a\nsubspace intervention makes the model's output behave as if the value of a\nfeature was changed, this effect may be achieved by activating a dormant\nparallel pathway leveraging another subspace that is causally disconnected from\nmodel outputs. We demonstrate this phenomenon in a distilled mathematical\nexample, in two real-world domains (the indirect object identification task and\nfactual recall), and present evidence for its prevalence in practice. In the\ncontext of factual recall, we further show a link to rank-1 fact editing,\nproviding a mechanistic explanation for previous work observing an\ninconsistency between fact editing performance and fact localization.\n  However, this does not imply that activation patching of subspaces is\nintrinsically unfit for interpretability. To contextualize our findings, we\nalso show what a success case looks like in a task (indirect object\nidentification) where prior manual circuit analysis informs an understanding of\nthe location of a feature. We explore the additional evidence needed to argue\nthat a patched subspace is faithful.","terms":["cs.LG","cs.AI","cs.CL"]},{"titles":"FRDiff: Feature Reuse for Exquisite Zero-shot Acceleration of Diffusion Models","summaries":"The substantial computational costs of diffusion models, particularly due to\nthe repeated denoising steps crucial for high-quality image generation, present\na major obstacle to their widespread adoption. While several studies have\nattempted to address this issue by reducing the number of score function\nevaluations using advanced ODE solvers without fine-tuning, the decreased\nnumber of denoising iterations misses the opportunity to update fine details,\nresulting in noticeable quality degradation. In our work, we introduce an\nadvanced acceleration technique that leverages the temporal redundancy inherent\nin diffusion models. Reusing feature maps with high temporal similarity opens\nup a new opportunity to save computation without sacrificing output quality. To\nrealize the practical benefits of this intuition, we conduct an extensive\nanalysis and propose a novel method, FRDiff. FRDiff is designed to harness the\nadvantages of both reduced NFE and feature reuse, achieving a Pareto frontier\nthat balances fidelity and latency trade-offs in various generative tasks.","terms":["cs.CV","cs.AI"]},{"titles":"Constrained Parameter Regularization","summaries":"Regularization is a critical component in deep learning training, with weight\ndecay being a commonly used approach. It applies a constant penalty coefficient\nuniformly across all parameters. This may be unnecessarily restrictive for some\nparameters, while insufficiently restricting others. To dynamically adjust\npenalty coefficients for different parameter groups, we present constrained\nparameter regularization (CPR) as an alternative to traditional weight decay.\nInstead of applying a single constant penalty to all parameters, we enforce an\nupper bound on a statistical measure (e.g., the L$_2$-norm) of parameter\ngroups. Consequently, learning becomes a constraint optimization problem, which\nwe address by an adaptation of the augmented Lagrangian method. CPR only\nrequires two hyperparameters and incurs no measurable runtime overhead.\nAdditionally, we propose a simple but efficient mechanism to adapt the upper\nbounds during the optimization. We provide empirical evidence of CPR's efficacy\nin experiments on the \"grokking\" phenomenon, computer vision, and language\nmodeling tasks. Our results demonstrate that CPR counteracts the effects of\ngrokking and consistently matches or outperforms traditional weight decay.","terms":["cs.LG"]},{"titles":"AnimatableDreamer: Text-Guided Non-rigid 3D Model Generation and Reconstruction with Canonical Score Distillation","summaries":"Text-to-3D model adaptations have advanced static 3D model quality, but\nsequential 3D model generation, particularly for animatable objects with large\nmotions, is still scarce. Our work proposes AnimatableDreamer, a text-to-4D\ngeneration framework capable of generating diverse categories of non-rigid\nobjects while adhering to the object motions extracted from a monocular video.\nAt its core, AnimatableDreamer is equipped with our novel optimization design\ndubbed Canonical Score Distillation (CSD), which simplifies the generation\ndimension from 4D to 3D by denoising over different frames in the time-varying\ncamera spaces while conducting the distillation process in a unique canonical\nspace shared per video. Concretely, CSD ensures that score gradients\nback-propagate to the canonical space through differentiable warping, hence\nguaranteeing the time-consistent generation and maintaining morphological\nplausibility across different poses. By lifting the 3D generator to 4D with\nwarping functions, AnimatableDreamer offers a novel perspective on non-rigid 3D\nmodel generation and reconstruction. Besides, with inductive knowledge from a\nmulti-view consistent diffusion model, CSD regularizes reconstruction from\nnovel views, thus cyclically enhancing the generation process. Extensive\nexperiments demonstrate the capability of our method in generating\nhigh-flexibility text-guided 3D models from the monocular video, while also\nshowing improved reconstruction performance over typical non-rigid\nreconstruction methods. Project page https:\/\/AnimatableDreamer.github.io.","terms":["cs.CV","I.4.5"]},{"titles":"Kandinsky 3.0 Technical Report","summaries":"We present Kandinsky 3.0, a large-scale text-to-image generation model based\non latent diffusion, continuing the series of text-to-image Kandinsky models\nand reflecting our progress to achieve higher quality and realism of image\ngeneration. Compared to previous versions of Kandinsky 2.x, Kandinsky 3.0\nleverages a two times larger U-Net backbone, a ten times larger text encoder\nand removes diffusion mapping. We describe the architecture of the model, the\ndata collection procedure, the training technique, and the production system of\nuser interaction. We focus on the key components that, as we have identified as\na result of a large number of experiments, had the most significant impact on\nimproving the quality of our model compared to the others. By our side-by-side\ncomparisons, Kandinsky becomes better in text understanding and works better on\nspecific domains. Project page: https:\/\/ai-forever.github.io\/Kandinsky-3","terms":["cs.CV","cs.LG","cs.MM"]},{"titles":"Solving Linear Inverse Problems using Higher-Order Annealed Langevin Diffusion","summaries":"We propose a solution for linear inverse problems based on higher-order\nLangevin diffusion. More precisely, we propose pre-conditioned second-order and\nthird-order Langevin dynamics that provably sample from the posterior\ndistribution of our unknown variables of interest while being computationally\nmore efficient than their first-order counterpart and the non-conditioned\nversions of both dynamics. Moreover, we prove that both pre-conditioned\ndynamics are well-defined and have the same unique invariant distributions as\nthe non-conditioned cases. We also incorporate an annealing procedure that has\nthe double benefit of further accelerating the convergence of the algorithm and\nallowing us to accommodate the case where the unknown variables are discrete.\nNumerical experiments in two different tasks in communications (MIMO symbol\ndetection and channel estimation) and in three tasks for images showcase the\ngenerality of our method and illustrate the high performance achieved relative\nto competing approaches (including learning-based ones) while having comparable\nor lower computational complexity.","terms":["stat.ML","eess.SP"]},{"titles":"Gravitational cell detection and tracking in fluorescence microscopy data","summaries":"Automatic detection and tracking of cells in microscopy images are major\napplications of computer vision technologies in both biomedical research and\nclinical practice. Though machine learning methods are increasingly common in\nthese fields, classical algorithms still offer significant advantages for both\ntasks, including better explainability, faster computation, lower hardware\nrequirements and more consistent performance. In this paper, we present a novel\napproach based on gravitational force fields that can compete with, and\npotentially outperform modern machine learning models when applied to\nfluorescence microscopy images. This method includes detection, segmentation,\nand tracking elements, with the results demonstrated on a Cell Tracking\nChallenge dataset.","terms":["cs.CV","q-bio.CB","I.4.6"]},{"titles":"Nash Learning from Human Feedback","summaries":"Reinforcement learning from human feedback (RLHF) has emerged as the main\nparadigm for aligning large language models (LLMs) with human preferences.\nTypically, RLHF involves the initial step of learning a reward model from human\nfeedback, often expressed as preferences between pairs of text generations\nproduced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by\noptimizing it to maximize the reward model through a reinforcement learning\nalgorithm. However, an inherent limitation of current reward models is their\ninability to fully represent the richness of human preferences and their\ndependency on the sampling distribution.\n  In this study, we introduce an alternative pipeline for the fine-tuning of\nLLMs using pairwise human feedback. Our approach entails the initial learning\nof a preference model, which is conditioned on two inputs given a prompt,\nfollowed by the pursuit of a policy that consistently generates responses\npreferred over those generated by any competing policy, thus defining the Nash\nequilibrium of this preference model. We term this approach Nash learning from\nhuman feedback (NLHF).\n  In the context of a tabular policy representation, we present a novel\nalgorithmic solution, Nash-MD, founded on the principles of mirror descent.\nThis algorithm produces a sequence of policies, with the last iteration\nconverging to the regularized Nash equilibrium. Additionally, we explore\nparametric representations of policies and introduce gradient descent\nalgorithms for deep-learning architectures. To demonstrate the effectiveness of\nour approach, we present experimental results involving the fine-tuning of a\nLLM for a text summarization task. We believe NLHF offers a compelling avenue\nfor preference learning and policy optimization with the potential of advancing\nthe field of aligning LLMs with human preferences.","terms":["stat.ML","cs.AI","cs.GT","cs.LG","cs.MA"]},{"titles":"Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models","summaries":"Recent advancements in diffusion models have unlocked unprecedented abilities\nin visual creation. However, current text-to-video generation models struggle\nwith the trade-off among movement range, action coherence and object\nconsistency. To mitigate this issue, we present a controllable text-to-video\n(T2V) diffusion model, called Control-A-Video, capable of maintaining\nconsistency while customizable video synthesis. Based on a pre-trained\nconditional text-to-image (T2I) diffusion model, our model aims to generate\nvideos conditioned on a sequence of control signals, such as edge or depth\nmaps. For the purpose of improving object consistency, Control-A-Video\nintegrates motion priors and content priors into video generation. We propose\ntwo motion-adaptive noise initialization strategies, which are based on pixel\nresidual and optical flow, to introduce motion priors from input videos,\nproducing more coherent videos. Moreover, a first-frame conditioned controller\nis proposed to generate videos from content priors of the first frame, which\nfacilitates the semantic alignment with text and allows longer video generation\nin an auto-regressive manner. With the proposed architecture and strategies,\nour model achieves resource-efficient convergence and generate consistent and\ncoherent videos with fine-grained control. Extensive experiments demonstrate\nits success in various video generative tasks such as video editing and video\nstyle transfer, outperforming previous methods in terms of consistency and\nquality.","terms":["cs.CV","cs.AI","cs.LG","cs.MM"]},{"titles":"Improving the Generalization of Segmentation Foundation Model under Distribution Shift via Weakly Supervised Adaptation","summaries":"The success of large language models has inspired the computer vision\ncommunity to explore image segmentation foundation model that is able to\nzero\/few-shot generalize through prompt engineering. Segment-Anything(SAM),\namong others, is the state-of-the-art image segmentation foundation model\ndemonstrating strong zero\/few-shot generalization. Despite the success, recent\nstudies reveal the weakness of SAM under strong distribution shift. In\nparticular, SAM performs awkwardly on corrupted natural images, camouflaged\nimages, medical images, etc. Motivated by the observations, we aim to develop a\nself-training based strategy to adapt SAM to target distribution. Given the\nunique challenges of large source dataset, high computation cost and incorrect\npseudo label, we propose a weakly supervised self-training architecture with\nanchor regularization and low-rank finetuning to improve the robustness and\ncomputation efficiency of adaptation. We validate the effectiveness on 5 types\nof downstream segmentation tasks including natural clean\/corrupted images,\nmedical images, camouflaged images and robotic images. Our proposed method is\ntask-agnostic in nature and outperforms pre-trained SAM and state-of-the-art\ndomain adaptation methods on almost all downstream tasks with the same testing\nprompt inputs.","terms":["cs.CV"]},{"titles":"Memory-free Online Change-point Detection: A Novel Neural Network Approach","summaries":"Change-point detection (CPD), which detects abrupt changes in the data\ndistribution, is recognized as one of the most significant tasks in time series\nanalysis. Despite the extensive literature on offline CPD, unsupervised online\nCPD still suffers from major challenges, including scalability, hyperparameter\ntuning, and learning constraints. To mitigate some of these challenges, in this\npaper, we propose a novel deep learning approach for unsupervised online CPD\nfrom multi-dimensional time series, named Adaptive LSTM-Autoencoder\nChange-Point Detection (ALACPD). ALACPD exploits an LSTM-autoencoder-based\nneural network to perform unsupervised online CPD. It continuously adapts to\nthe incoming samples without keeping the previously received input, thus being\nmemory-free. We perform an extensive evaluation on several real-world time\nseries CPD benchmarks. We show that ALACPD, on average, ranks first among\nstate-of-the-art CPD algorithms in terms of quality of the time series\nsegmentation, and it is on par with the best performer in terms of the accuracy\nof the estimated change-points. The implementation of ALACPD is available\nonline on Github\\footnote{\\url{https:\/\/github.com\/zahraatashgahi\/ALACPD}}.","terms":["cs.LG","cs.AI"]},{"titles":"Overcoming Generic Knowledge Loss with Selective Parameter Update","summaries":"Foundation models encompass an extensive knowledge base and offer remarkable\ntransferability. However, this knowledge becomes outdated or insufficient over\ntime. The challenge lies in continuously updating foundation models to\naccommodate novel information while retaining their original capabilities.\nLeveraging the fact that foundation models have initial knowledge on various\ntasks and domains, we propose a novel approach that, instead of updating all\nparameters equally, localizes the updates to a sparse set of parameters\nrelevant to the task being learned. We strike a balance between efficiency and\nnew task performance, while maintaining the transferability and\ngeneralizability of foundation models. We extensively evaluate our method on\nfoundational vision-language models with a diverse spectrum of continual\nlearning tasks. Our method achieves improvements on the accuracy of the newly\nlearned tasks up to 7% while preserving the pretraining knowledge with a\nnegligible decrease of 0.9% on a representative control set accuracy.","terms":["cs.CV"]},{"titles":"AnimateZero: Video Diffusion Models are Zero-Shot Image Animators","summaries":"Large-scale text-to-video (T2V) diffusion models have great progress in\nrecent years in terms of visual quality, motion and temporal consistency.\nHowever, the generation process is still a black box, where all attributes\n(e.g., appearance, motion) are learned and generated jointly without precise\ncontrol ability other than rough text descriptions. Inspired by image animation\nwhich decouples the video as one specific appearance with the corresponding\nmotion, we propose AnimateZero to unveil the pre-trained text-to-video\ndiffusion model, i.e., AnimateDiff, and provide more precise appearance and\nmotion control abilities for it. For appearance control, we borrow intermediate\nlatents and their features from the text-to-image (T2I) generation for ensuring\nthe generated first frame is equal to the given generated image. For temporal\ncontrol, we replace the global temporal attention of the original T2V model\nwith our proposed positional-corrected window attention to ensure other frames\nalign with the first frame well. Empowered by the proposed methods, AnimateZero\ncan successfully control the generating progress without further training. As a\nzero-shot image animator for given images, AnimateZero also enables multiple\nnew applications, including interactive video generation and real image\nanimation. The detailed experiments demonstrate the effectiveness of the\nproposed method in both T2V and related applications.","terms":["cs.CV"]},{"titles":"Learning From Scenarios for Stochastic Repairable Scheduling","summaries":"When optimizing problems with uncertain parameter values in a linear\nobjective, decision-focused learning enables end-to-end learning of these\nvalues. We are interested in a stochastic scheduling problem, in which\nprocessing times are uncertain, which brings uncertain values in the\nconstraints, and thus repair of an initial schedule may be needed. Historical\nrealizations of the stochastic processing times are available. We show how\nexisting decision-focused learning techniques based on stochastic smoothing can\nbe adapted to this scheduling problem. We include an extensive experimental\nevaluation to investigate in which situations decision-focused learning\noutperforms the state of the art for such situations: scenario-based stochastic\noptimization.","terms":["cs.LG","cs.AI"]},{"titles":"Precision of Individual Shapley Value Explanations","summaries":"Shapley values are extensively used in explainable artificial intelligence\n(XAI) as a framework to explain predictions made by complex machine learning\n(ML) models. In this work, we focus on conditional Shapley values for\npredictive models fitted to tabular data and explain the prediction\n$f(\\boldsymbol{x}^{*})$ for a single observation $\\boldsymbol{x}^{*}$ at the\ntime. Numerous Shapley value estimation methods have been proposed and\nempirically compared on an average basis in the XAI literature. However, less\nfocus has been devoted to analyzing the precision of the Shapley value\nexplanations on an individual basis. We extend our work in Olsen et al. (2023)\nby demonstrating and discussing that the explanations are systematically less\nprecise for observations on the outer region of the training data distribution\nfor all used estimation methods. This is expected from a statistical point of\nview, but to the best of our knowledge, it has not been systematically\naddressed in the Shapley value literature. This is crucial knowledge for\nShapley values practitioners, who should be more careful in applying these\nobservations' corresponding Shapley value explanations.","terms":["stat.ML","cs.LG","stat.AP","stat.CO"]},{"titles":"Self-Supervised Open-Ended Classification with Small Visual Language Models","summaries":"We present Self-Context Adaptation (SeCAt), a self-supervised approach that\nunlocks few-shot abilities for open-ended classification with small visual\nlanguage models. Our approach imitates image captions in a self-supervised way\nbased on clustering a large pool of images followed by assigning\nsemantically-unrelated names to clusters. By doing so, we construct a training\nsignal consisting of interleaved sequences of image and pseudocaption pairs and\na query image, which we denote as the 'self-context' sequence. Based on this\nsignal the model is trained to produce the right pseudo-caption. We demonstrate\nthe performance and flexibility of SeCAt on several multimodal few-shot\ndatasets, spanning various granularities. By using models with approximately 1B\nparameters we outperform the few-shot abilities of much larger models, such as\nFrozen and FROMAGe. SeCAt opens new possibilities for research and applications\nin open-ended few-shot learning that otherwise requires access to large or\nproprietary models.","terms":["cs.CV"]},{"titles":"Molecule Joint Auto-Encoding: Trajectory Pretraining with 2D and 3D Diffusion","summaries":"Recently, artificial intelligence for drug discovery has raised increasing\ninterest in both machine learning and chemistry domains. The fundamental\nbuilding block for drug discovery is molecule geometry and thus, the molecule's\ngeometrical representation is the main bottleneck to better utilize machine\nlearning techniques for drug discovery. In this work, we propose a pretraining\nmethod for molecule joint auto-encoding (MoleculeJAE). MoleculeJAE can learn\nboth the 2D bond (topology) and 3D conformation (geometry) information, and a\ndiffusion process model is applied to mimic the augmented trajectories of such\ntwo modalities, based on which, MoleculeJAE will learn the inherent chemical\nstructure in a self-supervised manner. Thus, the pretrained geometrical\nrepresentation in MoleculeJAE is expected to benefit downstream\ngeometry-related tasks. Empirically, MoleculeJAE proves its effectiveness by\nreaching state-of-the-art performance on 15 out of 20 tasks by comparing it\nwith 12 competitive baselines.","terms":["cs.LG","cs.AI","q-bio.BM"]},{"titles":"Memory-Efficient Optical Flow via Radius-Distribution Orthogonal Cost Volume","summaries":"The full 4D cost volume in Recurrent All-Pairs Field Transforms (RAFT) or\nglobal matching by Transformer achieves impressive performance for optical flow\nestimation. However, their memory consumption increases quadratically with\ninput resolution, rendering them impractical for high-resolution images. In\nthis paper, we present MeFlow, a novel memory-efficient method for\nhigh-resolution optical flow estimation. The key of MeFlow is a recurrent local\northogonal cost volume representation, which decomposes the 2D search space\ndynamically into two 1D orthogonal spaces, enabling our method to scale\neffectively to very high-resolution inputs. To preserve essential information\nin the orthogonal space, we utilize self attention to propagate feature\ninformation from the 2D space to the orthogonal space. We further propose a\nradius-distribution multi-scale lookup strategy to model the correspondences of\nlarge displacements at a negligible cost. We verify the efficiency and\neffectiveness of our method on the challenging Sintel and KITTI benchmarks, and\nreal-world 4K ($2160\\!\\times\\!3840$) images. Our method achieves competitive\nperformance on both Sintel and KITTI benchmarks, while maintaining the highest\nmemory efficiency on high-resolution inputs.","terms":["cs.CV"]},{"titles":"Search Strategies for Self-driving Laboratories with Pending Experiments","summaries":"Self-driving laboratories (SDLs) consist of multiple stations that perform\nmaterial synthesis and characterisation tasks. To minimize station downtime and\nmaximize experimental throughput, it is practical to run experiments in\nasynchronous parallel, in which multiple experiments are being performed at\nonce in different stages. Asynchronous parallelization of experiments, however,\nintroduces delayed feedback (i.e. \"pending experiments\"), which is known to\nreduce Bayesian optimiser performance. Here, we build a simulator for a\nmulti-stage SDL and compare optimisation strategies for dealing with delayed\nfeedback and asynchronous parallelized operation. Using data from a real SDL,\nwe build a ground truth Bayesian optimisation simulator from 177 previously run\nexperiments for maximizing the conductivity of functional coatings. We then\ncompare search strategies such as expected improvement, noisy expected\nimprovement, 4-mode exploration and random sampling. We evaluate their\nperformance in terms of amount of delay and problem dimensionality. Our\nsimulation results showcase the trade-off between the asynchronous parallel\noperation and delayed feedback.","terms":["cs.LG","cs.RO"]},{"titles":"Subnetwork-to-go: Elastic Neural Network with Dynamic Training and Customizable Inference","summaries":"Deploying neural networks to different devices or platforms is in general\nchallenging, especially when the model size is large or model complexity is\nhigh. Although there exist ways for model pruning or distillation, it is\ntypically required to perform a full round of model training or finetuning\nprocedure in order to obtain a smaller model that satisfies the model size or\ncomplexity constraints. Motivated by recent works on dynamic neural networks,\nwe propose a simple way to train a large network and flexibly extract a\nsubnetwork from it given a model size or complexity constraint during\ninference. We introduce a new way to allow a large model to be trained with\ndynamic depth and width during the training phase, and after the large model is\ntrained we can select a subnetwork from it with arbitrary depth and width\nduring the inference phase with a relatively better performance compared to\ntraining the subnetwork independently from scratch. Experiment results on a\nmusic source separation model show that our proposed method can effectively\nimprove the separation performance across different subnetwork sizes and\ncomplexities with a single large model, and training the large model takes\nsignificantly shorter time than training all the different subnetworks.","terms":["cs.LG","cs.SD","eess.AS"]},{"titles":"F3-Pruning: A Training-Free and Generalized Pruning Strategy towards Faster and Finer Text-to-Video Synthesis","summaries":"Recently Text-to-Video (T2V) synthesis has undergone a breakthrough by\ntraining transformers or diffusion models on large-scale datasets.\nNevertheless, inferring such large models incurs huge costs.Previous inference\nacceleration works either require costly retraining or are model-specific.To\naddress this issue, instead of retraining we explore the inference process of\ntwo mainstream T2V models using transformers and diffusion models.The\nexploration reveals the redundancy in temporal attention modules of both\nmodels, which are commonly utilized to establish temporal relations among\nframes.Consequently, we propose a training-free and generalized pruning\nstrategy called F3-Pruning to prune redundant temporal attention\nweights.Specifically, when aggregate temporal attention values are ranked below\na certain ratio, corresponding weights will be pruned.Extensive experiments on\nthree datasets using a classic transformer-based model CogVideo and a typical\ndiffusion-based model Tune-A-Video verify the effectiveness of F3-Pruning in\ninference acceleration, quality assurance and broad applicability.","terms":["cs.CV"]},{"titles":"Visual Data-Type Understanding does not emerge from Scaling Vision-Language Models","summaries":"Recent advances in the development of vision-language models (VLMs) are\nyielding remarkable success in recognizing visual semantic content, including\nimpressive instances of compositional image understanding. Here, we introduce\nthe novel task of Visual Data-Type Identification, a basic perceptual skill\nwith implications for data curation (e.g., noisy data-removal from large\ndatasets, domain-specific retrieval) and autonomous vision (e.g.,\ndistinguishing changing weather conditions from camera lens staining). We\ndevelop two datasets consisting of animal images altered across a diverse set\nof 27 visual data-types, spanning four broad categories. An extensive zero-shot\nevaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced\nperformance landscape. While VLMs are reasonably good at identifying certain\nstylistic \\textit{data-types}, such as cartoons and sketches, they struggle\nwith simpler data-types arising from basic manipulations like image rotations\nor additive noise. Our findings reveal that (i) model scaling alone yields\nmarginal gains for contrastively-trained models like CLIP, and (ii) there is a\npronounced drop in performance for the largest auto-regressively trained VLMs\nlike OpenFlamingo. This finding points to a blind spot in current frontier\nVLMs: they excel in recognizing semantic content but fail to acquire an\nunderstanding of visual data-types through scaling. By analyzing the\npre-training distributions of these models and incorporating data-type\ninformation into the captions during fine-tuning, we achieve a significant\nenhancement in performance. By exploring this previously uncharted task, we aim\nto set the stage for further advancing VLMs to equip them with visual data-type\nunderstanding. Code and datasets are released at\nhttps:\/\/github.com\/bethgelab\/DataTypeIdentification.","terms":["cs.CV","cs.CL","cs.LG"]},{"titles":"Function-Space Optimality of Neural Architectures With Multivariate Nonlinearities","summaries":"We investigate the function-space optimality (specifically, the Banach-space\noptimality) of a large class of shallow neural architectures with multivariate\nnonlinearities\/activation functions. To that end, we construct a new family of\nBanach spaces defined via a regularization operator, the $k$-plane transform,\nand a sparsity-promoting norm. We prove a representer theorem that states that\nthe solution sets to learning problems posed over these Banach spaces are\ncompletely characterized by neural architectures with multivariate\nnonlinearities. These optimal architectures have skip connections and are\ntightly connected to orthogonal weight normalization and multi-index models,\nboth of which have received recent interest in the neural network community.\nOur framework is compatible with a number of classical nonlinearities including\nthe rectified linear unit (ReLU) activation function, the norm activation\nfunction, and the radial basis functions found in the theory of\nthin-plate\/polyharmonic splines. We also show that the underlying spaces are\nspecial instances of reproducing kernel Banach spaces and variation spaces. Our\nresults shed light on the regularity of functions learned by neural networks\ntrained on data, particularly with multivariate nonlinearities, and provide new\ntheoretical motivation for several architectural choices found in practice.","terms":["stat.ML","cs.LG"]},{"titles":"FaceStudio: Put Your Face Everywhere in Seconds","summaries":"This study investigates identity-preserving image synthesis, an intriguing\ntask in image generation that seeks to maintain a subject's identity while\nadding a personalized, stylistic touch. Traditional methods, such as Textual\nInversion and DreamBooth, have made strides in custom image creation, but they\ncome with significant drawbacks. These include the need for extensive resources\nand time for fine-tuning, as well as the requirement for multiple reference\nimages. To overcome these challenges, our research introduces a novel approach\nto identity-preserving synthesis, with a particular focus on human images. Our\nmodel leverages a direct feed-forward mechanism, circumventing the need for\nintensive fine-tuning, thereby facilitating quick and efficient image\ngeneration. Central to our innovation is a hybrid guidance framework, which\ncombines stylized images, facial images, and textual prompts to guide the image\ngeneration process. This unique combination enables our model to produce a\nvariety of applications, such as artistic portraits and identity-blended\nimages. Our experimental results, including both qualitative and quantitative\nevaluations, demonstrate the superiority of our method over existing baseline\nmodels and previous works, particularly in its remarkable efficiency and\nability to preserve the subject's identity with high fidelity.","terms":["cs.CV","cs.AI"]},{"titles":"DyEdgeGAT: Dynamic Edge via Graph Attention for Early Fault Detection in IIoT Systems","summaries":"In the industrial Internet of Things, condition monitoring sensor signals\nfrom complex systems often exhibit strong nonlinear and stochastic\nspatial-temporal dynamics under varying operating conditions. Such complex\ndynamics make fault detection particularly challenging. Although previously\nproposed methods effectively model these dynamics, they often neglect the\ndynamic evolution of relationships between sensor signals. Undetected shifts in\nthese relationships can potentially result in significant system failures.\nAnother limitation is their inability to effectively distinguish between novel\noperating conditions and actual faults. To address this gap, we propose\nDyEdgeGAT (Dynamic Edge via Graph Attention), a novel approach capable of\ndetecting various faults, especially those characterized by relationship\nchanges at early stages, while distinguishing faults from novel operating\nconditions. DyEdgeGAT is a graph-based framework that provides a novel graph\ninference scheme for multivariate time series that dynamically constructs edges\nto represent and track the evolution of relationships between time series.\nAdditionally, it addresses a commonly overlooked aspect: the cause-and-effect\nrelationships within the system, such as between control inputs and\nmeasurements. By incorporating system-independent variables as contexts of\noperating conditions into node dynamics extraction, DyEdgeGAT enhances its\nrobustness against novel operating conditions. We rigorously evaluate\nDyEdgeGAT's performance using both a synthetic dataset, designed to simulate\nvarying levels of fault severity and a real-world industrial-scale benchmark\ncontaining a variety of fault types with different detection complexities. Our\nfindings demonstrate that DyEdgeGAT is highly effective in fault detection,\nshowing particular strength in early fault detection while maintaining\nrobustness under novel operating conditions.","terms":["cs.LG","cs.AI"]},{"titles":"Data-driven Crop Growth Simulation on Time-varying Generated Images using Multi-conditional Generative Adversarial Networks","summaries":"Image-based crop growth modeling can substantially contribute to precision\nagriculture by revealing spatial crop development over time, which allows an\nearly and location-specific estimation of relevant future plant traits, such as\nleaf area or biomass. A prerequisite for realistic and sharp crop image\ngeneration is the integration of multiple growth-influencing conditions in a\nmodel, such as an image of an initial growth stage, the associated growth time,\nand further information about the field treatment. We present a two-stage\nframework consisting first of an image prediction model and second of a growth\nestimation model, which both are independently trained. The image prediction\nmodel is a conditional Wasserstein generative adversarial network (CWGAN). In\nthe generator of this model, conditional batch normalization (CBN) is used to\nintegrate different conditions along with the input image. This allows the\nmodel to generate time-varying artificial images dependent on multiple\ninfluencing factors of different kinds. These images are used by the second\npart of the framework for plant phenotyping by deriving plant-specific traits\nand comparing them with those of non-artificial (real) reference images. For\nvarious crop datasets, the framework allows realistic, sharp image predictions\nwith a slight loss of quality from short-term to long-term predictions.\nSimulations of varying growth-influencing conditions performed with the trained\nframework provide valuable insights into how such factors relate to crop\nappearances, which is particularly useful in complex, less explored crop\nmixture systems. Further results show that adding process-based simulated\nbiomass as a condition increases the accuracy of the derived phenotypic traits\nfrom the predicted images. This demonstrates the potential of our framework to\nserve as an interface between an image- and process-based crop growth model.","terms":["cs.CV","stat.ML"]},{"titles":"High-Quality Facial Geometry and Appearance Capture at Home","summaries":"Facial geometry and appearance capture have demonstrated tremendous success\nin 3D scanning real humans in studios. Recent works propose to democratize this\ntechnique while keeping the results high quality. However, they are still\ninconvenient for daily usage. In addition, they focus on an easier problem of\nonly capturing facial skin. This paper proposes a novel method for high-quality\nface capture, featuring an easy-to-use system and the capability to model the\ncomplete face with skin, mouth interior, hair, and eyes. We reconstruct facial\ngeometry and appearance from a single co-located smartphone flashlight sequence\ncaptured in a dim room where the flashlight is the dominant light source (e.g.\nrooms with curtains or at night). To model the complete face, we propose a\nnovel hybrid representation to effectively model both eyes and other facial\nregions, along with novel techniques to learn it from images. We apply a\ncombined lighting model to compactly represent real illuminations and exploit a\nmorphable face albedo model as a reflectance prior to disentangle diffuse and\nspecular. Experiments show that our method can capture high-quality 3D\nrelightable scans.","terms":["cs.CV"]},{"titles":"UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity","summaries":"Existing text-based person retrieval datasets often have relatively\ncoarse-grained text annotations. This hinders the model to comprehend the\nfine-grained semantics of query texts in real scenarios. To address this\nproblem, we contribute a new benchmark named \\textbf{UFineBench} for text-based\nperson retrieval with ultra-fine granularity.\n  Firstly, we construct a new \\textbf{dataset} named UFine6926. We collect a\nlarge number of person images and manually annotate each image with two\ndetailed textual descriptions, averaging 80.8 words each. The average word\ncount is three to four times that of the previous datasets. In addition of\nstandard in-domain evaluation, we also propose a special \\textbf{evaluation\nparadigm} more representative of real scenarios. It contains a new evaluation\nset with cross domains, cross textual granularity and cross textual styles,\nnamed UFine3C, and a new evaluation metric for accurately measuring retrieval\nability, named mean Similarity Distribution (mSD). Moreover, we propose CFAM, a\nmore efficient \\textbf{algorithm} especially designed for text-based person\nretrieval with ultra fine-grained texts. It achieves fine granularity mining by\nadopting a shared cross-modal granularity decoder and hard negative match\nmechanism.\n  With standard in-domain evaluation, CFAM establishes competitive performance\nacross various datasets, especially on our ultra fine-grained UFine6926.\nFurthermore, by evaluating on UFine3C, we demonstrate that training on our\nUFine6926 significantly improves generalization to real scenarios compared with\nother coarse-grained datasets. The dataset and code will be made publicly\navailable at \\url{https:\/\/github.com\/Zplusdragon\/UFineBench}.","terms":["cs.CV"]},{"titles":"Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle","summaries":"We introduce Gaussian-Flow, a novel point-based approach for fast dynamic\nscene reconstruction and real-time rendering from both multi-view and monocular\nvideos. In contrast to the prevalent NeRF-based approaches hampered by slow\ntraining and rendering speeds, our approach harnesses recent advancements in\npoint-based 3D Gaussian Splatting (3DGS). Specifically, a novel Dual-Domain\nDeformation Model (DDDM) is proposed to explicitly model attribute deformations\nof each Gaussian point, where the time-dependent residual of each attribute is\ncaptured by a polynomial fitting in the time domain, and a Fourier series\nfitting in the frequency domain. The proposed DDDM is capable of modeling\ncomplex scene deformations across long video footage, eliminating the need for\ntraining separate 3DGS for each frame or introducing an additional implicit\nneural field to model 3D dynamics. Moreover, the explicit deformation modeling\nfor discretized Gaussian points ensures ultra-fast training and rendering of a\n4D scene, which is comparable to the original 3DGS designed for static 3D\nreconstruction. Our proposed approach showcases a substantial efficiency\nimprovement, achieving a $5\\times$ faster training speed compared to the\nper-frame 3DGS modeling. In addition, quantitative results demonstrate that the\nproposed Gaussian-Flow significantly outperforms previous leading methods in\nnovel view rendering quality. Project page:\nhttps:\/\/nju-3dv.github.io\/projects\/Gaussian-Flow","terms":["cs.CV"]},{"titles":"ShareCMP: Polarization-Aware RGB-P Semantic Segmentation","summaries":"Multimodal semantic segmentation is developing rapidly, but the modality of\nRGB-Polarization remains underexplored. To delve into this problem, we\nconstruct a UPLight RGB-P segmentation benchmark with 12 typical underwater\nsemantic classes which provides data support for Autonomous Underwater Vehicles\n(AUVs) to perform special perception tasks. In this work, we design the\nShareCMP, an RGB-P semantic segmentation framework with a shared dual-branch\narchitecture, which reduces the number of parameters by about 26-33% compared\nto previous dual-branch models. It encompasses a Polarization Generate\nAttention (PGA) module designed to generate polarization modal images with\nricher polarization properties for the encoder. In addition, we introduce the\nClass Polarization-Aware Loss (CPALoss) to improve the learning and\nunderstanding of the encoder for polarization modal information and to optimize\nthe PGA module. With extensive experiments on a total of three RGB-P\nbenchmarks, our ShareCMP achieves state-of-the-art performance in mIoU with\nfewer parameters on the UPLight (92.45%), ZJU (92.7%), and MCubeS (50.99%)\ndatasets. The code is available at https:\/\/github.com\/LEFTeyex\/ShareCMP.","terms":["cs.CV","I.4.6"]},{"titles":"Continual Driving Policy Optimization with Closed-Loop Individualized Curricula","summaries":"The safety of autonomous vehicles (AV) has been a long-standing top concern,\nstemming from the absence of rare and safety-critical scenarios in the\nlong-tail naturalistic driving distribution. To tackle this challenge, a surge\nof research in scenario-based autonomous driving has emerged, with a focus on\ngenerating high-risk driving scenarios and applying them to conduct\nsafety-critical testing of AV models. However, limited work has been explored\non the reuse of these extensive scenarios to iteratively improve AV models.\nMoreover, it remains intractable and challenging to filter through gigantic\nscenario libraries collected from other AV models with distinct behaviors,\nattempting to extract transferable information for current AV improvement.\nTherefore, we develop a continual driving policy optimization framework\nfeaturing Closed-Loop Individualized Curricula (CLIC), which we factorize into\na set of standardized sub-modules for flexible implementation choices: AV\nEvaluation, Scenario Selection, and AV Training. CLIC frames AV Evaluation as a\ncollision prediction task, where it estimates the chance of AV failures in\nthese scenarios at each iteration. Subsequently, by re-sampling from historical\nscenarios based on these failure probabilities, CLIC tailors individualized\ncurricula for downstream training, aligning them with the evaluated capability\nof AV. Accordingly, CLIC not only maximizes the utilization of the vast\npre-collected scenario library for closed-loop driving policy optimization but\nalso facilitates AV improvement by individualizing its training with more\nchallenging cases out of those poorly organized scenarios. Experimental results\nclearly indicate that CLIC surpasses other curriculum-based training\nstrategies, showing substantial improvement in managing risky scenarios, while\nstill maintaining proficiency in handling simpler cases.","terms":["cs.LG","cs.AI","cs.RO"]},{"titles":"SmoothQuant+: Accurate and Efficient 4-bit Post-Training WeightQuantization for LLM","summaries":"Large language models (LLMs) have shown remarkable capabilities in various\ntasks. However their huge model size and the consequent demand for\ncomputational and memory resources also pose challenges to model deployment.\nCurrently, 4-bit post-training quantization (PTQ) has achieved some success in\nLLMs, reducing the memory footprint by approximately 75% compared to FP16\nmodels, albeit with some accuracy loss. In this paper, we propose SmoothQuant+,\nan accurate and efficient 4-bit weight-only PTQ that requires no additional\ntraining, which enables lossless in accuracy for LLMs for the first time. Based\non the fact that the loss of weight quantization is amplified by the activation\noutliers, SmoothQuant+ smoothes the activation outliers by channel before\nquantization, while adjusting the corresponding weights for mathematical\nequivalence, and then performs group-wise 4-bit weight quantization for linear\nlayers. We have integrated SmoothQuant+ into the vLLM framework, an advanced\nhigh-throughput inference engine specially developed for LLMs, and equipped it\nwith an efficient W4A16 CUDA kernels, so that vLLM can seamlessly support\nSmoothQuant+ 4-bit weight quantization. Our results show that, with\nSmoothQuant+, the Code Llama-34B model can be quantized and deployed on a A100\n40GB GPU, achieving lossless accuracy and a throughput increase of 1.9 to 4.0\ntimes compared to the FP16 model deployed on two A100 40GB GPUs. Moreover, the\nlatency per token is only 68% of the FP16 model deployed on two A100 40GB GPUs.\nThis is the state-of-the-art 4-bit weight quantization for LLMs as we know.","terms":["cs.LG","cs.CL"]},{"titles":"Artist-Friendly Relightable and Animatable Neural Heads","summaries":"An increasingly common approach for creating photo-realistic digital avatars\nis through the use of volumetric neural fields. The original neural radiance\nfield (NeRF) allowed for impressive novel view synthesis of static heads when\ntrained on a set of multi-view images, and follow up methods showed that these\nneural representations can be extended to dynamic avatars. Recently, new\nvariants also surpassed the usual drawback of baked-in illumination in neural\nrepresentations, showing that static neural avatars can be relit in any\nenvironment. In this work we simultaneously tackle both the motion and\nillumination problem, proposing a new method for relightable and animatable\nneural heads. Our method builds on a proven dynamic avatar approach based on a\nmixture of volumetric primitives, combined with a recently-proposed lightweight\nhardware setup for relightable neural fields, and includes a novel architecture\nthat allows relighting dynamic neural avatars performing unseen expressions in\nany environment, even with nearfield illumination and viewpoints.","terms":["cs.CV","cs.GR"]},{"titles":"InfMLLM: A Unified Framework for Visual-Language Tasks","summaries":"Large language models (LLMs) have proven their remarkable versatility in\nhandling a comprehensive range of language-centric applications. To expand\nLLMs' capabilities to a broader spectrum of modal inputs, multimodal large\nlanguage models (MLLMs) have attracted growing interest. This work delves into\nenabling LLMs to tackle more vision-language-related tasks, particularly image\ncaptioning, visual question answering (VQA,) and visual grounding. To this end,\nwe implemented a three-stage training scheme: starting with lightweight\nalignment pretraining, then moderate-weight multitask hybrid training, and\nfinally, LLM fine-tuning to improve instruction following capability.\nThroughout the training process, the requirements on GPU memory gradually\nincrease. To effectively manage the number of visual embeddings passed to the\nLLM while preserving their positional information, we introduce a\nstraightforward visual adapter module dubbed pool-adapter. Our experiments\ndemonstrate that preserving the positional information of visual embeddings\nthrough the pool-adapter is particularly beneficial for tasks like visual\ngrounding. We name our proposed approach InfMLLM and have evaluated it\nextensively on various benchmark datasets. Our results demonstrate that InfMLLM\nachieves either state-of-the-art (SOTA) performance or performance comparable\nto recent MLLMs. The code and model will be made open-source at:\n\\url{https:\/\/github.com\/mightyzau\/InfMLLM}.","terms":["cs.CV"]},{"titles":"EvDNeRF: Reconstructing Event Data with Dynamic Neural Radiance Fields","summaries":"We present EvDNeRF, a pipeline for generating event data and training an\nevent-based dynamic NeRF, for the purpose of faithfully reconstructing\neventstreams on scenes with rigid and non-rigid deformations that may be too\nfast to capture with a standard camera. Event cameras register asynchronous\nper-pixel brightness changes at MHz rates with high dynamic range, making them\nideal for observing fast motion with almost no motion blur. Neural radiance\nfields (NeRFs) offer visual-quality geometric-based learnable rendering, but\nprior work with events has only considered reconstruction of static scenes. Our\nEvDNeRF can predict eventstreams of dynamic scenes from a static or moving\nviewpoint between any desired timestamps, thereby allowing it to be used as an\nevent-based simulator for a given scene. We show that by training on varied\nbatch sizes of events, we can improve test-time predictions of events at fine\ntime resolutions, outperforming baselines that pair standard dynamic NeRFs with\nevent generators. We release our simulated and real datasets, as well as code\nfor multi-view event-based data generation and the training and evaluation of\nEvDNeRF models (https:\/\/github.com\/anish-bhattacharya\/EvDNeRF).","terms":["cs.CV"]},{"titles":"SAIF: Sparse Adversarial and Imperceptible Attack Framework","summaries":"Adversarial attacks hamper the decision-making ability of neural networks by\nperturbing the input signal. The addition of calculated small distortion to\nimages, for instance, can deceive a well-trained image classification network.\nIn this work, we propose a novel attack technique called Sparse Adversarial and\nInterpretable Attack Framework (SAIF). Specifically, we design imperceptible\nattacks that contain low-magnitude perturbations at a small number of pixels\nand leverage these sparse attacks to reveal the vulnerability of classifiers.\nWe use the Frank-Wolfe (conditional gradient) algorithm to simultaneously\noptimize the attack perturbations for bounded magnitude and sparsity with\n$O(1\/\\sqrt{T})$ convergence. Empirical results show that SAIF computes highly\nimperceptible and interpretable adversarial examples, and outperforms\nstate-of-the-art sparse attack methods on the ImageNet dataset.","terms":["cs.CV"]},{"titles":"Pose Impact Estimation on Face Recognition using 3D-Aware Synthetic Data with Application to Quality Assessment","summaries":"Evaluating the quality of facial images is essential for operating face\nrecognition systems with sufficient accuracy. The recent advances in face\nquality standardisation (ISO\/IEC CD3 29794-5) recommend the usage of component\nquality measures for breaking down face quality into its individual factors,\nhence providing valuable feedback for operators to re-capture low-quality\nimages. In light of recent advances in 3D-aware generative adversarial\nnetworks, we propose a novel dataset, Syn-YawPitch, comprising 1000 identities\nwith varying yaw-pitch angle combinations. Utilizing this dataset, we\ndemonstrate that pitch angles beyond 30 degrees have a significant impact on\nthe biometric performance of current face recognition systems. Furthermore, we\npropose a lightweight and explainable pose quality predictor that adheres to\nthe draft international standard of ISO\/IEC CD3 29794-5 and benchmark it\nagainst state-of-the-art face image quality assessment algorithms","terms":["cs.CV","cs.HC"]},{"titles":"Run LoRA Run: Faster and Lighter LoRA Implementations","summaries":"LoRA is a technique that reduces the number of trainable parameters in a\nneural network by introducing low-rank adapters to linear layers. This\ntechnique is used both for fine-tuning (LoRA, QLoRA) and full train (ReLoRA).\nThis paper presents the RunLoRA framework for efficient implementations of LoRA\nthat significantly improves the speed of neural network training and\nfine-tuning using low-rank adapters. The proposed implementation optimizes the\ncomputation of LoRA operations based on dimensions of corresponding linear\nlayer, layer input dimensions and lora rank by choosing best forward and\nbackward computation graph based on FLOPs and time estimations, resulting in\nfaster training without sacrificing accuracy. The experimental results show up\nto 17% speedup on Llama family of models.","terms":["cs.LG"]},{"titles":"Time Regularization in Optimal Time Variable Learning","summaries":"Recently, optimal time variable learning in deep neural networks (DNNs) was\nintroduced in arXiv:2204.08528. In this manuscript we extend the concept by\nintroducing a regularization term that directly relates to the time horizon in\ndiscrete dynamical systems. Furthermore, we propose an adaptive pruning\napproach for Residual Neural Networks (ResNets), which reduces network\ncomplexity without compromising expressiveness, while simultaneously decreasing\ntraining time. The results are illustrated by applying the proposed concepts to\nclassification tasks on the well known MNIST and Fashion MNIST data sets. Our\nPyTorch code is available on\nhttps:\/\/github.com\/frederikkoehne\/time_variable_learning.","terms":["cs.LG","math.OC"]},{"titles":"Compressed Context Memory For Online Language Model Interaction","summaries":"This paper presents a novel context compression method for Transformer\nlanguage models in online scenarios such as ChatGPT, where the context\ncontinually expands. As the context lengthens, the attention process requires\nmore memory and computational resources, which in turn reduces the throughput\nof the language model. To this end, we propose a compressed context memory\nsystem that continually compresses the growing context into a compact memory\nspace. The compression process simply involves integrating a lightweight\nconditional LoRA into the language model's forward pass during inference. Based\non the compressed context memory, the language model can perform inference with\nreduced memory and attention operations. Through evaluations on conversation,\npersonalization, and multi-task learning, we demonstrate that our approach\nachieves the performance level of a full context model with $5\\times$ smaller\ncontext memory space. Codes are available at\nhttps:\/\/github.com\/snu-mllab\/context-memory.","terms":["cs.LG","cs.CL"]},{"titles":"Approximating Solutions to the Knapsack Problem using the Lagrangian Dual Framework","summaries":"The Knapsack Problem is a classic problem in combinatorial optimisation.\nSolving these problems may be computationally expensive. Recent years have seen\na growing interest in the use of deep learning methods to approximate the\nsolutions to such problems. A core problem is how to enforce or encourage\nconstraint satisfaction in predicted solutions. A promising approach for\npredicting solutions to constrained optimisation problems is the Lagrangian\nDual Framework which builds on the method of Lagrangian Relaxation. In this\npaper we develop neural network models to approximate Knapsack Problem\nsolutions using the Lagrangian Dual Framework while improving constraint\nsatisfaction. We explore the problems of output interpretation and model\nselection within this context. Experimental results show strong constraint\nsatisfaction with a minor reduction of optimality as compared to a baseline\nneural network which does not explicitly model the constraints.","terms":["cs.LG","cs.AI","math.OC"]},{"titles":"DeepPyramid+: Medical Image Segmentation using Pyramid View Fusion and Deformable Pyramid Reception","summaries":"Semantic Segmentation plays a pivotal role in many applications related to\nmedical image and video analysis. However, designing a neural network\narchitecture for medical image and surgical video segmentation is challenging\ndue to the diverse features of relevant classes, including heterogeneity,\ndeformability, transparency, blunt boundaries, and various distortions. We\npropose a network architecture, DeepPyramid+, which addresses diverse\nchallenges encountered in medical image and surgical video segmentation. The\nproposed DeepPyramid+ incorporates two major modules, namely \"Pyramid View\nFusion\" (PVF) and \"Deformable Pyramid Reception,\" (DPR), to address the\noutlined challenges. PVF replicates a deduction process within the neural\nnetwork, aligning with the human visual system, thereby enhancing the\nrepresentation of relative information at each pixel position. Complementarily,\nDPR introduces shape- and scale-adaptive feature extraction techniques using\ndilated deformable convolutions, enhancing accuracy and robustness in handling\nheterogeneous classes and deformable shapes. Extensive experiments conducted on\ndiverse datasets, including endometriosis videos, MRI images, OCT scans, and\ncataract and laparoscopy videos, demonstrate the effectiveness of DeepPyramid+\nin handling various challenges such as shape and scale variation, reflection,\nand blur degradation. DeepPyramid+ demonstrates significant improvements in\nsegmentation performance, achieving up to a 3.65% increase in Dice coefficient\nfor intra-domain segmentation and up to a 17% increase in Dice coefficient for\ncross-domain segmentation. DeepPyramid+ consistently outperforms\nstate-of-the-art networks across diverse modalities considering different\nbackbone networks, showcasing its versatility.","terms":["cs.CV"]},{"titles":"Open-sourced Data Ecosystem in Autonomous Driving: the Present and Future","summaries":"With the continuous maturation and application of autonomous driving\ntechnology, a systematic examination of open-source autonomous driving datasets\nbecomes instrumental in fostering the robust evolution of the industry\necosystem. Current autonomous driving datasets can broadly be categorized into\ntwo generations. The first-generation autonomous driving datasets are\ncharacterized by relatively simpler sensor modalities, smaller data scale, and\nis limited to perception-level tasks. KITTI, introduced in 2012, serves as a\nprominent representative of this initial wave. In contrast, the\nsecond-generation datasets exhibit heightened complexity in sensor modalities,\ngreater data scale and diversity, and an expansion of tasks from perception to\nencompass prediction and control. Leading examples of the second generation\ninclude nuScenes and Waymo, introduced around 2019. This comprehensive review,\nconducted in collaboration with esteemed colleagues from both academia and\nindustry, systematically assesses over seventy open-source autonomous driving\ndatasets from domestic and international sources. It offers insights into\nvarious aspects, such as the principles underlying the creation of high-quality\ndatasets, the pivotal role of data engine systems, and the utilization of\ngenerative foundation models to facilitate scalable data generation.\nFurthermore, this review undertakes an exhaustive analysis and discourse\nregarding the characteristics and data scales that future third-generation\nautonomous driving datasets should possess. It also delves into the scientific\nand technical challenges that warrant resolution. These endeavors are pivotal\nin advancing autonomous innovation and fostering technological enhancement in\ncritical domains. For further details, please refer to\nhttps:\/\/github.com\/OpenDriveLab\/DriveAGI.","terms":["cs.CV"]},{"titles":"Coherent Soft Imitation Learning","summaries":"Imitation learning methods seek to learn from an expert either through\nbehavioral cloning (BC) of the policy or inverse reinforcement learning (IRL)\nof the reward. Such methods enable agents to learn complex tasks from humans\nthat are difficult to capture with hand-designed reward functions. Choosing BC\nor IRL for imitation depends on the quality and state-action coverage of the\ndemonstrations, as well as additional access to the Markov decision process.\nHybrid strategies that combine BC and IRL are not common, as initial policy\noptimization against inaccurate rewards diminishes the benefit of pretraining\nthe policy with BC. This work derives an imitation method that captures the\nstrengths of both BC and IRL. In the entropy-regularized ('soft') reinforcement\nlearning setting, we show that the behaviour-cloned policy can be used as both\na shaped reward and a critic hypothesis space by inverting the regularized\npolicy update. This coherency facilitates fine-tuning cloned policies using the\nreward estimate and additional interactions with the environment. This approach\nconveniently achieves imitation learning through initial behaviour cloning,\nfollowed by refinement via RL with online or offline data sources. The\nsimplicity of the approach enables graceful scaling to high-dimensional and\nvision-based tasks, with stable learning and minimal hyperparameter tuning, in\ncontrast to adversarial approaches. For the open-source implementation and\nsimulation results, see https:\/\/joemwatson.github.io\/csil\/.","terms":["cs.LG"]},{"titles":"Generalized Contrastive Divergence: Joint Training of Energy-Based Model and Diffusion Model through Inverse Reinforcement Learning","summaries":"We present Generalized Contrastive Divergence (GCD), a novel objective\nfunction for training an energy-based model (EBM) and a sampler simultaneously.\nGCD generalizes Contrastive Divergence (Hinton, 2002), a celebrated algorithm\nfor training EBM, by replacing Markov Chain Monte Carlo (MCMC) distribution\nwith a trainable sampler, such as a diffusion model. In GCD, the joint training\nof EBM and a diffusion model is formulated as a minimax problem, which reaches\nan equilibrium when both models converge to the data distribution. The minimax\nlearning with GCD bears interesting equivalence to inverse reinforcement\nlearning, where the energy corresponds to a negative reward, the diffusion\nmodel is a policy, and the real data is expert demonstrations. We present\npreliminary yet promising results showing that joint training is beneficial for\nboth EBM and a diffusion model. GCD enables EBM training without MCMC while\nimproving the sample quality of a diffusion model.","terms":["cs.LG","cs.AI"]},{"titles":"Action Scene Graphs for Long-Form Understanding of Egocentric Videos","summaries":"We present Egocentric Action Scene Graphs (EASGs), a new representation for\nlong-form understanding of egocentric videos. EASGs extend standard\nmanually-annotated representations of egocentric videos, such as verb-noun\naction labels, by providing a temporally evolving graph-based description of\nthe actions performed by the camera wearer, including interacted objects, their\nrelationships, and how actions unfold in time. Through a novel annotation\nprocedure, we extend the Ego4D dataset by adding manually labeled Egocentric\nAction Scene Graphs offering a rich set of annotations designed for long-from\negocentric video understanding. We hence define the EASG generation task and\nprovide a baseline approach, establishing preliminary benchmarks. Experiments\non two downstream tasks, egocentric action anticipation and egocentric activity\nsummarization, highlight the effectiveness of EASGs for long-form egocentric\nvideo understanding. We will release the dataset and the code to replicate\nexperiments and annotations.","terms":["cs.CV"]},{"titles":"Novel class discovery meets foundation models for 3D semantic segmentation","summaries":"The task of Novel Class Discovery (NCD) in semantic segmentation entails\ntraining a model able to accurately segment unlabelled (novel) classes, relying\non the available supervision from annotated (base) classes. Although\nextensively investigated in 2D image data, the extension of the NCD task to the\ndomain of 3D point clouds represents a pioneering effort, characterized by\nassumptions and challenges that are not present in the 2D case. This paper\nrepresents an advancement in the analysis of point cloud data in four\ndirections. Firstly, it introduces the novel task of NCD for point cloud\nsemantic segmentation. Secondly, it demonstrates that directly transposing the\nonly existing NCD method for 2D image semantic segmentation to 3D data yields\nsuboptimal results. Thirdly, a new NCD approach based on online clustering,\nuncertainty estimation, and semantic distillation is presented. Lastly, a novel\nevaluation protocol is proposed to rigorously assess the performance of NCD in\npoint cloud semantic segmentation. Through comprehensive evaluations on the\nSemanticKITTI, SemanticPOSS, and S3DIS datasets, the paper demonstrates\nsubstantial superiority of the proposed method over the considered baselines.","terms":["cs.CV"]},{"titles":"An Infinite-Width Analysis on the Jacobian-Regularised Training of a Neural Network","summaries":"The recent theoretical analysis of deep neural networks in their\ninfinite-width limits has deepened our understanding of initialisation, feature\nlearning, and training of those networks, and brought new practical techniques\nfor finding appropriate hyperparameters, learning network weights, and\nperforming inference. In this paper, we broaden this line of research by\nshowing that this infinite-width analysis can be extended to the Jacobian of a\ndeep neural network. We show that a multilayer perceptron (MLP) and its\nJacobian at initialisation jointly converge to a Gaussian process (GP) as the\nwidths of the MLP's hidden layers go to infinity and characterise this GP. We\nalso prove that in the infinite-width limit, the evolution of the MLP under the\nso-called robust training (i.e., training with a regulariser on the Jacobian)\nis described by a linear first-order ordinary differential equation that is\ndetermined by a variant of the Neural Tangent Kernel. We experimentally show\nthe relevance of our theoretical claims to wide finite networks, and\nempirically analyse the properties of kernel regression solution to obtain an\ninsight into Jacobian regularisation.","terms":["cs.LG","stat.ML"]},{"titles":"Technical Report on Subspace Pyramid Fusion Network for Semantic Segmentation","summaries":"The following is a technical report to test the validity of the proposed\nSubspace Pyramid Fusion Module (SPFM) to capture multi-scale feature\nrepresentations, which is more useful for semantic segmentation. In this\ninvestigation, we have proposed the Efficient Shuffle Attention Module(ESAM) to\nreconstruct the skip-connections paths by fusing multi-level global context\nfeatures. Experimental results on two well-known semantic segmentation\ndatasets, including Camvid and Cityscapes, show the effectiveness of our\nproposed method.","terms":["cs.CV","cs.AI"]},{"titles":"Lite-Mind: Towards Efficient and Versatile Brain Representation Network","summaries":"Research in decoding visual information from the brain, particularly through\nthe non-invasive fMRI method, is rapidly progressing. The challenge arises from\nthe limited data availability and the low signal-to-noise ratio of fMRI\nsignals, leading to a low-precision task of fMRI-to-image retrieval.\nState-of-the-art MindEye remarkably improves fMRI-to-image retrieval\nperformance by leveraging a deep MLP with a high parameter count orders of\nmagnitude, i.e., a 996M MLP Backbone per subject, to align fMRI embeddings to\nthe final hidden layer of CLIP's vision transformer. However, significant\nindividual variations exist among subjects, even within identical experimental\nsetups, mandating the training of subject-specific models. The substantial\nparameters pose significant challenges in deploying fMRI decoding on practical\ndevices, especially with the necessitating of specific models for each subject.\nTo this end, we propose Lite-Mind, a lightweight, efficient, and versatile\nbrain representation network based on discrete Fourier transform, that\nefficiently aligns fMRI voxels to fine-grained information of CLIP. Our\nexperiments demonstrate that Lite-Mind achieves an impressive 94.3%\nfMRI-to-image retrieval accuracy on the NSD dataset for Subject 1, with 98.7%\nfewer parameters than MindEye. Lite-Mind is also proven to be able to be\nmigrated to smaller brain datasets and establishes a new state-of-the-art for\nzero-shot classification on the GOD dataset. The code is available at\nhttps:\/\/github.com\/gongzix\/Lite-Mind.","terms":["cs.CV","cs.AI"]},{"titles":"Riemannian Complex Matrix Convolution Network for PolSAR Image Classification","summaries":"Recently, deep learning methods have achieved superior performance for\nPolarimetric Synthetic Aperture Radar(PolSAR) image classification. Existing\ndeep learning methods learn PolSAR data by converting the covariance matrix\ninto a feature vector or complex-valued vector as the input. However, all these\nmethods cannot learn the structure of complex matrix directly and destroy the\nchannel correlation. To learn geometric structure of complex matrix, we propose\na Riemannian complex matrix convolution network for PolSAR image classification\nin Riemannian space for the first time, which directly utilizes the complex\nmatrix as the network input and defines the Riemannian operations to learn\ncomplex matrix's features. The proposed Riemannian complex matrix convolution\nnetwork considers PolSAR complex matrix endowed in Riemannian manifold, and\ndefines a series of new Riemannian convolution, ReLu and LogEig operations in\nRiemannian space, which breaks through the Euclidean constraint of conventional\nnetworks. Then, a CNN module is appended to enhance contextual Riemannian\nfeatures. Besides, a fast kernel learning method is developed for the proposed\nmethod to learn class-specific features and reduce the computation time\neffectively. Experiments are conducted on three sets of real PolSAR data with\ndifferent bands and sensors. Experiments results demonstrates the proposed\nmethod can obtain superior performance than the state-of-the-art methods.","terms":["cs.CV"]},{"titles":"Few-shot Hybrid Domain Adaptation of Image Generators","summaries":"Can a pre-trained generator be adapted to the hybrid of multiple target\ndomains and generate images with integrated attributes of them? In this work,\nwe introduce a new task -- Few-shot Hybrid Domain Adaptation (HDA). Given a\nsource generator and several target domains, HDA aims to acquire an adapted\ngenerator that preserves the integrated attributes of all target domains,\nwithout overriding the source domain's characteristics. Compared with Domain\nAdaptation (DA), HDA offers greater flexibility and versatility to adapt\ngenerators to more composite and expansive domains. Simultaneously, HDA also\npresents more challenges than DA as we have access only to images from\nindividual target domains and lack authentic images from the hybrid domain. To\naddress this issue, we introduce a discriminator-free framework that directly\nencodes different domains' images into well-separable subspaces. To achieve\nHDA, we propose a novel directional subspace loss comprised of a distance loss\nand a direction loss. Concretely, the distance loss blends the attributes of\nall target domains by reducing the distances from generated images to all\ntarget subspaces. The direction loss preserves the characteristics from the\nsource domain by guiding the adaptation along the perpendicular to subspaces.\nExperiments show that our method can obtain numerous domain-specific attributes\nin a single adapted generator, which surpasses the baseline methods in semantic\nsimilarity, image fidelity, and cross-domain consistency.","terms":["cs.CV","cs.AI"]},{"titles":"Physics Informed Neural Networks for Simulating Radiative Transfer","summaries":"We propose a novel machine learning algorithm for simulating radiative\ntransfer. Our algorithm is based on physics informed neural networks (PINNs),\nwhich are trained by minimizing the residual of the underlying radiative\ntranfer equations. We present extensive experiments and theoretical error\nestimates to demonstrate that PINNs provide a very easy to implement, fast,\nrobust and accurate method for simulating radiative transfer. We also present a\nPINN based algorithm for simulating inverse problems for radiative transfer\nefficiently.","terms":["cs.LG","stat.ML"]},{"titles":"Evaluating the point cloud of individual trees generated from images based on Neural Radiance fields (NeRF) method","summaries":"Three-dimensional (3D) reconstruction of trees has always been a key task in\nprecision forestry management and research. Due to the complex branch\nmorphological structure of trees themselves and the occlusions from tree stems,\nbranches and foliage, it is difficult to recreate a complete three-dimensional\ntree model from a two-dimensional image by conventional photogrammetric\nmethods. In this study, based on tree images collected by various cameras in\ndifferent ways, the Neural Radiance Fields (NeRF) method was used for\nindividual tree reconstruction and the exported point cloud models are compared\nwith point cloud derived from photogrammetric reconstruction and laser scanning\nmethods. The results show that the NeRF method performs well in individual tree\n3D reconstruction, as it has higher successful reconstruction rate, better\nreconstruction in the canopy area, it requires less amount of images as input.\nCompared with photogrammetric reconstruction method, NeRF has significant\nadvantages in reconstruction efficiency and is adaptable to complex scenes, but\nthe generated point cloud tends to be noisy and low resolution. The accuracy of\ntree structural parameters (tree height and diameter at breast height)\nextracted from the photogrammetric point cloud is still higher than those of\nderived from the NeRF point cloud. The results of this study illustrate the\ngreat potential of NeRF method for individual tree reconstruction, and it\nprovides new ideas and research directions for 3D reconstruction and\nvisualization of complex forest scenes.","terms":["cs.CV"]},{"titles":"A General Framework for Sequential Decision-Making under Adaptivity Constraints","summaries":"We take the first step in studying general sequential decision-making under\ntwo adaptivity constraints: rare policy switch and batch learning. First, we\nprovide a general class called the Eluder Condition class, which includes a\nwide range of reinforcement learning classes. Then, for the rare policy switch\nconstraint, we provide a generic algorithm to achieve a\n$\\widetilde{\\mathcal{O}}(\\log K) $ switching cost with a\n$\\widetilde{\\mathcal{O}}(\\sqrt{K})$ regret on the EC class. For the batch\nlearning constraint, we provide an algorithm that provides a\n$\\widetilde{\\mathcal{O}}(\\sqrt{K}+K\/B)$ regret with the number of batches $B.$\nThis paper is the first work considering rare policy switch and batch learning\nunder general function classes, which covers nearly all the models studied in\nthe previous works such as tabular MDP (Bai et al. 2019; Zhang et al. 2020),\nlinear MDP (Wang et al. 2021; Gao et al. 2021), low eluder dimension MDP (Kong\net al. 2021; Gao et al. 2021), generalized linear function approximation (Qiao\net al. 2023), and also some new classes such as the low $D_\\Delta$-type Bellman\neluder dimension problem, linear mixture MDP, kernelized nonlinear regulator\nand undercomplete partially observed Markov decision process (POMDP).","terms":["cs.LG","cs.AI"]},{"titles":"All the World's a (Hyper)Graph: A Data Drama","summaries":"We introduce Hyperbard, a dataset of diverse relational data representations\nderived from Shakespeare's plays. Our representations range from simple graphs\ncapturing character co-occurrence in single scenes to hypergraphs encoding\ncomplex communication settings and character contributions as hyperedges with\nedge-specific node weights. By making multiple intuitive representations\nreadily available for experimentation, we facilitate rigorous representation\nrobustness checks in graph learning, graph mining, and network analysis,\nhighlighting the advantages and drawbacks of specific representations.\nLeveraging the data released in Hyperbard, we demonstrate that many solutions\nto popular graph mining problems are highly dependent on the representation\nchoice, thus calling current graph curation practices into question. As an\nhomage to our data source, and asserting that science can also be art, we\npresent all our points in the form of a play.","terms":["cs.LG","cs.CL","cs.CY","cs.SI"]},{"titles":"RING-NeRF: A Versatile Architecture based on Residual Implicit Neural Grids","summaries":"Since their introduction, Neural Fields have become very popular for 3D\nreconstruction and new view synthesis. Recent researches focused on\naccelerating the process, as well as improving the robustness to variation of\nthe observation distance and limited number of supervised viewpoints. However,\nthose approaches often led to dedicated solutions that cannot be easily\ncombined. To tackle this issue, we introduce a new simple but efficient\narchitecture named RING-NeRF, based on Residual Implicit Neural Grids, that\nprovides a control on the level of detail of the mapping function between the\nscene and the latent spaces. Associated with a distance-aware forward mapping\nmechanism and a continuous coarse-to-fine reconstruction process, our versatile\narchitecture demonstrates both fast training and state-of-the-art performances\nin terms of: (1) anti-aliased rendering, (2) reconstruction quality from few\nsupervised viewpoints, and (3) robustness in the absence of appropriate\nscene-specific initialization for SDF-based NeRFs. We also demonstrate that our\narchitecture can dynamically add grids to increase the details of the\nreconstruction, opening the way to adaptive reconstruction.","terms":["cs.CV"]},{"titles":"On the variants of SVM methods applied to GPR data to classify tack coat characteristics in French pavements: two experimental case studies","summaries":"Among the commonly used non-destructive techniques, the Ground Penetrating\nRadar (GPR) is one of the most widely adopted today for assessing pavement\nconditions in France. However, conventional radar systems and their forward\nprocessing methods have shown their limitations for the physical and\ngeometrical characterization of very thin layers such as tack coats. However,\nthe use of Machine Learning methods applied to GPR with an inverse approach\nshowed that it was numerically possible to identify the tack coat\ncharacteristics despite masking effects due to low timefrequency resolution\nnoted in the raw B-scans. Thus, we propose in this paper to apply the inverse\napproach based on Machine Learning, already validated in previous works on\nnumerical data, on two experimental cases with different pavement structures.\nThe first case corresponds to a validation on known pavement structures on the\nGustave Eiffel University (Nantes, France) with its pavement fatigue carousel\nand the second case focuses on a new real road in Vend{\\'e}e department\n(France). In both case studies, the performances of SVM\/SVR methods showed the\nefficiency of supervised learning methods to classify and estimate the emulsion\nproportioning in the tack coats.","terms":["stat.ML","cs.LG"]},{"titles":"PointMoment:Mixed-Moment-based Self-Supervised Representation Learning for 3D Point Clouds","summaries":"Large and rich data is a prerequisite for effective training of deep neural\nnetworks. However, the irregularity of point cloud data makes manual annotation\ntime-consuming and laborious. Self-supervised representation learning, which\nleverages the intrinsic structure of large-scale unlabelled data to learn\nmeaningful feature representations, has attracted increasing attention in the\nfield of point cloud research. However, self-supervised representation learning\noften suffers from model collapse, resulting in reduced information and\ndiversity of the learned representation, and consequently degrading the\nperformance of downstream tasks. To address this problem, we propose\nPointMoment, a novel framework for point cloud self-supervised representation\nlearning that utilizes a high-order mixed moment loss function rather than the\nconventional contrastive loss function. Moreover, our framework does not\nrequire any special techniques such as asymmetric network architectures,\ngradient stopping, etc. Specifically, we calculate the high-order mixed moment\nof the feature variables and force them to decompose into products of their\nindividual moment, thereby making multiple variables more independent and\nminimizing the feature redundancy. We also incorporate a contrastive learning\napproach to maximize the feature invariance under different data augmentations\nof the same point cloud. Experimental results show that our approach\noutperforms previous unsupervised learning methods on the downstream task of 3D\npoint cloud classification and segmentation.","terms":["cs.CV"]},{"titles":"Predicting the Transportation Activities of Construction Waste Hauling Trucks: An Input-Output Hidden Markov Approach","summaries":"Construction waste hauling trucks (CWHTs), as one of the most commonly seen\nheavy-duty vehicles in major cities around the globe, are usually subject to a\nseries of regulations and spatial-temporal access restrictions because they not\nonly produce significant NOx and PM emissions but also causes on-road fugitive\ndust. The timely and accurate prediction of CWHTs' destinations and dwell times\nplay a key role in effective environmental management. To address this\nchallenge, we propose a prediction method based on an interpretable\nactivity-based model, input-output hidden Markov model (IOHMM), and validate it\non 300 CWHTs in Chengdu, China. Contextual factors are considered in the model\nto improve its prediction power. Results show that the IOHMM outperforms\nseveral baseline models, including Markov chains, linear regression, and long\nshort-term memory. Factors influencing the predictability of CWHTs'\ntransportation activities are also explored using linear regression models.\nResults suggest the proposed model holds promise in assisting authorities by\npredicting the upcoming transportation activities of CWHTs and administering\nintervention in a timely and effective manner.","terms":["cs.LG"]},{"titles":"Interpretable Mechanistic Representations for Meal-level Glycemic Control in the Wild","summaries":"Diabetes encompasses a complex landscape of glycemic control that varies\nwidely among individuals. However, current methods do not faithfully capture\nthis variability at the meal level. On the one hand, expert-crafted features\nlack the flexibility of data-driven methods; on the other hand, learned\nrepresentations tend to be uninterpretable which hampers clinical adoption. In\nthis paper, we propose a hybrid variational autoencoder to learn interpretable\nrepresentations of CGM and meal data. Our method grounds the latent space to\nthe inputs of a mechanistic differential equation, producing embeddings that\nreflect physiological quantities, such as insulin sensitivity, glucose\neffectiveness, and basal glucose levels. Moreover, we introduce a novel method\nto infer the glucose appearance rate, making the mechanistic model robust to\nunreliable meal logs. On a dataset of CGM and self-reported meals from\nindividuals with type-2 diabetes and pre-diabetes, our unsupervised\nrepresentation discovers a separation between individuals proportional to their\ndisease severity. Our embeddings produce clusters that are up to 4x better than\nnaive, expert, black-box, and pure mechanistic features. Our method provides a\nnuanced, yet interpretable, embedding space to compare glycemic control within\nand across individuals, directly learnable from in-the-wild data.","terms":["cs.LG","math.DS","stat.AP","stat.ML"]},{"titles":"Evaluating Point Cloud from Moving Camera Videos: A No-Reference Metric","summaries":"Point cloud is one of the most widely used digital representation formats for\nthree-dimensional (3D) contents, the visual quality of which may suffer from\nnoise and geometric shift distortions during the production procedure as well\nas compression and downsampling distortions during the transmission process. To\ntackle the challenge of point cloud quality assessment (PCQA), many PCQA\nmethods have been proposed to evaluate the visual quality levels of point\nclouds by assessing the rendered static 2D projections. Although such\nprojection-based PCQA methods achieve competitive performance with the\nassistance of mature image quality assessment (IQA) methods, they neglect that\nthe 3D model is also perceived in a dynamic viewing manner, where the viewpoint\nis continually changed according to the feedback of the rendering device.\nTherefore, in this paper, we evaluate the point clouds from moving camera\nvideos and explore the way of dealing with PCQA tasks via using video quality\nassessment (VQA) methods. First, we generate the captured videos by rotating\nthe camera around the point clouds through several circular pathways. Then we\nextract both spatial and temporal quality-aware features from the selected key\nframes and the video clips through using trainable 2D-CNN and pre-trained\n3D-CNN models respectively. Finally, the visual quality of point clouds is\nrepresented by the video quality values. The experimental results reveal that\nthe proposed method is effective for predicting the visual quality levels of\nthe point clouds and even competitive with full-reference (FR) PCQA methods.\nThe ablation studies further verify the rationality of the proposed framework\nand confirm the contributions made by the quality-aware features extracted via\nthe dynamic viewing manner. The code is available at\nhttps:\/\/github.com\/zzc-1998\/VQA_PC.","terms":["cs.CV","eess.IV"]},{"titles":"Online Vectorized HD Map Construction using Geometry","summaries":"The construction of online vectorized High-Definition (HD) maps is critical\nfor downstream prediction and planning. Recent efforts have built strong\nbaselines for this task, however, shapes and relations of instances in urban\nroad systems are still under-explored, such as parallelism, perpendicular, or\nrectangle-shape. In our work, we propose GeMap ($\\textbf{Ge}$ometry\n$\\textbf{Map}$), which end-to-end learns Euclidean shapes and relations of map\ninstances beyond basic perception. Specifically, we design a geometric loss\nbased on angle and distance clues, which is robust to rigid transformations. We\nalso decouple self-attention to independently handle Euclidean shapes and\nrelations. Our method achieves new state-of-the-art performance on the NuScenes\nand Argoverse 2 datasets. Remarkably, it reaches a 71.8% mAP on the large-scale\nArgoverse 2 dataset, outperforming MapTR V2 by +4.4% and surpassing the 70% mAP\nthreshold for the first time. Code is available at\nhttps:\/\/github.com\/cnzzx\/GeMap","terms":["cs.CV","cs.AI"]},{"titles":"PointJEM: Self-supervised Point Cloud Understanding for Reducing Feature Redundancy via Joint Entropy Maximization","summaries":"Most deep learning-based point cloud processing methods are supervised and\nrequire large scale of labeled data. However, manual labeling of point cloud\ndata is laborious and time-consuming. Self-supervised representation learning\ncan address the aforementioned issue by learning robust and generalized\nrepresentations from unlabeled datasets. Nevertheless, the embedded features\nobtained by representation learning usually contain redundant information, and\nmost current methods reduce feature redundancy by linear correlation\nconstraints. In this paper, we propose PointJEM, a self-supervised\nrepresentation learning method applied to the point cloud field. PointJEM\ncomprises an embedding scheme and a loss function based on joint entropy. The\nembedding scheme divides the embedding vector into different parts, each part\ncan learn a distinctive feature. To reduce redundant information in the\nfeatures, PointJEM maximizes the joint entropy between the different parts,\nthereby rendering the learned feature variables pairwise independent. To\nvalidate the effectiveness of our method, we conducted experiments on multiple\ndatasets. The results demonstrate that our method can significantly reduce\nfeature redundancy beyond linear correlation. Furthermore, PointJEM achieves\ncompetitive performance in downstream tasks such as classification and\nsegmentation.","terms":["cs.CV"]},{"titles":"Towards Learning a Generalist Model for Embodied Navigation","summaries":"Building a generalist agent that can interact with the world is the\nintriguing target of AI systems, thus spurring the research for embodied\nnavigation, where an agent is required to navigate according to instructions or\nrespond to queries. Despite the major progress attained, previous works\nprimarily focus on task-specific agents and lack generalizability to unseen\nscenarios. Recently, LLMs have presented remarkable capabilities across various\nfields, and provided a promising opportunity for embodied navigation. Drawing\non this, we propose the first generalist model for embodied navigation,\nNaviLLM. It adapts LLMs to embodied navigation by introducing schema-based\ninstruction. The schema-based instruction flexibly casts various tasks into\ngeneration problems, thereby unifying a wide range of tasks. This approach\nallows us to integrate diverse data sources from various datasets into the\ntraining, equipping NaviLLM with a wide range of capabilities required by\nembodied navigation. We conduct extensive experiments to evaluate the\nperformance and generalizability of our model. The experimental results\ndemonstrate that our unified model achieves state-of-the-art performance on\nCVDN, SOON, and ScanQA. Specifically, it surpasses the previous\nstats-of-the-art method by a significant margin of 29% in goal progress on\nCVDN. Moreover, our model also demonstrates strong generalizability and\npresents impressive results on unseen tasks, e.g., embodied question answering\nand 3D captioning.","terms":["cs.CV","cs.AI"]},{"titles":"$\\textbf{A}^2\\textbf{CiD}^2$: Accelerating Asynchronous Communication in Decentralized Deep Learning","summaries":"Distributed training of Deep Learning models has been critical to many recent\nsuccesses in the field. Current standard methods primarily rely on synchronous\ncentralized algorithms which induce major communication bottlenecks and\nsynchronization locks at scale. Decentralized asynchronous algorithms are\nemerging as a potential alternative but their practical applicability still\nlags. In order to mitigate the increase in communication cost that naturally\ncomes with scaling the number of workers, we introduce a principled\nasynchronous, randomized, gossip-based optimization algorithm which works\nthanks to a continuous local momentum named $\\textbf{A}^2\\textbf{CiD}^2$. Our\nmethod allows each worker to continuously process mini-batches without\nstopping, and run a peer-to-peer averaging routine in parallel, reducing idle\ntime. In addition to inducing a significant communication acceleration at no\ncost other than adding a local momentum variable, minimal adaptation is\nrequired to incorporate $\\textbf{A}^2\\textbf{CiD}^2$ to standard asynchronous\napproaches. Our theoretical analysis proves accelerated rates compared to\nprevious asynchronous decentralized baselines and we empirically show that\nusing our $\\textbf{A}^2\\textbf{CiD}^2$ momentum significantly decrease\ncommunication costs in poorly connected networks. In particular, we show\nconsistent improvement on the ImageNet dataset using up to 64 asynchronous\nworkers (A100 GPUs) and various communication network topologies.","terms":["cs.LG","cs.AI","cs.DC"]},{"titles":"From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces","summaries":"We provide a novel reduction from swap-regret minimization to external-regret\nminimization, which improves upon the classical reductions of Blum-Mansour\n[BM07] and Stolz-Lugosi [SL05] in that it does not require finiteness of the\nspace of actions. We show that, whenever there exists a no-external-regret\nalgorithm for some hypothesis class, there must also exist a no-swap-regret\nalgorithm for that same class. For the problem of learning with expert advice,\nour result implies that it is possible to guarantee that the swap regret is\nbounded by {\\epsilon} after $\\log(N)^{O(1\/\\epsilon)}$ rounds and with $O(N)$\nper iteration complexity, where $N$ is the number of experts, while the\nclassical reductions of Blum-Mansour and Stolz-Lugosi require $O(N\/\\epsilon^2)$\nrounds and at least $\\Omega(N^2)$ per iteration complexity. Our result comes\nwith an associated lower bound, which -- in contrast to that in [BM07] -- holds\nfor oblivious and $\\ell_1$-constrained adversaries and learners that can employ\ndistributions over experts, showing that the number of rounds must be\n$\\tilde\\Omega(N\/\\epsilon^2)$ or exponential in $1\/\\epsilon$.\n  Our reduction implies that, if no-regret learning is possible in some game,\nthen this game must have approximate correlated equilibria, of arbitrarily good\napproximation. This strengthens the folklore implication of no-regret learning\nthat approximate coarse correlated equilibria exist. Importantly, it provides a\nsufficient condition for the existence of correlated equilibrium which vastly\nextends the requirement that the action set is finite, thus answering a\nquestion left open by [DG22; Ass+23]. Moreover, it answers several outstanding\nquestions about equilibrium computation and learning in games.","terms":["cs.LG","cs.AI","cs.GT"]},{"titles":"Optimizing rgb-d semantic segmentation through multi-modal interaction and pooling attention","summaries":"Semantic segmentation of RGB-D images involves understanding the appearance\nand spatial relationships of objects within a scene, which requires careful\nconsideration of various factors. However, in indoor environments, the simple\ninput of RGB and depth images often results in a relatively limited acquisition\nof semantic and spatial information, leading to suboptimal segmentation\noutcomes. To address this, we propose the Multi-modal Interaction and Pooling\nAttention Network (MIPANet), a novel approach designed to harness the\ninteractive synergy between RGB and depth modalities, optimizing the\nutilization of complementary information. Specifically, we incorporate a\nMulti-modal Interaction Fusion Module (MIM) into the deepest layers of the\nnetwork. This module is engineered to facilitate the fusion of RGB and depth\ninformation, allowing for mutual enhancement and correction. Additionally, we\nintroduce a Pooling Attention Module (PAM) at various stages of the encoder.\nThis module serves to amplify the features extracted by the network and\nintegrates the module's output into the decoder in a targeted manner,\nsignificantly improving semantic segmentation performance. Our experimental\nresults demonstrate that MIPANet outperforms existing methods on two indoor\nscene datasets, NYUDv2 and SUN-RGBD, underscoring its effectiveness in\nenhancing RGB-D semantic segmentation.","terms":["cs.CV"]},{"titles":"Building Category Graphs Representation with Spatial and Temporal Attention for Visual Navigation","summaries":"Given an object of interest, visual navigation aims to reach the object's\nlocation based on a sequence of partial observations. To this end, an agent\nneeds to 1) learn a piece of certain knowledge about the relations of object\ncategories in the world during training and 2) look for the target object based\non the pre-learned object category relations and its moving trajectory in the\ncurrent unseen environment. In this paper, we propose a Category Relation Graph\n(CRG) to learn the knowledge of object category layout relations and a\nTemporal-Spatial-Region (TSR) attention architecture to perceive the long-term\nspatial-temporal dependencies of objects helping the navigation. We learn prior\nknowledge of object layout, establishing a category relationship graph to\ndeduce the positions of specific objects. Subsequently, we introduced TSR to\ncapture the relationships of objects in temporal, spatial, and regions within\nthe observation trajectories. Specifically, we propose a Temporal attention\nmodule (T) to model the temporal structure of the observation sequence, which\nimplicitly encodes the historical moving or trajectory information. Then, a\nSpatial attention module (S) is used to uncover the spatial context of the\ncurrent observation objects based on the category relation graph and past\nobservations. Last, a Region attention module (R) shifts the attention to the\ntarget-relevant region. Based on the visual representation extracted by our\nmethod, the agent can better perceive the environment and easily learn superior\nnavigation policy. Experiments on AI2-THOR demonstrate our CRG-TSR method\nsignificantly outperforms existing methods regarding both effectiveness and\nefficiency. The code has been included in the supplementary material and will\nbe publicly available.","terms":["cs.CV"]},{"titles":"GCFA:Geodesic Curve Feature Augmentation via Shape Space Theory","summaries":"Deep learning has yielded remarkable outcomes in various domains. However,\nthe challenge of requiring large-scale labeled samples still persists in deep\nlearning. Thus, data augmentation has been introduced as a critical strategy to\ntrain deep learning models. However, data augmentation suffers from information\nloss and poor performance in small sample environments. To overcome these\ndrawbacks, we propose a feature augmentation method based on shape space\ntheory, i.e., Geodesic curve feature augmentation, called GCFA in brevity.\nFirst, we extract features from the image with the neural network model. Then,\nthe multiple image features are projected into a pre-shape space as features.\nIn the pre-shape space, a Geodesic curve is built to fit the features. Finally,\nthe many generated features on the Geodesic curve are used to train the various\nmachine learning models. The GCFA module can be seamlessly integrated with most\nmachine learning methods. And the proposed method is simple, effective and\ninsensitive for the small sample datasets. Several examples demonstrate that\nthe GCFA method can greatly improve the performance of the data preprocessing\nmodel in a small sample environment.","terms":["cs.CV","cs.LG"]},{"titles":"KappaFace: Adaptive Additive Angular Margin Loss for Deep Face Recognition","summaries":"Feature learning is a widely used method employed for large-scale face\nrecognition. Recently, large-margin softmax loss methods have demonstrated\nsignificant enhancements on deep face recognition. These methods propose fixed\npositive margins in order to enforce intra-class compactness and inter-class\ndiversity. However, the majority of the proposed methods do not consider the\nclass imbalance issue, which is a major challenge in practice for developing\ndeep face recognition models. We hypothesize that it significantly affects the\ngeneralization ability of the deep face models. Inspired by this observation,\nwe introduce a novel adaptive strategy, called KappaFace, to modulate the\nrelative importance based on class difficultness and imbalance. With the\nsupport of the von Mises-Fisher distribution, our proposed KappaFace loss can\nintensify the margin's magnitude for hard learning or low concentration classes\nwhile relaxing it for counter classes. Experiments conducted on popular facial\nbenchmarks demonstrate that our proposed method achieves superior performance\nto the state-of-the-art.","terms":["cs.CV"]},{"titles":"FastPillars: A Deployment-friendly Pillar-based 3D Detector","summaries":"The deployment of 3D detectors strikes one of the major challenges in\nreal-world self-driving scenarios. Existing BEV-based (i.e., Bird Eye View)\ndetectors favor sparse convolutions (known as SPConv) to speed up training and\ninference, which puts a hard barrier for deployment, especially for on-device\napplications. In this paper, to tackle the challenge of efficient 3D object\ndetection from an industry perspective, we devise a deployment-friendly\npillar-based 3D detector, termed FastPillars. First, we introduce a novel\nlightweight Max-and-Attention Pillar Encoding (MAPE) module specially for\nenhancing small 3D objects. Second, we propose a simple yet effective principle\nfor designing a backbone in pillar-based 3D detection. We construct FastPillars\nbased on these designs, achieving high performance and low latency without\nSPConv. Extensive experiments on two large-scale datasets demonstrate the\neffectiveness and efficiency of FastPillars for on-device 3D detection\nregarding both performance and speed. Specifically, FastPillars delivers\nstate-of-the-art accuracy on Waymo Open Dataset with 1.8X speed up and 3.8\nmAPH\/L2 improvement over CenterPoint (SPConv-based). Our code is publicly\navailable at: https:\/\/github.com\/StiphyJay\/FastPillars.","terms":["cs.CV","cs.RO"]},{"titles":"Background Clustering Pre-training for Few-shot Segmentation","summaries":"Recent few-shot segmentation (FSS) methods introduce an extra pre-training\nstage before meta-training to obtain a stronger backbone, which has become a\nstandard step in few-shot learning. Despite the effectiveness, current\npre-training scheme suffers from the merged background problem: only base\nclasses are labelled as foregrounds, making it hard to distinguish between\nnovel classes and actual background. In this paper, we propose a new\npre-training scheme for FSS via decoupling the novel classes from background,\ncalled Background Clustering Pre-Training (BCPT). Specifically, we adopt online\nclustering to the pixel embeddings of merged background to explore the\nunderlying semantic structures, bridging the gap between pre-training and\nadaptation to novel classes. Given the clustering results, we further propose\nthe background mining loss and leverage base classes to guide the clustering\nprocess, improving the quality and stability of clustering results. Experiments\non PASCAL-5i and COCO-20i show that BCPT yields advanced performance. Code will\nbe available.","terms":["cs.CV"]},{"titles":"DreamPropeller: Supercharge Text-to-3D Generation with Parallel Sampling","summaries":"Recent methods such as Score Distillation Sampling (SDS) and Variational\nScore Distillation (VSD) using 2D diffusion models for text-to-3D generation\nhave demonstrated impressive generation quality. However, the long generation\ntime of such algorithms significantly degrades the user experience. To tackle\nthis problem, we propose DreamPropeller, a drop-in acceleration algorithm that\ncan be wrapped around any existing text-to-3D generation pipeline based on\nscore distillation. Our framework generalizes Picard iterations, a classical\nalgorithm for parallel sampling an ODE path, and can account for non-ODE paths\nsuch as momentum-based gradient updates and changes in dimensions during the\noptimization process as in many cases of 3D generation. We show that our\nalgorithm trades parallel compute for wallclock time and empirically achieves\nup to 4.7x speedup with a negligible drop in generation quality for all tested\nframeworks.","terms":["cs.CV","stat.ML"]},{"titles":"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models","summaries":"Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform\nmultimodal tasks, showing amazing emergent abilities in recent studies, such as\nwriting poems based on an image. However, it is difficult for these case\nstudies to fully reflect the performance of MLLM, lacking a comprehensive\nevaluation. In this paper, we fill in this blank, presenting the first\ncomprehensive MLLM Evaluation benchmark MME. It measures both perception and\ncognition abilities on a total of 14 subtasks. In order to avoid data leakage\nthat may arise from direct use of public datasets for evaluation, the\nannotations of instruction-answer pairs are all manually designed. The concise\ninstruction design allows us to fairly compare MLLMs, instead of struggling in\nprompt engineering. Besides, with such an instruction, we can also easily carry\nout quantitative statistics. A total of 30 advanced MLLMs are comprehensively\nevaluated on our MME, which not only suggests that existing MLLMs still have a\nlarge room for improvement, but also reveals the potential directions for the\nsubsequent model optimization.","terms":["cs.CV"]},{"titles":"Complementary Benefits of Contrastive Learning and Self-Training Under Distribution Shift","summaries":"Self-training and contrastive learning have emerged as leading techniques for\nincorporating unlabeled data, both under distribution shift (unsupervised\ndomain adaptation) and when it is absent (semi-supervised learning). However,\ndespite the popularity and compatibility of these techniques, their efficacy in\ncombination remains unexplored. In this paper, we undertake a systematic\nempirical investigation of this combination, finding that (i) in domain\nadaptation settings, self-training and contrastive learning offer significant\ncomplementary gains; and (ii) in semi-supervised learning settings,\nsurprisingly, the benefits are not synergistic. Across eight distribution shift\ndatasets (e.g., BREEDs, WILDS), we demonstrate that the combined method obtains\n3--8% higher accuracy than either approach independently. We then theoretically\nanalyze these techniques in a simplified model of distribution shift,\ndemonstrating scenarios under which the features produced by contrastive\nlearning can yield a good initialization for self-training to further amplify\ngains and achieve optimal performance, even when either method alone would\nfail.","terms":["cs.LG","cs.CV","stat.ML"]},{"titles":"Optimal Variable Clustering for High-Dimensional Matrix Valued Data","summaries":"Matrix valued data has become increasingly prevalent in many applications.\nMost of the existing clustering methods for this type of data are tailored to\nthe mean model and do not account for the dependence structure of the features,\nwhich can be very informative, especially in high-dimensional settings or when\nmean information is not available. To extract the information from the\ndependence structure for clustering, we propose a new latent variable model for\nthe features arranged in matrix form, with some unknown membership matrices\nrepresenting the clusters for the rows and columns. Under this model, we\nfurther propose a class of hierarchical clustering algorithms using the\ndifference of a weighted covariance matrix as the dissimilarity measure.\nTheoretically, we show that under mild conditions, our algorithm attains\nclustering consistency in the high-dimensional setting. While this consistency\nresult holds for our algorithm with a broad class of weighted covariance\nmatrices, the conditions for this result depend on the choice of the weight. To\ninvestigate how the weight affects the theoretical performance of our\nalgorithm, we establish the minimax lower bound for clustering under our latent\nvariable model in terms of some cluster separation metric. Given these results,\nwe identify the optimal weight in the sense that using this weight guarantees\nour algorithm to be minimax rate-optimal. The practical implementation of our\nalgorithm with the optimal weight is also discussed. Simulation studies show\nthat our algorithm performs better than existing methods in terms of the\nadjusted Rand index (ARI). The method is applied to a genomic dataset and\nyields meaningful interpretations.","terms":["stat.ML","cs.LG","math.ST","stat.ME","stat.TH"]},{"titles":"A Simple and Scalable Graph Neural Network for Large Directed Graphs","summaries":"Node classification is one of the hottest tasks in graph analysis. Though\nexisting studies have explored various node representations in directed and\nundirected graphs, they have overlooked the distinctions of their capabilities\nto capture the information of graphs. To tackle the limitation, we investigate\nvarious combinations of node representations (aggregated features vs. adjacency\nlists) and edge direction awareness within an input graph (directed vs.\nundirected). We address the first empirical study to benchmark the performance\nof various GNNs that use either combination of node representations and edge\ndirection awareness. Our experiments demonstrate that no single combination\nstably achieves state-of-the-art results across datasets, which indicates that\nwe need to select appropriate combinations depending on the dataset\ncharacteristics. In response, we propose a simple yet holistic classification\nmethod A2DUG which leverages all combinations of node representations in\ndirected and undirected graphs. We demonstrate that A2DUG stably performs well\non various datasets and improves the accuracy up to 11.29 compared with the\nstate-of-the-art methods. To spur the development of new methods, we publicly\nrelease our complete codebase under the MIT license.","terms":["cs.LG","cs.SI"]},{"titles":"On the Nystrom Approximation for Preconditioning in Kernel Machines","summaries":"Kernel methods are a popular class of nonlinear predictive models in machine\nlearning. Scalable algorithms for learning kernel models need to be iterative\nin nature, but convergence can be slow due to poor conditioning. Spectral\npreconditioning is an important tool to speed-up the convergence of such\niterative algorithms for training kernel models. However computing and storing\na spectral preconditioner can be expensive which can lead to large\ncomputational and storage overheads, precluding the application of kernel\nmethods to problems with large datasets. A Nystrom approximation of the\nspectral preconditioner is often cheaper to compute and store, and has\ndemonstrated success in practical applications. In this paper we analyze the\ntrade-offs of using such an approximated preconditioner. Specifically, we show\nthat a sample of logarithmic size (as a function of the size of the dataset)\nenables the Nystrom-based approximated preconditioner to accelerate gradient\ndescent nearly as well as the exact preconditioner, while also reducing the\ncomputational and storage overheads.","terms":["stat.ML","cs.LG"]},{"titles":"Benchmarking Continual Learning from Cognitive Perspectives","summaries":"Continual learning addresses the problem of continuously acquiring and\ntransferring knowledge without catastrophic forgetting of old concepts. While\nhumans achieve continual learning via diverse neurocognitive mechanisms, there\nis a mismatch between cognitive properties and evaluation methods of continual\nlearning models. First, the measurement of continual learning models mostly\nrelies on evaluation metrics at a micro-level, which cannot characterize\ncognitive capacities of the model. Second, the measurement is method-specific,\nemphasizing model strengths in one aspect while obscuring potential weaknesses\nin other respects. To address these issues, we propose to integrate model\ncognitive capacities and evaluation metrics into a unified evaluation paradigm.\nWe first characterize model capacities via desiderata derived from cognitive\nproperties supporting human continual learning. The desiderata concern (1)\nadaptability in varying lengths of task sequence; (2) sensitivity to dynamic\ntask variations; and (3) efficiency in memory usage and training time\nconsumption. Then we design evaluation protocols for each desideratum to assess\ncognitive capacities of recent continual learning models. Experimental results\nshow that no method we consider has satisfied all the desiderata and is still\nfar away from realizing truly continual learning. Although some methods exhibit\nsome degree of adaptability and efficiency, no method is able to identify task\nrelationships when encountering dynamic task variations, or achieve a trade-off\nin learning similarities and differences between tasks. Inspired by these\nresults, we discuss possible factors that influence model performance in these\ndesiderata and provide guidance for the improvement of continual learning\nmodels.","terms":["cs.LG","cs.AI"]},{"titles":"Balanced Marginal and Joint Distributional Learning via Mixture Cramer-Wold Distance","summaries":"In the process of training a generative model, it becomes essential to\nmeasure the discrepancy between two high-dimensional probability distributions:\nthe generative distribution and the ground-truth distribution of the observed\ndataset. Recently, there has been growing interest in an approach that involves\nslicing high-dimensional distributions, with the Cramer-Wold distance emerging\nas a promising method. However, we have identified that the Cramer-Wold\ndistance primarily focuses on joint distributional learning, whereas\nunderstanding marginal distributional patterns is crucial for effective\nsynthetic data generation. In this paper, we introduce a novel measure of\ndissimilarity, the mixture Cramer-Wold distance. This measure enables us to\ncapture both marginal and joint distributional information simultaneously, as\nit incorporates a mixture measure with point masses on standard basis vectors.\nBuilding upon the mixture Cramer-Wold distance, we propose a new generative\nmodel called CWDAE (Cramer-Wold Distributional AutoEncoder), which shows\nremarkable performance in generating synthetic data when applied to real\ntabular datasets. Furthermore, our model offers the flexibility to adjust the\nlevel of data privacy with ease.","terms":["stat.ML","cs.LG"]},{"titles":"Epistemic Graph: A Plug-And-Play Module For Hybrid Representation Learning","summaries":"In recent years, deep models have achieved remarkable success in various\nvision tasks. However, their performance heavily relies on large training\ndatasets. In contrast, humans exhibit hybrid learning, seamlessly integrating\nstructured knowledge for cross-domain recognition or relying on a smaller\namount of data samples for few-shot learning. Motivated by this human-like\nepistemic process, we aim to extend hybrid learning to computer vision tasks by\nintegrating structured knowledge with data samples for more effective\nrepresentation learning. Nevertheless, this extension faces significant\nchallenges due to the substantial gap between structured knowledge and deep\nfeatures learned from data samples, encompassing both dimensions and knowledge\ngranularity. In this paper, a novel Epistemic Graph Layer (EGLayer) is\nintroduced to enable hybrid learning, enhancing the exchange of information\nbetween deep features and a structured knowledge graph. Our EGLayer is composed\nof three major parts, including a local graph module, a query aggregation\nmodel, and a novel correlation alignment loss function to emulate human\nepistemic ability. Serving as a plug-and-play module that can replace the\nstandard linear classifier, EGLayer significantly improves the performance of\ndeep models. Extensive experiments demonstrates that EGLayer can greatly\nenhance representation learning for the tasks of cross-domain recognition and\nfew-shot learning, and the visualization of knowledge graphs can aid in model\ninterpretation.","terms":["cs.CV","cs.AI"]},{"titles":"X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion Model","summaries":"We introduce X-Adapter, a universal upgrader to enable the pretrained\nplug-and-play modules (e.g., ControlNet, LoRA) to work directly with the\nupgraded text-to-image diffusion model (e.g., SDXL) without further retraining.\nWe achieve this goal by training an additional network to control the frozen\nupgraded model with the new text-image data pairs. In detail, X-Adapter keeps a\nfrozen copy of the old model to preserve the connectors of different plugins.\nAdditionally, X-Adapter adds trainable mapping layers that bridge the decoders\nfrom models of different versions for feature remapping. The remapped features\nwill be used as guidance for the upgraded model. To enhance the guidance\nability of X-Adapter, we employ a null-text training strategy for the upgraded\nmodel. After training, we also introduce a two-stage denoising strategy to\nalign the initial latents of X-Adapter and the upgraded model. Thanks to our\nstrategies, X-Adapter demonstrates universal compatibility with various plugins\nand also enables plugins of different versions to work together, thereby\nexpanding the functionalities of diffusion community. To verify the\neffectiveness of the proposed method, we conduct extensive experiments and the\nresults show that X-Adapter may facilitate wider application in the upgraded\nfoundational diffusion model.","terms":["cs.CV","cs.AI","cs.MM"]},{"titles":"CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training","summaries":"Recent research on online Gradient Balancing (GraB) has revealed that there\nexist permutation-based example orderings for SGD that are guaranteed to\noutperform random reshuffling (RR). Whereas RR arbitrarily permutes training\nexamples, GraB leverages stale gradients from prior epochs to order examples --\nachieving a provably faster convergence rate than RR. However, GraB is limited\nby design: while it demonstrates an impressive ability to scale-up training on\ncentralized data, it does not naturally extend to modern distributed ML\nworkloads. We therefore propose Coordinated Distributed GraB (CD-GraB), which\nuses insights from prior work on kernel thinning to translate the benefits of\nprovably faster permutation-based example ordering to distributed settings.\nWith negligible overhead, CD-GraB exhibits a linear speedup in convergence rate\nover centralized GraB and outperforms distributed RR on a variety of benchmark\ntasks.","terms":["cs.LG","cs.DC","math.OC"]},{"titles":"DiffPMAE: Diffusion Masked Autoencoders for Point Cloud Reconstruction","summaries":"Point cloud streaming is increasingly getting popular, evolving into the norm\nfor interactive service delivery and the future Metaverse. However, the\nsubstantial volume of data associated with point clouds presents numerous\nchallenges, particularly in terms of high bandwidth consumption and large\nstorage capacity. Despite various solutions proposed thus far, with a focus on\npoint cloud compression, upsampling, and completion, these\nreconstruction-related methods continue to fall short in delivering high\nfidelity point cloud output. As a solution, in DiffPMAE, we propose an\neffective point cloud reconstruction architecture. Inspired by self-supervised\nlearning concepts, we combine Masked Auto-Encoding and Diffusion Model\nmechanism to remotely reconstruct point cloud data. By the nature of this\nreconstruction process, DiffPMAE can be extended to many related downstream\ntasks including point cloud compression, upsampling and completion. Leveraging\nShapeNet-55 and ModelNet datasets with over 60000 objects, we validate the\nperformance of DiffPMAE exceeding many state-of-the-art methods in-terms of\nauto-encoding and downstream tasks considered.","terms":["cs.CV"]},{"titles":"Towards Transferable Multi-modal Perception Representation Learning for Autonomy: NeRF-Supervised Masked AutoEncoder","summaries":"This work proposes a unified self-supervised pre-training framework for\ntransferable multi-modal perception representation learning via masked\nmulti-modal reconstruction in Neural Radiance Field (NeRF), namely\nNeRF-Supervised Masked AutoEncoder (NS-MAE). Specifically, conditioned on\ncertain view directions and locations, multi-modal embeddings extracted from\ncorrupted multi-modal input signals, i.e., Lidar point clouds and images, are\nrendered into projected multi-modal feature maps via neural rendering. Then,\noriginal multi-modal signals serve as reconstruction targets for the rendered\nmulti-modal feature maps to enable self-supervised representation learning.\nExtensive experiments show that the representation learned via NS-MAE shows\npromising transferability for diverse multi-modal and single-modal (camera-only\nand Lidar-only) perception models on diverse 3D perception downstream tasks (3D\nobject detection and BEV map segmentation) with diverse amounts of fine-tuning\nlabeled data. Moreover, we empirically find that NS-MAE enjoys the synergy of\nboth the mechanism of masked autoencoder and neural radiance field. We hope\nthis study can inspire exploration of more general multi-modal representation\nlearning for autonomous agents.","terms":["cs.CV","cs.AI","cs.LG","cs.RO"]},{"titles":"The SVHN Dataset Is Deceptive for Probabilistic Generative Models Due to a Distribution Mismatch","summaries":"The Street View House Numbers (SVHN) dataset is a popular benchmark dataset\nin deep learning. Originally designed for digit classification tasks, the SVHN\ndataset has been widely used as a benchmark for various other tasks including\ngenerative modeling. However, with this work, we aim to warn the community\nabout an issue of the SVHN dataset as a benchmark for generative modeling\ntasks: we discover that the official split into training set and test set of\nthe SVHN dataset are not drawn from the same distribution. We empirically show\nthat this distribution mismatch has little impact on the classification task\n(which may explain why this issue has not been detected before), but it\nseverely affects the evaluation of probabilistic generative models, such as\nVariational Autoencoders and diffusion models. As a workaround, we propose to\nmix and re-split the official training and test set when SVHN is used for tasks\nother than classification. We publish a new split and the indices we used to\ncreate it at https:\/\/jzenn.github.io\/svhn-remix\/ .","terms":["cs.CV","cs.LG","stat.ML"]},{"titles":"Enhancing Molecular Property Prediction via Mixture of Collaborative Experts","summaries":"Molecular Property Prediction (MPP) task involves predicting biochemical\nproperties based on molecular features, such as molecular graph structures,\ncontributing to the discovery of lead compounds in drug development. To address\ndata scarcity and imbalance in MPP, some studies have adopted Graph Neural\nNetworks (GNN) as an encoder to extract commonalities from molecular graphs.\nHowever, these approaches often use a separate predictor for each task,\nneglecting the shared characteristics among predictors corresponding to\ndifferent tasks. In response to this limitation, we introduce the GNN-MoCE\narchitecture. It employs the Mixture of Collaborative Experts (MoCE) as\npredictors, exploiting task commonalities while confronting the homogeneity\nissue in the expert pool and the decision dominance dilemma within the expert\ngroup. To enhance expert diversity for collaboration among all experts, the\nExpert-Specific Projection method is proposed to assign a unique projection\nperspective to each expert. To balance decision-making influence for\ncollaboration within the expert group, the Expert-Specific Loss is presented to\nintegrate individual expert loss into the weighted decision loss of the group\nfor more equitable training. Benefiting from the enhancements of MoCE in expert\ncreation, dynamic expert group formation, and experts' collaboration, our model\ndemonstrates superior performance over traditional methods on 24 MPP datasets,\nespecially in tasks with limited data or high imbalance.","terms":["cs.LG","cs.MA","q-bio.QM"]},{"titles":"On the Robustness of Large Multimodal Models Against Image Adversarial Attacks","summaries":"Recent advances in instruction tuning have led to the development of\nState-of-the-Art Large Multimodal Models (LMMs). Given the novelty of these\nmodels, the impact of visual adversarial attacks on LMMs has not been\nthoroughly examined. We conduct a comprehensive study of the robustness of\nvarious LMMs against different adversarial attacks, evaluated across tasks\nincluding image classification, image captioning, and Visual Question Answer\n(VQA). We find that in general LMMs are not robust to visual adversarial\ninputs. However, our findings suggest that context provided to the model via\nprompts, such as questions in a QA pair helps to mitigate the effects of visual\nadversarial inputs. Notably, the LMMs evaluated demonstrated remarkable\nresilience to such attacks on the ScienceQA task with only an 8.10% drop in\nperformance compared to their visual counterparts which dropped 99.73%. We also\npropose a new approach to real-world image classification which we term query\ndecomposition. By incorporating existence queries into our input prompt we\nobserve diminished attack effectiveness and improvements in image\nclassification accuracy. This research highlights a previously under-explored\nfacet of LMM robustness and sets the stage for future work aimed at\nstrengthening the resilience of multimodal systems in adversarial environments.","terms":["cs.CV"]},{"titles":"OMNIINPUT: A Model-centric Evaluation Framework through Output Distribution","summaries":"We propose a novel model-centric evaluation framework, OmniInput, to evaluate\nthe quality of an AI\/ML model's predictions on all possible inputs (including\nhuman-unrecognizable ones), which is crucial for AI safety and reliability.\nUnlike traditional data-centric evaluation based on pre-defined test sets, the\ntest set in OmniInput is self-constructed by the model itself and the model\nquality is evaluated by investigating its output distribution. We employ an\nefficient sampler to obtain representative inputs and the output distribution\nof the trained model, which, after selective annotation, can be used to\nestimate the model's precision and recall at different output values and a\ncomprehensive precision-recall curve. Our experiments demonstrate that\nOmniInput enables a more fine-grained comparison between models, especially\nwhen their performance is almost the same on pre-defined datasets, leading to\nnew findings and insights for how to train more robust, generalizable models.","terms":["cs.LG","cs.AI"]},{"titles":"A Kernel-Based Neural Network Test for High-dimensional Sequencing Data Analysis","summaries":"The recent development of artificial intelligence (AI) technology, especially\nthe advance of deep neural network (DNN) technology, has revolutionized many\nfields. While DNN plays a central role in modern AI technology, it has been\nrarely used in sequencing data analysis due to challenges brought by\nhigh-dimensional sequencing data (e.g., overfitting). Moreover, due to the\ncomplexity of neural networks and their unknown limiting distributions,\nbuilding association tests on neural networks for genetic association analysis\nremains a great challenge. To address these challenges and fill the important\ngap of using AI in high-dimensional sequencing data analysis, we introduce a\nnew kernel-based neural network (KNN) test for complex association analysis of\nsequencing data. The test is built on our previously developed KNN framework,\nwhich uses random effects to model the overall effects of high-dimensional\ngenetic data and adopts kernel-based neural network structures to model complex\ngenotype-phenotype relationships. Based on KNN, a Wald-type test is then\nintroduced to evaluate the joint association of high-dimensional genetic data\nwith a disease phenotype of interest, considering non-linear and non-additive\neffects (e.g., interaction effects). Through simulations, we demonstrated that\nour proposed method attained higher power compared to the sequence kernel\nassociation test (SKAT), especially in the presence of non-linear and\ninteraction effects. Finally, we apply the methods to the whole genome\nsequencing (WGS) dataset from the Alzheimer's Disease Neuroimaging Initiative\n(ADNI) study, investigating new genes associated with the hippocampal volume\nchange over time.","terms":["stat.ML","cs.LG","stat.ME"]},{"titles":"STEP CATFormer: Spatial-Temporal Effective Body-Part Cross Attention Transformer for Skeleton-based Action Recognition","summaries":"Graph convolutional networks (GCNs) have been widely used and achieved\nremarkable results in skeleton-based action recognition. We think the key to\nskeleton-based action recognition is a skeleton hanging in frames, so we focus\non how the Graph Convolutional Convolution networks learn different topologies\nand effectively aggregate joint features in the global temporal and local\ntemporal. In this work, we propose three Channel-wise Tolopogy Graph\nConvolution based on Channel-wise Topology Refinement Graph Convolution\n(CTR-GCN). Combining CTR-GCN with two joint cross-attention modules can capture\nthe upper-lower body part and hand-foot relationship skeleton features. After\nthat, to capture features of human skeletons changing in frames we design the\nTemporal Attention Transformers to extract skeletons effectively. The Temporal\nAttention Transformers can learn the temporal features of human skeleton\nsequences. Finally, we fuse the temporal features output scale with MLP and\nclassification. We develop a powerful graph convolutional network named Spatial\nTemporal Effective Body-part Cross Attention Transformer which notably\nhigh-performance on the NTU RGB+D, NTU RGB+D 120 datasets. Our code and models\nare available at https:\/\/github.com\/maclong01\/STEP-CATFormer","terms":["cs.CV","cs.AI","cs.LG","I.2.10"]},{"titles":"AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback","summaries":"Large language models (LLMs) such as ChatGPT have seen widespread adoption\ndue to their ability to follow user instructions well. Developing these LLMs\ninvolves a complex yet poorly understood workflow requiring training with human\nfeedback. Replicating and understanding this instruction-following process\nfaces three major challenges: the high cost of data collection, the lack of\ntrustworthy evaluation, and the absence of reference method implementations. We\naddress these challenges with AlpacaFarm, a simulator that enables research and\ndevelopment for learning from feedback at a low cost. First, we design LLM\nprompts to simulate human feedback that are 45x cheaper than crowdworkers and\ndisplay high agreement with humans. Second, we propose an automatic evaluation\nand validate it against human instructions obtained on real-world interactions.\nThird, we contribute reference implementations for several methods (PPO, DPO,\nbest-of-n, expert iteration, and more) that learn from pairwise feedback.\nFinally, as an end-to-end validation of AlpacaFarm, we train and evaluate\neleven models on 10k pairs of real human feedback and show that rankings of\nmodels trained in AlpacaFarm match rankings of models trained on human data. As\na demonstration of the research possible in AlpacaFarm, we find that methods\nthat use a reward model can substantially improve over supervised fine-tuning\nand that our reference PPO implementation leads to a +10% improvement in\nwin-rate against Davinci003. We release all components of AlpacaFarm at\nhttps:\/\/github.com\/tatsu-lab\/alpaca_farm.","terms":["cs.LG","cs.AI","cs.CL"]},{"titles":"Indirect Gradient Matching for Adversarial Robust Distillation","summaries":"Adversarial training significantly improves adversarial robustness, but\nsuperior performance is primarily attained with large models. This substantial\nperformance gap for smaller models has spurred active research into adversarial\ndistillation (AD) to mitigate the difference. Existing AD methods leverage the\nteacher's logits as a guide. In contrast to these approaches, we aim to\ntransfer another piece of knowledge from the teacher, the input gradient. In\nthis paper, we propose a distillation module termed Indirect Gradient\nDistillation Module (IGDM) that indirectly matches the student's input gradient\nwith that of the teacher. We hypothesize that students can better acquire the\nteacher's knowledge by matching the input gradient. Leveraging the observation\nthat adversarial training renders the model locally linear on the input space,\nwe employ Taylor approximation to effectively align gradients without directly\ncalculating them. Experimental results show that IGDM seamlessly integrates\nwith existing AD methods, significantly enhancing the performance of all AD\nmethods. Particularly, utilizing IGDM on the CIFAR-100 dataset improves the\nAutoAttack accuracy from 28.06% to 30.32% with the ResNet-18 model and from\n26.18% to 29.52% with the MobileNetV2 model when integrated into the SOTA\nmethod without additional data augmentation. The code will be made available.","terms":["cs.CV"]},{"titles":"The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation","summaries":"Denoising diffusion probabilistic models have transformed image generation\nwith their impressive fidelity and diversity. We show that they also excel in\nestimating optical flow and monocular depth, surprisingly, without\ntask-specific architectures and loss functions that are predominant for these\ntasks. Compared to the point estimates of conventional regression-based\nmethods, diffusion models also enable Monte Carlo inference, e.g., capturing\nuncertainty and ambiguity in flow and depth. With self-supervised pre-training,\nthe combined use of synthetic and real data for supervised training, and\ntechnical innovations (infilling and step-unrolled denoising diffusion\ntraining) to handle noisy-incomplete training data, and a simple form of\ncoarse-to-fine refinement, one can train state-of-the-art diffusion models for\ndepth and optical flow estimation. Extensive experiments focus on quantitative\nperformance against benchmarks, ablations, and the model's ability to capture\nuncertainty and multimodality, and impute missing values. Our model, DDVM\n(Denoising Diffusion Vision Model), obtains a state-of-the-art relative depth\nerror of 0.074 on the indoor NYU benchmark and an Fl-all outlier rate of 3.26\\%\non the KITTI optical flow benchmark, about 25\\% better than the best published\nmethod. For an overview see https:\/\/diffusion-vision.github.io.","terms":["cs.CV"]},{"titles":"Context-PIPs: Persistent Independent Particles Demands Spatial Context Features","summaries":"We tackle the problem of Persistent Independent Particles (PIPs), also called\nTracking Any Point (TAP), in videos, which specifically aims at estimating\npersistent long-term trajectories of query points in videos. Previous methods\nattempted to estimate these trajectories independently to incorporate longer\nimage sequences, therefore, ignoring the potential benefits of incorporating\nspatial context features. We argue that independent video point tracking also\ndemands spatial context features. To this end, we propose a novel framework\nContext-PIPs, which effectively improves point trajectory accuracy by\naggregating spatial context features in videos. Context-PIPs contains two main\nmodules: 1) a SOurse Feature Enhancement (SOFE) module, and 2) a TArget Feature\nAggregation (TAFA) module. Context-PIPs significantly improves PIPs all-sided,\nreducing 11.4% Average Trajectory Error of Occluded Points (ATE-Occ) on CroHD\nand increasing 11.8% Average Percentage of Correct Keypoint (A-PCK) on\nTAP-Vid-Kinectics. Demos are available at\nhttps:\/\/wkbian.github.io\/Projects\/Context-PIPs\/.","terms":["cs.CV"]},{"titles":"Manipulating the Label Space for In-Context Classification","summaries":"After pre-training by generating the next word conditional on previous words,\nthe Language Model (LM) acquires the ability of In-Context Learning (ICL) that\ncan learn a new task conditional on the context of the given in-context\nexamples (ICEs). Similarly, visually-conditioned Language Modelling is also\nused to train Vision-Language Models (VLMs) with ICL ability. However, such\nVLMs typically exhibit weaker classification abilities compared to contrastive\nlearning-based models like CLIP, since the Language Modelling objective does\nnot directly contrast whether an object is paired with a text. To improve the\nICL of classification, using more ICEs to provide more knowledge is a\nstraightforward way. However, this may largely increase the selection time, and\nmore importantly, the inclusion of additional in-context images tends to extend\nthe length of the in-context sequence beyond the processing capacity of a VLM.\nTo alleviate these limitations, we propose to manipulate the label space of\neach ICE to increase its knowledge density, allowing for fewer ICEs to convey\nas much information as a larger set would. Specifically, we propose two\nstrategies which are Label Distribution Enhancement and Visual Descriptions\nEnhancement to improve In-context classification performance on diverse\ndatasets, including the classic ImageNet and more fine-grained datasets like\nCUB-200. Specifically, using our approach on ImageNet, we increase accuracy\nfrom 74.70\\% in a 4-shot setting to 76.21\\% with just 2 shots. surpassing CLIP\nby 0.67\\%. On CUB-200, our method raises 1-shot accuracy from 48.86\\% to\n69.05\\%, 12.15\\% higher than CLIP. The code is given in\nhttps:\/\/anonymous.4open.science\/r\/MLS_ICC.","terms":["cs.CV"]},{"titles":"Anomaly Detection for Scalable Task Grouping in Reinforcement Learning-based RAN Optimization","summaries":"The use of learning-based methods for optimizing cellular radio access\nnetworks (RAN) has received increasing attention in recent years. This\ncoincides with a rapid increase in the number of cell sites worldwide, driven\nlargely by dramatic growth in cellular network traffic. Training and\nmaintaining learned models that work well across a large number of cell sites\nhas thus become a pertinent problem. This paper proposes a scalable framework\nfor constructing a reinforcement learning policy bank that can perform RAN\noptimization across a large number of cell sites with varying traffic patterns.\nCentral to our framework is a novel application of anomaly detection techniques\nto assess the compatibility between sites (tasks) and the policy bank. This\nallows our framework to intelligently identify when a policy can be reused for\na task, and when a new policy needs to be trained and added to the policy bank.\nOur results show that our approach to compatibility assessment leads to an\nefficient use of computational resources, by allowing us to construct a\nperformant policy bank without exhaustively training on all tasks, which makes\nit applicable under real-world constraints.","terms":["cs.LG"]},{"titles":"Early Autism Diagnosis based on Path Signature and Siamese Unsupervised Feature Compressor","summaries":"Autism Spectrum Disorder (ASD) has been emerging as a growing public health\nthreat. Early diagnosis of ASD is crucial for timely, effective intervention\nand treatment. However, conventional diagnosis methods based on communications\nand behavioral patterns are unreliable for children younger than 2 years of\nage. Given evidences of neurodevelopmental abnormalities in ASD infants, we\nresort to a novel deep learning-based method to extract key features from the\ninherently scarce, class-imbalanced, and heterogeneous structural MR images for\nearly autism diagnosis. Specifically, we propose a Siamese verification\nframework to extend the scarce data, and an unsupervised compressor to\nalleviate data imbalance by extracting key features. We also proposed weight\nconstraints to cope with sample heterogeneity by giving different samples\ndifferent voting weights during validation, and we used Path Signature to\nunravel meaningful developmental features from the two-time point data\nlongitudinally. We further extracted machine learning focused brain regions for\nautism diagnosis. Extensive experiments have shown that our method performed\nwell under practical scenarios, transcending existing machine learning methods\nand providing anatomical insights for autism early diagnosis.","terms":["cs.CV","q-bio.NC"]},{"titles":"Global and Local Semantic Completion Learning for Vision-Language Pre-training","summaries":"Cross-modal alignment plays a crucial role in vision-language pre-training\n(VLP) models, enabling them to capture meaningful associations across different\nmodalities. For this purpose, numerous masked modeling tasks have been proposed\nfor VLP to further promote cross-modal interactions. The core idea of previous\nmasked modeling tasks is to focus on reconstructing the masked tokens based on\nvisible context for learning local-local alignment. However, most of them pay\nlittle attention to the global semantic features generated for the masked data,\nresulting in a limited cross-modal alignment ability of global representations\nto local features of the other modality. Therefore, in this paper, we propose a\nnovel Global and Local Semantic Completion Learning (GLSCL) task to facilitate\nglobal-local alignment and local-local alignment simultaneously. Specifically,\nthe GLSCL task complements the missing semantics of masked data and recovers\nglobal and local features by cross-modal interactions. Our GLSCL consists of\nmasked global semantic completion (MGSC) and masked local token completion\n(MLTC). MGSC promotes learning more representative global features, which have\na great impact on the performance of downstream tasks, while MLTC reconstructs\nmodal-fusion local tokens, further enhancing accurate comprehension of\nmultimodal data. To evaluate the proposed approaches on cross-modal alignment,\nwe develop a validation benchmark called ALIGN-BENCH. Moreover, we present a\nflexible vision encoder, enabling our model to simultaneously perform\nimage-text and video-text multimodal tasks. Experimental results show that our\nproposed method obtains state-of-the-art performance on various vision-language\nbenchmarks, such as visual question answering, image-text retrieval, and\nvideo-text retrieval.","terms":["cs.CV"]},{"titles":"BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models","summaries":"Large Multimodal Models (LMMs) such as GPT-4V and LLaVA have shown remarkable\ncapabilities in visual reasoning with common image styles. However, their\nrobustness against diverse style shifts, crucial for practical applications,\nremains largely unexplored. In this paper, we propose a new benchmark,\nBenchLMM, to assess the robustness of LMMs against three different styles:\nartistic image style, imaging sensor style, and application style, where each\nstyle has five sub-styles. Utilizing BenchLMM, we comprehensively evaluate\nstate-of-the-art LMMs and reveal: 1) LMMs generally suffer performance\ndegradation when working with other styles; 2) An LMM performs better than\nanother model in common style does not guarantee its superior performance in\nother styles; 3) LMMs' reasoning capability can be enhanced by prompting LMMs\nto predict the style first, based on which we propose a versatile and\ntraining-free method for improving LMMs; 4) An intelligent LMM is expected to\ninterpret the causes of its errors when facing stylistic variations. We hope\nthat our benchmark and analysis can shed new light on developing more\nintelligent and versatile LMMs.","terms":["cs.CV"]},{"titles":"The Art of Camouflage: Few-shot Learning for Animal Detection and Segmentation","summaries":"Camouflaged object detection and segmentation is a new and challenging\nresearch topic in computer vision. There is a serious issue of lacking data of\ncamouflaged objects such as camouflaged animals in natural scenes. In this\npaper, we address the problem of few-shot learning for camouflaged object\ndetection and segmentation. To this end, we first collect a new dataset,\nCAMO-FS, for the benchmark. We then propose a novel method to efficiently\ndetect and segment the camouflaged objects in the images. In particular, we\nintroduce the instance triplet loss and the instance memory storage. The\nextensive experiments demonstrated that our proposed method achieves\nstate-of-the-art performance on the newly collected dataset.","terms":["cs.CV"]},{"titles":"PanoGRF: Generalizable Spherical Radiance Fields for Wide-baseline Panoramas","summaries":"Achieving an immersive experience enabling users to explore virtual\nenvironments with six degrees of freedom (6DoF) is essential for various\napplications such as virtual reality (VR). Wide-baseline panoramas are commonly\nused in these applications to reduce network bandwidth and storage\nrequirements. However, synthesizing novel views from these panoramas remains a\nkey challenge. Although existing neural radiance field methods can produce\nphotorealistic views under narrow-baseline and dense image captures, they tend\nto overfit the training views when dealing with \\emph{wide-baseline} panoramas\ndue to the difficulty in learning accurate geometry from sparse $360^{\\circ}$\nviews. To address this problem, we propose PanoGRF, Generalizable Spherical\nRadiance Fields for Wide-baseline Panoramas, which construct spherical radiance\nfields incorporating $360^{\\circ}$ scene priors. Unlike generalizable radiance\nfields trained on perspective images, PanoGRF avoids the information loss from\npanorama-to-perspective conversion and directly aggregates geometry and\nappearance features of 3D sample points from each panoramic view based on\nspherical projection. Moreover, as some regions of the panorama are only\nvisible from one view while invisible from others under wide baseline settings,\nPanoGRF incorporates $360^{\\circ}$ monocular depth priors into spherical depth\nestimation to improve the geometry features. Experimental results on multiple\npanoramic datasets demonstrate that PanoGRF significantly outperforms\nstate-of-the-art generalizable view synthesis methods for wide-baseline\npanoramas (e.g., OmniSyn) and perspective images (e.g., IBRNet, NeuRay).","terms":["cs.CV","cs.GR"]},{"titles":"SO-NeRF: Active View Planning for NeRF using Surrogate Objectives","summaries":"Despite the great success of Neural Radiance Fields (NeRF), its\ndata-gathering process remains vague with only a general rule of thumb of\nsampling as densely as possible. The lack of understanding of what actually\nconstitutes good views for NeRF makes it difficult to actively plan a sequence\nof views that yield the maximal reconstruction quality. We propose Surrogate\nObjectives for Active Radiance Fields (SOAR), which is a set of interpretable\nfunctions that evaluates the goodness of views using geometric and photometric\nvisual cues - surface coverage, geometric complexity, textural complexity, and\nray diversity. Moreover, by learning to infer the SOAR scores from a deep\nnetwork, SOARNet, we are able to effectively select views in mere seconds\ninstead of hours, without the need for prior visits to all the candidate views\nor training any radiance field during such planning. Our experiments show\nSOARNet outperforms the baselines with $\\sim$80x speed-up while achieving\nbetter or comparable reconstruction qualities. We finally show that SOAR is\nmodel-agnostic, thus it generalizes across fully neural-implicit to fully\nexplicit approaches.","terms":["cs.CV"]},{"titles":"Low-Cost High-Power Membership Inference by Boosting Relativity","summaries":"We present a robust membership inference attack (RMIA) that amplifies the\ndistinction between population data and the training data on any target model,\nby effectively leveraging both reference models and reference data in our\nlikelihood ratio test. Our algorithm exhibits superior test power\n(true-positive rate) when compared to prior methods, even at extremely low\nfalse-positive error rates (as low as 0). Also, under computation constraints,\nwhere only a limited number of reference models (as few as 1) are available,\nour method performs exceptionally well, unlike some prior attacks that approach\nrandom guessing in such scenarios. Our method lays the groundwork for\ncost-effective and practical yet powerful and robust privacy risk analysis of\nmachine learning algorithms.","terms":["stat.ML","cs.CR","cs.LG"]},{"titles":"f-FERM: A Scalable Framework for Robust Fair Empirical Risk Minimization","summaries":"Training and deploying machine learning models that meet fairness criteria\nfor protected groups are fundamental in modern artificial intelligence. While\nnumerous constraints and regularization terms have been proposed in the\nliterature to promote fairness in machine learning tasks, most of these methods\nare not amenable to stochastic optimization due to the complex and nonlinear\nstructure of constraints and regularizers. Here, the term \"stochastic\" refers\nto the ability of the algorithm to work with small mini-batches of data.\nMotivated by the limitation of existing literature, this paper presents a\nunified stochastic optimization framework for fair empirical risk minimization\nbased on f-divergence measures (f-FERM). The proposed stochastic algorithm\nenjoys theoretical convergence guarantees. In addition, our experiments\ndemonstrate the superiority of fairness-accuracy tradeoffs offered by f-FERM\nfor almost all batch sizes (ranging from full-batch to batch size of one).\nMoreover, we show that our framework can be extended to the case where there is\na distribution shift from training to the test data. Our extension is based on\na distributionally robust optimization reformulation of f-FERM objective under\n$L_p$ norms as uncertainty sets. Again, in this distributionally robust\nsetting, f-FERM not only enjoys theoretical convergence guarantees but also\noutperforms other baselines in the literature in the tasks involving\ndistribution shifts. An efficient stochastic implementation of $f$-FERM is\npublicly available.","terms":["cs.LG"]},{"titles":"CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale Recommendation Models","summaries":"Recently, the growing memory demands of embedding tables in Deep Learning\nRecommendation Models (DLRMs) pose great challenges for model training and\ndeployment. Existing embedding compression solutions cannot simultaneously meet\nthree key design requirements: memory efficiency, low latency, and adaptability\nto dynamic data distribution. This paper presents CAFE, a Compact, Adaptive,\nand Fast Embedding compression framework that addresses the above requirements.\nThe design philosophy of CAFE is to dynamically allocate more memory resources\nto important features (called hot features), and allocate less memory to\nunimportant ones. In CAFE, we propose a fast and lightweight sketch data\nstructure, named HotSketch, to capture feature importance and report hot\nfeatures in real time. For each reported hot feature, we assign it a unique\nembedding. For the non-hot features, we allow multiple features to share one\nembedding by using hash embedding technique. Guided by our design philosophy,\nwe further propose a multi-level hash embedding framework to optimize the\nembedding tables of non-hot features. We theoretically analyze the accuracy of\nHotSketch, and analyze the model convergence against deviation. Extensive\nexperiments show that CAFE significantly outperforms existing embedding\ncompression methods, yielding 3.92% and 3.68% superior testing AUC on Criteo\nKaggle dataset and CriteoTB dataset at a compression ratio of 10000x. The\nsource codes of CAFE are available at GitHub.","terms":["cs.LG"]},{"titles":"Seller-side Outcome Fairness in Online Marketplaces","summaries":"This paper aims to investigate and achieve seller-side fairness within online\nmarketplaces, where many sellers and their items are not sufficiently exposed\nto customers in an e-commerce platform. This phenomenon raises concerns\nregarding the potential loss of revenue associated with less exposed items as\nwell as less marketplace diversity. We introduce the notion of seller-side\noutcome fairness and build an optimization model to balance collected\nrecommendation rewards and the fairness metric. We then propose a\ngradient-based data-driven algorithm based on the duality and bandit theory.\nOur numerical experiments on real e-commerce data sets show that our algorithm\ncan lift seller fairness measures while not hurting metrics like collected\nGross Merchandise Value (GMV) and total purchases.","terms":["cs.LG","math.OC"]},{"titles":"FAAC: Facial Animation Generation with Anchor Frame and Conditional Control for Superior Fidelity and Editability","summaries":"Over recent years, diffusion models have facilitated significant advancements\nin video generation. Yet, the creation of face-related videos still confronts\nissues such as low facial fidelity, lack of frame consistency, limited\neditability and uncontrollable human poses. To address these challenges, we\nintroduce a facial animation generation method that enhances both face identity\nfidelity and editing capabilities while ensuring frame consistency. This\napproach incorporates the concept of an anchor frame to counteract the\ndegradation of generative ability in original text-to-image models when\nincorporating a motion module. We propose two strategies towards this\nobjective: training-free and training-based anchor frame methods. Our method's\nefficacy has been validated on multiple representative DreamBooth and LoRA\nmodels, delivering substantial improvements over the original outcomes in terms\nof facial fidelity, text-to-image editability, and video motion. Moreover, we\nintroduce conditional control using a 3D parametric face model to capture\naccurate facial movements and expressions. This solution augments the creative\npossibilities for facial animation generation through the integration of\nmultiple control signals. For additional samples, please visit\nhttps:\/\/anonymous.4open.science\/r\/FAAC.","terms":["cs.CV"]},{"titles":"OctreeOcc: Efficient and Multi-Granularity Occupancy Prediction Using Octree Queries","summaries":"Occupancy prediction has increasingly garnered attention in recent years for\nits fine-grained understanding of 3D scenes. Traditional approaches typically\nrely on dense, regular grid representations, which often leads to excessive\ncomputational demands and a loss of spatial details for small objects. This\npaper introduces OctreeOcc, an innovative 3D occupancy prediction framework\nthat leverages the octree representation to adaptively capture valuable\ninformation in 3D, offering variable granularity to accommodate object shapes\nand semantic regions of varying sizes and complexities. In particular, we\nincorporate image semantic information to improve the accuracy of initial\noctree structures and design an effective rectification mechanism to refine the\noctree structure iteratively. Our extensive evaluations show that OctreeOcc not\nonly surpasses state-of-the-art methods in occupancy prediction, but also\nachieves a 15%-24% reduction in computational overhead compared to\ndense-grid-based methods.","terms":["cs.CV"]},{"titles":"Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning","summaries":"Modular and composable transfer learning is an emerging direction in the\nfield of Parameter Efficient Fine-Tuning, as it enables neural networks to\nbetter organize various aspects of knowledge, leading to improved cross-task\ngeneralization. In this paper, we introduce a novel approach Customized\nPolytropon C-Poly that combines task-common skills and task-specific skills,\nwhile the skill parameters being highly parameterized using low-rank\ntechniques. Each task is associated with a customizable number of exclusive\nspecialized skills and also benefits from skills shared with peer tasks. A\nskill assignment matrix is jointly learned. To evaluate our approach, we\nconducted extensive experiments on the Super-NaturalInstructions and the\nSuperGLUE benchmarks. Our findings demonstrate that C-Poly outperforms\nfully-shared, task-specific, and skill-indistinguishable baselines,\nsignificantly enhancing the sample efficiency in multi-task learning scenarios.","terms":["cs.LG","cs.AI"]},{"titles":"Multicoated and Folded Graph Neural Networks with Strong Lottery Tickets","summaries":"The Strong Lottery Ticket Hypothesis (SLTH) demonstrates the existence of\nhigh-performing subnetworks within a randomly initialized model, discoverable\nthrough pruning a convolutional neural network (CNN) without any weight\ntraining. A recent study, called Untrained GNNs Tickets (UGT), expanded SLTH\nfrom CNNs to shallow graph neural networks (GNNs). However, discrepancies\npersist when comparing baseline models with learned dense weights.\nAdditionally, there remains an unexplored area in applying SLTH to deeper GNNs,\nwhich, despite delivering improved accuracy with additional layers, suffer from\nexcessive memory requirements. To address these challenges, this work utilizes\nMulticoated Supermasks (M-Sup), a scalar pruning mask method, and implements it\nin GNNs by proposing a strategy for setting its pruning thresholds adaptively.\nIn the context of deep GNNs, this research uncovers the existence of untrained\nrecurrent networks, which exhibit performance on par with their trained\nfeed-forward counterparts. This paper also introduces the Multi-Stage Folding\nand Unshared Masks methods to expand the search space in terms of both\narchitecture and parameters. Through the evaluation of various datasets,\nincluding the Open Graph Benchmark (OGB), this work establishes a triple-win\nscenario for SLTH-based GNNs: by achieving high sparsity, competitive\nperformance, and high memory efficiency with up to 98.7\\% reduction, it\ndemonstrates suitability for energy-efficient graph processing.","terms":["cs.LG","cs.AI","stat.ML"]},{"titles":"TongueSAM: An Universal Tongue Segmentation Model Based on SAM with Zero-Shot","summaries":"Tongue segmentation serves as the primary step in automated TCM tongue\ndiagnosis, which plays a significant role in the diagnostic results. Currently,\nnumerous deep learning based methods have achieved promising results. However,\nwhen confronted with tongue images that differ from the training set or possess\nchallenging backgrounds, these methods demonstrate limited performance. To\naddress this issue, this paper proposes a universal tongue segmentation model\nnamed TongueSAM based on SAM (Segment Anything Model). SAM is a large-scale\npretrained interactive segmentation model known for its powerful zero-shot\ngeneralization capability. Applying SAM to tongue segmentation leverages its\nlearned prior knowledge from natural images, enabling the achievement of\nzero-shot segmentation for various types of tongue images. In this study, a\nPrompt Generator based on object detection is integrated into SAM to enable an\nend-to-end automated tongue segmentation method. Experiments demonstrate that\nTongueSAM achieves exceptional performance across various of tongue\nsegmentation datasets, particularly under zero-shot. Even when dealing with\nchallenging background tongue images, TongueSAM achieves a mIoU of 95.23\\%\nunder zero-shot conditions, surpassing other segmentation methods. As far as we\nknow, this is the first application of large-scale pretrained model for tongue\nsegmentation. The project mentioned in this paper is currently publicly\navailable.","terms":["cs.CV"]},{"titles":"Leveraging Single-View Images for Unsupervised 3D Point Cloud Completion","summaries":"Point clouds captured by scanning devices are often incomplete due to\nocclusion. To overcome this limitation, point cloud completion methods have\nbeen developed to predict the complete shape of an object based on its partial\ninput. These methods can be broadly classified as supervised or unsupervised.\nHowever, both categories require a large number of 3D complete point clouds,\nwhich may be difficult to capture. In this paper, we propose Cross-PCC, an\nunsupervised point cloud completion method without requiring any 3D complete\npoint clouds. We only utilize 2D images of the complete objects, which are\neasier to capture than 3D complete and clean point clouds. Specifically, to\ntake advantage of the complementary information from 2D images, we use a\nsingle-view RGB image to extract 2D features and design a fusion module to fuse\nthe 2D and 3D features extracted from the partial point cloud. To guide the\nshape of predicted point clouds, we project the predicted points of the object\nto the 2D plane and use the foreground pixels of its silhouette maps to\nconstrain the position of the projected points. To reduce the outliers of the\npredicted point clouds, we propose a view calibrator to move the points\nprojected to the background into the foreground by the single-view silhouette\nimage. To the best of our knowledge, our approach is the first point cloud\ncompletion method that does not require any 3D supervision. The experimental\nresults of our method are superior to those of the state-of-the-art\nunsupervised methods by a large margin. Moreover, our method even achieves\ncomparable performance to some supervised methods. We will make the source code\npublicly available at https:\/\/github.com\/ltwu6\/cross-pcc.","terms":["cs.CV"]},{"titles":"AttriHuman-3D: Editable 3D Human Avatar Generation with Attribute Decomposition and Indexing","summaries":"Editable 3D-aware generation, which supports user-interacted editing, has\nwitnessed rapid development recently. However, existing editable 3D GANs either\nfail to achieve high-accuracy local editing or suffer from huge computational\ncosts. We propose AttriHuman-3D, an editable 3D human generation model, which\naddress the aforementioned problems with attribute decomposition and indexing.\nThe core idea of the proposed model is to generate all attributes (e.g. human\nbody, hair, clothes and so on) in an overall attribute space with six feature\nplanes, which are then decomposed and manipulated with different attribute\nindexes. To precisely extract features of different attributes from the\ngenerated feature planes, we propose a novel attribute indexing method as well\nas an orthogonal projection regularization to enhance the disentanglement. We\nalso introduce a hyper-latent training strategy and an attribute-specific\nsampling strategy to avoid style entanglement and misleading punishment from\nthe discriminator. Our method allows users to interactively edit selected\nattributes in the generated 3D human avatars while keeping others fixed. Both\nqualitative and quantitative experiments demonstrate that our model provides a\nstrong disentanglement between different attributes, allows fine-grained image\nediting and generates high-quality 3D human avatars.","terms":["cs.CV"]},{"titles":"SpaCE: The Spatial Confounding Environment","summaries":"Spatial confounding poses a significant challenge in scientific studies\ninvolving spatial data, where unobserved spatial variables can influence both\ntreatment and outcome, possibly leading to spurious associations. To address\nthis problem, we introduce SpaCE: The Spatial Confounding Environment, the\nfirst toolkit to provide realistic benchmark datasets and tools for\nsystematically evaluating causal inference methods designed to alleviate\nspatial confounding. Each dataset includes training data, true counterfactuals,\na spatial graph with coordinates, and smoothness and confounding scores\ncharacterizing the effect of a missing spatial confounder. It also includes\nrealistic semi-synthetic outcomes and counterfactuals, generated using\nstate-of-the-art machine learning ensembles, following best practices for\ncausal inference benchmarks. The datasets cover real treatment and covariates\nfrom diverse domains, including climate, health and social sciences. SpaCE\nfacilitates an automated end-to-end pipeline, simplifying data loading,\nexperimental setup, and evaluating machine learning and causal inference\nmodels. The SpaCE project provides several dozens of datasets of diverse sizes\nand spatial complexity. It is publicly available as a Python package,\nencouraging community feedback and contributions.","terms":["cs.LG","stat.ME","stat.ML"]},{"titles":"Deep Multimodal Fusion for Surgical Feedback Classification","summaries":"Quantification of real-time informal feedback delivered by an experienced\nsurgeon to a trainee during surgery is important for skill improvements in\nsurgical training. Such feedback in the live operating room is inherently\nmultimodal, consisting of verbal conversations (e.g., questions and answers) as\nwell as non-verbal elements (e.g., through visual cues like pointing to\nanatomic elements). In this work, we leverage a clinically-validated\nfive-category classification of surgical feedback: \"Anatomic\", \"Technical\",\n\"Procedural\", \"Praise\" and \"Visual Aid\". We then develop a multi-label machine\nlearning model to classify these five categories of surgical feedback from\ninputs of text, audio, and video modalities. The ultimate goal of our work is\nto help automate the annotation of real-time contextual surgical feedback at\nscale. Our automated classification of surgical feedback achieves AUCs ranging\nfrom 71.5 to 77.6 with the fusion improving performance by 3.1%. We also show\nthat high-quality manual transcriptions of feedback audio from experts improve\nAUCs to between 76.5 and 96.2, which demonstrates a clear path toward future\nimprovements. Empirically, we find that the Staged training strategy, with\nfirst pre-training each modality separately and then training them jointly, is\nmore effective than training different modalities altogether. We also present\nintuitive findings on the importance of modalities for different feedback\ncategories. This work offers an important first look at the feasibility of\nautomated classification of real-world live surgical feedback based on text,\naudio, and video modalities.","terms":["cs.LG","cs.AI","cs.CV","cs.HC","eess.AS"]},{"titles":"Improving Depth Gradient Continuity in Transformers: A Comparative Study on Monocular Depth Estimation with CNN","summaries":"Monocular depth estimation is an ongoing challenge in computer vision. Recent\nprogress with Transformer models has demonstrated notable advantages over\nconventional CNNs in this area. However, there's still a gap in understanding\nhow these models prioritize different regions in 2D images and how these\nregions affect depth estimation performance. To explore the differences between\nTransformers and CNNs, we employ a sparse pixel approach to contrastively\nanalyze the distinctions between the two. Our findings suggest that while\nTransformers excel in handling global context and intricate textures, they lag\nbehind CNNs in preserving depth gradient continuity. To further enhance the\nperformance of Transformer models in monocular depth estimation, we propose the\nDepth Gradient Refinement (DGR) module that refines depth estimation through\nhigh-order differentiation, feature fusion, and recalibration. Additionally, we\nleverage optimal transport theory, treating depth maps as spatial probability\ndistributions, and employ the optimal transport distance as a loss function to\noptimize our model. Experimental results demonstrate that models integrated\nwith the plug-and-play Depth Gradient Refinement (DGR) module and the proposed\nloss function enhance performance without increasing complexity and\ncomputational costs on both outdoor KITTI and indoor NYU-Depth-v2 datasets.\nThis research not only offers fresh insights into the distinctions between\nTransformers and CNNs in depth estimation but also paves the way for novel\ndepth estimation methodologies.","terms":["cs.CV"]},{"titles":"Human Body Model based ID using Shape and Pose Parameters","summaries":"We present a Human Body model based IDentification system (HMID) system that\nis jointly trained for shape, pose and biometric identification. HMID is based\non the Human Mesh Recovery (HMR) network and we propose additional losses to\nimprove and stabilize shape estimation and biometric identification while\nmaintaining the pose and shape output. We show that when our HMID network is\ntrained using additional shape and pose losses, it shows a significant\nimprovement in biometric identification performance when compared to an\nidentical model that does not use such losses. The HMID model uses raw images\ninstead of silhouettes and is able to perform robust recognition on images\ncollected at range and altitude as many anthropometric properties are\nreasonably invariant to clothing, view and range. We show results on the USF\ndataset as well as the BRIAR dataset which includes probes with both clothing\nand view changes. Our approach (using body model losses) shows a significant\nimprovement in Rank20 accuracy and True Accuracy Rate on the BRIAR evaluation\ndataset.","terms":["cs.CV","eess.IV"]},{"titles":"Rethinking Object Saliency Ranking: A Novel Whole-flow Processing Paradigm","summaries":"Existing salient object detection methods are capable of predicting binary\nmaps that highlight visually salient regions. However, these methods are\nlimited in their ability to differentiate the relative importance of multiple\nobjects and the relationships among them, which can lead to errors and reduced\naccuracy in downstream tasks that depend on the relative importance of multiple\nobjects. To conquer, this paper proposes a new paradigm for saliency ranking,\nwhich aims to completely focus on ranking salient objects by their \"importance\norder\". While previous works have shown promising performance, they still face\nill-posed problems. First, the saliency ranking ground truth (GT) orders\ngeneration methods are unreasonable since determining the correct ranking order\nis not well-defined, resulting in false alarms. Second, training a ranking\nmodel remains challenging because most saliency ranking methods follow the\nmulti-task paradigm, leading to conflicts and trade-offs among different tasks.\nThird, existing regression-based saliency ranking methods are complex for\nsaliency ranking models due to their reliance on instance mask-based saliency\nranking orders. These methods require a significant amount of data to perform\naccurately and can be challenging to implement effectively. To solve these\nproblems, this paper conducts an in-depth analysis of the causes and proposes a\nwhole-flow processing paradigm of saliency ranking task from the perspective of\n\"GT data generation\", \"network structure design\" and \"training protocol\". The\nproposed approach outperforms existing state-of-the-art methods on the\nwidely-used SALICON set, as demonstrated by extensive experiments with fair and\nreasonable comparisons. The saliency ranking task is still in its infancy, and\nour proposed unified framework can serve as a fundamental strategy to guide\nfuture work.","terms":["cs.CV"]},{"titles":"Feature-Learning Networks Are Consistent Across Widths At Realistic Scales","summaries":"We study the effect of width on the dynamics of feature-learning neural\nnetworks across a variety of architectures and datasets. Early in training,\nwide neural networks trained on online data have not only identical loss curves\nbut also agree in their point-wise test predictions throughout training. For\nsimple tasks such as CIFAR-5m this holds throughout training for networks of\nrealistic widths. We also show that structural properties of the models,\nincluding internal representations, preactivation distributions, edge of\nstability phenomena, and large learning rate effects are consistent across\nlarge widths. This motivates the hypothesis that phenomena seen in realistic\nmodels can be captured by infinite-width, feature-learning limits. For harder\ntasks (such as ImageNet and language modeling), and later training times,\nfinite-width deviations grow systematically. Two distinct effects cause these\ndeviations across widths. First, the network output has\ninitialization-dependent variance scaling inversely with width, which can be\nremoved by ensembling networks. We observe, however, that ensembles of narrower\nnetworks perform worse than a single wide network. We call this the bias of\nnarrower width. We conclude with a spectral perspective on the origin of this\nfinite-width bias.","terms":["cs.LG"]},{"titles":"Predicting Scores of Various Aesthetic Attribute Sets by Learning from Overall Score Labels","summaries":"Now many mobile phones embed deep-learning models for evaluation or guidance\non photography. These models cannot provide detailed results like human pose\nscores or scene color scores because of the rare of corresponding aesthetic\nattribute data. However, the annotation of image aesthetic attribute scores\nrequires experienced artists and professional photographers, which hinders the\ncollection of large-scale fully-annotated datasets. In this paper, we propose\nto replace image attribute labels with feature extractors. First, a novel\naesthetic attribute evaluation framework based on attribute features is\nproposed to predict attribute scores and overall scores. We call it the F2S\n(attribute features to attribute scores) model. We use networks from different\ntasks to provide attribute features to our F2S models. Then, we define an\naesthetic attribute contribution to describe the role of aesthetic attributes\nthroughout an image and use it with the attribute scores and the overall scores\nto train our F2S model. Sufficient experiments on publicly available datasets\ndemonstrate that our F2S model achieves comparable performance with those\ntrained on the datasets with fully-annotated aesthetic attribute score labels.\nOur method makes it feasible to learn meaningful attribute scores for various\naesthetic attribute sets in different types of images with only overall\naesthetic scores.","terms":["cs.CV"]},{"titles":"Adaptive Confidence Threshold for ByteTrack in Multi-Object Tracking","summaries":"We investigate the application of ByteTrack in the realm of multiple object\ntracking. ByteTrack, a simple tracking algorithm, enables the simultaneous\ntracking of multiple objects by strategically incorporating detections with a\nlow confidence threshold. Conventionally, objects are initially associated with\nhigh confidence threshold detections. When the association between objects and\ndetections becomes ambiguous, ByteTrack extends the association to lower\nconfidence threshold detections. One notable drawback of the existing ByteTrack\napproach is its reliance on a fixed threshold to differentiate between high and\nlow-confidence detections. In response to this limitation, we introduce a novel\nand adaptive approach. Our proposed method entails a dynamic adjustment of the\nconfidence threshold, leveraging insights derived from overall detections.\nThrough experimentation, we demonstrate the effectiveness of our adaptive\nconfidence threshold technique while maintaining running time compared to\nByteTrack.","terms":["cs.CV"]},{"titles":"Accelerated Gradient Algorithms with Adaptive Subspace Search for Instance-Faster Optimization","summaries":"Gradient-based minimax optimal algorithms have greatly promoted the\ndevelopment of continuous optimization and machine learning. One seminal work\ndue to Yurii Nesterov [Nes83a] established $\\tilde{\\mathcal{O}}(\\sqrt{L\/\\mu})$\ngradient complexity for minimizing an $L$-smooth $\\mu$-strongly convex\nobjective. However, an ideal algorithm would adapt to the explicit complexity\nof a particular objective function and incur faster rates for simpler problems,\ntriggering our reconsideration of two defeats of existing optimization modeling\nand analysis. (i) The worst-case optimality is neither the instance optimality\nnor such one in reality. (ii) Traditional $L$-smoothness condition may not be\nthe primary abstraction\/characterization for modern practical problems.\n  In this paper, we open up a new way to design and analyze gradient-based\nalgorithms with direct applications in machine learning, including linear\nregression and beyond. We introduce two factors $(\\alpha, \\tau_{\\alpha})$ to\nrefine the description of the degenerated condition of the optimization\nproblems based on the observation that the singular values of Hessian often\ndrop sharply. We design adaptive algorithms that solve simpler problems without\npre-known knowledge with reduced gradient or analogous oracle accesses. The\nalgorithms also improve the state-of-art complexities for several problems in\nmachine learning, thereby solving the open problem of how to design faster\nalgorithms in light of the known complexity lower bounds. Specially, with the\n$\\mathcal{O}(1)$-nuclear norm bounded, we achieve an optimal\n$\\tilde{\\mathcal{O}}(\\mu^{-1\/3})$ (v.s. $\\tilde{\\mathcal{O}}(\\mu^{-1\/2})$)\ngradient complexity for linear regression. We hope this work could invoke the\nrethinking for understanding the difficulty of modern problems in optimization.","terms":["cs.LG","cs.CC","stat.ML"]},{"titles":"SDSRA: A Skill-Driven Skill-Recombination Algorithm for Efficient Policy Learning","summaries":"In this paper, we introduce a novel algorithm - the Skill-Driven Skill\nRecombination Algorithm (SDSRA) - an innovative framework that significantly\nenhances the efficiency of achieving maximum entropy in reinforcement learning\ntasks. We find that SDSRA achieves faster convergence compared to the\ntraditional Soft Actor-Critic (SAC) algorithm and produces improved policies.\nBy integrating skill-based strategies within the robust Actor-Critic framework,\nSDSRA demonstrates remarkable adaptability and performance across a wide array\nof complex and diverse benchmarks.","terms":["cs.LG","cs.AI"]},{"titles":"Bootstrap Your Own Variance","summaries":"Understanding model uncertainty is important for many applications. We\npropose Bootstrap Your Own Variance (BYOV), combining Bootstrap Your Own Latent\n(BYOL), a negative-free Self-Supervised Learning (SSL) algorithm, with Bayes by\nBackprop (BBB), a Bayesian method for estimating model posteriors. We find that\nthe learned predictive std of BYOV vs. a supervised BBB model is well captured\nby a Gaussian distribution, providing preliminary evidence that the learned\nparameter posterior is useful for label free uncertainty estimation. BYOV\nimproves upon the deterministic BYOL baseline (+2.83% test ECE, +1.03% test\nBrier) and presents better calibration and reliability when tested with various\naugmentations (eg: +2.4% test ECE, +1.2% test Brier for Salt & Pepper noise).","terms":["cs.LG","stat.ML"]},{"titles":"Constrained Bayesian Optimization Under Partial Observations: Balanced Improvements and Provable Convergence","summaries":"The partially observable constrained optimization problems (POCOPs) impede\ndata-driven optimization techniques since an infeasible solution of POCOPs can\nprovide little information about the objective as well as the constraints. We\nendeavor to design an efficient and provable method for expensive POCOPs under\nthe framework of constrained Bayesian optimization. Our method consists of two\nkey components. Firstly, we present an improved design of the acquisition\nfunctions that introduces balanced exploration during optimization. We\nrigorously study the convergence properties of this design to demonstrate its\neffectiveness. Secondly, we propose a Gaussian process embedding different\nlikelihoods as the surrogate model for a partially observable constraint. This\nmodel leads to a more accurate representation of the feasible regions compared\nto traditional classification-based models. Our proposed method is empirically\nstudied on both synthetic and real-world problems. The results demonstrate the\ncompetitiveness of our method for solving POCOPs.","terms":["cs.LG","stat.ML"]},{"titles":"Cache Me if You Can: Accelerating Diffusion Models through Block Caching","summaries":"Diffusion models have recently revolutionized the field of image synthesis\ndue to their ability to generate photorealistic images. However, one of the\nmajor drawbacks of diffusion models is that the image generation process is\ncostly. A large image-to-image network has to be applied many times to\niteratively refine an image from random noise. While many recent works propose\ntechniques to reduce the number of required steps, they generally treat the\nunderlying denoising network as a black box. In this work, we investigate the\nbehavior of the layers within the network and find that 1) the layers' output\nchanges smoothly over time, 2) the layers show distinct patterns of change, and\n3) the change from step to step is often very small. We hypothesize that many\nlayer computations in the denoising network are redundant. Leveraging this, we\nintroduce block caching, in which we reuse outputs from layer blocks of\nprevious steps to speed up inference. Furthermore, we propose a technique to\nautomatically determine caching schedules based on each block's changes over\ntimesteps. In our experiments, we show through FID, human evaluation and\nqualitative analysis that Block Caching allows to generate images with higher\nvisual quality at the same computational cost. We demonstrate this for\ndifferent state-of-the-art models (LDM and EMU) and solvers (DDIM and DPM).","terms":["cs.CV"]},{"titles":"Satellite Imagery and AI: A New Era in Ocean Conservation, from Research to Deployment and Impact","summaries":"Illegal, unreported, and unregulated (IUU) fishing poses a global threat to\nocean habitats. Publicly available satellite data offered by NASA and the\nEuropean Space Agency (ESA) provide an opportunity to actively monitor this\nactivity. Effectively leveraging satellite data for maritime conservation\nrequires highly reliable machine learning models operating globally with\nminimal latency. This paper introduces three specialized computer vision models\ndesigned for synthetic aperture radar (Sentinel-1), optical imagery\n(Sentinel-2), and nighttime lights (Suomi-NPP\/NOAA-20). It also presents best\npractices for developing and delivering real-time computer vision services for\nconservation. These models have been deployed in Skylight, a real time maritime\nmonitoring platform, which is provided at no cost to users worldwide.","terms":["cs.CV"]},{"titles":"Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields","summaries":"3D scene representations have gained immense popularity in recent years.\nMethods that use Neural Radiance fields are versatile for traditional tasks\nsuch as novel view synthesis. In recent times, some work has emerged that aims\nto extend the functionality of NeRF beyond view synthesis, for semantically\naware tasks such as editing and segmentation using 3D feature field\ndistillation from 2D foundation models. However, these methods have two major\nlimitations: (a) they are limited by the rendering speed of NeRF pipelines, and\n(b) implicitly represented feature fields suffer from continuity artifacts\nreducing feature quality. Recently, 3D Gaussian Splatting has shown\nstate-of-the-art performance on real-time radiance field rendering. In this\nwork, we go one step further: in addition to radiance field rendering, we\nenable 3D Gaussian splatting on arbitrary-dimension semantic features via 2D\nfoundation model distillation. This translation is not straightforward: naively\nincorporating feature fields in the 3DGS framework leads to warp-level\ndivergence. We propose architectural and training changes to efficiently avert\nthis problem. Our proposed method is general, and our experiments showcase\nnovel view semantic segmentation, language-guided editing and segment anything\nthrough learning feature fields from state-of-the-art 2D foundation models such\nas SAM and CLIP-LSeg. Across experiments, our distillation method is able to\nprovide comparable or better results, while being significantly faster to both\ntrain and render. Additionally, to the best of our knowledge, we are the first\nmethod to enable point and bounding-box prompting for radiance field\nmanipulation, by leveraging the SAM model. Project website at:\nhttps:\/\/feature-3dgs.github.io\/","terms":["cs.CV"]},{"titles":"Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback","summaries":"The field of text-conditioned image generation has made unparalleled progress\nwith the recent advent of latent diffusion models. While remarkable, as the\ncomplexity of given text input increases, the state-of-the-art diffusion models\nmay still fail in generating images which accurately convey the semantics of\nthe given prompt. Furthermore, it has been observed that such misalignments are\noften left undetected by pretrained multi-modal models such as CLIP. To address\nthese problems, in this paper we explore a simple yet effective decompositional\napproach towards both evaluation and improvement of text-to-image alignment. In\nparticular, we first introduce a Decompositional-Alignment-Score which given a\ncomplex prompt decomposes it into a set of disjoint assertions. The alignment\nof each assertion with generated images is then measured using a VQA model.\nFinally, alignment scores for different assertions are combined aposteriori to\ngive the final text-to-image alignment score. Experimental analysis reveals\nthat the proposed alignment metric shows significantly higher correlation with\nhuman ratings as opposed to traditional CLIP, BLIP scores. Furthermore, we also\nfind that the assertion level alignment scores provide a useful feedback which\ncan then be used in a simple iterative procedure to gradually increase the\nexpression of different assertions in the final image outputs. Human user\nstudies indicate that the proposed approach surpasses previous state-of-the-art\nby 8.7% in overall text-to-image alignment accuracy. Project page for our paper\nis available at https:\/\/1jsingh.github.io\/divide-evaluate-and-refine","terms":["cs.CV","cs.AI","cs.LG","cs.MM","stat.ML"]},{"titles":"DiffusionAtlas: High-Fidelity Consistent Diffusion Video Editing","summaries":"We present a diffusion-based video editing framework, namely DiffusionAtlas,\nwhich can achieve both frame consistency and high fidelity in editing video\nobject appearance. Despite the success in image editing, diffusion models still\nencounter significant hindrances when it comes to video editing due to the\nchallenge of maintaining spatiotemporal consistency in the object's appearance\nacross frames. On the other hand, atlas-based techniques allow propagating\nedits on the layered representations consistently back to frames. However, they\noften struggle to create editing effects that adhere correctly to the\nuser-provided textual or visual conditions due to the limitation of editing the\ntexture atlas on a fixed UV mapping field. Our method leverages a\nvisual-textual diffusion model to edit objects directly on the diffusion\natlases, ensuring coherent object identity across frames. We design a loss term\nwith atlas-based constraints and build a pretrained text-driven diffusion model\nas pixel-wise guidance for refining shape distortions and correcting texture\ndeviations. Qualitative and quantitative experiments show that our method\noutperforms state-of-the-art methods in achieving consistent high-fidelity\nvideo-object editing.","terms":["cs.CV"]},{"titles":"FERGI: Automatic Annotation of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction","summaries":"Researchers have proposed to use data of human preference feedback to\nfine-tune text-to-image generative models. However, the scalability of human\nfeedback collection has been limited by its reliance on manual annotation.\nTherefore, we develop and test a method to automatically annotate user\npreferences from their spontaneous facial expression reaction to the generated\nimages. We collect a dataset of Facial Expression Reaction to Generated Images\n(FERGI) and show that the activations of multiple facial action units (AUs) are\nhighly correlated with user evaluations of the generated images. Specifically,\nAU4 (brow lowerer) is most consistently reflective of negative evaluations of\nthe generated image. This can be useful in two ways. Firstly, we can\nautomatically annotate user preferences between image pairs with substantial\ndifference in AU4 responses to them with an accuracy significantly\noutperforming state-of-the-art scoring models. Secondly, directly integrating\nthe AU4 responses with the scoring models improves their consistency with human\npreferences. Additionally, the AU4 response best reflects the user's evaluation\nof the image fidelity, making it complementary to the state-of-the-art scoring\nmodels, which are generally better at reflecting image-text alignment. Finally,\nthis method of automatic annotation with facial expression analysis can be\npotentially generalized to other generation tasks. The code is available at\nhttps:\/\/github.com\/ShuangquanFeng\/FERGI, and the dataset is also available at\nthe same link for research purposes.","terms":["cs.CV","cs.AI","cs.HC","cs.LG"]},{"titles":"Data-Driven Traffic Reconstruction and Kernel Methods for Identifying Stop-and-Go Congestion","summaries":"Identifying stop-and-go events (SAGs) in traffic flow presents an important\navenue for advancing data-driven research for climate change mitigation and\nsustainability, owing to their substantial impact on carbon emissions, travel\ntime, fuel consumption, and roadway safety. In fact, SAGs are estimated to\naccount for 33-50% of highway driving externalities. However, insufficient\nattention has been paid to precisely quantifying where, when, and how much\nthese SAGs take place -necessary for downstream decision making, such as\nintervention design and policy analysis. A key challenge is that the data\navailable to researchers and governments are typically sparse and aggregated to\na granularity that obscures SAGs. To overcome such data limitations, this study\nthus explores the use of traffic reconstruction techniques for SAG\nidentification. In particular, we introduce a kernel-based method for\nidentifying spatio-temporal features in traffic and leverage bootstrapping to\nquantify the uncertainty of the reconstruction process. Experimental results on\nCalifornia highway data demonstrate the promise of the method for capturing\nSAGs. This work contributes to a foundation for data-driven decision making to\nadvance sustainability of traffic systems.","terms":["cs.LG","cs.AI","cs.CY"]},{"titles":"T-Cal: An optimal test for the calibration of predictive models","summaries":"The prediction accuracy of machine learning methods is steadily increasing,\nbut the calibration of their uncertainty predictions poses a significant\nchallenge. Numerous works focus on obtaining well-calibrated predictive models,\nbut less is known about reliably assessing model calibration. This limits our\nability to know when algorithms for improving calibration have a real effect,\nand when their improvements are merely artifacts due to random noise in finite\ndatasets. In this work, we consider detecting mis-calibration of predictive\nmodels using a finite validation dataset as a hypothesis testing problem. The\nnull hypothesis is that the predictive model is calibrated, while the\nalternative hypothesis is that the deviation from calibration is sufficiently\nlarge.\n  We find that detecting mis-calibration is only possible when the conditional\nprobabilities of the classes are sufficiently smooth functions of the\npredictions. When the conditional class probabilities are H\\\"older continuous,\nwe propose T-Cal, a minimax optimal test for calibration based on a debiased\nplug-in estimator of the $\\ell_2$-Expected Calibration Error (ECE). We further\npropose Adaptive T-Cal, a version that is adaptive to unknown smoothness. We\nverify our theoretical findings with a broad range of experiments, including\nwith several popular deep neural net architectures and several standard\npost-hoc calibration methods. T-Cal is a practical general-purpose tool, which\n-- combined with classical tests for discrete-valued predictors -- can be used\nto test the calibration of virtually any probabilistic classification method.","terms":["stat.ML","cs.LG"]},{"titles":"Using Curiosity for an Even Representation of Tasks in Continual Offline Reinforcement Learning","summaries":"In this work, we investigate the means of using curiosity on replay buffers\nto improve offline multi-task continual reinforcement learning when tasks,\nwhich are defined by the non-stationarity in the environment, are non labeled\nand not evenly exposed to the learner in time. In particular, we investigate\nthe use of curiosity both as a tool for task boundary detection and as a\npriority metric when it comes to retaining old transition tuples, which we\nrespectively use to propose two different buffers. Firstly, we propose a Hybrid\nReservoir Buffer with Task Separation (HRBTS), where curiosity is used to\ndetect task boundaries that are not known due to the task agnostic nature of\nthe problem. Secondly, by using curiosity as a priority metric when it comes to\nretaining old transition tuples, a Hybrid Curious Buffer (HCB) is proposed. We\nultimately show that these buffers, in conjunction with regular reinforcement\nlearning algorithms, can be used to alleviate the catastrophic forgetting issue\nsuffered by the state of the art on replay buffers when the agent's exposure to\ntasks is not equal along time. We evaluate catastrophic forgetting and the\nefficiency of our proposed buffers against the latest works such as the Hybrid\nReservoir Buffer (HRB) and the Multi-Time Scale Replay Buffer (MTR) in three\ndifferent continual reinforcement learning settings. Experiments were done on\nclassical control tasks and Metaworld environment. Experiments show that our\nproposed replay buffers display better immunity to catastrophic forgetting\ncompared to existing works in most of the settings.","terms":["cs.LG","cs.AI"]},{"titles":"Active Learning for Abrupt Shifts Change-point Detection via Derivative-Aware Gaussian Processes","summaries":"Change-point detection (CPD) is crucial for identifying abrupt shifts in\ndata, which influence decision-making and efficient resource allocation across\nvarious domains. To address the challenges posed by the costly and\ntime-intensive data acquisition in CPD, we introduce the Derivative-Aware\nChange Detection (DACD) method. It leverages the derivative process of a\nGaussian process (GP) for Active Learning (AL), aiming to pinpoint change-point\nlocations effectively. DACD balances the exploitation and exploration of\nderivative processes through multiple data acquisition functions (AFs). By\nutilizing GP derivative mean and variance as criteria, DACD sequentially\nselects the next sampling data point, thus enhancing algorithmic efficiency and\nensuring reliable and accurate results. We investigate the effectiveness of\nDACD method in diverse scenarios and show it outperforms other active learning\nchange-point detection approaches.","terms":["cs.LG"]},{"titles":"TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs","summaries":"Precise hardware performance models play a crucial role in code\noptimizations. They can assist compilers in making heuristic decisions or aid\nautotuners in identifying the optimal configuration for a given program. For\nexample, the autotuner for XLA, a machine learning compiler, discovered 10-20%\nspeedup on state-of-the-art models serving substantial production traffic at\nGoogle. Although there exist a few datasets for program performance prediction,\nthey target small sub-programs such as basic blocks or kernels. This paper\nintroduces TpuGraphs, a performance prediction dataset on full tensor programs,\nrepresented as computational graphs, running on Tensor Processing Units (TPUs).\nEach graph in the dataset represents the main computation of a machine learning\nworkload, e.g., a training epoch or an inference step. Each data sample\ncontains a computational graph, a compilation configuration, and the execution\ntime of the graph when compiled with the configuration. The graphs in the\ndataset are collected from open-source machine learning programs, featuring\npopular model architectures, e.g., ResNet, EfficientNet, Mask R-CNN, and\nTransformer. TpuGraphs provides 25x more graphs than the largest graph property\nprediction dataset (with comparable graph sizes), and 770x larger graphs on\naverage compared to existing performance prediction datasets on machine\nlearning programs. This graph-level prediction task on large graphs introduces\nnew challenges in learning, ranging from scalability, training efficiency, to\nmodel quality.","terms":["cs.LG","cs.AR","cs.SI"]},{"titles":"Inherent Inconsistencies of Feature Importance","summaries":"The rapid advancement and widespread adoption of machine learning-driven\ntechnologies have underscored the practical and ethical need for creating\ninterpretable artificial intelligence systems. Feature importance, a method\nthat assigns scores to the contribution of individual features on prediction\noutcomes, seeks to bridge this gap as a tool for enhancing human comprehension\nof these systems. Feature importance serves as an explanation of predictions in\ndiverse contexts, whether by providing a global interpretation of a phenomenon\nacross the entire dataset or by offering a localized explanation for the\noutcome of a specific data point. Furthermore, feature importance is being used\nboth for explaining models and for identifying plausible causal relations in\nthe data, independently from the model. However, it is worth noting that these\nvarious contexts have traditionally been explored in isolation, with limited\ntheoretical foundations.\n  This paper presents an axiomatic framework designed to establish coherent\nrelationships among the different contexts of feature importance scores.\nNotably, our work unveils a surprising conclusion: when we combine the proposed\nproperties with those previously outlined in the literature, we demonstrate the\nexistence of an inconsistency. This inconsistency highlights that certain\nessential properties of feature importance scores cannot coexist harmoniously\nwithin a single framework.","terms":["cs.LG","cs.AI","cs.HC"]},{"titles":"DreamInpainter: Text-Guided Subject-Driven Image Inpainting with Diffusion Models","summaries":"This study introduces Text-Guided Subject-Driven Image Inpainting, a novel\ntask that combines text and exemplar images for image inpainting. While both\ntext and exemplar images have been used independently in previous efforts,\ntheir combined utilization remains unexplored. Simultaneously accommodating\nboth conditions poses a significant challenge due to the inherent balance\nrequired between editability and subject fidelity. To tackle this challenge, we\npropose a two-step approach DreamInpainter. First, we compute dense subject\nfeatures to ensure accurate subject replication. Then, we employ a\ndiscriminative token selection module to eliminate redundant subject details,\npreserving the subject's identity while allowing changes according to other\nconditions such as mask shape and text prompts. Additionally, we introduce a\ndecoupling regularization technique to enhance text control in the presence of\nexemplar images. Our extensive experiments demonstrate the superior performance\nof our method in terms of visual quality, identity preservation, and text\ncontrol, showcasing its effectiveness in the context of text-guided\nsubject-driven image inpainting.","terms":["cs.CV"]},{"titles":"Deep Set Neural Networks for forecasting asynchronous bioprocess timeseries","summaries":"Cultivation experiments often produce sparse and irregular time series.\nClassical approaches based on mechanistic models, like Maximum Likelihood\nfitting or Monte-Carlo Markov chain sampling, can easily account for sparsity\nand time-grid irregularities, but most statistical and Machine Learning tools\nare not designed for handling sparse data out-of-the-box. Among popular\napproaches there are various schemes for filling missing values (imputation)\nand interpolation into a regular grid (alignment). However, such methods\ntransfer the biases of the interpolation or imputation models to the target\nmodel. We show that Deep Set Neural Networks equipped with triplet encoding of\nthe input data can successfully handle bio-process data without any need for\nimputation or alignment procedures. The method is agnostic to the particular\nnature of the time series and can be adapted for any task, for example, online\nmonitoring, predictive control, design of experiments, etc. In this work, we\nfocus on forecasting. We argue that such an approach is especially suitable for\ntypical cultivation processes, demonstrate the performance of the method on\nseveral forecasting tasks using data generated from macrokinetic growth models\nunder realistic conditions, and compare the method to a conventional fitting\nprocedure and methods based on imputation and alignment.","terms":["cs.LG"]},{"titles":"Deep Learning for Fast Inference of Mechanistic Models' Parameters","summaries":"Inferring parameters of macro-kinetic growth models, typically represented by\nOrdinary Differential Equations (ODE), from the experimental data is a crucial\nstep in bioprocess engineering. Conventionally, estimates of the parameters are\nobtained by fitting the mechanistic model to observations. Fitting, however,\nrequires a significant computational power. Specifically, during the\ndevelopment of new bioprocesses that use previously unknown organisms or\nstrains, efficient, robust, and computationally cheap methods for parameter\nestimation are of great value. In this work, we propose using Deep Neural\nNetworks (NN) for directly predicting parameters of mechanistic models given\nobservations. The approach requires spending computational resources for\ntraining a NN, nonetheless, once trained, such a network can provide parameter\nestimates orders of magnitude faster than conventional methods. We consider a\ntraining procedure that combines Neural Networks and mechanistic models. We\ndemonstrate the performance of the proposed algorithms on data sampled from\nseveral mechanistic models used in bioengineering describing a typical\nindustrial batch process and compare the proposed method, a typical\ngradient-based fitting procedure, and the combination of the two. We find that,\nwhile Neural Network estimates are slightly improved by further fitting, these\nestimates are measurably better than the fitting procedure alone.","terms":["cs.LG","q-bio.QM"]},{"titles":"Data-Driven Target Localization Using Adaptive Radar Processing and Convolutional Neural Networks","summaries":"Leveraging the advanced functionalities of modern radio frequency (RF)\nmodeling and simulation tools, specifically designed for adaptive radar\nprocessing applications, this paper presents a data-driven approach to improve\naccuracy in radar target localization post adaptive radar detection. To this\nend, we generate a large number of radar returns by randomly placing targets of\nvariable strengths in a predefined area, using RFView, a high-fidelity,\nsite-specific, RF modeling & simulation tool. We produce heatmap tensors from\nthe radar returns, in range, azimuth [and Doppler], of the normalized adaptive\nmatched filter (NAMF) test statistic. We then train a regression convolutional\nneural network (CNN) to estimate target locations from these heatmap tensors,\nand we compare the target localization accuracy of this approach with that of\npeak-finding and local search methods. This empirical study shows that our\nregression CNN achieves a considerable improvement in target location\nestimation accuracy. The regression CNN offers significant gains and reasonable\naccuracy even at signal-to-clutter-plus-noise ratio (SCNR) regimes that are\nclose to the breakdown threshold SCNR of the NAMF. We also study the robustness\nof our trained CNN to mismatches in the radar data, where the CNN is tested on\nheatmap tensors collected from areas that it was not trained on. We show that\nour CNN can be made robust to mismatches in the radar data through few-shot\nlearning, using a relatively small number of new training samples.","terms":["cs.CV","eess.SP"]},{"titles":"HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces","summaries":"Neural radiance fields provide state-of-the-art view synthesis quality but\ntend to be slow to render. One reason is that they make use of volume\nrendering, thus requiring many samples (and model queries) per ray at render\ntime. Although this representation is flexible and easy to optimize, most\nreal-world objects can be modeled more efficiently with surfaces instead of\nvolumes, requiring far fewer samples per ray. This observation has spurred\nconsiderable progress in surface representations such as signed distance\nfunctions, but these may struggle to model semi-opaque and thin structures. We\npropose a method, HybridNeRF, that leverages the strengths of both\nrepresentations by rendering most objects as surfaces while modeling the\n(typically) small fraction of challenging regions volumetrically. We evaluate\nHybridNeRF against the challenging Eyeful Tower dataset along with other\ncommonly used view synthesis datasets. When comparing to state-of-the-art\nbaselines, including recent rasterization-based approaches, we improve error\nrates by 15-30% while achieving real-time framerates (at least 36 FPS) for\nvirtual-reality resolutions (2Kx2K).","terms":["cs.CV","cs.GR","cs.LG"]},{"titles":"Effective Backdoor Mitigation Depends on the Pre-training Objective","summaries":"Despite the advanced capabilities of contemporary machine learning (ML)\nmodels, they remain vulnerable to adversarial and backdoor attacks. This\nvulnerability is particularly concerning in real-world deployments, where\ncompromised models may exhibit unpredictable behavior in critical scenarios.\nSuch risks are heightened by the prevalent practice of collecting massive,\ninternet-sourced datasets for pre-training multimodal models, as these datasets\nmay harbor backdoors. Various techniques have been proposed to mitigate the\neffects of backdooring in these models such as CleanCLIP which is the current\nstate-of-the-art approach. In this work, we demonstrate that the efficacy of\nCleanCLIP in mitigating backdoors is highly dependent on the particular\nobjective used during model pre-training. We observe that stronger pre-training\nobjectives correlate with harder to remove backdoors behaviors. We show this by\ntraining multimodal models on two large datasets consisting of 3 million (CC3M)\nand 6 million (CC6M) datapoints, under various pre-training objectives,\nfollowed by poison removal using CleanCLIP. We find that CleanCLIP is\nineffective when stronger pre-training objectives are used, even with extensive\nhyperparameter tuning. Our findings underscore critical considerations for ML\npractitioners who pre-train models using large-scale web-curated data and are\nconcerned about potential backdoor threats. Notably, our results suggest that\nsimpler pre-training objectives are more amenable to effective backdoor\nremoval. This insight is pivotal for practitioners seeking to balance the\ntrade-offs between using stronger pre-training objectives and security against\nbackdoor attacks.","terms":["cs.LG","cs.AI","cs.CV"]},{"titles":"ViscoNet: Bridging and Harmonizing Visual and Textual Conditioning for ControlNet","summaries":"This paper introduces ViscoNet, a novel method that enhances text-to-image\nhuman generation models with visual prompting. Unlike existing methods that\nrely on lengthy text descriptions to control the image structure, ViscoNet\nallows users to specify the visual appearance of the target object with a\nreference image. ViscoNet disentangles the object's appearance from the image\nbackground and injects it into a pre-trained latent diffusion model (LDM) model\nvia a ControlNet branch. This way, ViscoNet mitigates the style mode collapse\nproblem and enables precise and flexible visual control. We demonstrate the\neffectiveness of ViscoNet on human image generation, where it can manipulate\nvisual attributes and artistic styles with text and image prompts. We also show\nthat ViscoNet can learn visual conditioning from small and specific object\ndomains while preserving the generative power of the LDM backbone.","terms":["cs.CV"]},{"titles":"Multitask Learning Can Improve Worst-Group Outcomes","summaries":"In order to create machine learning systems that serve a variety of users\nwell, it is vital to not only achieve high average performance but also ensure\nequitable outcomes across diverse groups. However, most machine learning\nmethods are designed to improve a model's average performance on a chosen end\ntask without consideration for their impact on worst group error. Multitask\nlearning (MTL) is one such widely used technique. In this paper, we seek not\nonly to understand the impact of MTL on worst-group accuracy but also to\nexplore its potential as a tool to address the challenge of group-wise\nfairness. We primarily consider the common setting of fine-tuning a pre-trained\nmodel, where, following recent work (Gururangan et al., 2020; Dery et al.,\n2023), we multitask the end task with the pre-training objective constructed\nfrom the end task data itself. In settings with few or no group annotations, we\nfind that multitasking often, but not always, achieves better worst-group\naccuracy than Just-Train-Twice (JTT; Liu et al. (2021)) -- a representative\ndistributionally robust optimization (DRO) method. Leveraging insights from\nsynthetic data experiments, we propose to modify standard MTL by regularizing\nthe joint multitask representation space. We run a large number of fine-tuning\nexperiments across computer vision and natural language and find that our\nregularized MTL approach consistently outperforms JTT on both worst and average\ngroup outcomes. Our official code can be found here:\nhttps:\/\/github.com\/atharvajk98\/MTL-group-robustness.","terms":["cs.LG"]},{"titles":"Neural parameter calibration and uncertainty quantification for epidemic forecasting","summaries":"The recent COVID-19 pandemic has thrown the importance of accurately\nforecasting contagion dynamics and learning infection parameters into sharp\nfocus. At the same time, effective policy-making requires knowledge of the\nuncertainty on such predictions, in order, for instance, to be able to ready\nhospitals and intensive care units for a worst-case scenario without needlessly\nwasting resources. In this work, we apply a novel and powerful computational\nmethod to the problem of learning probability densities on contagion parameters\nand providing uncertainty quantification for pandemic projections. Using a\nneural network, we calibrate an ODE model to data of the spread of COVID-19 in\nBerlin in 2020, achieving both a significantly more accurate calibration and\nprediction than Markov-Chain Monte Carlo (MCMC)-based sampling schemes. The\nuncertainties on our predictions provide meaningful confidence intervals e.g.\non infection figures and hospitalisation rates, while training and running the\nneural scheme takes minutes where MCMC takes hours. We show convergence of our\nmethod to the true posterior on a simplified SIR model of epidemics, and also\ndemonstrate our method's learning capabilities on a reduced dataset, where a\ncomplex model is learned from a small number of compartments for which data is\navailable.","terms":["cs.LG","math.OC","physics.soc-ph","49-02, 92-02, 68-02","J.3; G.1.6; I.2.1; G.3"]},{"titles":"FlexModel: A Framework for Interpretability of Distributed Large Language Models","summaries":"With the growth of large language models, now incorporating billions of\nparameters, the hardware prerequisites for their training and deployment have\nseen a corresponding increase. Although existing tools facilitate model\nparallelization and distributed training, deeper model interactions, crucial\nfor interpretability and responsible AI techniques, still demand thorough\nknowledge of distributed computing. This often hinders contributions from\nresearchers with machine learning expertise but limited distributed computing\nbackground. Addressing this challenge, we present FlexModel, a software package\nproviding a streamlined interface for engaging with models distributed across\nmulti-GPU and multi-node configurations. The library is compatible with\nexisting model distribution libraries and encapsulates PyTorch models. It\nexposes user-registerable HookFunctions to facilitate straightforward\ninteraction with distributed model internals, bridging the gap between\ndistributed and single-device model paradigms. Primarily, FlexModel enhances\naccessibility by democratizing model interactions and promotes more inclusive\nresearch in the domain of large-scale neural networks. The package is found at\nhttps:\/\/github.com\/VectorInstitute\/flex_model.","terms":["cs.LG","cs.AI","cs.CL","cs.DC"]},{"titles":"Delegated Classification","summaries":"When machine learning is outsourced to a rational agent, conflicts of\ninterest might arise and severely impact predictive performance. In this work,\nwe propose a theoretical framework for incentive-aware delegation of machine\nlearning tasks. We model delegation as a principal-agent game, in which\naccurate learning can be incentivized by the principal using performance-based\ncontracts. Adapting the economic theory of contract design to this setting, we\ndefine budget-optimal contracts and prove they take a simple threshold form\nunder reasonable assumptions. In the binary-action case, the optimality of such\ncontracts is shown to be equivalent to the classic Neyman-Pearson lemma,\nestablishing a formal connection between contract design and statistical\nhypothesis testing. Empirically, we demonstrate that budget-optimal contracts\ncan be constructed using small-scale data, leveraging recent advances in the\nstudy of learning curves and scaling laws. Performance and economic outcomes\nare evaluated using synthetic and real-world classification tasks.","terms":["cs.LG","cs.CY","cs.GT"]},{"titles":"Model Reprogramming: Resource-Efficient Cross-Domain Machine Learning","summaries":"In data-rich domains such as vision, language, and speech, deep learning\nprevails to deliver high-performance task-specific models and can even learn\ngeneral task-agnostic representations for efficient finetuning to downstream\ntasks. However, deep learning in resource-limited domains still faces multiple\nchallenges including (i) limited data, (ii) constrained model development cost,\nand (iii) lack of adequate pre-trained models for effective finetuning. This\npaper provides an overview of model reprogramming to bridge this gap. Model\nreprogramming enables resource-efficient cross-domain machine learning by\nrepurposing and reusing a well-developed pre-trained model from a source domain\nto solve tasks in a target domain without model finetuning, where the source\nand target domains can be vastly different. In many applications, model\nreprogramming outperforms transfer learning and training from scratch. This\npaper elucidates the methodology of model reprogramming, summarizes existing\nuse cases, provides a theoretical explanation of the success of model\nreprogramming, and concludes with a discussion on open-ended research questions\nand opportunities. A list of model reprogramming studies is actively maintained\nand updated at https:\/\/github.com\/IBM\/model-reprogramming.","terms":["cs.LG","cs.AI"]},{"titles":"PromptonomyViT: Multi-Task Prompt Learning Improves Video Transformers using Synthetic Scene Data","summaries":"Action recognition models have achieved impressive results by incorporating\nscene-level annotations, such as objects, their relations, 3D structure, and\nmore. However, obtaining annotations of scene structure for videos requires a\nsignificant amount of effort to gather and annotate, making these methods\nexpensive to train. In contrast, synthetic datasets generated by graphics\nengines provide powerful alternatives for generating scene-level annotations\nacross multiple tasks. In this work, we propose an approach to leverage\nsynthetic scene data for improving video understanding. We present a multi-task\nprompt learning approach for video transformers, where a shared video\ntransformer backbone is enhanced by a small set of specialized parameters for\neach task. Specifically, we add a set of \"task prompts\", each corresponding to\na different task, and let each prompt predict task-related annotations. This\ndesign allows the model to capture information shared among synthetic scene\ntasks as well as information shared between synthetic scene tasks and a real\nvideo downstream task throughout the entire network. We refer to this approach\nas \"Promptonomy\", since the prompts model task-related structure. We propose\nthe PromptonomyViT model (PViT), a video transformer that incorporates various\ntypes of scene-level information from synthetic data using the \"Promptonomy\"\napproach. PViT shows strong performance improvements on multiple video\nunderstanding tasks and datasets. Project page:\n\\url{https:\/\/ofir1080.github.io\/PromptonomyViT}","terms":["cs.CV"]},{"titles":"The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning","summaries":"With the advance of the powerful heterogeneous, parallel and distributed\ncomputing systems and ever increasing immense amount of data, machine learning\nhas become an indispensable part of cutting-edge technology, scientific\nresearch and consumer products. In this study, we present a review of modern\nmachine and deep learning. We provide a high-level overview for the latest\nadvanced machine learning algorithms, applications, and frameworks. Our\ndiscussion encompasses parallel distributed learning, deep learning as well as\nfederated learning. As a result, our work serves as an introductory text to the\nvast field of modern machine learning.","terms":["cs.LG","cs.AI","cs.DC"]},{"titles":"AI-SAM: Automatic and Interactive Segment Anything Model","summaries":"Semantic segmentation is a core task in computer vision. Existing methods are\ngenerally divided into two categories: automatic and interactive. Interactive\napproaches, exemplified by the Segment Anything Model (SAM), have shown promise\nas pre-trained models. However, current adaptation strategies for these models\ntend to lean towards either automatic or interactive approaches. Interactive\nmethods depend on prompts user input to operate, while automatic ones bypass\nthe interactive promptability entirely. Addressing these limitations, we\nintroduce a novel paradigm and its first model: the Automatic and Interactive\nSegment Anything Model (AI-SAM). In this paradigm, we conduct a comprehensive\nanalysis of prompt quality and introduce the pioneering Automatic and\nInteractive Prompter (AI-Prompter) that automatically generates initial point\nprompts while accepting additional user inputs. Our experimental results\ndemonstrate AI-SAM's effectiveness in the automatic setting, achieving\nstate-of-the-art performance. Significantly, it offers the flexibility to\nincorporate additional user prompts, thereby further enhancing its performance.\nThe project page is available at https:\/\/github.com\/ymp5078\/AI-SAM.","terms":["cs.CV"]},{"titles":"Cross-feature Contrastive Loss for Decentralized Deep Learning on Heterogeneous Data","summaries":"The current state-of-the-art decentralized learning algorithms mostly assume\nthe data distribution to be Independent and Identically Distributed (IID).\nHowever, in practical scenarios, the distributed datasets can have\nsignificantly heterogeneous data distributions across the agents. In this work,\nwe present a novel approach for decentralized learning on heterogeneous data,\nwhere data-free knowledge distillation through contrastive loss on\ncross-features is utilized to improve performance. Cross-features for a pair of\nneighboring agents are the features (i.e., last hidden layer activations)\nobtained from the data of an agent with respect to the model parameters of the\nother agent. We demonstrate the effectiveness of the proposed technique through\nan exhaustive set of experiments on various Computer Vision datasets (CIFAR-10,\nCIFAR-100, Fashion MNIST, Imagenette, and ImageNet), model architectures, and\nnetwork topologies. Our experiments show that the proposed method achieves\nsuperior performance (0.2-4% improvement in test accuracy) compared to other\nexisting techniques for decentralized learning on heterogeneous data.","terms":["cs.LG"]},{"titles":"Vicious Classifiers: Data Reconstruction Attack at Inference Time","summaries":"Privacy-preserving inference in edge computing paradigms encourages the users\nof machine-learning services to locally run a model on their private input, for\na target task, and only share the model's outputs with the server. We study how\na vicious server can reconstruct the input data by observing only the model's\noutputs, while keeping the target accuracy very close to that of a honest\nserver: by jointly training a target model (to run at users' side) and an\nattack model for data reconstruction (to secretly use at server's side). We\npresent a new measure to assess the reconstruction risk in edge inference. Our\nevaluations on six benchmark datasets demonstrate that the model's input can be\napproximately reconstructed from the outputs of a single target inference. We\npropose a potential defense mechanism that helps to distinguish vicious versus\nhonest classifiers at inference time. We discuss open challenges and directions\nfor future studies and release our code as a benchmark for future work.","terms":["cs.LG","cs.CR","cs.IT","math.IT"]},{"titles":"Learning Deep O($n$)-Equivariant Hyperspheres","summaries":"This paper presents an approach to learning (deep) $n$D features equivariant\nunder orthogonal transformations, utilizing hyperspheres and regular\n$n$-simplexes. Our main contributions are theoretical and tackle major\nchallenges in geometric deep learning such as equivariance and invariance under\ngeometric transformations. Namely, we enrich the recently developed theory of\nsteerable 3D spherical neurons -- SO(3)-equivariant filter banks based on\nneurons with spherical decision surfaces -- by extending said neurons to $n$D,\nwhich we call deep equivariant hyperspheres, and enabling their multi-layer\nconstruction. Using synthetic and real-world data in $n$D, we experimentally\nverify our theoretical contributions and find that our approach is superior to\nthe competing methods for benchmark datasets in all but one case, additionally\ndemonstrating a better speed\/performance trade-off in all but one other case.","terms":["cs.LG"]},{"titles":"The Use of Multi-Scale Fiducial Markers To Aid Takeoff and Landing Navigation by Rotorcraft","summaries":"This paper quantifies the performance of visual SLAM that leverages\nmulti-scale fiducial markers (i.e., artificial landmarks that can be detected\nat a wide range of distances) to show its potential for reliable takeoff and\nlanding navigation in rotorcraft. Prior work has shown that square markers with\na black-and-white pattern of grid cells can be used to improve the performance\nof visual SLAM with color cameras. We extend this prior work to allow nested\nmarker layouts. We evaluate performance during semi-autonomous takeoff and\nlanding operations in a variety of environmental conditions by a DJI Matrice\n300 RTK rotorcraft with two FLIR Blackfly color cameras, using RTK GNSS to\nobtain ground truth pose estimates. Performance measures include absolute\ntrajectory error and the fraction of the number of estimated poses to the total\nframe. We release all of our results -- our dataset and the code of the\nimplementation of the visual SLAM with fiducial markers -- to the public as\nopen-source.","terms":["cs.CV","cs.RO"]},{"titles":"Unknown Sample Discovery for Source Free Open Set Domain Adaptation","summaries":"Open Set Domain Adaptation (OSDA) aims to adapt a model trained on a source\ndomain to a target domain that undergoes distribution shift and contains\nsamples from novel classes outside the source domain. Source-free OSDA\n(SF-OSDA) techniques eliminate the need to access source domain samples, but\ncurrent SF-OSDA methods utilize only the known classes in the target domain for\nadaptation, and require access to the entire target domain even during\ninference after adaptation, to make the distinction between known and unknown\nsamples. In this paper, we introduce Unknown Sample Discovery (USD) as an\nSF-OSDA method that utilizes a temporally ensembled teacher model to conduct\nknown-unknown target sample separation and adapts the student model to the\ntarget domain over all classes using co-training and temporal consistency\nbetween the teacher and the student. USD promotes Jensen-Shannon distance (JSD)\nas an effective measure for known-unknown sample separation. Our\nteacher-student framework significantly reduces error accumulation resulting\nfrom imperfect known-unknown sample separation, while curriculum guidance helps\nto reliably learn the distinction between target known and target unknown\nsubspaces. USD appends the target model with an unknown class node, thus\nreadily classifying a target sample into any of the known or unknown classes in\nsubsequent post-adaptation inference stages. Empirical results show that USD is\nsuperior to existing SF-OSDA methods and is competitive with current OSDA\nmodels that utilize both source and target domains during adaptation.","terms":["cs.CV","cs.AI"]},{"titles":"Functional Flow Matching","summaries":"We propose Functional Flow Matching (FFM), a function-space generative model\nthat generalizes the recently-introduced Flow Matching model to operate in\ninfinite-dimensional spaces. Our approach works by first defining a path of\nprobability measures that interpolates between a fixed Gaussian measure and the\ndata distribution, followed by learning a vector field on the underlying space\nof functions that generates this path of measures. Our method does not rely on\nlikelihoods or simulations, making it well-suited to the function space\nsetting. We provide both a theoretical framework for building such models and\nan empirical evaluation of our techniques. We demonstrate through experiments\non several real-world benchmarks that our proposed FFM method outperforms\nseveral recently proposed function-space generative models.","terms":["cs.LG","stat.ML"]},{"titles":"Precise Asymptotic Generalization for Multiclass Classification with Overparameterized Linear Models","summaries":"We study the asymptotic generalization of an overparameterized linear model\nfor multiclass classification under the Gaussian covariates bi-level model\nintroduced in Subramanian et al.~'22, where the number of data points,\nfeatures, and classes all grow together. We fully resolve the conjecture posed\nin Subramanian et al.~'22, matching the predicted regimes for generalization.\nFurthermore, our new lower bounds are akin to an information-theoretic strong\nconverse: they establish that the misclassification rate goes to 0 or 1\nasymptotically. One surprising consequence of our tight results is that the\nmin-norm interpolating classifier can be asymptotically suboptimal relative to\nnoninterpolating classifiers in the regime where the min-norm interpolating\nregressor is known to be optimal.\n  The key to our tight analysis is a new variant of the Hanson-Wright\ninequality which is broadly useful for multiclass problems with sparse labels.\nAs an application, we show that the same type of analysis can be used to\nanalyze the related multilabel classification problem under the same bi-level\nensemble.","terms":["cs.LG","stat.ML"]},{"titles":"An Automated Machine Learning Approach for Detecting Anomalous Peak Patterns in Time Series Data from a Research Watershed in the Northeastern United States Critical Zone","summaries":"This paper presents an automated machine learning framework designed to\nassist hydrologists in detecting anomalies in time series data generated by\nsensors in a research watershed in the northeastern United States critical\nzone. The framework specifically focuses on identifying peak-pattern anomalies,\nwhich may arise from sensor malfunctions or natural phenomena. However, the use\nof classification methods for anomaly detection poses challenges, such as the\nrequirement for labeled data as ground truth and the selection of the most\nsuitable deep learning model for the given task and dataset. To address these\nchallenges, our framework generates labeled datasets by injecting synthetic\npeak patterns into synthetically generated time series data and incorporates an\nautomated hyperparameter optimization mechanism. This mechanism generates an\noptimized model instance with the best architectural and training parameters\nfrom a pool of five selected models, namely Temporal Convolutional Network\n(TCN), InceptionTime, MiniRocket, Residual Networks (ResNet), and Long\nShort-Term Memory (LSTM). The selection is based on the user's preferences\nregarding anomaly detection accuracy and computational cost. The framework\nemploys Time-series Generative Adversarial Networks (TimeGAN) as the synthetic\ndataset generator. The generated model instances are evaluated using a\ncombination of accuracy and computational cost metrics, including training time\nand memory, during the anomaly detection process. Performance evaluation of the\nframework was conducted using a dataset from a watershed, demonstrating\nconsistent selection of the most fitting model instance that satisfies the\nuser's preferences.","terms":["cs.LG","cs.AI"]},{"titles":"zkDL: Efficient Zero-Knowledge Proofs of Deep Learning Training","summaries":"The recent advancements in deep learning have brought about significant\nchanges in various aspects of people's lives. Meanwhile, these rapid\ndevelopments have raised concerns about the legitimacy of the training process\nof deep neural networks. To protect the intellectual properties of AI\ndevelopers, directly examining the training process by accessing the model\nparameters and training data is often prohibited for verifiers.\n  In response to this challenge, we present zero-knowledge deep learning\n(zkDL), an efficient zero-knowledge proof for deep learning training. To\naddress the long-standing challenge of verifiable computations of\nnon-linearities in deep learning training, we introduce zkReLU, a specialized\nproof for the ReLU activation and its backpropagation. zkReLU turns the\ndisadvantage of non-arithmetic relations into an advantage, leading to the\ncreation of FAC4DNN, our specialized arithmetic circuit design for modelling\nneural networks. This design aggregates the proofs over different layers and\ntraining steps, without being constrained by their sequential order in the\ntraining process.\n  With our new CUDA implementation that achieves full compatibility with the\ntensor structures and the aggregated proof design, zkDL enables the generation\nof complete and sound proofs in less than a second per batch update for an\n8-layer neural network with 10M parameters and a batch size of 64, while\nprovably ensuring the privacy of data and model parameters. To our best\nknowledge, we are not aware of any existing work on zero-knowledge proof of\ndeep learning training that is scalable to million-size networks.","terms":["cs.LG","cs.CR"]},{"titles":"Unsupervised Keypoints from Pretrained Diffusion Models","summaries":"Unsupervised learning of keypoints and landmarks has seen significant\nprogress with the help of modern neural network architectures, but performance\nis yet to match the supervised counterpart, making their practicability\nquestionable. We leverage the emergent knowledge within text-to-image diffusion\nmodels, towards more robust unsupervised keypoints. Our core idea is to find\ntext embeddings that would cause the generative model to consistently attend to\ncompact regions in images (i.e. keypoints). To do so, we simply optimize the\ntext embedding such that the cross-attention maps within the denoising network\nare localized as Gaussians with small standard deviations. We validate our\nperformance on multiple datasets: the CelebA, CUB-200-2011, Tai-Chi-HD,\nDeepFashion, and Human3.6m datasets. We achieve significantly improved\naccuracy, sometimes even outperforming supervised ones, particularly for data\nthat is non-aligned and less curated. Our code is publicly available and can be\nfound through our project page: https:\/\/ubc-vision.github.io\/StableKeypoints\/","terms":["cs.CV"]},{"titles":"Incidental Polysemanticity","summaries":"Polysemantic neurons (neurons that activate for a set of unrelated features)\nhave been seen as a significant obstacle towards interpretability of\ntask-optimized deep networks, with implications for AI safety. The classic\norigin story of polysemanticity is that the data contains more \"features\" than\nneurons, such that learning to perform a task forces the network to co-allocate\nmultiple unrelated features to the same neuron, endangering our ability to\nunderstand the network's internal processing. In this work, we present a second\nand non-mutually exclusive origin story of polysemanticity. We show that\npolysemanticity can arise incidentally, even when there are ample neurons to\nrepresent all features in the data, using a combination of theory and\nexperiments. This second type of polysemanticity occurs because random\ninitialization can, by chance alone, initially assign multiple features to the\nsame neuron, and the training dynamics then strengthen such overlap. Due to its\norigin, we term this \\textit{incidental polysemanticity}.","terms":["cs.LG","cs.AI","cs.NE"]},{"titles":"Similarity-based Knowledge Transfer for Cross-Domain Reinforcement Learning","summaries":"Transferring knowledge in cross-domain reinforcement learning is a\nchallenging setting in which learning is accelerated by reusing knowledge from\na task with different observation and\/or action space. However, it is often\nnecessary to carefully select the source of knowledge for the receiving end to\nbenefit from the transfer process. In this article, we study how to measure the\nsimilarity between cross-domain reinforcement learning tasks to select a source\nof knowledge that will improve the performance of the learning agent. We\ndeveloped a semi-supervised alignment loss to match different spaces with a set\nof encoder-decoders, and use them to measure similarity and transfer policies\nacross tasks. In comparison to prior works, our method does not require data to\nbe aligned, paired or collected by expert policies. Experimental results, on a\nset of varied Mujoco control tasks, show the robustness of our method in\neffectively selecting and transferring knowledge, without the supervision of a\ntailored set of source tasks.","terms":["cs.LG","cs.AI","68T37, 68T42, 68T07, 68T05"]},{"titles":"Gaussian3Diff: 3D Gaussian Diffusion for 3D Full Head Synthesis and Editing","summaries":"We present a novel framework for generating photorealistic 3D human head and\nsubsequently manipulating and reposing them with remarkable flexibility. The\nproposed approach leverages an implicit function representation of 3D human\nheads, employing 3D Gaussians anchored on a parametric face model. To enhance\nrepresentational capabilities and encode spatial information, we embed a\nlightweight tri-plane payload within each Gaussian rather than directly storing\ncolor and opacity. Additionally, we parameterize the Gaussians in a 2D UV space\nvia a 3DMM, enabling effective utilization of the diffusion model for 3D head\navatar generation. Our method facilitates the creation of diverse and realistic\n3D human heads with fine-grained editing over facial features and expressions.\nExtensive experiments demonstrate the effectiveness of our method.","terms":["cs.CV","cs.GR","cs.LG"]},{"titles":"ScAR: Scaling Adversarial Robustness for LiDAR Object Detection","summaries":"The adversarial robustness of a model is its ability to resist adversarial\nattacks in the form of small perturbations to input data. Universal adversarial\nattack methods such as Fast Sign Gradient Method (FSGM) and Projected Gradient\nDescend (PGD) are popular for LiDAR object detection, but they are often\ndeficient compared to task-specific adversarial attacks. Additionally, these\nuniversal methods typically require unrestricted access to the model's\ninformation, which is difficult to obtain in real-world applications. To\naddress these limitations, we present a black-box Scaling Adversarial\nRobustness (ScAR) method for LiDAR object detection. By analyzing the\nstatistical characteristics of 3D object detection datasets such as KITTI,\nWaymo, and nuScenes, we have found that the model's prediction is sensitive to\nscaling of 3D instances. We propose three black-box scaling adversarial attack\nmethods based on the available information: model-aware attack,\ndistribution-aware attack, and blind attack. We also introduce a strategy for\ngenerating scaling adversarial examples to improve the model's robustness\nagainst these three scaling adversarial attacks. Comparison with other methods\non public datasets under different 3D object detection architectures\ndemonstrates the effectiveness of our proposed method.","terms":["cs.CV"]},{"titles":"Colour versus Shape Goal Misgeneralization in Reinforcement Learning: A Case Study","summaries":"We explore colour versus shape goal misgeneralization originally demonstrated\nby Di Langosco et al. (2022) in the Procgen Maze environment, where, given an\nambiguous choice, the agents seem to prefer generalization based on colour\nrather than shape. After training over 1,000 agents in a simplified version of\nthe environment and evaluating them on over 10 million episodes, we conclude\nthat the behaviour can be attributed to the agents learning to detect the goal\nobject through a specific colour channel. This choice is arbitrary.\nAdditionally, we show how, due to underspecification, the preferences can\nchange when retraining the agents using exactly the same procedure except for\nusing a different random seed for the training run. Finally, we demonstrate the\nexistence of outliers in out-of-distribution behaviour based on training random\nseed alone.","terms":["cs.LG","cs.AI"]},{"titles":"InfoBot: Transfer and Exploration via the Information Bottleneck","summaries":"A central challenge in reinforcement learning is discovering effective\npolicies for tasks where rewards are sparsely distributed. We postulate that in\nthe absence of useful reward signals, an effective exploration strategy should\nseek out {\\it decision states}. These states lie at critical junctions in the\nstate space from where the agent can transition to new, potentially unexplored\nregions. We propose to learn about decision states from prior experience. By\ntraining a goal-conditioned policy with an information bottleneck, we can\nidentify decision states by examining where the model actually leverages the\ngoal state. We find that this simple mechanism effectively identifies decision\nstates, even in partially observed settings. In effect, the model learns the\nsensory cues that correlate with potential subgoals. In new environments, this\nmodel can then identify novel subgoals for further exploration, guiding the\nagent through a sequence of potential decision states and through new regions\nof the state space.","terms":["stat.ML","cs.LG"]},{"titles":"LooseControl: Lifting ControlNet for Generalized Depth Conditioning","summaries":"We present LooseControl to allow generalized depth conditioning for\ndiffusion-based image generation. ControlNet, the SOTA for depth-conditioned\nimage generation, produces remarkable results but relies on having access to\ndetailed depth maps for guidance. Creating such exact depth maps, in many\nscenarios, is challenging. This paper introduces a generalized version of depth\nconditioning that enables many new content-creation workflows. Specifically, we\nallow (C1) scene boundary control for loosely specifying scenes with only\nboundary conditions, and (C2) 3D box control for specifying layout locations of\nthe target objects rather than the exact shape and appearance of the objects.\nUsing LooseControl, along with text guidance, users can create complex\nenvironments (e.g., rooms, street views, etc.) by specifying only scene\nboundaries and locations of primary objects. Further, we provide two editing\nmechanisms to refine the results: (E1) 3D box editing enables the user to\nrefine images by changing, adding, or removing boxes while freezing the style\nof the image. This yields minimal changes apart from changes induced by the\nedited boxes. (E2) Attribute editing proposes possible editing directions to\nchange one particular aspect of the scene, such as the overall object density\nor a particular object. Extensive tests and comparisons with baselines\ndemonstrate the generality of our method. We believe that LooseControl can\nbecome an important design tool for easily creating complex environments and be\nextended to other forms of guidance channels. Code and more information are\navailable at https:\/\/shariqfarooq123.github.io\/loose-control\/ .","terms":["cs.CV","cs.GR"]},{"titles":"ReconFusion: 3D Reconstruction with Diffusion Priors","summaries":"3D reconstruction methods such as Neural Radiance Fields (NeRFs) excel at\nrendering photorealistic novel views of complex scenes. However, recovering a\nhigh-quality NeRF typically requires tens to hundreds of input images,\nresulting in a time-consuming capture process. We present ReconFusion to\nreconstruct real-world scenes using only a few photos. Our approach leverages a\ndiffusion prior for novel view synthesis, trained on synthetic and multiview\ndatasets, which regularizes a NeRF-based 3D reconstruction pipeline at novel\ncamera poses beyond those captured by the set of input images. Our method\nsynthesizes realistic geometry and texture in underconstrained regions while\npreserving the appearance of observed regions. We perform an extensive\nevaluation across various real-world datasets, including forward-facing and\n360-degree scenes, demonstrating significant performance improvements over\nprevious few-view NeRF reconstruction approaches.","terms":["cs.CV"]},{"titles":"GPT4Point: A Unified Framework for Point-Language Understanding and Generation","summaries":"Multimodal Large Language Models (MLLMs) have excelled in 2D image-text\ncomprehension and image generation, but their understanding of the 3D world is\nnotably deficient, limiting progress in 3D language understanding and\ngeneration. To solve this problem, we introduce GPT4Point, an innovative\ngroundbreaking point-language multimodal model designed specifically for\nunified 3D object understanding and generation within the MLLM framework.\nGPT4Point as a powerful 3D MLLM seamlessly can execute a variety of point-text\nreference tasks such as point-cloud captioning and Q&A. Additionally, GPT4Point\nis equipped with advanced capabilities for controllable 3D generation, it can\nget high-quality results through a low-quality point-text feature maintaining\nthe geometric shapes and colors. To support the expansive needs of 3D\nobject-text pairs, we develop Pyramid-XL, a point-language dataset annotation\nengine. It constructs a large-scale database over 1M objects of varied text\ngranularity levels from the Objaverse-XL dataset, essential for training\nGPT4Point. A comprehensive benchmark has been proposed to evaluate 3D\npoint-language understanding capabilities. In extensive evaluations, GPT4Point\nhas demonstrated superior performance in understanding and generation.","terms":["cs.CV"]},{"titles":"DiffusionPCR: Diffusion Models for Robust Multi-Step Point Cloud Registration","summaries":"Point Cloud Registration (PCR) estimates the relative rigid transformation\nbetween two point clouds. We propose formulating PCR as a denoising diffusion\nprobabilistic process, mapping noisy transformations to the ground truth.\nHowever, using diffusion models for PCR has nontrivial challenges, such as\nadapting a generative model to a discriminative task and leveraging the\nestimated nonlinear transformation from the previous step. Instead of training\na diffusion model to directly map pure noise to ground truth, we map the\npredictions of an off-the-shelf PCR model to ground truth. The predictions of\noff-the-shelf models are often imperfect, especially in challenging cases where\nthe two points clouds have low overlap, and thus could be seen as noisy\nversions of the real rigid transformation. In addition, we transform the\nrotation matrix into a spherical linear space for interpolation between samples\nin the forward process, and convert rigid transformations into auxiliary\ninformation to implicitly exploit last-step estimations in the reverse process.\nAs a result, conditioned on time step, the denoising model adapts to the\nincreasing accuracy across steps and refines registrations. Our extensive\nexperiments showcase the effectiveness of our DiffusionPCR, yielding\nstate-of-the-art registration recall rates (95.3%\/81.6%) on 3DMatch and\n3DLoMatch. The code will be made public upon publication.","terms":["cs.CV"]},{"titles":"Describing Differences in Image Sets with Natural Language","summaries":"How do two sets of images differ? Discerning set-level differences is crucial\nfor understanding model behaviors and analyzing datasets, yet manually sifting\nthrough thousands of images is impractical. To aid in this discovery process,\nwe explore the task of automatically describing the differences between two\n$\\textbf{sets}$ of images, which we term Set Difference Captioning. This task\ntakes in image sets $D_A$ and $D_B$, and outputs a description that is more\noften true on $D_A$ than $D_B$. We outline a two-stage approach that first\nproposes candidate difference descriptions from image sets and then re-ranks\nthe candidates by checking how well they can differentiate the two sets. We\nintroduce VisDiff, which first captions the images and prompts a language model\nto propose candidate descriptions, then re-ranks these descriptions using CLIP.\nTo evaluate VisDiff, we collect VisDiffBench, a dataset with 187 paired image\nsets with ground truth difference descriptions. We apply VisDiff to various\ndomains, such as comparing datasets (e.g., ImageNet vs. ImageNetV2), comparing\nclassification models (e.g., zero-shot CLIP vs. supervised ResNet), summarizing\nmodel failure modes (supervised ResNet), characterizing differences between\ngenerative models (e.g., StableDiffusionV1 and V2), and discovering what makes\nimages memorable. Using VisDiff, we are able to find interesting and previously\nunknown differences in datasets and models, demonstrating its utility in\nrevealing nuanced insights.","terms":["cs.CV","cs.CL","cs.CY","cs.LG"]},{"titles":"GauHuman: Articulated Gaussian Splatting from Monocular Human Videos","summaries":"We present, GauHuman, a 3D human model with Gaussian Splatting for both fast\ntraining (1 ~ 2 minutes) and real-time rendering (up to 189 FPS), compared with\nexisting NeRF-based implicit representation modelling frameworks demanding\nhours of training and seconds of rendering per frame. Specifically, GauHuman\nencodes Gaussian Splatting in the canonical space and transforms 3D Gaussians\nfrom canonical space to posed space with linear blend skinning (LBS), in which\neffective pose and LBS refinement modules are designed to learn fine details of\n3D humans under negligible computational cost. Moreover, to enable fast\noptimization of GauHuman, we initialize and prune 3D Gaussians with 3D human\nprior, while splitting\/cloning via KL divergence guidance, along with a novel\nmerge operation for further speeding up. Extensive experiments on ZJU_Mocap and\nMonoCap datasets demonstrate that GauHuman achieves state-of-the-art\nperformance quantitatively and qualitatively with fast training and real-time\nrendering speed. Notably, without sacrificing rendering quality, GauHuman can\nfast model the 3D human performer with ~13k 3D Gaussians.","terms":["cs.CV"]},{"titles":"Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models","summaries":"Solving complex visual tasks such as \"Who invented the musical instrument on\nthe right?\" involves a composition of skills: understanding space, recognizing\ninstruments, and also retrieving prior knowledge. Recent work shows promise by\ndecomposing such tasks using a large language model (LLM) into an executable\nprogram that invokes specialized vision models. However, generated programs are\nerror-prone: they omit necessary steps, include spurious ones, and are unable\nto recover when the specialized models give incorrect outputs. Moreover, they\nrequire loading multiple models, incurring high latency and computation costs.\nWe propose Visual Program Distillation (VPD), an instruction tuning framework\nthat produces a vision-language model (VLM) capable of solving complex visual\ntasks with a single forward pass. VPD distills the reasoning ability of LLMs by\nusing them to sample multiple candidate programs, which are then executed and\nverified to identify a correct one. It translates each correct program into a\nlanguage description of the reasoning steps, which are then distilled into a\nVLM. Extensive experiments show that VPD improves the VLM's ability to count,\nunderstand spatial relations, and reason compositionally. Our VPD-trained\nPaLI-X outperforms all prior VLMs, achieving state-of-the-art performance\nacross complex vision tasks, including MMBench, OK-VQA, A-OKVQA, TallyQA, POPE,\nand Hateful Memes. An evaluation with human annotators also confirms that VPD\nimproves model response factuality and consistency. Finally, experiments on\ncontent moderation demonstrate that VPD is also helpful for adaptation to\nreal-world applications with limited data.","terms":["cs.CV","cs.CL"]},{"titles":"Alchemist: Parametric Control of Material Properties with Diffusion Models","summaries":"We propose a method to control material attributes of objects like roughness,\nmetallic, albedo, and transparency in real images. Our method capitalizes on\nthe generative prior of text-to-image models known for photorealism, employing\na scalar value and instructions to alter low-level material properties.\nAddressing the lack of datasets with controlled material attributes, we\ngenerated an object-centric synthetic dataset with physically-based materials.\nFine-tuning a modified pre-trained text-to-image model on this synthetic\ndataset enables us to edit material properties in real-world images while\npreserving all other attributes. We show the potential application of our model\nto material edited NeRFs.","terms":["cs.CV","cs.AI","cs.GR"]},{"titles":"Geometry-Aware Normalizing Wasserstein Flows for Optimal Causal Inference","summaries":"This manuscript enriches the framework of continuous normalizing flows (CNFs)\nwithin causal inference, primarily to augment the geometric properties of\nparametric submodels used in targeted maximum likelihood estimation (TMLE). By\nintroducing an innovative application of CNFs, we construct a refined series of\nparametric submodels that enable a directed interpolation between the prior\ndistribution $p_0$ and the empirical distribution $p_1$. This proposed\nmethodology serves to optimize the semiparametric efficiency bound in causal\ninference by orchestrating CNFs to align with Wasserstein gradient flows. Our\napproach not only endeavors to minimize the mean squared error in the\nestimation but also imbues the estimators with geometric sophistication,\nthereby enhancing robustness against misspecification. This robustness is\ncrucial, as it alleviates the dependence on the standard $n^{\\frac{1}{4}}$ rate\nfor a doubly-robust perturbation direction in TMLE. By incorporating robust\noptimization principles and differential geometry into the estimators, the\ndeveloped geometry-aware CNFs represent a significant advancement in the\npursuit of doubly robust causal inference.","terms":["cs.LG","stat.ML"]},{"titles":"AmbiGen: Generating Ambigrams from Pre-trained Diffusion Model","summaries":"Ambigrams are calligraphic designs that have different meanings depending on\nthe viewing orientation. Creating ambigrams is a challenging task even for\nskilled artists, as it requires maintaining the meaning under two different\nviewpoints at the same time. In this work, we propose to generate ambigrams by\ndistilling a large-scale vision and language diffusion model, namely DeepFloyd\nIF, to optimize the letters' outline for legibility in the two viewing\norientations. Empirically, we demonstrate that our approach outperforms\nexisting ambigram generation methods. On the 500 most common words in English,\nour method achieves more than an 11.6% increase in word accuracy and at least a\n41.9% reduction in edit distance.","terms":["cs.CV"]},{"titles":"Generating Interpretable Networks using Hypernetworks","summaries":"An essential goal in mechanistic interpretability to decode a network, i.e.,\nto convert a neural network's raw weights to an interpretable algorithm. Given\nthe difficulty of the decoding problem, progress has been made to understand\nthe easier encoding problem, i.e., to convert an interpretable algorithm into\nnetwork weights. Previous works focus on encoding existing algorithms into\nnetworks, which are interpretable by definition. However, focusing on encoding\nlimits the possibility of discovering new algorithms that humans have never\nstumbled upon, but that are nevertheless interpretable. In this work, we\nexplore the possibility of using hypernetworks to generate interpretable\nnetworks whose underlying algorithms are not yet known. The hypernetwork is\ncarefully designed such that it can control network complexity, leading to a\ndiverse family of interpretable algorithms ranked by their complexity. All of\nthem are interpretable in hindsight, although some of them are less intuitive\nto humans, hence providing new insights regarding how to \"think\" like a neural\nnetwork. For the task of computing L1 norms, hypernetworks find three\nalgorithms: (a) the double-sided algorithm, (b) the convexity algorithm, (c)\nthe pudding algorithm, although only the first algorithm was expected by the\nauthors before experiments. We automatically classify these algorithms and\nanalyze how these algorithmic phases develop during training, as well as how\nthey are affected by complexity control. Furthermore, we show that a trained\nhypernetwork can correctly construct models for input dimensions not seen in\ntraining, demonstrating systematic generalization.","terms":["cs.LG","cs.AI","cs.NE","68T07","I.2.6"]},{"titles":"Learning High-Dimensional Differential Graphs From Multi-Attribute Data","summaries":"We consider the problem of estimating differences in two Gaussian graphical\nmodels (GGMs) which are known to have similar structure. The GGM structure is\nencoded in its precision (inverse covariance) matrix. In many applications one\nis interested in estimating the difference in two precision matrices to\ncharacterize underlying changes in conditional dependencies of two sets of\ndata. Existing methods for differential graph estimation are based on\nsingle-attribute (SA) models where one associates a scalar random variable with\neach node. In multi-attribute (MA) graphical models, each node represents a\nrandom vector. In this paper, we analyze a group lasso penalized D-trace loss\nfunction approach for differential graph learning from multi-attribute data. An\nalternating direction method of multipliers (ADMM) algorithm is presented to\noptimize the objective function. Theoretical analysis establishing consistency\nin support recovery and estimation in high-dimensional settings is provided.\nNumerical results based on synthetic as well as real data are presented.","terms":["stat.ML","cs.LG","eess.SP"]},{"titles":"Diffusion-SS3D: Diffusion Model for Semi-supervised 3D Object Detection","summaries":"Semi-supervised object detection is crucial for 3D scene understanding,\nefficiently addressing the limitation of acquiring large-scale 3D bounding box\nannotations. Existing methods typically employ a teacher-student framework with\npseudo-labeling to leverage unlabeled point clouds. However, producing reliable\npseudo-labels in a diverse 3D space still remains challenging. In this work, we\npropose Diffusion-SS3D, a new perspective of enhancing the quality of\npseudo-labels via the diffusion model for semi-supervised 3D object detection.\nSpecifically, we include noises to produce corrupted 3D object size and class\nlabel distributions, and then utilize the diffusion model as a denoising\nprocess to obtain bounding box outputs. Moreover, we integrate the diffusion\nmodel into the teacher-student framework, so that the denoised bounding boxes\ncan be used to improve pseudo-label generation, as well as the entire\nsemi-supervised learning process. We conduct experiments on the ScanNet and SUN\nRGB-D benchmark datasets to demonstrate that our approach achieves\nstate-of-the-art performance against existing methods. We also present\nextensive analysis to understand how our diffusion model design affects\nperformance in semi-supervised learning.","terms":["cs.CV"]},{"titles":"MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures","summaries":"In this era, the success of large language models and text-to-image models\ncan be attributed to the driving force of large-scale datasets. However, in the\nrealm of 3D vision, while remarkable progress has been made with models trained\non large-scale synthetic and real-captured object data like Objaverse and\nMVImgNet, a similar level of progress has not been observed in the domain of\nhuman-centric tasks partially due to the lack of a large-scale human dataset.\nExisting datasets of high-fidelity 3D human capture continue to be mid-sized\ndue to the significant challenges in acquiring large-scale high-quality 3D\nhuman data. To bridge this gap, we present MVHumanNet, a dataset that comprises\nmulti-view human action sequences of 4,500 human identities. The primary focus\nof our work is on collecting human data that features a large number of diverse\nidentities and everyday clothing using a multi-view human capture system, which\nfacilitates easily scalable data collection. Our dataset contains 9,000 daily\noutfits, 60,000 motion sequences and 645 million frames with extensive\nannotations, including human masks, camera parameters, 2D and 3D keypoints,\nSMPL\/SMPLX parameters, and corresponding textual descriptions. To explore the\npotential of MVHumanNet in various 2D and 3D visual tasks, we conducted pilot\nstudies on view-consistent action recognition, human NeRF reconstruction,\ntext-driven view-unconstrained human image generation, as well as 2D\nview-unconstrained human image and 3D avatar generation. Extensive experiments\ndemonstrate the performance improvements and effective applications enabled by\nthe scale provided by MVHumanNet. As the current largest-scale 3D human\ndataset, we hope that the release of MVHumanNet data with annotations will\nfoster further innovations in the domain of 3D human-centric tasks at scale.","terms":["cs.CV"]},{"titles":"HIG: Hierarchical Interlacement Graph Approach to Scene Graph Generation in Video Understanding","summaries":"Visual interactivity understanding within visual scenes presents a\nsignificant challenge in computer vision. Existing methods focus on complex\ninteractivities while leveraging a simple relationship model. These methods,\nhowever, struggle with a diversity of appearance, situation, position,\ninteraction, and relation in videos. This limitation hinders the ability to\nfully comprehend the interplay within the complex visual dynamics of subjects.\nIn this paper, we delve into interactivities understanding within visual\ncontent by deriving scene graph representations from dense interactivities\namong humans and objects. To achieve this goal, we first present a new dataset\ncontaining Appearance-Situation-Position-Interaction-Relation predicates, named\nASPIRe, offering an extensive collection of videos marked by a wide range of\ninteractivities. Then, we propose a new approach named Hierarchical\nInterlacement Graph (HIG), which leverages a unified layer and graph within a\nhierarchical structure to provide deep insights into scene changes across five\ndistinct tasks. Our approach demonstrates superior performance to other methods\nthrough extensive experiments conducted in various scenarios.","terms":["cs.CV"]},{"titles":"Harnessing Discrete Representations For Continual Reinforcement Learning","summaries":"Reinforcement learning (RL) agents make decisions using nothing but\nobservations from the environment, and consequently, heavily rely on the\nrepresentations of those observations. Though some recent breakthroughs have\nused vector-based categorical representations of observations, often referred\nto as discrete representations, there is little work explicitly assessing the\nsignificance of such a choice. In this work, we provide a thorough empirical\ninvestigation of the advantages of representing observations as vectors of\ncategorical values within the context of reinforcement learning. We perform\nevaluations on world-model learning, model-free RL, and ultimately continual RL\nproblems, where the benefits best align with the needs of the problem setting.\nWe find that, when compared to traditional continuous representations, world\nmodels learned over discrete representations accurately model more of the world\nwith less capacity, and that agents trained with discrete representations learn\nbetter policies with less data. In the context of continual RL, these benefits\ntranslate into faster adapting agents. Additionally, our analysis suggests that\nthe observed performance improvements can be attributed to the information\ncontained within the latent vectors and potentially the encoding of the\ndiscrete representation itself.","terms":["cs.LG","cs.AI"]},{"titles":"Classification for everyone : Building geography agnostic models for fairer recognition","summaries":"In this paper, we analyze different methods to mitigate inherent geographical\nbiases present in state of the art image classification models. We first\nquantitatively present this bias in two datasets - The Dollar Street Dataset\nand ImageNet, using images with location information. We then present different\nmethods which can be employed to reduce this bias. Finally, we analyze the\neffectiveness of the different techniques on making these models more robust to\ngeographical locations of the images.","terms":["cs.CV","cs.AI","cs.CY","cs.LG"]},{"titles":"Investigation of UAV Detection in Images with Complex Backgrounds and Rainy Artifacts","summaries":"To detect unmanned aerial vehicles (UAVs) in real-time, computer vision and\ndeep learning approaches are evolving research areas. Interest in this problem\nhas grown due to concerns regarding the possible hazards and misuse of\nemploying UAVs in many applications. These include potential privacy\nviolations. To address the concerns, vision-based object detection methods have\nbeen developed for UAV detection. However, UAV detection in images with complex\nbackgrounds and weather artifacts like rain has yet to be reasonably studied.\nHence, for this purpose, we prepared two training datasets. The first dataset\nhas the sky as its background and is called the Sky Background Dataset (SBD).\nThe second training dataset has more complex scenes (with diverse backgrounds)\nand is named the Complex Background Dataset (CBD). Additionally, two test sets\nwere prepared: one containing clear images and the other with images with three\nrain artifacts, named the Rainy Test Set (RTS). This work also focuses on\nbenchmarking state-of-the-art object detection models, and to the best of our\nknowledge, it is the first to investigate the performance of recent and popular\nvision-based object detection methods for UAV detection under challenging\nconditions such as complex backgrounds, varying UAV sizes, and low-to-heavy\nrainy conditions. The findings presented in the paper shall help provide\ninsights concerning the performance of the selected models for UAV detection\nunder challenging conditions and pave the way to develop more robust UAV\ndetection methods. The codes and datasets are available at:\nhttps:\/\/github.com\/AdnanMunir294\/UAVD-CBRA.","terms":["cs.CV","cs.MM","eess.IV"]},{"titles":"DGInStyle: Domain-Generalizable Semantic Segmentation with Image Diffusion Models and Stylized Semantic Control","summaries":"Large, pretrained latent diffusion models (LDMs) have demonstrated an\nextraordinary ability to generate creative content, specialize to user data\nthrough few-shot fine-tuning, and condition their output on other modalities,\nsuch as semantic maps. However, are they usable as large-scale data generators,\ne.g., to improve tasks in the perception stack, like semantic segmentation? We\ninvestigate this question in the context of autonomous driving, and answer it\nwith a resounding \"yes\". We propose an efficient data generation pipeline\ntermed DGInStyle. First, we examine the problem of specializing a pretrained\nLDM to semantically-controlled generation within a narrow domain. Second, we\ndesign a Multi-resolution Latent Fusion technique to overcome the bias of LDMs\ntowards dominant objects. Third, we propose a Style Swap technique to endow the\nrich generative prior with the learned semantic control. Using DGInStyle, we\ngenerate a diverse dataset of street scenes, train a domain-agnostic semantic\nsegmentation model on it, and evaluate the model on multiple popular autonomous\ndriving datasets. Our approach consistently increases the performance of\nseveral domain generalization methods, in some cases by +2.5 mIoU compared to\nthe previous state-of-the-art method without our generative augmentation\nscheme. Source code and dataset are available at https:\/\/dginstyle.github.io .","terms":["cs.CV"]},{"titles":"LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models","summaries":"With the recent significant advancements in large multi-modal models (LMMs),\nthe importance of their grounding capability in visual chat is increasingly\nrecognized. Despite recent efforts to enable LMMs to support grounding, their\ncapabilities for grounding and chat are usually separate, and their chat\nperformance drops dramatically when asked to ground. The problem is the lack of\na dataset for grounded visual chat (GVC). Existing grounding datasets only\ncontain short captions. To address this issue, we have created GVC data that\nallows for the combination of grounding and chat capabilities. To better\nevaluate the GVC capabilities, we have introduced a benchmark called\nGrounding-Bench. Additionally, we have proposed a model design that can support\nGVC and various types of visual prompts by connecting segmentation models with\nlanguage models. Experimental results demonstrate that our model outperforms\nother LMMs on Grounding-Bench. Furthermore, our model achieves competitive\nperformance on classic grounding benchmarks like RefCOCO\/+\/g and Flickr30K\nEntities. Our code will be released at\nhttps:\/\/github.com\/UX-Decoder\/LLaVA-Grounding .","terms":["cs.CV"]},{"titles":"Drag-A-Video: Non-rigid Video Editing with Point-based Interaction","summaries":"Video editing is a challenging task that requires manipulating videos on both\nthe spatial and temporal dimensions. Existing methods for video editing mainly\nfocus on changing the appearance or style of the objects in the video, while\nkeeping their structures unchanged. However, there is no existing method that\nallows users to interactively ``drag'' any points of instances on the first\nframe to precisely reach the target points with other frames consistently\ndeformed. In this paper, we propose a new diffusion-based method for\ninteractive point-based video manipulation, called Drag-A-Video. Our method\nallows users to click pairs of handle points and target points as well as masks\non the first frame of an input video. Then, our method transforms the inputs\ninto point sets and propagates these sets across frames. To precisely modify\nthe contents of the video, we employ a new video-level motion supervision to\nupdate the features of the video and introduce the latent offsets to achieve\nthis update at multiple denoising timesteps. We propose a temporal-consistent\npoint tracking module to coordinate the movement of the points in the handle\npoint sets. We demonstrate the effectiveness and flexibility of our method on\nvarious videos. The website of our work is available here:\nhttps:\/\/drag-a-video.github.io\/.","terms":["cs.CV"]},{"titles":"Uncertainty Quantification in Multivariable Regression for Material Property Prediction with Bayesian Neural Networks","summaries":"With the increased use of data-driven approaches and machine learning-based\nmethods in material science, the importance of reliable uncertainty\nquantification (UQ) of the predicted variables for informed decision-making\ncannot be overstated. UQ in material property prediction poses unique\nchallenges, including the multi-scale and multi-physics nature of advanced\nmaterials, intricate interactions between numerous factors, limited\navailability of large curated datasets for model training, etc. Recently,\nBayesian Neural Networks (BNNs) have emerged as a promising approach for UQ,\noffering a probabilistic framework for capturing uncertainties within neural\nnetworks. In this work, we introduce an approach for UQ within physics-informed\nBNNs, which integrates knowledge from governing laws in material modeling to\nguide the models toward physically consistent predictions. To evaluate the\neffectiveness of this approach, we present case studies for predicting the\ncreep rupture life of steel alloys. Experimental validation with three datasets\nof collected measurements from creep tests demonstrates the ability of BNNs to\nproduce accurate point and uncertainty estimates that are competitive or exceed\nthe performance of the conventional method of Gaussian Process Regression.\nSimilarly, we evaluated the suitability of BNNs for UQ in an active learning\napplication and reported competitive performance. The most promising framework\nfor creep life prediction is BNNs based on Markov Chain Monte Carlo\napproximation of the posterior distribution of network parameters, as it\nprovided more reliable results in comparison to BNNs based on variational\ninference approximation or related NNs with probabilistic outputs. The codes\nare available at:\nhttps:\/\/github.com\/avakanski\/Creep-uncertainty-quantification.","terms":["cs.LG","cond-mat.mtrl-sci","I.2.6"]},{"titles":"LivePhoto: Real Image Animation with Text-guided Motion Control","summaries":"Despite the recent progress in text-to-video generation, existing studies\nusually overlook the issue that only spatial contents but not temporal motions\nin synthesized videos are under the control of text. Towards such a challenge,\nthis work presents a practical system, named LivePhoto, which allows users to\nanimate an image of their interest with text descriptions. We first establish a\nstrong baseline that helps a well-learned text-to-image generator (i.e., Stable\nDiffusion) take an image as a further input. We then equip the improved\ngenerator with a motion module for temporal modeling and propose a carefully\ndesigned training pipeline to better link texts and motions. In particular,\nconsidering the facts that (1) text can only describe motions roughly (e.g.,\nregardless of the moving speed) and (2) text may include both content and\nmotion descriptions, we introduce a motion intensity estimation module as well\nas a text re-weighting module to reduce the ambiguity of text-to-motion\nmapping. Empirical evidence suggests that our approach is capable of well\ndecoding motion-related textual instructions into videos, such as actions,\ncamera movements, or even conjuring new contents from thin air (e.g., pouring\nwater into an empty glass). Interestingly, thanks to the proposed intensity\nlearning mechanism, our system offers users an additional control signal (i.e.,\nthe motion intensity) besides text for video customization.","terms":["cs.CV"]},{"titles":"MagicStick: Controllable Video Editing via Control Handle Transformations","summaries":"Text-based video editing has recently attracted considerable interest in\nchanging the style or replacing the objects with a similar structure. Beyond\nthis, we demonstrate that properties such as shape, size, location, motion,\netc., can also be edited in videos. Our key insight is that the keyframe\ntransformations of the specific internal feature (e.g., edge maps of objects or\nhuman pose), can easily propagate to other frames to provide generation\nguidance. We thus propose MagicStick, a controllable video editing method that\nedits the video properties by utilizing the transformation on the extracted\ninternal control signals. In detail, to keep the appearance, we inflate both\nthe pretrained image diffusion model and ControlNet to the temporal dimension\nand train low-rank adaptions (LORA) layers to fit the specific scenes. Then, in\nediting, we perform an inversion and editing framework. Differently, finetuned\nControlNet is introduced in both inversion and generation for attention\nguidance with the proposed attention remix between the spatial attention maps\nof inversion and editing. Yet succinct, our method is the first method to show\nthe ability of video property editing from the pre-trained text-to-image model.\nWe present experiments on numerous examples within our unified framework. We\nalso compare with shape-aware text-based editing and handcrafted motion video\ngeneration, demonstrating our superior temporal consistency and editing\ncapability than previous works. The code and models will be made publicly\navailable.","terms":["cs.CV"]},{"titles":"Interactive Segmentation for Diverse Gesture Types Without Context","summaries":"Interactive segmentation entails a human marking an image to guide how a\nmodel either creates or edits a segmentation. Our work addresses limitations of\nexisting methods: they either only support one gesture type for marking an\nimage (e.g., either clicks or scribbles) or require knowledge of the gesture\ntype being employed, and require specifying whether marked regions should be\nincluded versus excluded in the final segmentation. We instead propose a\nsimplified interactive segmentation task where a user only must mark an image,\nwhere the input can be of any gesture type without specifying the gesture type.\nWe support this new task by introducing the first interactive segmentation\ndataset with multiple gesture types as well as a new evaluation metric capable\nof holistically evaluating interactive segmentation algorithms. We then analyze\nnumerous interactive segmentation algorithms, including ones adapted for our\nnovel task. While we observe promising performance overall, we also highlight\nareas for future improvement. To facilitate further extensions of this work, we\npublicly share our new dataset at https:\/\/github.com\/joshmyersdean\/dig.","terms":["cs.CV"]},{"titles":"Split & Merge: Unlocking the Potential of Visual Adapters via Sparse Training","summaries":"With the rapid growth in the scale of pre-trained foundation models,\nparameter-efficient fine-tuning techniques have gained significant attention,\namong which Adapter Tuning is the most widely used. Despite achieving\nefficiency, Adapter Tuning still underperforms full fine-tuning, and the\nperformance improves at the cost of an increase in parameters. Recent efforts\naddress this issue by pruning the original adapters, but it also introduces\ntraining instability and suboptimal performance on certain datasets. Motivated\nby this, we propose Mixture of Sparse Adapters, or MoSA, as a novel Adapter\nTuning method to fully unleash the potential of each parameter in the adapter.\nWe first split the standard adapter into multiple non-overlapping modules, then\nstochastically activate modules for sparse training, and finally merge them to\nform a complete adapter after tuning. In this way, MoSA can achieve\nsignificantly better performance than standard adapters without any additional\ncomputational or storage overhead. Furthermore, we propose a hierarchical\nsparse strategy to better leverage limited training data. Extensive experiments\non a series of 27 visual tasks demonstrate that MoSA consistently outperforms\nother Adapter Tuning methods as well as other baselines by a significant\nmargin. Furthermore, in two challenging scenarios with low-resource and\nmulti-task settings, MoSA achieves satisfactory results, further demonstrating\nthe effectiveness of our design. Our code will be released.","terms":["cs.CV"]},{"titles":"Fine-grained Controllable Video Generation via Object Appearance and Context","summaries":"Text-to-video generation has shown promising results. However, by taking only\nnatural languages as input, users often face difficulties in providing detailed\ninformation to precisely control the model's output. In this work, we propose\nfine-grained controllable video generation (FACTOR) to achieve detailed\ncontrol. Specifically, FACTOR aims to control objects' appearances and context,\nincluding their location and category, in conjunction with the text prompt. To\nachieve detailed control, we propose a unified framework to jointly inject\ncontrol signals into the existing text-to-video model. Our model consists of a\njoint encoder and adaptive cross-attention layers. By optimizing the encoder\nand the inserted layer, we adapt the model to generate videos that are aligned\nwith both text prompts and fine-grained control. Compared to existing methods\nrelying on dense control signals such as edge maps, we provide a more intuitive\nand user-friendly interface to allow object-level fine-grained control. Our\nmethod achieves controllability of object appearances without finetuning, which\nreduces the per-subject optimization efforts for the users. Extensive\nexperiments on standard benchmark datasets and user-provided inputs validate\nthat our model obtains a 70% improvement in controllability metrics over\ncompetitive baselines.","terms":["cs.CV"]},{"titles":"Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and Fidelity for All-in-One Image Restoration","summaries":"Despite substantial progress, all-in-one image restoration (IR) grapples with\npersistent challenges in handling intricate real-world degradations. This paper\nintroduces MPerceiver: a novel multimodal prompt learning approach that\nharnesses Stable Diffusion (SD) priors to enhance adaptiveness,\ngeneralizability and fidelity for all-in-one image restoration. Specifically,\nwe develop a dual-branch module to master two types of SD prompts: textual for\nholistic representation and visual for multiscale detail representation. Both\nprompts are dynamically adjusted by degradation predictions from the CLIP image\nencoder, enabling adaptive responses to diverse unknown degradations. Moreover,\na plug-in detail refinement module improves restoration fidelity via direct\nencoder-to-decoder information transformation. To assess our method, MPerceiver\nis trained on 9 tasks for all-in-one IR and outperforms state-of-the-art\ntask-specific methods across most tasks. Post multitask pre-training,\nMPerceiver attains a generalized representation in low-level vision, exhibiting\nremarkable zero-shot and few-shot capabilities in unseen tasks. Extensive\nexperiments on 16 IR tasks and 26 benchmarks underscore the superiority of\nMPerceiver in terms of adaptiveness, generalizability and fidelity.","terms":["cs.CV"]},{"titles":"MIND: Multi-Task Incremental Network Distillation","summaries":"The recent surge in pervasive devices generating dynamic data streams has\nunderscored the necessity for learning systems to adapt to data distributional\nshifts continually. To tackle this challenge, the research community has put\nforth a spectrum of methodologies, including the demanding pursuit of\nclass-incremental learning without replay data. In this study, we present MIND,\na parameter isolation method that aims to significantly enhance the performance\nof replay-free solutions and achieve state-of-the-art results on several widely\nstudied datasets. Our approach introduces two main contributions: two\nalternative distillation procedures that significantly improve the efficiency\nof MIND increasing the accumulated knowledge of each sub-network, and the\noptimization of the BachNorm layers across tasks inside the sub-networks.\nOverall, MIND outperforms all the state-of-the-art methods for rehearsal-free\nClass-Incremental learning (with an increment in classification accuracy of\napprox. +6% on CIFAR-100\/10 and +10% on TinyImageNet\/10) reaching up to approx.\n+40% accuracy in Domain-Incremental scenarios. Moreover, we ablated each\ncontribution to demonstrate its impact on performance improvement. Our results\nshowcase the superior performance of MIND indicating its potential for\naddressing the challenges posed by Class-incremental and Domain-Incremental\nlearning in resource-constrained environments.","terms":["cs.CV","cs.LG"]},{"titles":"Multi-task Image Restoration Guided By Robust DINO Features","summaries":"Multi-task image restoration has gained significant interest due to its\ninherent versatility and efficiency compared to its single-task counterpart.\nDespite its potential, performance degradation is observed with an increase in\nthe number of tasks, primarily attributed to the distinct nature of each\nrestoration task. Addressing this challenge, we introduce\n\\mbox{\\textbf{DINO-IR}}, a novel multi-task image restoration approach\nleveraging robust features extracted from DINOv2. Our empirical analysis shows\nthat while shallow features of DINOv2 capture rich low-level image\ncharacteristics, the deep features ensure a robust semantic representation\ninsensitive to degradations while preserving high-frequency contour details.\nBuilding on these features, we devise specialized components, including\nmulti-layer semantic fusion module, DINO-Restore adaption and fusion module,\nand DINO perception contrastive loss, to integrate DINOv2 features into the\nrestoration paradigm. Equipped with the aforementioned components, our DINO-IR\nperforms favorably against existing multi-task image restoration approaches in\nvarious tasks by a large margin, indicating the superiority and necessity of\nreinforcing the robust features for multi-task image restoration.","terms":["cs.CV"]},{"titles":"Realistic Scatterer Based Adversarial Attacks on SAR Image Classifiers","summaries":"Adversarial attacks have highlighted the vulnerability of classifiers based\non machine learning for Synthetic Aperture Radar (SAR) Automatic Target\nRecognition (ATR) tasks. An adversarial attack perturbs SAR images of on-ground\ntargets such that the classifiers are misled into making incorrect predictions.\nHowever, many existing attacking techniques rely on arbitrary manipulation of\nSAR images while overlooking the feasibility of executing the attacks on\nreal-world SAR imagery. Instead, adversarial attacks should be able to be\nimplemented by physical actions, for example, placing additional false objects\nas scatterers around the on-ground target to perturb the SAR image and fool the\nSAR ATR.\n  In this paper, we propose the On-Target Scatterer Attack (OTSA), a\nscatterer-based physical adversarial attack. To ensure the feasibility of its\nphysical execution, we enforce a constraint on the positioning of the\nscatterers. Specifically, we restrict the scatterers to be placed only on the\ntarget instead of in the shadow regions or the background. To achieve this, we\nintroduce a positioning score based on Gaussian kernels and formulate an\noptimization problem for our OTSA attack. Using a gradient ascent method to\nsolve the optimization problem, the OTSA can generate a vector of parameters\ndescribing the positions, shapes, sizes and amplitudes of the scatterers to\nguide the physical execution of the attack that will mislead SAR image\nclassifiers. The experimental results show that our attack obtains\nsignificantly higher success rates under the positioning constraint compared\nwith the existing method.","terms":["cs.CV"]},{"titles":"End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes","summaries":"Meta-Bayesian optimisation (meta-BO) aims to improve the sample efficiency of\nBayesian optimisation by leveraging data from related tasks. While previous\nmethods successfully meta-learn either a surrogate model or an acquisition\nfunction independently, joint training of both components remains an open\nchallenge. This paper proposes the first end-to-end differentiable meta-BO\nframework that generalises neural processes to learn acquisition functions via\ntransformer architectures. We enable this end-to-end framework with\nreinforcement learning (RL) to tackle the lack of labelled acquisition data.\nEarly on, we notice that training transformer-based neural processes from\nscratch with RL is challenging due to insufficient supervision, especially when\nrewards are sparse. We formalise this claim with a combinatorial analysis\nshowing that the widely used notion of regret as a reward signal exhibits a\nlogarithmic sparsity pattern in trajectory lengths. To tackle this problem, we\naugment the RL objective with an auxiliary task that guides part of the\narchitecture to learn a valid probabilistic model as an inductive bias. We\ndemonstrate that our method achieves state-of-the-art regret results against\nvarious baselines in experiments on standard hyperparameter optimisation tasks\nand also outperforms others in the real-world problems of mixed-integer\nprogramming tuning, antibody design, and logic synthesis for electronic design\nautomation.","terms":["cs.LG"]},{"titles":"HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting","summaries":"3D head animation has seen major quality and runtime improvements over the\nlast few years, particularly empowered by the advances in differentiable\nrendering and neural radiance fields. Real-time rendering is a highly desirable\ngoal for real-world applications. We propose HeadGaS, the first model to use 3D\nGaussian Splats (3DGS) for 3D head reconstruction and animation. In this paper\nwe introduce a hybrid model that extends the explicit representation from 3DGS\nwith a base of learnable latent features, which can be linearly blended with\nlow-dimensional parameters from parametric head models to obtain\nexpression-dependent final color and opacity values. We demonstrate that\nHeadGaS delivers state-of-the-art results in real-time inference frame rates,\nwhich surpasses baselines by up to ~2dB, while accelerating rendering speed by\nover x10.","terms":["cs.CV"]},{"titles":"Diversified in-domain synthesis with efficient fine-tuning for few-shot classification","summaries":"Few-shot image classification aims to learn an image classifier using only a\nsmall set of labeled examples per class. A recent research direction for\nimproving few-shot classifiers involves augmenting the labelled samples with\nsynthetic images created by state-of-the-art text-to-image generation models.\nFollowing this trend, we propose Diversified in-domain synthesis with efficient\nfine-tuning (DISEF), a novel approach which addresses the generalization\nchallenge in few-shot learning using synthetic data. DISEF consists of two main\ncomponents. First, we propose a novel text-to-image augmentation pipeline that,\nby leveraging the real samples and their rich semantics coming from an advanced\ncaptioning model, promotes in-domain sample diversity for better\ngeneralization. Second, we emphasize the importance of effective model\nfine-tuning in few-shot recognition, proposing to use Low-Rank Adaptation\n(LoRA) for joint adaptation of the text and image encoders in a Vision Language\nModel. We validate our method in ten different benchmarks, consistently\noutperforming baselines and establishing a new state-of-the-art for few-shot\nclassification. Code is available at \\url{https:\/\/github.com\/vturrisi\/disef}","terms":["cs.CV"]},{"titles":"FroSSL: Frobenius Norm Minimization for Self-Supervised Learning","summaries":"Self-supervised learning (SSL) is an increasingly popular paradigm for\nrepresentation learning. Recent methods can be classified as\nsample-contrastive, dimension-contrastive, or asymmetric network-based, with\neach family having its own approach to avoiding informational collapse. While\ndimension-contrastive methods converge to similar solutions as\nsample-contrastive methods, it can be empirically shown that some methods\nrequire more epochs of training to converge. Motivated by closing this divide,\nwe present the objective function FroSSL which is both sample- and\ndimension-contrastive up to embedding normalization. FroSSL works by minimizing\ncovariance Frobenius norms for avoiding collapse and minimizing mean-squared\nerror for augmentation invariance. We show that FroSSL converges more quickly\nthan a variety of other SSL methods and provide theoretical and empirical\nsupport that this faster convergence is due to how FroSSL affects the\neigenvalues of the embedding covariance matrices. We also show that FroSSL\nlearns competitive representations on linear probe evaluation when used to\ntrain a ResNet18 on the CIFAR-10, CIFAR-100, STL-10, and ImageNet datasets.","terms":["cs.LG"]},{"titles":"Concept Drift Adaptation in Text Stream Mining Settings: A Comprehensive Review","summaries":"Due to the advent and increase in the popularity of the Internet, people have\nbeen producing and disseminating textual data in several ways, such as reviews,\nsocial media posts, and news articles. As a result, numerous researchers have\nbeen working on discovering patterns in textual data, especially because social\nmedia posts function as social sensors, indicating peoples' opinions,\ninterests, etc. However, most tasks regarding natural language processing are\naddressed using traditional machine learning methods and static datasets. This\nsetting can lead to several problems, such as an outdated dataset, which may\nnot correspond to reality, and an outdated model, which has its performance\ndegrading over time. Concept drift is another aspect that emphasizes these\nissues, which corresponds to data distribution and pattern changes. In a text\nstream scenario, it is even more challenging due to its characteristics, such\nas the high speed and data arriving sequentially. In addition, models for this\ntype of scenario must adhere to the constraints mentioned above while learning\nfrom the stream by storing texts for a limited time and consuming low memory.\nIn this study, we performed a systematic literature review regarding concept\ndrift adaptation in text stream scenarios. Considering well-defined criteria,\nwe selected 40 papers to unravel aspects such as text drift categories, types\nof text drift detection, model update mechanism, the addressed stream mining\ntasks, types of text representations, and text representation update mechanism.\nIn addition, we discussed drift visualization and simulation and listed\nreal-world datasets used in the selected papers. Therefore, this paper\ncomprehensively reviews the concept drift adaptation in text stream mining\nscenarios.","terms":["cs.LG","cs.CL","cs.IR"]},{"titles":"VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence","summaries":"Current diffusion-based video editing primarily focuses on\nstructure-preserved editing by utilizing various dense correspondences to\nensure temporal consistency and motion alignment. However, these approaches are\noften ineffective when the target edit involves a shape change. To embark on\nvideo editing with shape change, we explore customized video subject swapping\nin this work, where we aim to replace the main subject in a source video with a\ntarget subject having a distinct identity and potentially different shape. In\ncontrast to previous methods that rely on dense correspondences, we introduce\nthe VideoSwap framework that exploits semantic point correspondences, inspired\nby our observation that only a small number of semantic points are necessary to\nalign the subject's motion trajectory and modify its shape. We also introduce\nvarious user-point interactions (\\eg, removing points and dragging points) to\naddress various semantic point correspondence. Extensive experiments\ndemonstrate state-of-the-art video subject swapping results across a variety of\nreal-world videos.","terms":["cs.CV"]},{"titles":"Customization Assistant for Text-to-image Generation","summaries":"Customizing pre-trained text-to-image generation model has attracted massive\nresearch interest recently, due to its huge potential in real-world\napplications. Although existing methods are able to generate creative content\nfor a novel concept contained in single user-input image, their capability are\nstill far from perfection. Specifically, most existing methods require\nfine-tuning the generative model on testing images. Some existing methods do\nnot require fine-tuning, while their performance are unsatisfactory.\nFurthermore, the interaction between users and models are still limited to\ndirective and descriptive prompts such as instructions and captions. In this\nwork, we build a customization assistant based on pre-trained large language\nmodel and diffusion model, which can not only perform customized generation in\na tuning-free manner, but also enable more user-friendly interactions: users\ncan chat with the assistant and input either ambiguous text or clear\ninstruction. Specifically, we propose a new framework consists of a new model\ndesign and a novel training strategy. The resulting assistant can perform\ncustomized generation in 2-5 seconds without any test time fine-tuning.\nExtensive experiments are conducted, competitive results have been obtained\nacross different domains, illustrating the effectiveness of the proposed\nmethod.","terms":["cs.CV"]},{"titles":"Towards More Practical Group Activity Detection: A New Benchmark and Model","summaries":"Group activity detection (GAD) is the task of identifying members of each\ngroup and classifying the activity of the group at the same time in a video.\nWhile GAD has been studied recently, there is still much room for improvement\nin both dataset and methodology due to their limited capability to address\npractical GAD scenarios. To resolve these issues, we first present a new\ndataset, dubbed Caf\\'e. Unlike existing datasets, Caf\\'e is constructed\nprimarily for GAD and presents more practical evaluation scenarios and metrics,\nas well as being large-scale and providing rich annotations. Along with the\ndataset, we propose a new GAD model that deals with an unknown number of groups\nand latent group members efficiently and effectively. We evaluated our model on\nthree datasets including Caf\\'e, where it outperformed previous work in terms\nof both accuracy and inference speed. Both our dataset and code base will be\nopen to the public to promote future research on GAD.","terms":["cs.CV"]},{"titles":"A Dynamic Network for Efficient Point Cloud Registration","summaries":"For the point cloud registration task, a significant challenge arises from\nnon-overlapping points that consume extensive computational resources while\nnegatively affecting registration accuracy. In this paper, we introduce a\ndynamic approach, widely utilized to improve network efficiency in computer\nvision tasks, to the point cloud registration task. We employ an iterative\nregistration process on point cloud data multiple times to identify regions\nwhere matching points cluster, ultimately enabling us to remove noisy points.\nSpecifically, we begin with deep global sampling to perform coarse global\nregistration. Subsequently, we employ the proposed refined node proposal module\nto further narrow down the registration region and perform local registration.\nFurthermore, we utilize a spatial consistency-based classifier to evaluate the\nresults of each registration stage. The model terminates once it reaches\nsufficient confidence, avoiding unnecessary computations. Extended experiments\ndemonstrate that our model significantly reduces time consumption compared to\nother methods with similar results, achieving a speed improvement of over 41%\non indoor dataset (3DMatch) and 33% on outdoor datasets (KITTI) while\nmaintaining competitive registration recall requirements.","terms":["cs.CV"]},{"titles":"On the Identifiability of Quantized Factors","summaries":"Disentanglement aims to recover meaningful latent ground-truth factors from\nthe observed distribution solely, and is formalized through the theory of\nidentifiability. The identifiability of independent latent factors is proven to\nbe impossible in the unsupervised i.i.d. setting under a general nonlinear map\nfrom factors to observations. In this work, however, we demonstrate that it is\npossible to recover quantized latent factors under a generic nonlinear\ndiffeomorphism. We only assume that the latent factors have independent\ndiscontinuities in their density, without requiring the factors to be\nstatistically independent. We introduce this novel form of identifiability,\ntermed quantized factor identifiability, and provide a comprehensive proof of\nthe recovery of the quantized factors.","terms":["cs.LG","cs.AI"]},{"titles":"A Practical Approach to Novel Class Discovery in Tabular Data","summaries":"The problem of Novel Class Discovery (NCD) consists in extracting knowledge\nfrom a labeled set of known classes to accurately partition an unlabeled set of\nnovel classes. While NCD has recently received a lot of attention from the\ncommunity, it is often solved on computer vision problems and under unrealistic\nconditions. In particular, the number of novel classes is usually assumed to be\nknown in advance, and their labels are sometimes used to tune hyperparameters.\nMethods that rely on these assumptions are not applicable in real-world\nscenarios. In this work, we focus on solving NCD in tabular data when no prior\nknowledge of the novel classes is available. To this end, we propose to tune\nthe hyperparameters of NCD methods by adapting the $k$-fold cross-validation\nprocess and hiding some of the known classes in each fold. Since we have found\nthat methods with too many hyperparameters are likely to overfit these hidden\nclasses, we define a simple deep NCD model. This method is composed of only the\nessential elements necessary for the NCD problem and performs impressively well\nunder realistic conditions. Furthermore, we find that the latent space of this\nmethod can be used to reliably estimate the number of novel classes.\nAdditionally, we adapt two unsupervised clustering algorithms ($k$-means and\nSpectral Clustering) to leverage the knowledge of the known classes. Extensive\nexperiments are conducted on 7 tabular datasets and demonstrate the\neffectiveness of the proposed method and hyperparameter tuning process, and\nshow that the NCD problem can be solved without relying on knowledge from the\nnovel classes.","terms":["cs.LG"]},{"titles":"Toward autocorrection of chemical process flowsheets using large language models","summaries":"The process engineering domain widely uses Process Flow Diagrams (PFDs) and\nProcess and Instrumentation Diagrams (P&IDs) to represent process flows and\nequipment configurations. However, the P&IDs and PFDs, hereafter called\nflowsheets, can contain errors causing safety hazards, inefficient operation,\nand unnecessary expenses. Correcting and verifying flowsheets is a tedious,\nmanual process. We propose a novel generative AI methodology for automatically\nidentifying errors in flowsheets and suggesting corrections to the user, i.e.,\nautocorrecting flowsheets. Inspired by the breakthrough of Large Language\nModels (LLMs) for grammatical autocorrection of human language, we investigate\nLLMs for the autocorrection of flowsheets. The input to the model is a\npotentially erroneous flowsheet and the output of the model are suggestions for\na corrected flowsheet. We train our autocorrection model on a synthetic dataset\nin a supervised manner. The model achieves a top-1 accuracy of 80% and a top-5\naccuracy of 84% on an independent test dataset of synthetically generated\nflowsheets. The results suggest that the model can learn to autocorrect the\nsynthetic flowsheets. We envision that flowsheet autocorrection will become a\nuseful tool for chemical engineers.","terms":["cs.LG","cs.AI"]},{"titles":"Experimental Insights Towards Explainable and Interpretable Pedestrian Crossing Prediction","summaries":"In the context of autonomous driving, pedestrian crossing prediction is a key\ncomponent for improving road safety. Presently, the focus of these predictions\nextends beyond achieving trustworthy results; it is shifting towards the\nexplainability and interpretability of these predictions. This research\nintroduces a novel neuro-symbolic approach that combines deep learning and\nfuzzy logic for an explainable and interpretable pedestrian crossing\nprediction. We have developed an explainable predictor (ExPedCross), which\nutilizes a set of explainable features and employs a fuzzy inference system to\npredict whether the pedestrian will cross or not. Our approach was evaluated on\nboth the PIE and JAAD datasets. The results offer experimental insights into\nachieving explainability and interpretability in the pedestrian crossing\nprediction task. Furthermore, the testing results yield a set of guidelines and\nrecommendations regarding the process of dataset selection, feature selection,\nand explainability.","terms":["cs.LG","cs.AI","cs.NE","cs.SY","eess.SY"]},{"titles":"Grounding Everything: Emerging Localization Properties in Vision-Language Transformers","summaries":"Vision-language foundation models have shown remarkable performance in\nvarious zero-shot settings such as image retrieval, classification, or\ncaptioning. But so far, those models seem to fall behind when it comes to\nzero-shot localization of referential expressions and objects in images. As a\nresult, they need to be fine-tuned for this task. In this paper, we show that\npretrained vision-language (VL) models allow for zero-shot open-vocabulary\nobject localization without any fine-tuning. To leverage those capabilities, we\npropose a Grounding Everything Module (GEM) that generalizes the idea of\nvalue-value attention introduced by CLIPSurgery to a self-self attention path.\nWe show that the concept of self-self attention corresponds to clustering, thus\nenforcing groups of tokens arising from the same object to be similar while\npreserving the alignment with the language space. To further guide the group\nformation, we propose a set of regularizations that allows the model to finally\ngeneralize across datasets and backbones. We evaluate the proposed GEM\nframework on various benchmark tasks and datasets for semantic segmentation. It\nshows that GEM not only outperforms other training-free open-vocabulary\nlocalization methods, but also achieves state-of-the-art results on the\nrecently proposed OpenImagesV7 large-scale segmentation benchmark.","terms":["cs.CV","cs.AI"]},{"titles":"Attention-enhanced neural differential equations for physics-informed deep learning of ion transport","summaries":"Species transport models typically combine partial differential equations\n(PDEs) with relations from hindered transport theory to quantify\nelectromigrative, convective, and diffusive transport through complex\nnanoporous systems; however, these formulations are frequently substantial\nsimplifications of the governing dynamics, leading to the poor generalization\nperformance of PDE-based models. Given the growing interest in deep learning\nmethods for the physical sciences, we develop a machine learning-based approach\nto characterize ion transport across nanoporous membranes. Our proposed\nframework centers around attention-enhanced neural differential equations that\nincorporate electroneutrality-based inductive biases to improve generalization\nperformance relative to conventional PDE-based methods. In addition, we study\nthe role of the attention mechanism in illuminating physically-meaningful\nion-pairing relationships across diverse mixture compositions. Further, we\ninvestigate the importance of pre-training on simulated data from PDE-based\nmodels, as well as the performance benefits from hard vs. soft inductive\nbiases. Our results indicate that physics-informed deep learning solutions can\noutperform their classical PDE-based counterparts and provide promising avenues\nfor modelling complex transport phenomena across diverse applications.","terms":["cs.LG","math-ph","math.MP","physics.comp-ph"]},{"titles":"REST: Enhancing Group Robustness in DNNs through Reweighted Sparse Training","summaries":"The deep neural network (DNN) has been proven effective in various domains.\nHowever, they often struggle to perform well on certain minority groups during\ninference, despite showing strong performance on the majority of data groups.\nThis is because over-parameterized models learned \\textit{bias attributes} from\na large number of \\textit{bias-aligned} training samples. These bias attributes\nare strongly spuriously correlated with the target variable, causing the models\nto be biased towards spurious correlations (i.e., \\textit{bias-conflicting}).\nTo tackle this issue, we propose a novel \\textbf{re}weighted \\textbf{s}parse\n\\textbf{t}raining framework, dubbed as \\textit{\\textbf{REST}}, which aims to\nenhance the performance of biased data while improving computation and memory\nefficiency. Our proposed REST framework has been experimentally validated on\nthree datasets, demonstrating its effectiveness in exploring unbiased\nsubnetworks. We found that REST reduces the reliance on spuriously correlated\nfeatures, leading to better performance across a wider range of data groups\nwith fewer training and inference resources. We highlight that the\n\\textit{REST} framework represents a promising approach for improving the\nperformance of DNNs on biased data, while simultaneously improving computation\nand memory efficiency. By reducing the reliance on spurious correlations, REST\nhas the potential to enhance the robustness of DNNs and improve their\ngeneralization capabilities. Code is released at\n\\url{https:\/\/github.com\/zhao1402072392\/REST}","terms":["cs.LG"]},{"titles":"Semi-Supervised Health Index Monitoring with Feature Generation and Fusion","summaries":"The Health Index (HI) is crucial for evaluating system health, aiding tasks\nlike anomaly detection and predicting remaining useful life for systems\ndemanding high safety and reliability. Tight monitoring is crucial for\nachieving high precision at a lower cost, with applications such as spray\ncoating. Obtaining HI labels in real-world applications is often\ncost-prohibitive, requiring continuous, precise health measurements. Therefore,\nit is more convenient to leverage run-to failure datasets that may provide\npotential indications of machine wear condition, making it necessary to apply\nsemi-supervised tools for HI construction. In this study, we adapt the Deep\nSemi-supervised Anomaly Detection (DeepSAD) method for HI construction. We use\nthe DeepSAD embedding as a condition indicators to address interpretability\nchallenges and sensitivity to system-specific factors. Then, we introduce a\ndiversity loss to enrich condition indicators. We employ an alternating\nprojection algorithm with isotonic constraints to transform the DeepSAD\nembedding into a normalized HI with an increasing trend. Validation on the PHME\n2010 milling dataset, a recognized benchmark with ground truth HIs demonstrates\nmeaningful HIs estimations. Our methodology is then applied to monitor wear\nstates of thermal spray coatings using high-frequency voltage. Our\ncontributions create opportunities for more accessible and reliable HI\nestimation, particularly in cases where obtaining ground truth HI labels is\nunfeasible.","terms":["cs.LG","stat.ME"]},{"titles":"Lessons from Usable ML Deployments and Application to Wind Turbine Monitoring","summaries":"Through past experiences deploying what we call usable ML (one step beyond\nexplainable ML, including both explanations and other augmenting information)\nto real-world domains, we have learned three key lessons. First, many\norganizations are beginning to hire people who we call ``bridges'' because they\nbridge the gap between ML developers and domain experts, and these people fill\na valuable role in developing usable ML applications. Second, a configurable\nsystem that enables easily iterating on usable ML interfaces during\ncollaborations with bridges is key. Finally, there is a need for continuous,\nin-deployment evaluations to quantify the real-world impact of usable ML.\nThroughout this paper, we apply these lessons to the task of wind turbine\nmonitoring, an essential task in the renewable energy domain. Turbine engineers\nand data analysts must decide whether to perform costly in-person\ninvestigations on turbines to prevent potential cases of brakepad failure, and\nwell-tuned usable ML interfaces can aid with this decision-making process.\nThrough the applications of our lessons to this task, we hope to demonstrate\nthe potential real-world impact of usable ML in the renewable energy domain.","terms":["cs.LG"]},{"titles":"Expert-guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems","summaries":"Domain experts often possess valuable physical insights that are overlooked\nin fully automated decision-making processes such as Bayesian optimisation. In\nthis article we apply high-throughput (batch) Bayesian optimisation alongside\nanthropological decision theory to enable domain experts to influence the\nselection of optimal experiments. Our methodology exploits the hypothesis that\nhumans are better at making discrete choices than continuous ones and enables\nexperts to influence critical early decisions. At each iteration we solve an\naugmented multi-objective optimisation problem across a number of alternate\nsolutions, maximising both the sum of their utility function values and the\ndeterminant of their covariance matrix, equivalent to their total variability.\nBy taking the solution at the knee point of the Pareto front, we return a set\nof alternate solutions at each iteration that have both high utility values and\nare reasonably distinct, from which the expert selects one for evaluation. We\ndemonstrate that even in the case of an uninformed practitioner, our algorithm\nrecovers the regret of standard Bayesian optimisation.","terms":["cs.LG","cs.HC","math.OC"]},{"titles":"A Unified Theory of Diversity in Ensemble Learning","summaries":"We present a theory of ensemble diversity, explaining the nature of diversity\nfor a wide range of supervised learning scenarios. This challenge, of\nunderstanding ensemble diversity, has been referred to as the \"holy grail\" of\nensemble learning, an open research issue for over 30 years. Our framework\nreveals that diversity is in fact a hidden dimension in the bias-variance\ndecomposition of the ensemble loss. We prove a family of exact\nbias-variance-diversity decompositions, for both regression and classification,\ne.g., squared, cross-entropy, and Poisson losses. For losses where an additive\nbias-variance decomposition is not available (e.g., 0\/1 loss) we present an\nalternative approach, which precisely quantifies the effects of diversity,\nturning out to be dependent on the label distribution. Experiments show how we\ncan use our framework to understand the diversity-encouraging mechanisms of\npopular methods: Bagging, Boosting, and Random Forests.","terms":["cs.LG","cs.AI","stat.ML"]},{"titles":"One-step Diffusion with Distribution Matching Distillation","summaries":"Diffusion models generate high-quality images but require dozens of forward\npasses. We introduce Distribution Matching Distillation (DMD), a procedure to\ntransform a diffusion model into a one-step image generator with minimal impact\non image quality. We enforce the one-step image generator match the diffusion\nmodel at distribution level, by minimizing an approximate KL divergence whose\ngradient can be expressed as the difference between 2 score functions, one of\nthe target distribution and the other of the synthetic distribution being\nproduced by our one-step generator. The score functions are parameterized as\ntwo diffusion models trained separately on each distribution. Combined with a\nsimple regression loss matching the large-scale structure of the multi-step\ndiffusion outputs, our method outperforms all published few-step diffusion\napproaches, reaching 2.62 FID on ImageNet 64x64 and 11.49 FID on zero-shot\nCOCO-30k, comparable to Stable Diffusion but orders of magnitude faster.\nUtilizing FP16 inference, our model generates images at 20 FPS on modern\nhardware.","terms":["cs.CV"]},{"titles":"Balance is Essence: Accelerating Sparse Training via Adaptive Gradient Correction","summaries":"Despite impressive performance, deep neural networks require significant\nmemory and computation costs, prohibiting their application in\nresource-constrained scenarios. Sparse training is one of the most common\ntechniques to reduce these costs, however, the sparsity constraints add\ndifficulty to the optimization, resulting in an increase in training time and\ninstability. In this work, we aim to overcome this problem and achieve\nspace-time co-efficiency. To accelerate and stabilize the convergence of sparse\ntraining, we analyze the gradient changes and develop an adaptive gradient\ncorrection method. Specifically, we approximate the correlation between the\ncurrent and previous gradients, which is used to balance the two gradients to\nobtain a corrected gradient. Our method can be used with the most popular\nsparse training pipelines under both standard and adversarial setups.\nTheoretically, we prove that our method can accelerate the convergence rate of\nsparse training. Extensive experiments on multiple datasets, model\narchitectures, and sparsities demonstrate that our method outperforms leading\nsparse training methods by up to \\textbf{5.0\\%} in accuracy given the same\nnumber of training epochs, and reduces the number of training epochs by up to\n\\textbf{52.1\\%} to achieve the same accuracy. Our code is available on:\n\\url{https:\/\/github.com\/StevenBoys\/AGENT}.","terms":["cs.LG","cs.AI","cs.CV"]},{"titles":"Stable Segment Anything Model","summaries":"The Segment Anything Model (SAM) achieves remarkable promptable segmentation\ngiven high-quality prompts which, however, often require good skills to\nspecify. To make SAM robust to casual prompts, this paper presents the first\ncomprehensive analysis on SAM's segmentation stability across a diverse\nspectrum of prompt qualities, notably imprecise bounding boxes and insufficient\npoints. Our key finding reveals that given such low-quality prompts, SAM's mask\ndecoder tends to activate image features that are biased towards the background\nor confined to specific object parts. To mitigate this issue, our key idea\nconsists of calibrating solely SAM's mask attention by adjusting the sampling\nlocations and amplitudes of image features, while the original SAM model\narchitecture and weights remain unchanged. Consequently, our deformable\nsampling plugin (DSP) enables SAM to adaptively shift attention to the prompted\ntarget regions in a data-driven manner, facilitated by our effective robust\ntraining strategy (RTS). During inference, dynamic routing plugin (DRP) is\nproposed that toggles SAM between the deformable and regular grid sampling\nmodes, conditioned on the input prompt quality. Thus, our solution, termed\nStable-SAM, offers several advantages: 1) improved SAM's segmentation stability\nacross a wide range of prompt qualities, while 2) retaining SAM's powerful\npromptable segmentation efficiency and generality, with 3) minimal learnable\nparameters (0.08 M) and fast adaptation (by 1 training epoch). Extensive\nexperiments across multiple datasets validate the effectiveness and advantages\nof our approach, underscoring Stable-SAM as a more robust solution for\nsegmenting anything. Codes will be released upon acceptance.\nhttps:\/\/github.com\/fanq15\/Stable-SAM","terms":["cs.CV"]},{"titles":"Transformer-Based Deep Learning Model for Bored Pile Load-Deformation Prediction in Bangkok Subsoil","summaries":"This paper presents a novel deep learning model based on the transformer\narchitecture to predict the load-deformation behavior of large bored piles in\nBangkok subsoil. The model encodes the soil profile and pile features as\ntokenization input, and generates the load-deformation curve as output. The\nmodel also incorporates the previous sequential data of load-deformation curve\ninto the decoder to improve the prediction accuracy. The model also\nincorporates the previous sequential data of load-deformation curve into the\ndecoder. The model shows a satisfactory accuracy and generalization ability for\nthe load-deformation curve prediction, with a mean absolute error of 5.72% for\nthe test data. The model could also be used for parametric analysis and design\noptimization of piles under different soil and pile conditions, pile cross\nsection, pile length and type of pile.","terms":["cs.LG","cs.CE"]},{"titles":"Are Vision Transformers More Data Hungry Than Newborn Visual Systems?","summaries":"Vision transformers (ViTs) are top performing models on many computer vision\nbenchmarks and can accurately predict human behavior on object recognition\ntasks. However, researchers question the value of using ViTs as models of\nbiological learning because ViTs are thought to be more data hungry than\nbrains, with ViTs requiring more training data to reach similar levels of\nperformance. To test this assumption, we directly compared the learning\nabilities of ViTs and animals, by performing parallel controlled rearing\nexperiments on ViTs and newborn chicks. We first raised chicks in impoverished\nvisual environments containing a single object, then simulated the training\ndata available in those environments by building virtual animal chambers in a\nvideo game engine. We recorded the first-person images acquired by agents\nmoving through the virtual chambers and used those images to train self\nsupervised ViTs that leverage time as a teaching signal, akin to biological\nvisual systems. When ViTs were trained through the eyes of newborn chicks, the\nViTs solved the same view invariant object recognition tasks as the chicks.\nThus, ViTs were not more data hungry than newborn visual systems: both learned\nview invariant object representations in impoverished visual environments. The\nflexible and generic attention based learning mechanism in ViTs combined with\nthe embodied data streams available to newborn animals appears sufficient to\ndrive the development of animal-like object recognition.","terms":["cs.CV","cs.AI","cs.LG","cs.NE"]},{"titles":"MIMONets: Multiple-Input-Multiple-Output Neural Networks Exploiting Computation in Superposition","summaries":"With the advent of deep learning, progressively larger neural networks have\nbeen designed to solve complex tasks. We take advantage of these capacity-rich\nmodels to lower the cost of inference by exploiting computation in\nsuperposition. To reduce the computational burden per input, we propose\nMultiple-Input-Multiple-Output Neural Networks (MIMONets) capable of handling\nmany inputs at once. MIMONets augment various deep neural network architectures\nwith variable binding mechanisms to represent an arbitrary number of inputs in\na compositional data structure via fixed-width distributed representations.\nAccordingly, MIMONets adapt nonlinear neural transformations to process the\ndata structure holistically, leading to a speedup nearly proportional to the\nnumber of superposed input items in the data structure. After processing in\nsuperposition, an unbinding mechanism recovers each transformed input of\ninterest. MIMONets also provide a dynamic trade-off between accuracy and\nthroughput by an instantaneous on-demand switching between a set of\naccuracy-throughput operating points, yet within a single set of fixed\nparameters. We apply the concept of MIMONets to both CNN and Transformer\narchitectures resulting in MIMOConv and MIMOFormer, respectively. Empirical\nevaluations show that MIMOConv achieves about 2-4 x speedup at an accuracy\ndelta within [+0.68, -3.18]% compared to WideResNet CNNs on CIFAR10 and\nCIFAR100. Similarly, MIMOFormer can handle 2-4 inputs at once while maintaining\na high average accuracy within a [-1.07, -3.43]% delta on the long range arena\nbenchmark. Finally, we provide mathematical bounds on the interference between\nsuperposition channels in MIMOFormer. Our code is available at\nhttps:\/\/github.com\/IBM\/multiple-input-multiple-output-nets.","terms":["cs.LG","cs.AI","stat.ML"]},{"titles":"Convergence Rates for Stochastic Approximation: Biased Noise with Unbounded Variance, and Applications","summaries":"The Stochastic Approximation (SA) algorithm introduced by Robbins and Monro\nin 1951 has been a standard method for solving equations of the form\n$\\mathbf{f}({\\boldsymbol {\\theta}}) = \\mathbf{0}$, when only noisy measurements\nof $\\mathbf{f}(\\cdot)$ are available. If $\\mathbf{f}({\\boldsymbol {\\theta}}) =\n\\nabla J({\\boldsymbol {\\theta}})$ for some function $J(\\cdot)$, then SA can\nalso be used to find a stationary point of $J(\\cdot)$. In much of the\nliterature, it is assumed that the error term ${\\boldsymbol {xi}}_{t+1}$ has\nzero conditional mean, and that its conditional variance is bounded as a\nfunction of $t$ (though not necessarily with respect to ${\\boldsymbol\n{\\theta}}_t$). Also, for the most part, the emphasis has been on\n``synchronous'' SA, whereby, at each time $t$, \\textit{every} component of\n${\\boldsymbol {\\theta}}_t$ is updated. Over the years, SA has been applied to a\nvariety of areas, out of which two are the focus in this paper: Convex and\nnonconvex optimization, and Reinforcement Learning (RL). As it turns out, in\nthese applications, the above-mentioned assumptions do not always hold. In\nzero-order methods, the error neither has zero mean nor bounded conditional\nvariance. In the present paper, we extend SA theory to encompass errors with\nnonzero conditional mean and\/or unbounded conditional variance, and also\nasynchronous SA. In addition, we derive estimates for the rate of convergence\nof the algorithm. Then we apply the new results to problems in nonconvex\noptimization, and to Markovian SA, a recently emerging area in RL. We prove\nthat SA converges in these situations, and compute the ``optimal step size\nsequences'' to maximize the estimated rate of convergence.","terms":["stat.ML","cs.LG","math.OC","math.PR","62L20, 60G17, 93D05"]},{"titles":"Calibrated Adaptive Teacher for Domain Adaptive Intelligent Fault Diagnosis","summaries":"Intelligent Fault Diagnosis (IFD) based on deep learning has proven to be an\neffective and flexible solution, attracting extensive research. Deep neural\nnetworks can learn rich representations from vast amounts of representative\nlabeled data for various applications. In IFD, they achieve high classification\nperformance from signals in an end-to-end manner, without requiring extensive\ndomain knowledge. However, deep learning models usually only perform well on\nthe data distribution they have been trained on. When applied to a different\ndistribution, they may experience performance drops. This is also observed in\nIFD, where assets are often operated in working conditions different from those\nin which labeled data have been collected. Unsupervised domain adaptation (UDA)\ndeals with the scenario where labeled data are available in a source domain,\nand only unlabeled data are available in a target domain, where domains may\ncorrespond to operating conditions. Recent methods rely on training with\nconfident pseudo-labels for target samples. However, the confidence-based\nselection of pseudo-labels is hindered by poorly calibrated confidence\nestimates in the target domain, primarily due to over-confident predictions,\nwhich limits the quality of pseudo-labels and leads to error accumulation. In\nthis paper, we propose a novel UDA method called Calibrated Adaptive Teacher\n(CAT), where we propose to calibrate the predictions of the teacher network\nthroughout the self-training process, leveraging post-hoc calibration\ntechniques. We evaluate CAT on domain-adaptive IFD and perform extensive\nexperiments on the Paderborn benchmark for bearing fault diagnosis under\nvarying operating conditions. Our proposed method achieves state-of-the-art\nperformance on most transfer tasks.","terms":["cs.LG","cs.AI","eess.SP","stat.ML","68T07, 62H30","I.2.6; J.2"]},{"titles":"RotaTR: Detection Transformer for Dense and Rotated Object","summaries":"Detecting the objects in dense and rotated scenes is a challenging task.\nRecent works on this topic are mostly based on Faster RCNN or Retinanet. As\nthey are highly dependent on the pre-set dense anchors and the NMS operation,\nthe approach is indirect and suboptimal.The end-to-end DETR-based detectors\nhave achieved great success in horizontal object detection and many other areas\nlike segmentation, tracking, action recognition and etc.However, the DETR-based\ndetectors perform poorly on dense rotated target tasks and perform worse than\nmost modern CNN-based detectors. In this paper, we find the most significant\nreason for the poor performance is that the original attention can not\naccurately focus on the oriented targets. Accordingly, we propose Rotated\nobject detection TRansformer (RotaTR) as an extension of DETR to oriented\ndetection. Specifically, we design Rotation Sensitive deformable (RSDeform)\nattention to enhance the DETR's ability to detect oriented targets. It is used\nto build the feature alignment module and rotation-sensitive decoder for our\nmodel. We test RotaTR on four challenging-oriented benchmarks. It shows a great\nadvantage in detecting dense and oriented objects compared to the original\nDETR. It also achieves competitive results when compared to the\nstate-of-the-art.","terms":["cs.CV"]},{"titles":"Revitalizing Legacy Video Content: Deinterlacing with Bidirectional Information Propagation","summaries":"Due to old CRT display technology and limited transmission bandwidth, early\nfilm and TV broadcasts commonly used interlaced scanning. This meant each field\ncontained only half of the information. Since modern displays require full\nframes, this has spurred research into deinterlacing, i.e. restoring the\nmissing information in legacy video content. In this paper, we present a\ndeep-learning-based method for deinterlacing animated and live-action content.\nOur proposed method supports bidirectional spatio-temporal information\npropagation across multiple scales to leverage information in both space and\ntime. More specifically, we design a Flow-guided Refinement Block (FRB) which\nperforms feature refinement including alignment, fusion, and rectification.\nAdditionally, our method can process multiple fields simultaneously, reducing\nper-frame processing time, and potentially enabling real-time processing. Our\nexperimental results demonstrate that our proposed method achieves superior\nperformance compared to existing methods.","terms":["cs.CV"]},{"titles":"Sample-based Dynamic Hierarchical Transformer with Layer and Head Flexibility via Contextual Bandit","summaries":"Transformer requires a fixed number of layers and heads which makes them\ninflexible to the complexity of individual samples and expensive in training\nand inference. To address this, we propose a sample-based Dynamic Hierarchical\nTransformer (DHT) model whose layers and heads can be dynamically configured\nwith single data samples via solving contextual bandit problems. To determine\nthe number of layers and heads, we use the Uniform Confidence Bound while we\ndeploy combinatorial Thompson Sampling in order to select specific head\ncombinations given their number. Different from previous work that focuses on\ncompressing trained networks for inference only, DHT is not only advantageous\nfor adaptively optimizing the underlying network architecture during training\nbut also has a flexible network for efficient inference. To the best of our\nknowledge, this is the first comprehensive data-driven dynamic transformer\nwithout any additional auxiliary neural networks that implement the dynamic\nsystem. According to the experiment results, we achieve up to 74% computational\nsavings for both training and inference with a minimal loss of accuracy.","terms":["cs.LG","cs.AI","cs.NE"]},{"titles":"Deterministic Guidance Diffusion Model for Probabilistic Weather Forecasting","summaries":"Weather forecasting requires not only accuracy but also the ability to\nperform probabilistic prediction. However, deterministic weather forecasting\nmethods do not support probabilistic predictions, and conversely, probabilistic\nmodels tend to be less accurate. To address these challenges, in this paper, we\nintroduce the \\textbf{\\textit{D}}eterministic \\textbf{\\textit{G}}uidance\n\\textbf{\\textit{D}}iffusion \\textbf{\\textit{M}}odel (DGDM) for probabilistic\nweather forecasting, integrating benefits of both deterministic and\nprobabilistic approaches. During the forward process, both the deterministic\nand probabilistic models are trained end-to-end. In the reverse process,\nweather forecasting leverages the predicted result from the deterministic\nmodel, using as an intermediate starting point for the probabilistic model. By\nfusing deterministic models with probabilistic models in this manner, DGDM is\ncapable of providing accurate forecasts while also offering probabilistic\npredictions. To evaluate DGDM, we assess it on the global weather forecasting\ndataset (WeatherBench) and the common video frame prediction benchmark (Moving\nMNIST). We also introduce and evaluate the Pacific Northwest Windstorm\n(PNW)-Typhoon weather satellite dataset to verify the effectiveness of DGDM in\nhigh-resolution regional forecasting. As a result of our experiments, DGDM\nachieves state-of-the-art results not only in global forecasting but also in\nregional forecasting. The code is available at:\n\\url{https:\/\/github.com\/DongGeun-Yoon\/DGDM}.","terms":["cs.CV"]},{"titles":"BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models","summaries":"Diffusion models have made tremendous progress in text-driven image and video\ngeneration. Now text-to-image foundation models are widely applied to various\ndownstream image synthesis tasks, such as controllable image generation and\nimage editing, while downstream video synthesis tasks are less explored for\nseveral reasons. First, it requires huge memory and compute overhead to train a\nvideo generation foundation model. Even with video foundation models,\nadditional costly training is still required for downstream video synthesis\ntasks. Second, although some works extend image diffusion models into videos in\na training-free manner, temporal consistency cannot be well kept. Finally,\nthese adaption methods are specifically designed for one task and fail to\ngeneralize to different downstream video synthesis tasks. To mitigate these\nissues, we propose a training-free general-purpose video synthesis framework,\ncoined as BIVDiff, via bridging specific image diffusion models and general\ntext-to-video foundation diffusion models. Specifically, we first use an image\ndiffusion model (like ControlNet, Instruct Pix2Pix) for frame-wise video\ngeneration, then perform Mixed Inversion on the generated video, and finally\ninput the inverted latents into the video diffusion model for temporal\nsmoothing. Decoupling image and video models enables flexible image model\nselection for different purposes, which endows the framework with strong task\ngeneralization and high efficiency. To validate the effectiveness and general\nuse of BIVDiff, we perform a wide range of video generation tasks, including\ncontrollable video generation video editing, video inpainting and outpainting.\nOur project page is available at https:\/\/bivdiff.github.io.","terms":["cs.CV","cs.AI"]},{"titles":"Score-Aware Policy-Gradient Methods and Performance Guarantees using Local Lyapunov Conditions: Applications to Product-Form Stochastic Networks and Queueing Systems","summaries":"Stochastic networks and queueing systems often lead to Markov decision\nprocesses (MDPs) with large state and action spaces as well as nonconvex\nobjective functions, which hinders the convergence of many reinforcement\nlearning (RL) algorithms. Policy-gradient methods perform well on MDPs with\nlarge state and action spaces, but they sometimes experience slow convergence\ndue to the high variance of the gradient estimator. In this paper, we show that\nsome of these difficulties can be circumvented by exploiting the structure of\nthe underlying MDP. We first introduce a new family of gradient estimators\ncalled score-aware gradient estimators (SAGEs). When the stationary\ndistribution of the MDP belongs to an exponential family parametrized by the\npolicy parameters, SAGEs allow us to estimate the policy gradient without\nrelying on value-function estimation, contrary to classical policy-gradient\nmethods like actor-critic. To demonstrate their applicability, we examine two\ncommon control problems arising in stochastic networks and queueing systems\nwhose stationary distributions have a product-form, a special case of\nexponential families. As a second contribution, we show that, under appropriate\nassumptions, the policy under a SAGE-based policy-gradient method has a large\nprobability of converging to an optimal policy, provided that it starts\nsufficiently close to it, even with a nonconvex objective function and multiple\nmaximizers. Our key assumptions are that, locally around a maximizer, a\nnondegeneracy property of the Hessian of the objective function holds and a\nLyapunov function exists. Finally, we conduct a numerical comparison between a\nSAGE-based policy-gradient method and an actor-critic algorithm. The results\ndemonstrate that the SAGE-based method finds close-to-optimal policies more\nrapidly, highlighting its superior performance over the traditional\nactor-critic method.","terms":["cs.LG","cs.PF","math.OC","math.PR"]},{"titles":"NeuroMixGDP: A Neural Collapse-Inspired Random Mixup for Private Data Release","summaries":"Privacy-preserving data release algorithms have gained increasing attention\nfor their ability to protect user privacy while enabling downstream machine\nlearning tasks. However, the utility of current popular algorithms is not\nalways satisfactory. Mixup of raw data provides a new way of data augmentation,\nwhich can help improve utility. However, its performance drastically\ndeteriorates when differential privacy (DP) noise is added. To address this\nissue, this paper draws inspiration from the recently observed Neural Collapse\n(NC) phenomenon, which states that the last layer features of a neural network\nconcentrate on the vertices of a simplex as Equiangular Tight Frame (ETF). We\npropose a scheme to mixup the Neural Collapse features to exploit the ETF\nsimplex structure and release noisy mixed features to enhance the utility of\nthe released data. By using Gaussian Differential Privacy (GDP), we obtain an\nasymptotic rate for the optimal mixup degree. To further enhance the utility\nand address the label collapse issue when the mixup degree is large, we propose\na Hierarchical sampling method to stratify the mixup samples on a small number\nof classes. This method remarkably improves utility when the number of classes\nis large. Extensive experiments demonstrate the effectiveness of our proposed\nmethod in protecting against attacks and improving utility. In particular, our\napproach shows significantly improved utility compared to directly training\nclassification networks with DPSGD on CIFAR100 and MiniImagenet datasets,\nhighlighting the benefits of using privacy-preserving data release. We release\nreproducible code in https:\/\/github.com\/Lidonghao1996\/NeuroMixGDP.","terms":["cs.LG","cs.CV"]},{"titles":"Weakly Supervised Detection of Hallucinations in LLM Activations","summaries":"We propose an auditing method to identify whether a large language model\n(LLM) encodes patterns such as hallucinations in its internal states, which may\npropagate to downstream tasks. We introduce a weakly supervised auditing\ntechnique using a subset scanning approach to detect anomalous patterns in LLM\nactivations from pre-trained models. Importantly, our method does not need\nknowledge of the type of patterns a-priori. Instead, it relies on a reference\ndataset devoid of anomalies during testing. Further, our approach enables the\nidentification of pivotal nodes responsible for encoding these patterns, which\nmay offer crucial insights for fine-tuning specific sub-networks for bias\nmitigation. We introduce two new scanning methods to handle LLM activations for\nanomalous sentences that may deviate from the expected distribution in either\ndirection. Our results confirm prior findings of BERT's limited internal\ncapacity for encoding hallucinations, while OPT appears capable of encoding\nhallucination information internally. Importantly, our scanning approach,\nwithout prior exposure to false statements, performs comparably to a fully\nsupervised out-of-distribution classifier.","terms":["cs.LG","cs.CL"]},{"titles":"PMMTalk: Speech-Driven 3D Facial Animation from Complementary Pseudo Multi-modal Features","summaries":"Speech-driven 3D facial animation has improved a lot recently while most\nrelated works only utilize acoustic modality and neglect the influence of\nvisual and textual cues, leading to unsatisfactory results in terms of\nprecision and coherence. We argue that visual and textual cues are not trivial\ninformation. Therefore, we present a novel framework, namely PMMTalk, using\ncomplementary Pseudo Multi-Modal features for improving the accuracy of facial\nanimation. The framework entails three modules: PMMTalk encoder, cross-modal\nalignment module, and PMMTalk decoder. Specifically, the PMMTalk encoder\nemploys the off-the-shelf talking head generation architecture and speech\nrecognition technology to extract visual and textual information from speech,\nrespectively. Subsequently, the cross-modal alignment module aligns the\naudio-image-text features at temporal and semantic levels. Then PMMTalk decoder\nis employed to predict lip-syncing facial blendshape coefficients. Contrary to\nprior methods, PMMTalk only requires an additional random reference face image\nbut yields more accurate results. Additionally, it is artist-friendly as it\nseamlessly integrates into standard animation production workflows by\nintroducing facial blendshape coefficients. Finally, given the scarcity of 3D\ntalking face datasets, we introduce a large-scale 3D Chinese Audio-Visual\nFacial Animation (3D-CAVFA) dataset. Extensive experiments and user studies\nshow that our approach outperforms the state of the art. We recommend watching\nthe supplementary video.","terms":["cs.CV","cs.AI"]},{"titles":"Scaling Laws for Adversarial Attacks on Language Model Activations","summaries":"We explore a class of adversarial attacks targeting the activations of\nlanguage models. By manipulating a relatively small subset of model\nactivations, $a$, we demonstrate the ability to control the exact prediction of\na significant number (in some cases up to 1000) of subsequent tokens $t$. We\nempirically verify a scaling law where the maximum number of target tokens\n$t_\\mathrm{max}$ predicted depends linearly on the number of tokens $a$ whose\nactivations the attacker controls as $t_\\mathrm{max} = \\kappa a$. We find that\nthe number of bits of control in the input space needed to control a single bit\nin the output space (what we call attack resistance $\\chi$) is remarkably\nconstant between $\\approx 16$ and $\\approx 25$ over 2 orders of magnitude of\nmodel sizes for different language models. Compared to attacks on tokens,\nattacks on activations are predictably much stronger, however, we identify a\nsurprising regularity where one bit of input steered either via activations or\nvia tokens is able to exert control over a similar amount of output bits. This\ngives support for the hypothesis that adversarial attacks are a consequence of\ndimensionality mismatch between the input and output spaces. A practical\nimplication of the ease of attacking language model activations instead of\ntokens is for multi-modal and selected retrieval models, where additional data\nsources are added as activations directly, sidestepping the tokenized input.\nThis opens up a new, broad attack surface. By using language models as a\ncontrollable test-bed to study adversarial attacks, we were able to experiment\nwith input-output dimensions that are inaccessible in computer vision,\nespecially where the output dimension dominates.","terms":["cs.LG","cs.CL","cs.CR"]},{"titles":"Generating Fine-Grained Human Motions Using ChatGPT-Refined Descriptions","summaries":"Recently, significant progress has been made in text-based motion generation,\nenabling the generation of diverse and high-quality human motions that conform\nto textual descriptions. However, it remains challenging to generate\nfine-grained or stylized motions due to the lack of datasets annotated with\ndetailed textual descriptions. By adopting a divide-and-conquer strategy, we\npropose a new framework named Fine-Grained Human Motion Diffusion Model\n(FG-MDM) for human motion generation. Specifically, we first parse previous\nvague textual annotation into fine-grained description of different body parts\nby leveraging a large language model (GPT-3.5). We then use these fine-grained\ndescriptions to guide a transformer-based diffusion model. FG-MDM can generate\nfine-grained and stylized motions even outside of the distribution of the\ntraining data. Our experimental results demonstrate the superiority of FG-MDM\nover previous methods, especially the strong generalization capability. We will\nrelease our fine-grained textual annotations for HumanML3D and KIT.","terms":["cs.CV"]},{"titles":"Learning \"Look-Ahead\" Nonlocal Traffic Dynamics in a Ring Road","summaries":"The macroscopic traffic flow model is widely used for traffic control and\nmanagement. To incorporate drivers' anticipative behaviors and to remove\nimpractical speed discontinuity inherent in the classic\nLighthill-Whitham-Richards (LWR) traffic model, nonlocal partial differential\nequation (PDE) models with ``look-ahead\" dynamics have been proposed, which\nassume that the speed is a function of weighted downstream traffic density.\nHowever, it lacks data validation on two important questions: whether there\nexist nonlocal dynamics, and how the length and weight of the ``look-ahead\"\nwindow affect the spatial temporal propagation of traffic densities. In this\npaper, we adopt traffic trajectory data from a ring-road experiment and design\na physics-informed neural network to learn the fundamental diagram and\nlook-ahead kernel that best fit the data, and reinvent a data-enhanced nonlocal\nLWR model via minimizing the loss function combining the data discrepancy and\nthe nonlocal model discrepancy. Results show that the learned nonlocal LWR\nyields a more accurate prediction of traffic wave propagation in three\ndifferent scenarios: stop-and-go oscillations, congested, and free traffic. We\nfirst demonstrate the existence of ``look-ahead\" effect with real traffic data.\nThe optimal nonlocal kernel is found out to take a length of around 35 to 50\nmeters, and the kernel weight within 5 meters accounts for the majority of the\nnonlocal effect. Our results also underscore the importance of choosing a\npriori physics in machine learning models.","terms":["cs.LG","cs.SY","eess.SY"]},{"titles":"SEVA: Leveraging sketches to evaluate alignment between human and machine visual abstraction","summaries":"Sketching is a powerful tool for creating abstract images that are sparse but\nmeaningful. Sketch understanding poses fundamental challenges for\ngeneral-purpose vision algorithms because it requires robustness to the\nsparsity of sketches relative to natural visual inputs and because it demands\ntolerance for semantic ambiguity, as sketches can reliably evoke multiple\nmeanings. While current vision algorithms have achieved high performance on a\nvariety of visual tasks, it remains unclear to what extent they understand\nsketches in a human-like way. Here we introduce SEVA, a new benchmark dataset\ncontaining approximately 90K human-generated sketches of 128 object concepts\nproduced under different time constraints, and thus systematically varying in\nsparsity. We evaluated a suite of state-of-the-art vision algorithms on their\nability to correctly identify the target concept depicted in these sketches and\nto generate responses that are strongly aligned with human response patterns on\nthe same sketch recognition task. We found that vision algorithms that better\npredicted human sketch recognition performance also better approximated human\nuncertainty about sketch meaning, but there remains a sizable gap between model\nand human response patterns. To explore the potential of models that emulate\nhuman visual abstraction in generative tasks, we conducted further evaluations\nof a recently developed sketch generation algorithm (Vinker et al., 2022)\ncapable of generating sketches that vary in sparsity. We hope that public\nrelease of this dataset and evaluation protocol will catalyze progress towards\nalgorithms with enhanced capacities for human-like visual abstraction.","terms":["cs.CV"]},{"titles":"Alleviating Exposure Bias in Diffusion Models through Sampling with Shifted Time Steps","summaries":"Diffusion Probabilistic Models (DPM) have shown remarkable efficacy in the\nsynthesis of high-quality images. However, their inference process\ncharacteristically requires numerous, potentially hundreds, of iterative steps,\nwhich could exaggerate the problem of exposure bias due to the training and\ninference discrepancy. Previous work has attempted to mitigate this issue by\nperturbing inputs during training, which consequently mandates the retraining\nof the DPM. In this work, we conduct a systematic study of exposure bias in DPM\nand, intriguingly, we find that the exposure bias could be alleviated with a\nnovel sampling method that we propose, without retraining the model. We\nempirically and theoretically show that, during inference, for each backward\ntime step $t$ and corresponding state $\\hat{x}_t$, there might exist another\ntime step $t_s$ which exhibits superior coupling with $\\hat{x}_t$. Based on\nthis finding, we introduce a sampling method named Time-Shift Sampler. Our\nframework can be seamlessly integrated to existing sampling algorithms, such as\nDDPM, DDIM and other high-order solvers, inducing merely minimal additional\ncomputations. Experimental results show our method brings significant and\nconsistent improvements in FID scores on different datasets and sampling\nmethods. For example, integrating Time-Shift Sampler to F-PNDM yields a\nFID=3.88, achieving 44.49\\% improvements as compared to F-PNDM, on CIFAR-10\nwith 10 sampling steps, which is more performant than the vanilla DDIM with 100\nsampling steps. We will release the code upon acceptance.","terms":["cs.CV"]},{"titles":"Robust Reinforcement Learning in Continuous Control Tasks with Uncertainty Set Regularization","summaries":"Reinforcement learning (RL) is recognized as lacking generalization and\nrobustness under environmental perturbations, which excessively restricts its\napplication for real-world robotics. Prior work claimed that adding\nregularization to the value function is equivalent to learning a robust policy\nwith uncertain transitions. Although the regularization-robustness\ntransformation is appealing for its simplicity and efficiency, it is still\nlacking in continuous control tasks. In this paper, we propose a new\nregularizer named $\\textbf{U}$ncertainty $\\textbf{S}$et $\\textbf{R}$egularizer\n(USR), by formulating the uncertainty set on the parameter space of the\ntransition function. In particular, USR is flexible enough to be plugged into\nany existing RL framework. To deal with unknown uncertainty sets, we further\npropose a novel adversarial approach to generate them based on the value\nfunction. We evaluate USR on the Real-world Reinforcement Learning (RWRL)\nbenchmark, demonstrating improvements in the robust performance for perturbed\ntesting environments.","terms":["cs.LG","cs.AI","cs.SY","eess.SY"]},{"titles":"Topological Graph Signal Compression","summaries":"Recently emerged Topological Deep Learning (TDL) methods aim to extend\ncurrent Graph Neural Networks (GNN) by naturally processing higher-order\ninteractions, going beyond the pairwise relations and local neighborhoods\ndefined by graph representations. In this paper we propose a novel TDL-based\nmethod for compressing signals over graphs, consisting in two main steps:\nfirst, disjoint sets of higher-order structures are inferred based on the\noriginal signal --by clustering $N$ datapoints into $K\\ll N$ collections; then,\na topological-inspired message passing gets a compressed representation of the\nsignal within those multi-element sets. Our results show that our framework\nimproves both standard GNN and feed-forward architectures in compressing\ntemporal link-based signals from two real-word Internet Service Provider\nNetworks' datasets --from $30\\%$ up to $90\\%$ better reconstruction errors\nacross all evaluation scenarios--, suggesting that it better captures and\nexploits spatial and temporal correlations over the whole graph-based network\nstructure.","terms":["cs.LG","cs.AI","cs.NI"]},{"titles":"C-NERF: Representing Scene Changes as Directional Consistency Difference-based NeRF","summaries":"In this work, we aim to detect the changes caused by object variations in a\nscene represented by the neural radiance fields (NeRFs). Given an arbitrary\nview and two sets of scene images captured at different timestamps, we can\npredict the scene changes in that view, which has significant potential\napplications in scene monitoring and measuring. We conducted preliminary\nstudies and found that such an exciting task cannot be easily achieved by\nutilizing existing NeRFs and 2D change detection methods with many false or\nmissing detections. The main reason is that the 2D change detection is based on\nthe pixel appearance difference between spatial-aligned image pairs and\nneglects the stereo information in the NeRF. To address the limitations, we\npropose the C-NERF to represent scene changes as directional consistency\ndifference-based NeRF, which mainly contains three modules. We first perform\nthe spatial alignment of two NeRFs captured before and after changes. Then, we\nidentify the change points based on the direction-consistent constraint; that\nis, real change points have similar change representations across view\ndirections, but fake change points do not. Finally, we design the change map\nrendering process based on the built NeRFs and can generate the change map of\nan arbitrarily specified view direction. To validate the effectiveness, we\nbuild a new dataset containing ten scenes covering diverse scenarios with\ndifferent changing objects. Our approach surpasses state-of-the-art 2D change\ndetection and NeRF-based methods by a significant margin.","terms":["cs.CV"]},{"titles":"On minimizing the training set fill distance in machine learning regression","summaries":"For regression tasks one often leverages large datasets for training\npredictive machine learning models. However, using large datasets may not be\nfeasible due to computational limitations or high data labelling costs.\nTherefore, suitably selecting small training sets from large pools of\nunlabelled data points is essential to maximize model performance while\nmaintaining efficiency. In this work, we study Farthest Point Sampling (FPS), a\ndata selection approach that aims to minimize the fill distance of the selected\nset. We derive an upper bound for the maximum expected prediction error,\nconditional to the location of the unlabelled data points, that linearly\ndepends on the training set fill distance. For empirical validation, we perform\nexperiments using two regression models on three datasets. We empirically show\nthat selecting a training set by aiming to minimize the fill distance, thereby\nminimizing our derived bound, significantly reduces the maximum prediction\nerror of various regression models, outperforming alternative sampling\napproaches by a large margin. Furthermore, we show that selecting training sets\nwith the FPS can also increase model stability for the specific case of\nGaussian kernel regression approaches.","terms":["cs.LG"]},{"titles":"GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models","summaries":"In recent times, the generation of 3D assets from text prompts has shown\nimpressive results. Both 2D and 3D diffusion models can help generate decent 3D\nobjects based on prompts. 3D diffusion models have good 3D consistency, but\ntheir quality and generalization are limited as trainable 3D data is expensive\nand hard to obtain. 2D diffusion models enjoy strong abilities of\ngeneralization and fine generation, but 3D consistency is hard to guarantee.\nThis paper attempts to bridge the power from the two types of diffusion models\nvia the recent explicit and efficient 3D Gaussian splatting representation. A\nfast 3D object generation framework, named as GaussianDreamer, is proposed,\nwhere the 3D diffusion model provides priors for initialization and the 2D\ndiffusion model enriches the geometry and appearance. Operations of noisy point\ngrowing and color perturbation are introduced to enhance the initialized\nGaussians. Our GaussianDreamer can generate a high-quality 3D instance or 3D\navatar within 15 minutes on one GPU, much faster than previous methods, while\nthe generated instances can be directly rendered in real time. Demos and code\nare available at https:\/\/taoranyi.com\/gaussiandreamer\/.","terms":["cs.CV","cs.GR"]},{"titles":"CROP: Towards Distributional-Shift Robust Reinforcement Learning using Compact Reshaped Observation Processing","summaries":"The safe application of reinforcement learning (RL) requires generalization\nfrom limited training data to unseen scenarios. Yet, fulfilling tasks under\nchanging circumstances is a key challenge in RL. Current state-of-the-art\napproaches for generalization apply data augmentation techniques to increase\nthe diversity of training data. Even though this prevents overfitting to the\ntraining environment(s), it hinders policy optimization. Crafting a suitable\nobservation, only containing crucial information, has been shown to be a\nchallenging task itself. To improve data efficiency and generalization\ncapabilities, we propose Compact Reshaped Observation Processing (CROP) to\nreduce the state information used for policy optimization. By providing only\nrelevant information, overfitting to a specific training layout is precluded\nand generalization to unseen environments is improved. We formulate three CROPs\nthat can be applied to fully observable observation- and action-spaces and\nprovide methodical foundation. We empirically show the improvements of CROP in\na distributionally shifted safety gridworld. We furthermore provide benchmark\ncomparisons to full observability and data-augmentation in two different-sized\nprocedurally generated mazes.","terms":["cs.LG"]},{"titles":"LExCI: A Framework for Reinforcement Learning with Embedded Systems","summaries":"Advances in artificial intelligence (AI) have led to its application in many\nareas of everyday life. In the context of control engineering, reinforcement\nlearning (RL) represents a particularly promising approach as it is centred\naround the idea of allowing an agent to freely interact with its environment to\nfind an optimal strategy. One of the challenges professionals face when\ntraining and deploying RL agents is that the latter often have to run on\ndedicated embedded devices. This could be to integrate them into an existing\ntoolchain or to satisfy certain performance criteria like real-time\nconstraints. Conventional RL libraries, however, cannot be easily utilised in\nconjunction with that kind of hardware. In this paper, we present a framework\nnamed LExCI, the Learning and Experiencing Cycle Interface, which bridges this\ngap and provides end-users with a free and open-source tool for training agents\non embedded systems using the open-source library RLlib. Its operability is\ndemonstrated with two state-of-the-art RL-algorithms and a rapid control\nprototyping system.","terms":["cs.LG"]},{"titles":"Physics-informed neural networks with unknown measurement noise","summaries":"Physics-informed neural networks (PINNs) constitute a flexible approach to\nboth finding solutions and identifying parameters of partial differential\nequations. Most works on the topic assume noiseless data, or data contaminated\nwith weak Gaussian noise. We show that the standard PINN framework breaks down\nin case of non-Gaussian noise. We give a way of resolving this fundamental\nissue and we propose to jointly train an energy-based model (EBM) to learn the\ncorrect noise distribution. We illustrate the improved performance of our\napproach using multiple examples.","terms":["stat.ML","cs.LG"]},{"titles":"Leveraging Model Fusion for Improved License Plate Recognition","summaries":"License Plate Recognition (LPR) plays a critical role in various\napplications, such as toll collection, parking management, and traffic law\nenforcement. Although LPR has witnessed significant advancements through the\ndevelopment of deep learning, there has been a noticeable lack of studies\nexploring the potential improvements in results by fusing the outputs from\nmultiple recognition models. This research aims to fill this gap by\ninvestigating the combination of up to 12 different models using\nstraightforward approaches, such as selecting the most confident prediction or\nemploying majority vote-based strategies. Our experiments encompass a wide\nrange of datasets, revealing substantial benefits of fusion approaches in both\nintra- and cross-dataset setups. Essentially, fusing multiple models reduces\nconsiderably the likelihood of obtaining subpar performance on a particular\ndataset\/scenario. We also found that combining models based on their speed is\nan appealing approach. Specifically, for applications where the recognition\ntask can tolerate some additional time, though not excessively, an effective\nstrategy is to combine 4-6 models. These models may not be the most accurate\nindividually, but their fusion strikes an optimal balance between speed and\naccuracy.","terms":["cs.CV"]},{"titles":"Towards Measuring Representational Similarity of Large Language Models","summaries":"Understanding the similarity of the numerous released large language models\n(LLMs) has many uses, e.g., simplifying model selection, detecting illegal\nmodel reuse, and advancing our understanding of what makes LLMs perform well.\nIn this work, we measure the similarity of representations of a set of LLMs\nwith 7B parameters. Our results suggest that some LLMs are substantially\ndifferent from others. We identify challenges of using representational\nsimilarity measures that suggest the need of careful study of similarity scores\nto avoid false conclusions.","terms":["cs.LG","cs.CL"]},{"titles":"Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain","summaries":"Time series has been left behind in the era of pre-training and transfer\nlearning. While research in the fields of natural language processing and\ncomputer vision are enjoying progressively larger datasets to train massive\nmodels, the most popular time series datasets consist of only tens of thousands\nof time steps, limiting our ability to study the effectiveness of pre-training\nand scaling. Recent studies have also cast doubt on the need for expressive\nmodels and scale. To alleviate these issues, we introduce three large-scale\ntime series forecasting datasets from the cloud operations (CloudOps) domain,\nthe largest having billions of observations, enabling further study into\npre-training and scaling of time series models. We build the empirical\ngroundwork for studying pre-training and scaling of time series models and pave\nthe way for future research by identifying a promising candidate architecture.\nWe show that it is a strong zero-shot baseline and benefits from further\nscaling, both in model and dataset size. Accompanying these datasets and\nresults is a suite of comprehensive benchmark results comparing classical and\ndeep learning baselines to our pre-trained method - achieving a 27% reduction\nin error on the largest dataset. Code and datasets can be found\nhttps:\/\/github.com\/SalesforceAIResearch\/pretrain-time-series-cloudops.","terms":["cs.LG"]},{"titles":"LiDAR-based Person Re-identification","summaries":"Camera-based person re-identification (ReID) systems have been widely applied\nin the field of public security. However, cameras often lack the perception of\n3D morphological information of human and are susceptible to various\nlimitations, such as inadequate illumination, complex background, and personal\nprivacy. In this paper, we propose a LiDAR-based ReID framework, ReID3D, that\nutilizes pre-training strategy to retrieve features of 3D body shape and\nintroduces Graph-based Complementary Enhancement Encoder for extracting\ncomprehensive features. Due to the lack of LiDAR datasets, we build LReID, the\nfirst LiDAR-based person ReID dataset, which is collected in several outdoor\nscenes with variations in natural conditions. Additionally, we introduce\nLReID-sync, a simulated pedestrian dataset designed for pre-training encoders\nwith tasks of point cloud completion and shape parameter learning. Extensive\nexperiments on LReID show that ReID3D achieves exceptional performance with a\nrank-1 accuracy of 94.0, highlighting the significant potential of LiDAR in\naddressing person ReID tasks. To the best of our knowledge, we are the first to\npropose a solution for LiDAR-based ReID. The code and datasets will be released\nsoon.","terms":["cs.CV"]},{"titles":"R3D-SWIN:Use Shifted Window Attention for Single-View 3D Reconstruction","summaries":"Recently, vision transformers have performed well in various computer vision\ntasks, including voxel 3D reconstruction. However, the windows of the vision\ntransformer are not multi-scale, and there is no connection between the\nwindows, which limits the accuracy of voxel 3D reconstruction . Therefore, we\npropose a shifted windows attention voxel 3D reconstruction network. To the\nbest of our knowledge, this is the first work to apply shifted window attention\nto voxel 3D reconstruction. Experimental results on ShapeNet verify our method\nachieves SOTA accuracy in single-view reconstruction.","terms":["cs.CV"]},{"titles":"Towards the Inferrence of Structural Similarity of Combinatorial Landscapes","summaries":"One of the most common problem-solving heuristics is by analogy. For a given\nproblem, a solver can be viewed as a strategic walk on its fitness landscape.\nThus if a solver works for one problem instance, we expect it will also be\neffective for other instances whose fitness landscapes essentially share\nstructural similarities with each other. However, due to the black-box nature\nof combinatorial optimization, it is far from trivial to infer such similarity\nin real-world scenarios. To bridge this gap, by using local optima network as a\nproxy of fitness landscapes, this paper proposed to leverage graph data mining\ntechniques to conduct qualitative and quantitative analyses to explore the\nlatent topological structural information embedded in those landscapes. By\nconducting large-scale empirical experiments on three classic combinatorial\noptimization problems, we gain concrete evidence to support the existence of\nstructural similarity between landscapes of the same classes within neighboring\ndimensions. We also interrogated the relationship between landscapes of\ndifferent problem classes.","terms":["cs.LG","cs.AI"]},{"titles":"TR3D: Towards Real-Time Indoor 3D Object Detection","summaries":"Recently, sparse 3D convolutions have changed 3D object detection. Performing\non par with the voting-based approaches, 3D CNNs are memory-efficient and scale\nto large scenes better. However, there is still room for improvement. With a\nconscious, practice-oriented approach to problem-solving, we analyze the\nperformance of such methods and localize the weaknesses. Applying modifications\nthat resolve the found issues one by one, we end up with TR3D: a fast\nfully-convolutional 3D object detection model trained end-to-end, that achieves\nstate-of-the-art results on the standard benchmarks, ScanNet v2, SUN RGB-D, and\nS3DIS. Moreover, to take advantage of both point cloud and RGB inputs, we\nintroduce an early fusion of 2D and 3D features. We employ our fusion module to\nmake conventional 3D object detection methods multimodal and demonstrate an\nimpressive boost in performance. Our model with early feature fusion, which we\nrefer to as TR3D+FF, outperforms existing 3D object detection approaches on the\nSUN RGB-D dataset. Overall, besides being accurate, both TR3D and TR3D+FF\nmodels are lightweight, memory-efficient, and fast, thereby marking another\nmilestone on the way toward real-time 3D object detection. Code is available at\nhttps:\/\/github.com\/SamsungLabs\/tr3d .","terms":["cs.CV"]},{"titles":"Self-Evolving Neural Radiance Fields","summaries":"Recently, neural radiance field (NeRF) has shown remarkable performance in\nnovel view synthesis and 3D reconstruction. However, it still requires abundant\nhigh-quality images, limiting its applicability in real-world scenarios. To\novercome this limitation, recent works have focused on training NeRF only with\nsparse viewpoints by giving additional regularizations, often called few-shot\nNeRF. We observe that due to the under-constrained nature of the task, solely\nusing additional regularization is not enough to prevent the model from\noverfitting to sparse viewpoints. In this paper, we propose a novel framework,\ndubbed Self-Evolving Neural Radiance Fields (SE-NeRF), that applies a\nself-training framework to NeRF to address these problems. We formulate\nfew-shot NeRF into a teacher-student framework to guide the network to learn a\nmore robust representation of the scene by training the student with additional\npseudo labels generated from the teacher. By distilling ray-level pseudo labels\nusing distinct distillation schemes for reliable and unreliable rays obtained\nwith our novel reliability estimation method, we enable NeRF to learn a more\naccurate and robust geometry of the 3D scene. We show and evaluate that\napplying our self-training framework to existing models improves the quality of\nthe rendered images and achieves state-of-the-art performance in multiple\nsettings.","terms":["cs.CV"]},{"titles":"RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model","summaries":"Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text\npaired data have demonstrated unprecedented image-text association\ncapabilities, achieving remarkable results across various downstream tasks. A\ncritical challenge is how to make use of existing large-scale pre-trained VLMs,\nwhich are trained on common objects, to perform the domain-specific transfer\nfor accomplishing domain-related downstream tasks. A critical challenge is how\nto make use of existing large-scale pre-trained VLMs, which are trained on\ncommon objects, to perform the domain-specific transfer for accomplishing\ndomain-related downstream tasks. In this paper, we propose a new framework that\nincludes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap\nbetween the General Vision-Language Model (GVLM) and domain-specific downstream\ntasks. Moreover, we present an image-text paired dataset in the field of remote\nsensing (RS), RS5M, which has 5 million RS images with English descriptions.\nThe dataset is obtained from filtering publicly available image-text paired\ndatasets and captioning label-only RS datasets with pre-trained VLM. These\nconstitute the first large-scale RS image-text paired dataset. Additionally, we\nfine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning\nmethods on RS5M to implement the DVLM. Experimental results show that our\nproposed dataset is highly effective for various tasks, and our model GeoRSCLIP\nimproves upon the baseline or previous state-of-the-art model by $3\\%\\sim20\\%$\nin Zero-shot Classification (ZSC), $3\\%\\sim6\\%$ in Remote Sensing Cross-Modal\nText-Image Retrieval (RSCTIR) and $4\\%\\sim5\\%$ in Semantic Localization (SeLo)\ntasks. Dataset and models have been released in:\n\\url{https:\/\/github.com\/om-ai-lab\/RS5M}.","terms":["cs.CV","cs.AI","cs.CL","cs.MM"]},{"titles":"Revisiting Hidden Representations in Transfer Learning for Medical Imaging","summaries":"While a key component to the success of deep learning is the availability of\nmassive amounts of training data, medical image datasets are often limited in\ndiversity and size. Transfer learning has the potential to bridge the gap\nbetween related yet different domains. For medical applications, however, it\nremains unclear whether it is more beneficial to pre-train on natural or\nmedical images. We aim to shed light on this problem by comparing\ninitialization on ImageNet and RadImageNet on seven medical classification\ntasks. Our work includes a replication study, which yields results contrary to\npreviously published findings. In our experiments, ResNet50 models pre-trained\non ImageNet tend to outperform those trained on RadImageNet. To gain further\ninsights, we investigate the learned representations using Canonical\nCorrelation Analysis (CCA) and compare the predictions of the different models.\nOur results indicate that, contrary to intuition, ImageNet and RadImageNet may\nconverge to distinct intermediate representations, which appear to diverge\nfurther during fine-tuning. Despite these distinct representations, the\npredictions of the models remain similar. Our findings show that the similarity\nbetween networks before and after fine-tuning does not correlate with\nperformance gains, suggesting that the advantages of transfer learning might\nnot solely originate from the reuse of features in the early layers of a\nconvolutional neural network.","terms":["cs.CV","cs.LG"]},{"titles":"TriDeNT: Triple Deep Network Training for Privileged Knowledge Distillation in Histopathology","summaries":"Computational pathology models rarely utilise data that will not be available\nfor inference. This means most models cannot learn from highly informative data\nsuch as additional immunohistochemical (IHC) stains and spatial\ntranscriptomics. We present TriDeNT, a novel self-supervised method for\nutilising privileged data that is not available during inference to improve\nperformance. We demonstrate the efficacy of this method for a range of\ndifferent paired data including immunohistochemistry, spatial transcriptomics\nand expert nuclei annotations. In all settings, TriDeNT outperforms other\nstate-of-the-art methods in downstream tasks, with observed improvements of up\nto 101%. Furthermore, we provide qualitative and quantitative measurements of\nthe features learned by these models and how they differ from baselines.\nTriDeNT offers a novel method to distil knowledge from scarce or costly data\nduring training, to create significantly better models for routine inputs.","terms":["cs.CV","cs.AI","cs.LG","q-bio.TO"]},{"titles":"(Provable) Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More","summaries":"A machine learning model is traditionally considered robust if its prediction\nremains (almost) constant under input perturbations with small norm. However,\nreal-world tasks like molecular property prediction or point cloud segmentation\nhave inherent equivariances, such as rotation or permutation equivariance. In\nsuch tasks, even perturbations with large norm do not necessarily change an\ninput's semantic content. Furthermore, there are perturbations for which a\nmodel's prediction explicitly needs to change. For the first time, we propose a\nsound notion of adversarial robustness that accounts for task equivariance. We\nthen demonstrate that provable robustness can be achieved by (1) choosing a\nmodel that matches the task's equivariances (2) certifying traditional\nadversarial robustness. Certification methods are, however, unavailable for\nmany models, such as those with continuous equivariances. We close this gap by\ndeveloping the framework of equivariance-preserving randomized smoothing, which\nenables architecture-agnostic certification. We additionally derive the first\narchitecture-specific graph edit distance certificates, i.e. sound robustness\nguarantees for isomorphism equivariant tasks like node classification. Overall,\na sound notion of robustness is an important prerequisite for future work at\nthe intersection of robust and geometric machine learning.","terms":["cs.LG","cs.CR","stat.ML"]},{"titles":"Unified learning-based lossy and lossless JPEG recompression","summaries":"JPEG is still the most widely used image compression algorithm. Most image\ncompression algorithms only consider uncompressed original image, while\nignoring a large number of already existing JPEG images. Recently, JPEG\nrecompression approaches have been proposed to further reduce the size of JPEG\nfiles. However, those methods only consider JPEG lossless recompression, which\nis just a special case of the rate-distortion theorem. In this paper, we\npropose a unified lossly and lossless JPEG recompression framework, which\nconsists of learned quantization table and Markovian hierarchical variational\nautoencoders. Experiments show that our method can achieve arbitrarily low\ndistortion when the bitrate is close to the upper bound, namely the bitrate of\nthe lossless compression model. To the best of our knowledge, this is the first\nlearned method that bridges the gap between lossy and lossless recompression of\nJPEG images.","terms":["cs.CV","cs.AI"]},{"titles":"Solving Inverse Physics Problems with Score Matching","summaries":"We propose to solve inverse problems involving the temporal evolution of\nphysics systems by leveraging recent advances from diffusion models. Our method\nmoves the system's current state backward in time step by step by combining an\napproximate inverse physics simulator and a learned correction function. A\ncentral insight of our work is that training the learned correction with a\nsingle-step loss is equivalent to a score matching objective, while recursively\npredicting longer parts of the trajectory during training relates to maximum\nlikelihood training of a corresponding probability flow. We highlight the\nadvantages of our algorithm compared to standard denoising score matching and\nimplicit score matching, as well as fully learned baselines for a wide range of\ninverse physics problems. The resulting inverse solver has excellent accuracy\nand temporal stability and, in contrast to other learned inverse solvers,\nallows for sampling the posterior of the solutions.","terms":["cs.LG","physics.data-an"]},{"titles":"MyPortrait: Morphable Prior-Guided Personalized Portrait Generation","summaries":"Generating realistic talking faces is an interesting and long-standing topic\nin the field of computer vision. Although significant progress has been made,\nit is still challenging to generate high-quality dynamic faces with\npersonalized details. This is mainly due to the inability of the general model\nto represent personalized details and the generalization problem to unseen\ncontrollable parameters. In this work, we propose Myportrait, a simple,\ngeneral, and flexible framework for neural portrait generation. We incorporate\npersonalized prior in a monocular video and morphable prior in 3D face\nmorphable space for generating personalized details under novel controllable\nparameters. Our proposed framework supports both video-driven and audio-driven\nface animation given a monocular video of a single person. Distinguished by\nwhether the test data is sent to training or not, our method provides a\nreal-time online version and a high-quality offline version. Comprehensive\nexperiments in various metrics demonstrate the superior performance of our\nmethod over the state-of-the-art methods. The code will be publicly available.","terms":["cs.CV"]},{"titles":"Neural Sign Actors: A diffusion model for 3D sign language production from text","summaries":"Sign Languages (SL) serve as the predominant mode of communication for the\nDeaf and Hard of Hearing communities. The advent of deep learning has aided\nnumerous methods in SL recognition and translation, achieving remarkable\nresults. However, Sign Language Production (SLP) poses a challenge for the\ncomputer vision community as the motions generated must be realistic and have\nprecise semantic meanings. Most SLP methods rely on 2D data, thus impeding\ntheir ability to attain a necessary level of realism. In this work, we propose\na diffusion-based SLP model trained on a curated large-scale dataset of 4D\nsigning avatars and their corresponding text transcripts. The proposed method\ncan generate dynamic sequences of 3D avatars from an unconstrained domain of\ndiscourse using a diffusion process formed on a novel and anatomically informed\ngraph neural network defined on the SMPL-X body skeleton. Through a series of\nquantitative and qualitative experiments, we show that the proposed method\nconsiderably outperforms previous methods of SLP. We believe that this work\npresents an important and necessary step towards realistic neural sign avatars,\nbridging the communication gap between Deaf and hearing communities. The code,\nmethod and generated data will be made publicly available.","terms":["cs.CV"]},{"titles":"Revisit Human-Scene Interaction via Space Occupancy","summaries":"Human-scene Interaction (HSI) generation is a challenging task and crucial\nfor various downstream tasks. However, one of the major obstacles is the\nlimited data scale. High-quality data with simultaneously captured human and 3D\nenvironments is rare, resulting in limited data diversity and complexity. In\nthis work, we argue that interaction with a scene is essentially interacting\nwith the space occupancy of the scene from an abstract physical perspective,\nleading us to a unified novel view of Human-Occupancy Interaction. By treating\npure motion sequences as records of humans interacting with invisible scene\noccupancy, we can aggregate motion-only data into a large-scale paired\nhuman-occupancy interaction database: Motion Occupancy Base (MOB). Thus, the\nneed for costly paired motion-scene datasets with high-quality scene scans can\nbe substantially alleviated. With this new unified view of Human-Occupancy\ninteraction, a single motion controller is proposed to reach the target state\ngiven the surrounding occupancy. Once trained on MOB with complex occupancy\nlayout, the controller could handle cramped scenes and generalize well to\ngeneral scenes with limited complexity. With no GT 3D scenes for training, our\nmethod can generate realistic and stable HSI motions in diverse scenarios,\nincluding both static and dynamic scenes. Our code and data would be made\npublicly available at https:\/\/foruck.github.io\/occu-page\/.","terms":["cs.CV"]},{"titles":"Enhancing Vehicle Entrance and Parking Management: Deep Learning Solutions for Efficiency and Security","summaries":"The auto-management of vehicle entrance and parking in any organization is a\ncomplex challenge encompassing record-keeping, efficiency, and security\nconcerns. Manual methods for tracking vehicles and finding parking spaces are\nslow and a waste of time. To solve the problem of auto management of vehicle\nentrance and parking, we have utilized state-of-the-art deep learning models\nand automated the process of vehicle entrance and parking into any\norganization. To ensure security, our system integrated vehicle detection,\nlicense number plate verification, and face detection and recognition models to\nensure that the person and vehicle are registered with the organization. We\nhave trained multiple deep-learning models for vehicle detection, license\nnumber plate detection, face detection, and recognition, however, the YOLOv8n\nmodel outperformed all the other models. Furthermore, License plate recognition\nis facilitated by Google's Tesseract-OCR Engine. By integrating these\ntechnologies, the system offers efficient vehicle detection, precise\nidentification, streamlined record keeping, and optimized parking slot\nallocation in buildings, thereby enhancing convenience, accuracy, and security.\nFuture research opportunities lie in fine-tuning system performance for a wide\nrange of real-world applications.","terms":["cs.CV","cs.AI"]},{"titles":"Analyzing and Improving the Training Dynamics of Diffusion Models","summaries":"Diffusion models currently dominate the field of data-driven image synthesis\nwith their unparalleled scaling to large datasets. In this paper, we identify\nand rectify several causes for uneven and ineffective training in the popular\nADM diffusion model architecture, without altering its high-level structure.\nObserving uncontrolled magnitude changes and imbalances in both the network\nactivations and weights over the course of training, we redesign the network\nlayers to preserve activation, weight, and update magnitudes on expectation. We\nfind that systematic application of this philosophy eliminates the observed\ndrifts and imbalances, resulting in considerably better networks at equal\ncomputational complexity. Our modifications improve the previous record FID of\n2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic\nsampling.\n  As an independent contribution, we present a method for setting the\nexponential moving average (EMA) parameters post-hoc, i.e., after completing\nthe training run. This allows precise tuning of EMA length without the cost of\nperforming several training runs, and reveals its surprising interactions with\nnetwork architecture, training time, and guidance.","terms":["cs.CV","cs.AI","cs.LG","cs.NE","stat.ML"]},{"titles":"Unraveling the Enigma of Double Descent: An In-depth Analysis through the Lens of Learned Feature Space","summaries":"Double descent presents a counter-intuitive aspect within the machine\nlearning domain, and researchers have observed its manifestation in various\nmodels and tasks. While some theoretical explanations have been proposed for\nthis phenomenon in specific contexts, an accepted theory to account for its\noccurrence in deep learning remains yet to be established. In this study, we\nrevisit the phenomenon of double descent and demonstrate that its occurrence is\nstrongly influenced by the presence of noisy data. Through conducting a\ncomprehensive analysis of the feature space of learned representations, we\nunveil that double descent arises in imperfect models trained with noisy data.\nWe argue that double descent is a consequence of the model first learning the\nnoisy data until interpolation and then adding implicit regularization via\nover-parameterization acquiring therefore capability to separate the\ninformation from the noise.","terms":["cs.LG"]},{"titles":"UPOCR: Towards Unified Pixel-Level OCR Interface","summaries":"In recent years, the optical character recognition (OCR) field has been\nproliferating with plentiful cutting-edge approaches for a wide spectrum of\ntasks. However, these approaches are task-specifically designed with divergent\nparadigms, architectures, and training strategies, which significantly\nincreases the complexity of research and maintenance and hinders the fast\ndeployment in applications. To this end, we propose UPOCR, a\nsimple-yet-effective generalist model for Unified Pixel-level OCR interface.\nSpecifically, the UPOCR unifies the paradigm of diverse OCR tasks as\nimage-to-image transformation and the architecture as a vision Transformer\n(ViT)-based encoder-decoder. Learnable task prompts are introduced to push the\ngeneral feature representations extracted by the encoder toward task-specific\nspaces, endowing the decoder with task awareness. Moreover, the model training\nis uniformly aimed at minimizing the discrepancy between the generated and\nground-truth images regardless of the inhomogeneity among tasks. Experiments\nare conducted on three pixel-level OCR tasks including text removal, text\nsegmentation, and tampered text detection. Without bells and whistles, the\nexperimental results showcase that the proposed method can simultaneously\nachieve state-of-the-art performance on three tasks with a unified single\nmodel, which provides valuable strategies and insights for future research on\ngeneralist OCR models. Code will be publicly available.","terms":["cs.CV"]},{"titles":"Policy Gradient with Kernel Quadrature","summaries":"Reward evaluation of episodes becomes a bottleneck in a broad range of\nreinforcement learning tasks. Our aim in this paper is to select a small but\nrepresentative subset of a large batch of episodes, only on which we actually\ncompute rewards for more efficient policy gradient iterations. We build a\nGaussian process modeling of discounted returns or rewards to derive a positive\ndefinite kernel on the space of episodes, run an ``episodic\" kernel quadrature\nmethod to compress the information of sample episodes, and pass the reduced\nepisodes to the policy network for gradient updates. We present the theoretical\nbackground of this procedure as well as its numerical illustrations in MuJoCo\ntasks.","terms":["cs.LG","cs.AI"]},{"titles":"DeepPointMap: Advancing LiDAR SLAM with Unified Neural Descriptors","summaries":"Point clouds have shown significant potential in various domains, including\nSimultaneous Localization and Mapping (SLAM). However, existing approaches\neither rely on dense point clouds to achieve high localization accuracy or use\ngeneralized descriptors to reduce map size. Unfortunately, these two aspects\nseem to conflict with each other. To address this limitation, we propose a\nunified architecture, DeepPointMap, achieving excellent preference on both\naspects. We utilize neural network to extract highly representative and sparse\nneural descriptors from point clouds, enabling memory-efficient map\nrepresentation and accurate multi-scale localization tasks (e.g., odometry and\nloop-closure). Moreover, we showcase the versatility of our framework by\nextending it to more challenging multi-agent collaborative SLAM. The promising\nresults obtained in these scenarios further emphasize the effectiveness and\npotential of our approach.","terms":["cs.CV","cs.LG","cs.RO"]},{"titles":"H-GAP: Humanoid Control with a Generalist Planner","summaries":"Humanoid control is an important research challenge offering avenues for\nintegration into human-centric infrastructures and enabling physics-driven\nhumanoid animations. The daunting challenges in this field stem from the\ndifficulty of optimizing in high-dimensional action spaces and the instability\nintroduced by the bipedal morphology of humanoids. However, the extensive\ncollection of human motion-captured data and the derived datasets of humanoid\ntrajectories, such as MoCapAct, paves the way to tackle these challenges. In\nthis context, we present Humanoid Generalist Autoencoding Planner (H-GAP), a\nstate-action trajectory generative model trained on humanoid trajectories\nderived from human motion-captured data, capable of adeptly handling downstream\ncontrol tasks with Model Predictive Control (MPC). For 56 degrees of freedom\nhumanoid, we empirically demonstrate that H-GAP learns to represent and\ngenerate a wide range of motor behaviours. Further, without any learning from\nonline interactions, it can also flexibly transfer these behaviors to solve\nnovel downstream control tasks via planning. Notably, H-GAP excels established\nMPC baselines that have access to the ground truth dynamics model, and is\nsuperior or comparable to offline RL methods trained for individual tasks.\nFinally, we do a series of empirical studies on the scaling properties of\nH-GAP, showing the potential for performance gains via additional data but not\ncomputing. Code and videos are available at\nhttps:\/\/ycxuyingchen.github.io\/hgap\/.","terms":["cs.LG","cs.AI","cs.RO"]},{"titles":"Domain-wise Invariant Learning for Panoptic Scene Graph Generation","summaries":"Panoptic Scene Graph Generation (PSG) involves the detection of objects and\nthe prediction of their corresponding relationships (predicates). However, the\npresence of biased predicate annotations poses a significant challenge for PSG\nmodels, as it hinders their ability to establish a clear decision boundary\namong different predicates. This issue substantially impedes the practical\nutility and real-world applicability of PSG models. To address the intrinsic\nbias above, we propose a novel framework to infer potentially biased\nannotations by measuring the predicate prediction risks within each\nsubject-object pair (domain), and adaptively transfer the biased annotations to\nconsistent ones by learning invariant predicate representation embeddings.\nExperiments show that our method significantly improves the performance of\nbenchmark models, achieving a new state-of-the-art performance, and shows great\ngeneralization and effectiveness on PSG dataset.","terms":["cs.CV"]},{"titles":"Zero-Shot Point Cloud Registration","summaries":"Learning-based point cloud registration approaches have significantly\noutperformed their traditional counterparts. However, they typically require\nextensive training on specific datasets. In this paper, we propose , the first\nzero-shot point cloud registration approach that eliminates the need for\ntraining on point cloud datasets. The cornerstone of ZeroReg is the novel\ntransfer of image features from keypoints to the point cloud, enriched by\naggregating information from 3D geometric neighborhoods. Specifically, we\nextract keypoints and features from 2D image pairs using a frozen pretrained 2D\nbackbone. These features are then projected in 3D, and patches are constructed\nby searching for neighboring points. We integrate the geometric and visual\nfeatures of each point using our novel parameter-free geometric decoder.\nSubsequently, the task of determining correspondences between point clouds is\nformulated as an optimal transport problem. Extensive evaluations of ZeroReg\ndemonstrate its competitive performance against both traditional and\nlearning-based methods. On benchmarks such as 3DMatch, 3DLoMatch, and ScanNet,\nZeroReg achieves impressive Recall Ratios (RR) of over 84%, 46%, and 75%,\nrespectively.","terms":["cs.CV"]},{"titles":"Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving?","summaries":"End-to-end autonomous driving recently emerged as a promising research\ndirection to target autonomy from a full-stack perspective. Along this line,\nmany of the latest works follow an open-loop evaluation setting on nuScenes to\nstudy the planning behavior. In this paper, we delve deeper into the problem by\nconducting thorough analyses and demystifying more devils in the details. We\ninitially observed that the nuScenes dataset, characterized by relatively\nsimple driving scenarios, leads to an under-utilization of perception\ninformation in end-to-end models incorporating ego status, such as the ego\nvehicle's velocity. These models tend to rely predominantly on the ego\nvehicle's status for future path planning. Beyond the limitations of the\ndataset, we also note that current metrics do not comprehensively assess the\nplanning quality, leading to potentially biased conclusions drawn from existing\nbenchmarks. To address this issue, we introduce a new metric to evaluate\nwhether the predicted trajectories adhere to the road. We further propose a\nsimple baseline able to achieve competitive results without relying on\nperception annotations. Given the current limitations on the benchmark and\nmetrics, we suggest the community reassess relevant prevailing research and be\ncautious whether the continued pursuit of state-of-the-art would yield\nconvincing and universal conclusions. Code and models are available at\n\\url{https:\/\/github.com\/NVlabs\/BEV-Planner}","terms":["cs.CV"]},{"titles":"Contour-based Interactive Segmentation","summaries":"Recent advances in interactive segmentation (IS) allow speeding up and\nsimplifying image editing and labeling greatly. The majority of modern IS\napproaches accept user input in the form of clicks. However, using clicks may\nrequire too many user interactions, especially when selecting small objects,\nminor parts of an object, or a group of objects of the same type. In this\npaper, we consider such a natural form of user interaction as a loose contour,\nand introduce a contour-based IS method. We evaluate the proposed method on the\nstandard segmentation benchmarks, our novel UserContours dataset, and its\nsubset UserContours-G containing difficult segmentation cases. Through\nexperiments, we demonstrate that a single contour provides the same accuracy as\nmultiple clicks, thus reducing the required amount of user interactions.","terms":["cs.CV","I.4.6"]},{"titles":"Amortized Bayesian Decision Making for simulation-based models","summaries":"Simulation-based inference (SBI) provides a powerful framework for inferring\nposterior distributions of stochastic simulators in a wide range of domains. In\nmany settings, however, the posterior distribution is not the end goal itself\n-- rather, the derived parameter values and their uncertainties are used as a\nbasis for deciding what actions to take. Unfortunately, because posterior\ndistributions provided by SBI are (potentially crude) approximations of the\ntrue posterior, the resulting decisions can be suboptimal. Here, we address the\nquestion of how to perform Bayesian decision making on stochastic simulators,\nand how one can circumvent the need to compute an explicit approximation to the\nposterior. Our method trains a neural network on simulated data and can predict\nthe expected cost given any data and action, and can, thus, be directly used to\ninfer the action with lowest cost. We apply our method to several benchmark\nproblems and demonstrate that it induces similar cost as the true posterior\ndistribution. We then apply the method to infer optimal actions in a real-world\nsimulator in the medical neurosciences, the Bayesian Virtual Epileptic Patient,\nand demonstrate that it allows to infer actions associated with low cost after\nfew simulations.","terms":["cs.LG","cs.AI","stat.ML"]},{"titles":"Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection? An Investigation and the HOI-Synth Domain Adaptation Benchmark","summaries":"In this study, we investigate the effectiveness of synthetic data in\nenhancing hand-object interaction detection within the egocentric vision\ndomain. We introduce a simulator able to generate synthetic images of\nhand-object interactions automatically labeled with hand-object contact states,\nbounding boxes, and pixel-wise segmentation masks. Through comprehensive\nexperiments and comparative analyses on three egocentric datasets, VISOR,\nEgoHOS, and ENIGMA-51, we demonstrate that the use of synthetic data and domain\nadaptation techniques allows for comparable performance to conventional\nsupervised methods while requiring annotations on only a fraction of the real\ndata. When tested with in-domain synthetic data generated from 3D models of\nreal target environments and objects, our best models show consistent\nperformance improvements with respect to standard fully supervised approaches\nbased on labeled real data only. Our study also sets a new benchmark of domain\nadaptation for egocentric hand-object interaction detection (HOI-Synth) and\nprovides baseline results to encourage the community to engage in this\nchallenging task. We release the generated data, code, and the simulator at the\nfollowing link: https:\/\/iplab.dmi.unict.it\/HOI-Synth\/.","terms":["cs.CV"]},{"titles":"Learning a Sparse Representation of Barron Functions with the Inverse Scale Space Flow","summaries":"This paper presents a method for finding a sparse representation of Barron\nfunctions. Specifically, given an $L^2$ function $f$, the inverse scale space\nflow is used to find a sparse measure $\\mu$ minimising the $L^2$ loss between\nthe Barron function associated to the measure $\\mu$ and the function $f$. The\nconvergence properties of this method are analysed in an ideal setting and in\nthe cases of measurement noise and sampling bias. In an ideal setting the\nobjective decreases strictly monotone in time to a minimizer with\n$\\mathcal{O}(1\/t)$, and in the case of measurement noise or sampling bias the\noptimum is achieved up to a multiplicative or additive constant. This\nconvergence is preserved on discretization of the parameter space, and the\nminimizers on increasingly fine discretizations converge to the optimum on the\nfull parameter space.","terms":["stat.ML","cs.LG","cs.NA","math.FA","math.NA","47A52, 68T07, 65K10, 90C25","I.2.6; F.2.1; G.1.6"]},{"titles":"Generating Visually Realistic Adversarial Patch","summaries":"Deep neural networks (DNNs) are vulnerable to various types of adversarial\nexamples, bringing huge threats to security-critical applications. Among these,\nadversarial patches have drawn increasing attention due to their good\napplicability to fool DNNs in the physical world. However, existing works often\ngenerate patches with meaningless noise or patterns, making it conspicuous to\nhumans. To address this issue, we explore how to generate visually realistic\nadversarial patches to fool DNNs. Firstly, we analyze that a high-quality\nadversarial patch should be realistic, position irrelevant, and printable to be\ndeployed in the physical world. Based on this analysis, we propose an effective\nattack called VRAP, to generate visually realistic adversarial patches.\nSpecifically, VRAP constrains the patch in the neighborhood of a real image to\nensure the visual reality, optimizes the patch at the poorest position for\nposition irrelevance, and adopts Total Variance loss as well as gamma\ntransformation to make the generated patch printable without losing\ninformation. Empirical evaluations on the ImageNet dataset demonstrate that the\nproposed VRAP exhibits outstanding attack performance in the digital world.\nMoreover, the generated adversarial patches can be disguised as the scrawl or\nlogo in the physical world to fool the deep models without being detected,\nbringing significant threats to DNNs-enabled applications.","terms":["cs.CV"]},{"titles":"Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians","summaries":"Creating high-fidelity 3D head avatars has always been a research hotspot,\nbut there remains a great challenge under lightweight sparse view setups. In\nthis paper, we propose Gaussian Head Avatar represented by controllable 3D\nGaussians for high-fidelity head avatar modeling. We optimize the neutral 3D\nGaussians and a fully learned MLP-based deformation field to capture complex\nexpressions. The two parts benefit each other, thereby our method can model\nfine-grained dynamic details while ensuring expression accuracy. Furthermore,\nwe devise a well-designed geometry-guided initialization strategy based on\nimplicit SDF and Deep Marching Tetrahedra for the stability and convergence of\nthe training procedure. Experiments show our approach outperforms other\nstate-of-the-art sparse-view methods, achieving ultra high-fidelity rendering\nquality at 2K resolution even under exaggerated expressions.","terms":["cs.CV","cs.GR"]},{"titles":"A Self-Commissioning Edge Computing Method for Data-Driven Anomaly Detection in Power Electronic Systems","summaries":"Ensuring the reliability of power electronic converters is a matter of great\nimportance, and data-driven condition monitoring techniques are cementing\nthemselves as an important tool for this purpose. However, translating methods\nthat work well in controlled lab environments to field applications presents\nsignificant challenges, notably because of the limited diversity and accuracy\nof the lab training data. By enabling the use of field data, online machine\nlearning can be a powerful tool to overcome this problem, but it introduces\nadditional challenges in ensuring the stability and predictability of the\ntraining processes. This work presents an edge computing method that mitigates\nthese shortcomings with minimal additional memory usage, by employing an\nautonomous algorithm that prioritizes the storage of training samples with\nlarger prediction errors. The method is demonstrated on the use case of a\nself-commissioning condition monitoring system, in the form of a thermal\nanomaly detection scheme for a variable frequency motor drive, where the\nalgorithm self-learned to distinguish normal and anomalous operation with\nminimal prior knowledge. The obtained results, based on experimental data, show\na significant improvement in prediction accuracy and training speed, when\ncompared to equivalent models trained online without the proposed data\nselection process.","terms":["cs.LG","eess.SP"]},{"titles":"Do AI models produce better weather forecasts than physics-based models? A quantitative evaluation case study of Storm Ciar\u00e1n","summaries":"There has been huge recent interest in the potential of making operational\nweather forecasts using machine learning techniques. As they become a part of\nthe weather forecasting toolbox, there is a pressing need to understand how\nwell current machine learning models can simulate high-impactweather events. We\ncompare forecasts of Storm Ciar\\'an, a European windstorm that caused sixteen\ndeaths and extensive damage in Northern Europe, made by machine learning and\nnumericalweather prediction models. The four machine learning models considered\n(FourCastNet, Pangu-Weather, GraphCast and FourCastNet-v2) produce forecasts\nthat accurately capture the synoptic-scale structure of the cyclone including\nthe position of the cloud head, shape of the warm sector and location of warm\nconveyor belt jet, and the large-scale dynamical drivers important for the\nrapid storm development such as the position of the storm relative to the\nupper-level jet exit. However, their ability to resolve the more detailed\nstructures important for issuing weather warnings is more mixed. All of the\nmachine learning models underestimate the peak amplitude of winds associated\nwith the storm, only some machine learning models resolve the warm core\nseclusion and none of the machine learning models capture the sharp bent-back\nwarm frontal gradient. Our study shows there is a great deal about the\nperformance and properties of machine learning weather forecasts that can be\nderived from case studies of high-impact weather events such as Storm Ciar\\'an.","terms":["cs.LG","physics.ao-ph"]},{"titles":"Value Functions are Control Barrier Functions: Verification of Safe Policies using Control Theory","summaries":"Guaranteeing safe behaviour of reinforcement learning (RL) policies poses\nsignificant challenges for safety-critical applications, despite RL's\ngenerality and scalability. To address this, we propose a new approach to apply\nverification methods from control theory to learned value functions. By\nanalyzing task structures for safety preservation, we formalize original\ntheorems that establish links between value functions and control barrier\nfunctions. Further, we propose novel metrics for verifying value functions in\nsafe control tasks and practical implementation details to improve learning.\nOur work presents a novel method for certificate learning, which unlocks a\ndiversity of verification techniques from control theory for RL policies, and\nmarks a significant step towards a formal framework for the general, scalable,\nand verifiable design of RL-based control systems. Code and videos are\navailable at this https url: https:\/\/rl-cbf.github.io\/","terms":["cs.LG","cs.AI","cs.RO"]},{"titles":"TPA3D: Triplane Attention for Fast Text-to-3D Generation","summaries":"Due to the lack of large-scale text-3D correspondence data, recent text-to-3D\ngeneration works mainly rely on utilizing 2D diffusion models for synthesizing\n3D data. Since diffusion-based methods typically require significant\noptimization time for both training and inference, the use of GAN-based models\nwould still be desirable for fast 3D generation. In this work, we propose\nTriplane Attention for text-guided 3D generation (TPA3D), an end-to-end\ntrainable GAN-based deep learning model for fast text-to-3D generation. With\nonly 3D shape data and their rendered 2D images observed during training, our\nTPA3D is designed to retrieve detailed visual descriptions for synthesizing the\ncorresponding 3D mesh data. This is achieved by the proposed attention\nmechanisms on the extracted sentence and word-level text features. In our\nexperiments, we show that TPA3D generates high-quality 3D textured shapes\naligned with fine-grained descriptions, while impressive computation efficiency\ncan be observed.","terms":["cs.CV"]},{"titles":"SAMSGL: Series-Aligned Multi-Scale Graph Learning for Spatio-Temporal Forecasting","summaries":"Spatio-temporal forecasting in various domains, like traffic prediction and\nweather forecasting, is a challenging endeavor, primarily due to the\ndifficulties in modeling propagation dynamics and capturing high-dimensional\ninteractions among nodes. Despite the significant strides made by graph-based\nnetworks in spatio-temporal forecasting, there remain two pivotal factors\nclosely related to forecasting performance that need further consideration:\ntime delays in propagation dynamics and multi-scale high-dimensional\ninteractions. In this work, we present a Series-Aligned Multi-Scale Graph\nLearning (SAMSGL) framework, aiming to enhance forecasting performance. In\norder to handle time delays in spatial interactions, we propose a\nseries-aligned graph convolution layer to facilitate the aggregation of\nnon-delayed graph signals, thereby mitigating the influence of time delays for\nthe improvement in accuracy. To understand global and local spatio-temporal\ninteractions, we develop a spatio-temporal architecture via multi-scale graph\nlearning, which encompasses two essential components: multi-scale graph\nstructure learning and graph-fully connected (Graph-FC) blocks. The multi-scale\ngraph structure learning includes a global graph structure to learn both\ndelayed and non-delayed node embeddings, as well as a local one to learn node\nvariations influenced by neighboring factors. The Graph-FC blocks\nsynergistically fuse spatial and temporal information to boost prediction\naccuracy. To evaluate the performance of SAMSGL, we conduct experiments on\nmeteorological and traffic forecasting datasets, which demonstrate its\neffectiveness and superiority.","terms":["cs.LG","cs.AI"]},{"titles":"Rethinking Radiology Report Generation via Causal Reasoning and Counterfactual Augmentation","summaries":"Radiology Report Generation (RRG) draws attention as an interaction between\nvision and language fields. Previous works inherited the ideology of\nvision-to-language generation tasks,aiming to generate paragraphs with high\nconsistency as reports. However, one unique characteristic of RRG, the\nindependence between diseases, was neglected, leading to the injection of\ndisease co-occurrence as a confounder that effects the results through backdoor\npath. Unfortunately, this confounder confuses the process of report generation\nworse because of the biased RRG data distribution. In this paper, to rethink\nthis issue thoroughly, we reason about its causes and effects from a novel\nperspective of statistics and causality, where the Joint Vision Coupling and\nthe Conditional Sentence Coherence Coupling are two aspects prone to implicitly\ndecrease the accuracy of reports. Then, a counterfactual augmentation strategy\nthat contains the Counterfactual Sample Synthesis and the Counterfactual Report\nReconstruction sub-methods is proposed to break these two aspects of spurious\neffects. Experimental results and further analyses on two widely used datasets\njustify our reasoning and proposed methods.","terms":["cs.CV","cs.CL","cs.MM"]},{"titles":"Synchronization is All You Need: Exocentric-to-Egocentric Transfer for Temporal Action Segmentation with Unlabeled Synchronized Video Pairs","summaries":"We consider the problem of transferring a temporal action segmentation system\ninitially designed for exocentric (fixed) cameras to an egocentric scenario,\nwhere wearable cameras capture video data. The conventional supervised approach\nrequires the collection and labeling of a new set of egocentric videos to adapt\nthe model, which is costly and time-consuming. Instead, we propose a novel\nmethodology which performs the adaptation leveraging existing labeled\nexocentric videos and a new set of unlabeled, synchronized\nexocentric-egocentric video pairs, for which temporal action segmentation\nannotations do not need to be collected. We implement the proposed methodology\nwith an approach based on knowledge distillation, which we investigate both at\nthe feature and model level. To evaluate our approach, we introduce a new\nbenchmark based on the Assembly101 dataset. Results demonstrate the feasibility\nand effectiveness of the proposed method against classic unsupervised domain\nadaptation and temporal sequence alignment approaches. Remarkably, without\nbells and whistles, our best model performs on par with supervised approaches\ntrained on labeled egocentric data, without ever seeing a single egocentric\nlabel, achieving a +15.99% (28.59% vs 12.60%) improvement in the edit score on\nthe Assembly101 dataset compared to a baseline model trained solely on\nexocentric data.","terms":["cs.CV"]},{"titles":"General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History","summaries":"Developing clinical prediction models (e.g., mortality prediction) based on\nelectronic health records (EHRs) typically relies on expert opinion for feature\nselection and adjusting observation window size. This burdens experts and\ncreates a bottleneck in the development process. We propose Retrieval-Enhanced\nMedical prediction model (REMed) to address such challenges. REMed can\nessentially evaluate an unlimited number of clinical events, select the\nrelevant ones, and make predictions. This approach effectively eliminates the\nneed for manual feature selection and enables an unrestricted observation\nwindow. We verified these properties through experiments on 27 clinical tasks\nand two independent cohorts from publicly available EHR datasets, where REMed\noutperformed other contemporary architectures that aim to handle as many events\nas possible. Notably, we found that the preferences of REMed align closely with\nthose of medical experts. We expect our approach to significantly expedite the\ndevelopment of EHR prediction models by minimizing clinicians' need for manual\ninvolvement.","terms":["cs.LG","cs.CL"]},{"titles":"Stable Diffusion Exposed: Gender Bias from Prompt to Image","summaries":"Recent studies have highlighted biases in generative models, shedding light\non their predisposition towards gender-based stereotypes and imbalances. This\npaper contributes to this growing body of research by introducing an evaluation\nprotocol designed to automatically analyze the impact of gender indicators on\nStable Diffusion images. Leveraging insights from prior work, we explore how\ngender indicators not only affect gender presentation but also the\nrepresentation of objects and layouts within the generated images. Our findings\ninclude the existence of differences in the depiction of objects, such as\ninstruments tailored for specific genders, and shifts in overall layouts. We\nalso reveal that neutral prompts tend to produce images more aligned with\nmasculine prompts than their feminine counterparts, providing valuable insights\ninto the nuanced gender biases inherent in Stable Diffusion.","terms":["cs.CV"]},{"titles":"Spintronics for image recognition: performance benchmarking via ultrafast data-driven simulations","summaries":"We present a demonstration of image classification using an echo-state\nnetwork (ESN) relying on a single simulated spintronic nanostructure known as\nthe vortex-based spin-torque oscillator (STVO) delayed in time. We employ an\nultrafast data-driven simulation framework called the data-driven Thiele\nequation approach (DD-TEA) to simulate the STVO dynamics. This allows us to\navoid the challenges associated with repeated experimental manipulation of such\na nanostructured system. We showcase the versatility of our solution by\nsuccessfully applying it to solve classification challenges with the MNIST,\nEMNIST-letters and Fashion MNIST datasets. Through our simulations, we\ndetermine that within a large ESN the results obtained using the STVO dynamics\nas an activation function are comparable to the ones obtained with other\nconventional nonlinear activation functions like the reLU and the sigmoid.\nWhile achieving state-of-the-art accuracy levels on the MNIST dataset, our\nmodel's performance on EMNIST-letters and Fashion MNIST is lower due to the\nrelative simplicity of the system architecture and the increased complexity of\nthe tasks. We expect that the DD-TEA framework will enable the exploration of\ndeeper architectures, ultimately leading to improved classification accuracy.","terms":["cs.CV"]},{"titles":"Learning Content-enhanced Mask Transformer for Domain Generalized Urban-Scene Segmentation","summaries":"Domain-generalized urban-scene semantic segmentation (USSS) aims to learn\ngeneralized semantic predictions across diverse urban-scene styles. Unlike\ndomain gap challenges, USSS is unique in that the semantic categories are often\nsimilar in different urban scenes, while the styles can vary significantly due\nto changes in urban landscapes, weather conditions, lighting, and other\nfactors. Existing approaches typically rely on convolutional neural networks\n(CNNs) to learn the content of urban scenes.\n  In this paper, we propose a Content-enhanced Mask TransFormer (CMFormer) for\ndomain-generalized USSS. The main idea is to enhance the focus of the\nfundamental component, the mask attention mechanism, in Transformer\nsegmentation models on content information. To achieve this, we introduce a\nnovel content-enhanced mask attention mechanism. It learns mask queries from\nboth the image feature and its down-sampled counterpart, as lower-resolution\nimage features usually contain more robust content information and are less\nsensitive to style variations. These features are fused into a Transformer\ndecoder and integrated into a multi-resolution content-enhanced mask attention\nlearning scheme.\n  Extensive experiments conducted on various domain-generalized urban-scene\nsegmentation datasets demonstrate that the proposed CMFormer significantly\noutperforms existing CNN-based methods for domain-generalized semantic\nsegmentation, achieving improvements of up to 14.00\\% in terms of mIoU (mean\nintersection over union). The source code is publicly available at\n\\url{https:\/\/github.com\/BiQiWHU\/CMFormer}.","terms":["cs.CV"]},{"titles":"LLaFS: When Large-Language Models Meet Few-Shot Segmentation","summaries":"This paper proposes LLaFS, the first attempt to leverage large language\nmodels (LLMs) in few-shot segmentation. In contrast to the conventional\nfew-shot segmentation methods that only rely on the limited and biased\ninformation from the annotated support images, LLaFS leverages the vast prior\nknowledge gained by LLM as an effective supplement and directly uses the LLM to\nsegment images in a few-shot manner. To enable the text-based LLM to handle\nimage-related tasks, we carefully design an input instruction that allows the\nLLM to produce segmentation results represented as polygons, and propose a\nregion-attribute table to simulate the human visual mechanism and provide\nmulti-modal guidance. We also synthesize pseudo samples and use curriculum\nlearning for pretraining to augment data and achieve better optimization. LLaFS\nachieves state-of-the-art results on multiple datasets, showing the potential\nof using LLMs for few-shot computer vision tasks. Code will be available at\nhttps:\/\/github.com\/lanyunzhu99\/LLaFS.","terms":["cs.CV"]},{"titles":"Diffusion Noise Feature: Accurate and Fast Generated Image Detection","summaries":"Generative models have reached an advanced stage where they can produce\nremarkably realistic images. However, this remarkable generative capability\nalso introduces the risk of disseminating false or misleading information.\nNotably, existing image detectors for generated images encounter challenges\nsuch as low accuracy and limited generalization. This paper seeks to address\nthis issue by seeking a representation with strong generalization capabilities\nto enhance the detection of generated images. Our investigation has revealed\nthat real and generated images display distinct latent Gaussian representations\nwhen subjected to an inverse diffusion process within a pre-trained diffusion\nmodel. Exploiting this disparity, we can amplify subtle artifacts in generated\nimages. Building upon this insight, we introduce a novel image representation\nknown as Diffusion Noise Feature (DNF). DNF is an ensemble representation that\nestimates the noise generated during the inverse diffusion process. A simple\nclassifier, e.g., ResNet, trained on DNF achieves high accuracy, robustness,\nand generalization capabilities for detecting generated images, even from\npreviously unseen classes or models. We conducted experiments using a widely\nrecognized and standard dataset, achieving state-of-the-art effects of\nDetection.","terms":["cs.CV"]},{"titles":"On the Initialization of Graph Neural Networks","summaries":"Graph Neural Networks (GNNs) have displayed considerable promise in graph\nrepresentation learning across various applications. The core learning process\nrequires the initialization of model weight matrices within each GNN layer,\nwhich is typically accomplished via classic initialization methods such as\nXavier initialization. However, these methods were originally motivated to\nstabilize the variance of hidden embeddings and gradients across layers of\nFeedforward Neural Networks (FNNs) and Convolutional Neural Networks (CNNs) to\navoid vanishing gradients and maintain steady information flow. In contrast,\nwithin the GNN context classical initializations disregard the impact of the\ninput graph structure and message passing on variance. In this paper, we\nanalyze the variance of forward and backward propagation across GNN layers and\nshow that the variance instability of GNN initializations comes from the\ncombined effect of the activation function, hidden dimension, graph structure\nand message passing. To better account for these influence factors, we propose\na new initialization method for Variance Instability Reduction within GNN\nOptimization (Virgo), which naturally tends to equate forward and backward\nvariances across successive layers. We conduct comprehensive experiments on 15\ndatasets to show that Virgo can lead to superior model performance and more\nstable variance at initialization on node classification, link prediction and\ngraph classification tasks. Codes are in\nhttps:\/\/github.com\/LspongebobJH\/virgo_icml2023.","terms":["cs.LG","cs.AI"]},{"titles":"NeuRAD: Neural Rendering for Autonomous Driving","summaries":"Neural radiance fields (NeRFs) have gained popularity in the autonomous\ndriving (AD) community. Recent methods show NeRFs' potential for closed-loop\nsimulation, enabling testing of AD systems, and as an advanced training data\naugmentation technique. However, existing methods often require long training\ntimes, dense semantic supervision, or lack generalizability. This, in turn,\nhinders the application of NeRFs for AD at scale. In this paper, we propose\nNeuRAD, a robust novel view synthesis method tailored to dynamic AD data. Our\nmethod features simple network design, extensive sensor modeling for both\ncamera and lidar -- including rolling shutter, beam divergence and ray dropping\n-- and is applicable to multiple datasets out of the box. We verify its\nperformance on five popular AD datasets, achieving state-of-the-art performance\nacross the board. To encourage further development, we will openly release the\nNeuRAD source code. See https:\/\/github.com\/georghess\/NeuRAD .","terms":["cs.CV"]},{"titles":"The Bayesian Stability Zoo","summaries":"We show that many definitions of stability found in the learning theory\nliterature are equivalent to one another. We distinguish between two families\nof definitions of stability: distribution-dependent and\ndistribution-independent Bayesian stability. Within each family, we establish\nequivalences between various definitions, encompassing approximate differential\nprivacy, pure differential privacy, replicability, global stability, perfect\ngeneralization, TV stability, mutual information stability, KL-divergence\nstability, and R\\'enyi-divergence stability. Along the way, we prove boosting\nresults that enable the amplification of the stability of a learning rule. This\nwork is a step towards a more systematic taxonomy of stability notions in\nlearning theory, which can promote clarity and an improved understanding of an\narray of stability concepts that have emerged in recent years.","terms":["cs.LG"]},{"titles":"Rethinking and Simplifying Bootstrapped Graph Latents","summaries":"Graph contrastive learning (GCL) has emerged as a representative paradigm in\ngraph self-supervised learning, where negative samples are commonly regarded as\nthe key to preventing model collapse and producing distinguishable\nrepresentations. Recent studies have shown that GCL without negative samples\ncan achieve state-of-the-art performance as well as scalability improvement,\nwith bootstrapped graph latent (BGRL) as a prominent step forward. However,\nBGRL relies on a complex architecture to maintain the ability to scatter\nrepresentations, and the underlying mechanisms enabling the success remain\nlargely unexplored. In this paper, we introduce an instance-level decorrelation\nperspective to tackle the aforementioned issue and leverage it as a springboard\nto reveal the potential unnecessary model complexity within BGRL. Based on our\nfindings, we present SGCL, a simple yet effective GCL framework that utilizes\nthe outputs from two consecutive iterations as positive pairs, eliminating the\nnegative samples. SGCL only requires a single graph augmentation and a single\ngraph encoder without additional parameters. Extensive experiments conducted on\nvarious graph benchmarks demonstrate that SGCL can achieve competitive\nperformance with fewer parameters, lower time and space costs, and significant\nconvergence speedup.","terms":["cs.LG"]},{"titles":"DreaMo: Articulated 3D Reconstruction From A Single Casual Video","summaries":"Articulated 3D reconstruction has valuable applications in various domains,\nyet it remains costly and demands intensive work from domain experts. Recent\nadvancements in template-free learning methods show promising results with\nmonocular videos. Nevertheless, these approaches necessitate a comprehensive\ncoverage of all viewpoints of the subject in the input video, thus limiting\ntheir applicability to casually captured videos from online sources. In this\nwork, we study articulated 3D shape reconstruction from a single and casually\ncaptured internet video, where the subject's view coverage is incomplete. We\npropose DreaMo that jointly performs shape reconstruction while solving the\nchallenging low-coverage regions with view-conditioned diffusion prior and\nseveral tailored regularizations. In addition, we introduce a skeleton\ngeneration strategy to create human-interpretable skeletons from the learned\nneural bones and skinning weights. We conduct our study on a self-collected\ninternet video collection characterized by incomplete view coverage. DreaMo\nshows promising quality in novel-view rendering, detailed articulated shape\nreconstruction, and skeleton generation. Extensive qualitative and quantitative\nstudies validate the efficacy of each proposed component, and show existing\nmethods are unable to solve correct geometry due to the incomplete view\ncoverage.","terms":["cs.CV","cs.GR"]},{"titles":"Facilitating the Production of Well-tailored Video Summaries for Sharing on Social Media","summaries":"This paper presents a web-based tool that facilitates the production of\ntailored summaries for online sharing on social media. Through an interactive\nuser interface, it supports a ``one-click'' video summarization process. Based\non the integrated AI models for video summarization and aspect ratio\ntransformation, it facilitates the generation of multiple summaries of a\nfull-length video according to the needs of target platforms with regard to the\nvideo's length and aspect ratio.","terms":["cs.CV"]},{"titles":"Projection Regret: Reducing Background Bias for Novelty Detection via Diffusion Models","summaries":"Novelty detection is a fundamental task of machine learning which aims to\ndetect abnormal ($\\textit{i.e.}$ out-of-distribution (OOD)) samples. Since\ndiffusion models have recently emerged as the de facto standard generative\nframework with surprising generation results, novelty detection via diffusion\nmodels has also gained much attention. Recent methods have mainly utilized the\nreconstruction property of in-distribution samples. However, they often suffer\nfrom detecting OOD samples that share similar background information to the\nin-distribution data. Based on our observation that diffusion models can\n\\emph{project} any sample to an in-distribution sample with similar background\ninformation, we propose \\emph{Projection Regret (PR)}, an efficient novelty\ndetection method that mitigates the bias of non-semantic information. To be\nspecific, PR computes the perceptual distance between the test image and its\ndiffusion-based projection to detect abnormality. Since the perceptual distance\noften fails to capture semantic changes when the background information is\ndominant, we cancel out the background bias by comparing it against recursive\nprojections. Extensive experiments demonstrate that PR outperforms the prior\nart of generative-model-based novelty detection methods by a significant\nmargin.","terms":["cs.LG","cs.CV"]},{"titles":"Prompt Optimization via Adversarial In-Context Learning","summaries":"We propose a new method, Adversarial In-Context Learning (adv-ICL), to\noptimize prompt for in-context learning (ICL) by employing one LLM as a\ngenerator, another as a discriminator, and a third as a prompt modifier. As in\ntraditional adversarial learning, adv-ICL is implemented as a two-player game\nbetween the generator and discriminator, where the generator tries to generate\nrealistic enough output to fool the discriminator. In each round, given an\ninput prefixed by task instructions and several exemplars, the generator\nproduces an output. The discriminator is then tasked with classifying the\ngenerator input-output pair as model-generated or real data. Based on the\ndiscriminator loss, the prompt modifier proposes possible edits to the\ngenerator and discriminator prompts, and the edits that most improve the\nadversarial loss are selected. We show that adv-ICL results in significant\nimprovements over state-of-the-art prompt optimization techniques for both open\nand closed-source models on 11 generation and classification tasks including\nsummarization, arithmetic reasoning, machine translation, data-to-text\ngeneration, and the MMLU and big-bench hard benchmarks. In addition, because\nour method uses pre-trained models and updates only prompts rather than model\nparameters, it is computationally efficient, easy to extend to any LLM and\ntask, and effective in low-resource settings.","terms":["cs.LG","cs.CL"]},{"titles":"A Unified Simulation Framework for Visual and Behavioral Fidelity in Crowd Analysis","summaries":"Simulation is a powerful tool to easily generate annotated data, and a highly\ndesirable feature, especially in those domains where learning models need large\ntraining datasets. Machine learning and deep learning solutions, have proven to\nbe extremely data-hungry and sometimes, the available real-world data are not\nsufficient to effectively model the given task. Despite the initial skepticism\nof a portion of the scientific community, the potential of simulation has been\nlargely confirmed in many application areas, and the recent developments in\nterms of rendering and virtualization engines, have shown a good ability also\nin representing complex scenes. This includes environmental factors, such as\nweather conditions and surface reflectance, as well as human-related events,\nlike human actions and behaviors. We present a human crowd simulator, called\nUniCrowd, and its associated validation pipeline. We show how the simulator can\ngenerate annotated data, suitable for computer vision tasks, in particular for\ndetection and segmentation, as well as the related applications, as crowd\ncounting, human pose estimation, trajectory analysis and prediction, and\nanomaly detection.","terms":["cs.CV","cs.MA"]},{"titles":"Privacy-Aware Data Acquisition under Data Similarity in Regression Markets","summaries":"Data markets facilitate decentralized data exchange for applications such as\nprediction, learning, or inference. The design of these markets is challenged\nby varying privacy preferences as well as data similarity among data owners.\nRelated works have often overlooked how data similarity impacts pricing and\ndata value through statistical information leakage. We demonstrate that data\nsimilarity and privacy preferences are integral to market design and propose a\nquery-response protocol using local differential privacy for a two-party data\nacquisition mechanism. In our regression data market model, we analyze\nstrategic interactions between privacy-aware owners and the learner as a\nStackelberg game over the asked price and privacy factor. Finally, we\nnumerically evaluate how data similarity affects market participation and\ntraded data value.","terms":["cs.LG","cs.CR","cs.GT"]},{"titles":"Grounding Foundation Models through Federated Transfer Learning: A General Framework","summaries":"Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and\npowerful emergent abilities have achieved remarkable success in various natural\nlanguage processing and computer vision tasks. Grounding FMs by adapting them\nto domain-specific tasks or augmenting them with domain-specific knowledge\nenables us to exploit the full potential of FMs. However, grounding FMs faces\nseveral challenges, stemming primarily from constrained computing resources,\ndata privacy, model heterogeneity, and model ownership. Federated Transfer\nLearning (FTL), the combination of federated learning and transfer learning,\nprovides promising solutions to address these challenges. In recent years, the\nneed for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in\nboth academia and industry. Motivated by the strong growth in FTL-FM research\nand the potential impact of FTL-FM on industrial applications, we propose an\nFTL-FM framework that formulates problems of grounding FMs in the federated\nlearning setting, construct a detailed taxonomy based on the FTL-FM framework\nto categorize state-of-the-art FTL-FM works, and comprehensively overview\nFTL-FM works based on the proposed taxonomy. We also establish correspondences\nbetween FTL-FM and conventional phases of adapting FM so that FM practitioners\ncan align their research works with FTL-FM. In addition, we overview advanced\nefficiency-improving and privacy-preserving techniques because efficiency and\nprivacy are critical concerns in FTL-FM. Last, we discuss opportunities and\nfuture research directions of FTL-FM.","terms":["cs.LG","cs.AI"]},{"titles":"Panoptica -- instance-wise evaluation of 3D semantic and instance segmentation maps","summaries":"This paper introduces panoptica, a versatile and performance-optimized\npackage designed for computing instance-wise segmentation quality metrics from\n2D and 3D segmentation maps. panoptica addresses the limitations of existing\nmetrics and provides a modular framework that complements the original\nintersection over union-based panoptic quality with other metrics, such as the\ndistance metric Average Symmetric Surface Distance. The package is open-source,\nimplemented in Python, and accompanied by comprehensive documentation and\ntutorials. panoptica employs a three-step metrics computation process to cover\ndiverse use cases. The efficacy of panoptica is demonstrated on various\nreal-world biomedical datasets, where an instance-wise evaluation is\ninstrumental for an accurate representation of the underlying clinical task.\nOverall, we envision panoptica as a valuable tool facilitating in-depth\nevaluation of segmentation methods.","terms":["cs.CV","cs.AI","cs.LG","eess.IV"]},{"titles":"Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation","summaries":"Recently, a new line of works has emerged to understand and improve\nself-attention in Transformers by treating it as a kernel machine. However,\nexisting works apply the methods for symmetric kernels to the asymmetric\nself-attention, resulting in a nontrivial gap between the analytical\nunderstanding and numerical implementation. In this paper, we provide a new\nperspective to represent and optimize self-attention through asymmetric Kernel\nSingular Value Decomposition (KSVD), which is also motivated by the low-rank\nproperty of self-attention normally observed in deep layers. Through asymmetric\nKSVD, $i$) a primal-dual representation of self-attention is formulated, where\nthe optimization objective is cast to maximize the projection variances in the\nattention outputs; $ii$) a novel attention mechanism, i.e., Primal-Attention,\nis proposed via the primal representation of KSVD, avoiding explicit\ncomputation of the kernel matrix in the dual; $iii$) with KKT conditions, we\nprove that the stationary solution to the KSVD optimization in Primal-Attention\nyields a zero-value objective. In this manner, KSVD optimization can be\nimplemented by simply minimizing a regularization loss, so that low-rank\nproperty is promoted without extra decomposition. Numerical experiments show\nstate-of-the-art performance of our Primal-Attention with improved efficiency.\nMoreover, we demonstrate that the deployed KSVD optimization regularizes\nPrimal-Attention with a sharper singular value decay than that of the canonical\nself-attention, further verifying the great potential of our method. To the\nbest of our knowledge, this is the first work that provides a primal-dual\nrepresentation for the asymmetric kernel in self-attention and successfully\napplies it to modeling and optimization.","terms":["cs.LG","cs.AI","cs.CV"]},{"titles":"Efficient Deep Learning Models for Privacy-preserving People Counting on Low-resolution Infrared Arrays","summaries":"Ultra-low-resolution Infrared (IR) array sensors offer a low-cost,\nenergy-efficient, and privacy-preserving solution for people counting, with\napplications such as occupancy monitoring. Previous work has shown that Deep\nLearning (DL) can yield superior performance on this task. However, the\nliterature was missing an extensive comparative analysis of various efficient\nDL architectures for IR array-based people counting, that considers not only\ntheir accuracy, but also the cost of deploying them on memory- and\nenergy-constrained Internet of Things (IoT) edge nodes. In this work, we\naddress this need by comparing 6 different DL architectures on a novel dataset\ncomposed of IR images collected from a commercial 8x8 array, which we made\nopenly available. With a wide architectural exploration of each model type, we\nobtain a rich set of Pareto-optimal solutions, spanning cross-validated\nbalanced accuracy scores in the 55.70-82.70% range. When deployed on a\ncommercial Microcontroller (MCU) by STMicroelectronics, the STM32L4A6ZG, these\nmodels occupy 0.41-9.28kB of memory, and require 1.10-7.74ms per inference,\nwhile consuming 17.18-120.43 $\\mu$J of energy. Our models are significantly\nmore accurate than a previous deterministic method (up to +39.9%), while being\nup to 3.53x faster and more energy efficient. Further, our models' accuracy is\ncomparable to state-of-the-art DL solutions on similar resolution sensors,\ndespite a much lower complexity. All our models enable continuous, real-time\ninference on a MCU-based IoT node, with years of autonomous operation without\nbattery recharging.","terms":["cs.CV","cs.LG"]},{"titles":"TSVR+: Twin support vector regression with privileged information","summaries":"In the realm of machine learning, the data may contain additional attributes,\nknown as privileged information (PI). The main purpose of PI is to assist in\nthe training of the model and then utilize the acquired knowledge to make\npredictions for unseen samples. Support vector regression (SVR) is an effective\nregression model, however, it has a low learning speed due to solving a convex\nquadratic problem (QP) subject to a pair of constraints. In contrast, twin\nsupport vector regression (TSVR) is more efficient than SVR as it solves two\nQPs each subject to one set of constraints. However, TSVR and its variants are\ntrained only on regular features and do not use privileged features for\ntraining. To fill this gap, we introduce a fusion of TSVR with learning using\nprivileged information (LUPI) and propose a novel approach called twin support\nvector regression with privileged information (TSVR+). The regularization terms\nin the proposed TSVR+ capture the essence of statistical learning theory and\nimplement the structural risk minimization principle. We use the successive\noverrelaxation (SOR) technique to solve the optimization problem of the\nproposed TSVR+, which enhances the training efficiency. As far as our knowledge\nextends, the integration of the LUPI concept into twin variants of regression\nmodels is a novel advancement. The numerical experiments conducted on UCI,\nstock and time series data collectively demonstrate the superiority of the\nproposed model.","terms":["cs.LG"]},{"titles":"DINO-Mix: Enhancing Visual Place Recognition with Foundational Vision Model and Feature Mixing","summaries":"Utilizing visual place recognition (VPR) technology to ascertain the\ngeographical location of publicly available images is a pressing issue for\nreal-world VPR applications. Although most current VPR methods achieve\nfavorable results under ideal conditions, their performance in complex\nenvironments, characterized by lighting variations, seasonal changes, and\nocclusions caused by moving objects, is generally unsatisfactory. In this\nstudy, we utilize the DINOv2 model as the backbone network for trimming and\nfine-tuning to extract robust image features. We propose a novel VPR\narchitecture called DINO-Mix, which combines a foundational vision model with\nfeature aggregation. This architecture relies on the powerful image feature\nextraction capabilities of foundational vision models. We employ an\nMLP-Mixer-based mix module to aggregate image features, resulting in globally\nrobust and generalizable descriptors that enable high-precision VPR. We\nexperimentally demonstrate that the proposed DINO-Mix architecture\nsignificantly outperforms current state-of-the-art (SOTA) methods. In test sets\nhaving lighting variations, seasonal changes, and occlusions (Tokyo24\/7,\nNordland, SF-XL-Testv1), our proposed DINO-Mix architecture achieved Top-1\naccuracy rates of 91.75%, 80.18%, and 82%, respectively. Compared with SOTA\nmethods, our architecture exhibited an average accuracy improvement of 5.14%.","terms":["cs.CV"]},{"titles":"FRAPP\u00c9: A Post-Processing Framework for Group Fairness Regularization","summaries":"Post-processing mitigation techniques for group fairness generally adjust the\ndecision threshold of a base model in order to improve fairness. Methods in\nthis family exhibit several advantages that make them appealing in practice:\npost-processing requires no access to the model training pipeline, is agnostic\nto the base model architecture, and offers a reduced computation cost compared\nto in-processing. Despite these benefits, existing methods face other\nchallenges that limit their applicability: they require knowledge of the\nsensitive attributes at inference time and are oftentimes outperformed by\nin-processing. In this paper, we propose a general framework to transform any\nin-processing method with a penalized objective into a post-processing\nprocedure. The resulting method is specifically designed to overcome the\naforementioned shortcomings of prior post-processing approaches. Furthermore,\nwe show theoretically and through extensive experiments on real-world data that\nthe resulting post-processing method matches or even surpasses the\nfairness-error trade-off offered by the in-processing counterpart.","terms":["cs.LG","cs.CY"]},{"titles":"Out-of-distribution Detection Learning with Unreliable Out-of-distribution Sources","summaries":"Out-of-distribution (OOD) detection discerns OOD data where the predictor\ncannot make valid predictions as in-distribution (ID) data, thereby increasing\nthe reliability of open-world classification. However, it is typically hard to\ncollect real out-of-distribution (OOD) data for training a predictor capable of\ndiscerning ID and OOD patterns. This obstacle gives rise to data\ngeneration-based learning methods, synthesizing OOD data via data generators\nfor predictor training without requiring any real OOD data. Related methods\ntypically pre-train a generator on ID data and adopt various selection\nprocedures to find those data likely to be the OOD cases. However, generated\ndata may still coincide with ID semantics, i.e., mistaken OOD generation\nremains, confusing the predictor between ID and OOD data. To this end, we\nsuggest that generated data (with mistaken OOD generation) can be used to\ndevise an auxiliary OOD detection task to facilitate real OOD detection.\nSpecifically, we can ensure that learning from such an auxiliary task is\nbeneficial if the ID and the OOD parts have disjoint supports, with the help of\na well-designed training procedure for the predictor. Accordingly, we propose a\npowerful data generation-based learning method named Auxiliary Task-based OOD\nLearning (ATOL) that can relieve the mistaken OOD generation. We conduct\nextensive experiments under various OOD detection setups, demonstrating the\neffectiveness of our method against its advanced counterparts.","terms":["cs.LG","cs.MM"]},{"titles":"TokenCut: Segmenting Objects in Images and Videos with Self-supervised Transformer and Normalized Cut","summaries":"In this paper, we describe a graph-based algorithm that uses the features\nobtained by a self-supervised transformer to detect and segment salient objects\nin images and videos. With this approach, the image patches that compose an\nimage or video are organised into a fully connected graph, where the edge\nbetween each pair of patches is labeled with a similarity score between patches\nusing features learned by the transformer. Detection and segmentation of\nsalient objects is then formulated as a graph-cut problem and solved using the\nclassical Normalized Cut algorithm. Despite the simplicity of this approach, it\nachieves state-of-the-art results on several common image and video detection\nand segmentation tasks. For unsupervised object discovery, this approach\noutperforms the competing approaches by a margin of 6.1%, 5.7%, and 2.6%,\nrespectively, when tested with the VOC07, VOC12, and COCO20K datasets. For the\nunsupervised saliency detection task in images, this method improves the score\nfor Intersection over Union (IoU) by 4.4%, 5.6% and 5.2%. When tested with the\nECSSD, DUTS, and DUT-OMRON datasets, respectively, compared to current\nstate-of-the-art techniques. This method also achieves competitive results for\nunsupervised video object segmentation tasks with the DAVIS, SegTV2, and FBMS\ndatasets.","terms":["cs.CV","stat.ML"]},{"titles":"Discovering Interpretable Physical Models using Symbolic Regression and Discrete Exterior Calculus","summaries":"Computational modeling is a key resource to gather insight into physical\nsystems in modern scientific research and engineering. While access to large\namount of data has fueled the use of Machine Learning (ML) to recover physical\nmodels from experiments and increase the accuracy of physical simulations,\npurely data-driven models have limited generalization and interpretability. To\novercome these limitations, we propose a framework that combines Symbolic\nRegression (SR) and Discrete Exterior Calculus (DEC) for the automated\ndiscovery of physical models starting from experimental data. Since these\nmodels consist of mathematical expressions, they are interpretable and amenable\nto analysis, and the use of a natural, general-purpose discrete mathematical\nlanguage for physics favors generalization with limited input data.\nImportantly, DEC provides building blocks for the discrete analogue of field\ntheories, which are beyond the state-of-the-art applications of SR to physical\nproblems. Further, we show that DEC allows to implement a strongly-typed SR\nprocedure that guarantees the mathematical consistency of the recovered models\nand reduces the search space of symbolic expressions. Finally, we prove the\neffectiveness of our methodology by re-discovering three models of Continuum\nPhysics from synthetic experimental data: Poisson equation, the Euler's\nElastica and the equations of Linear Elasticity. Thanks to their\ngeneral-purpose nature, the methods developed in this paper may be applied to\ndiverse contexts of physical modeling.","terms":["cs.LG","cs.DM","cs.NE"]},{"titles":"T3D: Towards 3D Medical Image Understanding through Vision-Language Pre-training","summaries":"Expert annotation of 3D medical image for downstream analysis is\nresource-intensive, posing challenges in clinical applications. Visual\nself-supervised learning (vSSL), though effective for learning visual\ninvariance, neglects the incorporation of domain knowledge from medicine. To\nincorporate medical knowledge into visual representation learning,\nvision-language pre-training (VLP) has shown promising results in 2D image.\nHowever, existing VLP approaches become generally impractical when applied to\nhigh-resolution 3D medical images due to GPU hardware constraints and the\npotential loss of critical details caused by downsampling, which is the\nintuitive solution to hardware constraints. To address the above limitations,\nwe introduce T3D, the first VLP framework designed for high-resolution 3D\nmedical images. T3D incorporates two text-informed pretext tasks:\n(\\lowerromannumeral{1}) text-informed contrastive learning;\n(\\lowerromannumeral{2}) text-informed image restoration. These tasks focus on\nlearning 3D visual representations from high-resolution 3D medical images and\nintegrating clinical knowledge from radiology reports, without distorting\ninformation through forced alignment of downsampled volumes with detailed\nanatomical text. Trained on a newly curated large-scale dataset of 3D medical\nimages and radiology reports, T3D significantly outperforms current vSSL\nmethods in tasks like organ and tumor segmentation, as well as disease\nclassification. This underlines T3D's potential in representation learning for\n3D medical image analysis. All data and code will be available upon acceptance.","terms":["cs.CV","cs.CL","cs.LG","eess.IV"]},{"titles":"Hardware Resilience Properties of Text-Guided Image Classifiers","summaries":"This paper presents a novel method to enhance the reliability of image\nclassification models during deployment in the face of transient hardware\nerrors. By utilizing enriched text embeddings derived from GPT-3 with question\nprompts per class and CLIP pretrained text encoder, we investigate their impact\nas an initialization for the classification layer. Our approach achieves a\nremarkable $5.5\\times$ average increase in hardware reliability (and up to\n$14\\times$) across various architectures in the most critical layer, with\nminimal accuracy drop ($0.3\\%$ on average) compared to baseline PyTorch models.\nFurthermore, our method seamlessly integrates with any image classification\nbackbone, showcases results across various network architectures, decreases\nparameter and FLOPs overhead, and follows a consistent training recipe. This\nresearch offers a practical and efficient solution to bolster the robustness of\nimage classification models against hardware failures, with potential\nimplications for future studies in this domain. Our code and models are\nreleased at https:\/\/github.com\/TalalWasim\/TextGuidedResilience.","terms":["cs.CV"]},{"titles":"Cost-effective On-device Continual Learning over Memory Hierarchy with Miro","summaries":"Continual learning (CL) trains NN models incrementally from a continuous\nstream of tasks. To remember previously learned knowledge, prior studies store\nold samples over a memory hierarchy and replay them when new tasks arrive. Edge\ndevices that adopt CL to preserve data privacy are typically energy-sensitive\nand thus require high model accuracy while not compromising energy efficiency,\ni.e., cost-effectiveness. Our work is the first to explore the design space of\nhierarchical memory replay-based CL to gain insights into achieving\ncost-effectiveness on edge devices. We present Miro, a novel system runtime\nthat carefully integrates our insights into the CL framework by enabling it to\ndynamically configure the CL system based on resource states for the best\ncost-effectiveness. To reach this goal, Miro also performs online profiling on\nparameters with clear accuracy-energy trade-offs and adapts to optimal values\nwith low overhead. Extensive evaluations show that Miro significantly\noutperforms baseline systems we build for comparison, consistently achieving\nhigher cost-effectiveness.","terms":["cs.LG","cs.AI","cs.AR"]},{"titles":"An Integrated System for Spatio-Temporal Summarization of 360-degrees Videos","summaries":"In this work, we present an integrated system for spatiotemporal\nsummarization of 360-degrees videos. The video summary production mainly\ninvolves the detection of salient events and their synopsis into a concise\nsummary. The analysis relies on state-of-the-art methods for saliency detection\nin 360-degrees video (ATSal and SST-Sal) and video summarization (CA-SUM). It\nalso contains a mechanism that classifies a 360-degrees video based on the use\nof static or moving camera during recording and decides which saliency\ndetection method will be used, as well as a 2D video production component that\nis responsible to create a conventional 2D video containing the salient events\nin the 360-degrees video. Quantitative evaluations using two datasets for\n360-degrees video saliency detection (VR-EyeTracking, Sports-360) show the\naccuracy and positive impact of the developed decision mechanism, and justify\nour choice to use two different methods for detecting the salient events. A\nqualitative analysis using content from these datasets, gives further insights\nabout the functionality of the decision mechanism, shows the pros and cons of\neach used saliency detection method and demonstrates the advanced performance\nof the trained summarization method against a more conventional approach.","terms":["cs.CV"]},{"titles":"UTBoost: A Tree-boosting based System for Uplift Modeling","summaries":"Uplift modeling refers to the set of machine learning techniques that a\nmanager may use to estimate customer uplift, that is, the net effect of an\naction on some customer outcome. By identifying the subset of customers for\nwhom a treatment will have the greatest effect, uplift models assist\ndecision-makers in optimizing resource allocations and maximizing overall\nreturns. Accurately estimating customer uplift poses practical challenges, as\nit requires assessing the difference between two mutually exclusive outcomes\nfor each individual. In this paper, we propose two innovative adaptations of\nthe well-established Gradient Boosting Decision Trees (GBDT) algorithm, which\nlearn the causal effect in a sequential way and overcome the counter-factual\nnature. Both approaches innovate existing techniques in terms of ensemble\nlearning method and learning objectives, respectively. Experiments on\nlarge-scale datasets demonstrate the usefulness of the proposed methods, which\noften yielding remarkable improvements over base models. To facilitate the\napplication, we develop the UTBoost, an end-to-end tree boosting system\nspecifically designed for uplift modeling. The package is open source and has\nbeen optimized for training speed to meet the needs of real industrial\napplications.","terms":["cs.LG","cs.AI"]},{"titles":"Data Upcycling Knowledge Distillation for Image Super-Resolution","summaries":"Knowledge distillation (KD) emerges as a promising yet challenging technique\nfor compressing deep neural networks, aiming to transfer extensive learning\nrepresentations from proficient and computationally intensive teacher models to\ncompact student models. However, current KD methods for super-resolution (SR)\nmodels have limited performance and restricted applications, since the\ncharacteristics of SR tasks are overlooked. In this paper, we put forth an\napproach from the perspective of effective data utilization, namely, the Data\nUpcycling Knowledge Distillation (DUKD), which facilitates the student model by\nthe prior knowledge the teacher provided through the upcycled in-domain data\nderived from the input images. Besides, for the first time, we realize the\nlabel consistency regularization in KD for SR models, which is implemented by\nthe paired invertible data augmentations. It constrains the training process of\nKD and leads to better generalization capability of the student model. The\nDUKD, due to its versatility, can be applied across a broad spectrum of\nteacher-student architectures (e.g., CNN and Transformer models) and SR tasks,\nsuch as single image SR, real-world SR, and SR quantization, and is in parallel\nwith other compression techniques. Comprehensive experiments on diverse\nbenchmarks demonstrate that the DUKD method significantly outperforms previous\nart.","terms":["cs.CV","cs.AI"]},{"titles":"Prompt2NeRF-PIL: Fast NeRF Generation via Pretrained Implicit Latent","summaries":"This paper explores promptable NeRF generation (e.g., text prompt or single\nimage prompt) for direct conditioning and fast generation of NeRF parameters\nfor the underlying 3D scenes, thus undoing complex intermediate steps while\nproviding full 3D generation with conditional control. Unlike previous\ndiffusion-CLIP-based pipelines that involve tedious per-prompt optimizations,\nPrompt2NeRF-PIL is capable of generating a variety of 3D objects with a single\nforward pass, leveraging a pre-trained implicit latent space of NeRF\nparameters. Furthermore, in zero-shot tasks, our experiments demonstrate that\nthe NeRFs produced by our method serve as semantically informative\ninitializations, significantly accelerating the inference process of existing\nprompt-to-NeRF methods. Specifically, we will show that our approach speeds up\nthe text-to-NeRF model DreamFusion and the 3D reconstruction speed of the\nimage-to-NeRF method Zero-1-to-3 by 3 to 5 times.","terms":["cs.CV"]},{"titles":"Think Twice Before Selection: Federated Evidential Active Learning for Medical Image Analysis with Domain Shifts","summaries":"Federated learning facilitates the collaborative learning of a global model\nacross multiple distributed medical institutions without centralizing data.\nNevertheless, the expensive cost of annotation on local clients remains an\nobstacle to effectively utilizing local data. To mitigate this issue, federated\nactive learning methods suggest leveraging local and global model predictions\nto select a relatively small amount of informative local data for annotation.\nHowever, existing methods mainly focus on all local data sampled from the same\ndomain, making them unreliable in realistic medical scenarios with domain\nshifts among different clients. In this paper, we make the first attempt to\nassess the informativeness of local data derived from diverse domains and\npropose a novel methodology termed Federated Evidential Active Learning (FEAL)\nto calibrate the data evaluation under domain shift. Specifically, we introduce\na Dirichlet prior distribution in both local and global models to treat the\nprediction as a distribution over the probability simplex and capture both\naleatoric and epistemic uncertainties by using the Dirichlet-based evidential\nmodel. Then we employ the epistemic uncertainty to calibrate the aleatoric\nuncertainty. Afterward, we design a diversity relaxation strategy to reduce\ndata redundancy and maintain data diversity. Extensive experiments and analyses\nare conducted to show the superiority of FEAL over the state-of-the-art active\nlearning methods and the efficiency of FEAL under the federated active learning\nframework.","terms":["cs.CV"]},{"titles":"Uni3DL: Unified Model for 3D and Language Understanding","summaries":"In this work, we present Uni3DL, a unified model for 3D and Language\nunderstanding. Distinct from existing unified vision-language models in 3D\nwhich are limited in task variety and predominantly dependent on projected\nmulti-view images, Uni3DL operates directly on point clouds. This approach\nsignificantly expands the range of supported tasks in 3D, encompassing both\nvision and vision-language tasks in 3D. At the core of Uni3DL, a query\ntransformer is designed to learn task-agnostic semantic and mask outputs by\nattending to 3D visual features, and a task router is employed to selectively\ngenerate task-specific outputs required for diverse tasks. With a unified\narchitecture, our Uni3DL model enjoys seamless task decomposition and\nsubstantial parameter sharing across tasks. Uni3DL has been rigorously\nevaluated across diverse 3D vision-language understanding tasks, including\nsemantic segmentation, object detection, instance segmentation, visual\ngrounding, 3D captioning, and text-3D cross-modal retrieval. It demonstrates\nperformance on par with or surpassing state-of-the-art (SOTA) task-specific\nmodels. We hope our benchmark and Uni3DL model will serve as a solid step to\nease future research in unified models in the realm of 3D and language\nunderstanding. Project page: https:\/\/uni3dl.github.io.","terms":["cs.CV"]},{"titles":"Structured World Representations in Maze-Solving Transformers","summaries":"Transformer models underpin many recent advances in practical machine\nlearning applications, yet understanding their internal behavior continues to\nelude researchers. Given the size and complexity of these models, forming a\ncomprehensive picture of their inner workings remains a significant challenge.\nTo this end, we set out to understand small transformer models in a more\ntractable setting: that of solving mazes. In this work, we focus on the\nabstractions formed by these models and find evidence for the consistent\nemergence of structured internal representations of maze topology and valid\npaths. We demonstrate this by showing that the residual stream of only a single\ntoken can be linearly decoded to faithfully reconstruct the entire maze. We\nalso find that the learned embeddings of individual tokens have spatial\nstructure. Furthermore, we take steps towards deciphering the circuity of\npath-following by identifying attention heads (dubbed $\\textit{adjacency\nheads}$), which are implicated in finding valid subsequent tokens.","terms":["cs.LG","cs.AI"]},{"titles":"Deep Learning in Computed Tomography Pulmonary Angiography Imaging: A Dual-Pronged Approach for Pulmonary Embolism Detection","summaries":"The increasing reliance on Computed Tomography Pulmonary Angiography for\nPulmonary Embolism (PE) diagnosis presents challenges and a pressing need for\nimproved diagnostic solutions. The primary objective of this study is to\nleverage deep learning techniques to enhance the Computer Assisted Diagnosis of\nPE. In this study, we propose a classifier-guided detection approach that\neffectively leverages the classifier's probabilistic inference to direct the\ndetection predictions, marking a novel contribution in the domain of automated\nPE diagnosis. Our end-to-end classification framework introduces an\nAttention-Guided Convolutional Neural Network (AG-CNN) that leverages local\ncontext by utilizing an attention mechanism. This approach emulates the\nattention of a human expert by looking at both global appearances and local\nlesion regions before forming a conclusive decision. The classifier achieves a\nnotable AUROC, sensitivity, specificity and F1-score of 0.927, 0.862, 0.879 and\n0.805 respectively on the FUMPE dataset with Inception-v3 backbone\narchitecture. Moreover, AG-CNN outperforms the baseline DenseNet-121 model,\nachieving an 8.1% AUROC gain. While prior studies have primarily focused on PE\ndetection in main arteries, our utilization of state-of-the-art object\ndetection models and ensembling techniques significantly enhances detection\naccuracy for small embolisms in the peripheral arteries. Finally, our proposed\nclassifier-guided detection approach further refines the detection metrics\ncontributing new state-of-the-art to the community: mAP$_{50}$, sensitivity and\nF1-score of 0.846, 0.901 and 0.779 respectively outperforming the former\nbenchmark with a significant 3.7% improvement in mAP$_{50}$. Our research aims\nto elevate PE patient care by integrating AI solutions into clinical workflows,\nhighlighting the potential of human-AI collaboration in medical diagnostics.","terms":["cs.CV","cs.AI"]},{"titles":"Regularization Trade-offs with Fake Features","summaries":"Recent successes of massively overparameterized models have inspired a new\nline of work investigating the underlying conditions that enable\noverparameterized models to generalize well. This paper considers a framework\nwhere the possibly overparametrized model includes fake features, i.e.,\nfeatures that are present in the model but not in the data. We present a\nnon-asymptotic high-probability bound on the generalization error of the ridge\nregression problem under the model misspecification of having fake features.\nOur highprobability results provide insights into the interplay between the\nimplicit regularization provided by the fake features and the explicit\nregularization provided by the ridge parameter. Numerical results illustrate\nthe trade-off between the number of fake features and how the optimal ridge\nparameter may heavily depend on the number of fake features.","terms":["cs.LG","eess.SP","stat.ML"]},{"titles":"Denoising Diffusion Bridge Models","summaries":"Diffusion models are powerful generative models that map noise to data using\nstochastic processes. However, for many applications such as image editing, the\nmodel input comes from a distribution that is not random noise. As such,\ndiffusion models must rely on cumbersome methods like guidance or projected\nsampling to incorporate this information in the generative process. In our\nwork, we propose Denoising Diffusion Bridge Models (DDBMs), a natural\nalternative to this paradigm based on diffusion bridges, a family of processes\nthat interpolate between two paired distributions given as endpoints. Our\nmethod learns the score of the diffusion bridge from data and maps from one\nendpoint distribution to the other by solving a (stochastic) differential\nequation based on the learned score. Our method naturally unifies several\nclasses of generative models, such as score-based diffusion models and\nOT-Flow-Matching, allowing us to adapt existing design and architectural\nchoices to our more general problem. Empirically, we apply DDBMs to challenging\nimage datasets in both pixel and latent space. On standard image translation\nproblems, DDBMs achieve significant improvement over baseline methods, and,\nwhen we reduce the problem to image generation by setting the source\ndistribution to random noise, DDBMs achieve comparable FID scores to\nstate-of-the-art methods despite being built for a more general task.","terms":["cs.CV","cs.AI"]},{"titles":"Fairness in Medical Image Analysis and Healthcare: A Literature Survey","summaries":"Machine learning-enabled medical imaging analysis has become a vital part of\nthe automatic diagnosis system. However, machine learning, especially deep\nlearning models have been shown to demonstrate a systematic bias towards\ncertain subgroups of people. For instance, they yield a preferential predictive\nperformance to males over females, which is unfair and potentially harmful\nespecially in healthcare scenarios. In this literature survey, we give a\ncomprehensive review of the current progress of fairness studies in medical\nimage analysis (MedIA) and healthcare. Specifically, we first discuss the\ndefinitions of fairness, the source of unfairness and potential solutions.\nThen, we discuss current research on fairness for MedIA categorized by fairness\nevaluation and unfairness mitigation. Furthermore, we conduct extensive\nexperiments to evaluate the fairness of different medical imaging tasks.\nFinally, we discuss the challenges and future directions in developing fair\nMedIA and healthcare applications","terms":["cs.CV"]},{"titles":"ULMA: Unified Language Model Alignment with Demonstration and Point-wise Human Preference","summaries":"Language model alignment is a cutting-edge technique in large language model\ntraining to align the model output to user's intent, e.g., being helpful and\nharmless. Recent alignment framework consists of two steps: supervised\nfine-tuning with demonstration data and preference learning with human\npreference data. Previous preference learning methods, such as RLHF and DPO,\nmainly focus on pair-wise preference data. However, in many real-world\nscenarios where human feedbacks are intrinsically point-wise, these methods\nwill suffer from information loss or even fail. To fill this gap, in this\npaper, we first develop a preference learning method called point-wise DPO to\ntackle point-wise preference data. Further revelation on the connection between\nsupervised fine-tuning and point-wise preference learning enables us to develop\na unified framework for both human demonstration and point-wise preference\ndata, which sheds new light on the construction of preference dataset.\nExtensive experiments on point-wise datasets with binary or continuous labels\ndemonstrate the superior performance and efficiency of our proposed methods. A\nnew dataset with high-quality demonstration samples on harmlessness is\nconstructed and made publicly available.","terms":["cs.LG","cs.CL"]},{"titles":"Stackelberg Driver Model for Continual Policy Improvement in Scenario-Based Closed-Loop Autonomous Driving","summaries":"The deployment of autonomous vehicles (AVs) has faced hurdles due to the\ndominance of rare but critical corner cases within the long-tail distribution\nof driving scenarios, which negatively affects their overall performance. To\naddress this challenge, adversarial generation methods have emerged as a class\nof efficient approaches to synthesize safety-critical scenarios for AV testing.\nHowever, these generated scenarios are often underutilized for AV training,\nresulting in the potential for continual AV policy improvement remaining\nuntapped, along with a deficiency in the closed-loop design needed to achieve\nit. Therefore, we tailor the Stackelberg Driver Model (SDM) to accurately\ncharacterize the hierarchical nature of vehicle interaction dynamics,\nfacilitating iterative improvement by engaging background vehicles (BVs) and AV\nin a sequential game-like interaction paradigm. With AV acting as the leader\nand BVs as followers, this leader-follower modeling ensures that AV would\nconsistently refine its policy, always taking into account the additional\ninformation that BVs play the best response to challenge AV. Extensive\nexperiments have shown that our algorithm exhibits superior performance\ncompared to several baselines especially in higher dimensional scenarios,\nleading to substantial advancements in AV capabilities while continually\ngenerating progressively challenging scenarios. Code is available at\nhttps:\/\/github.com\/BlueCat-de\/SDM.","terms":["cs.LG","cs.AI","cs.RO"]},{"titles":"DemaFormer: Damped Exponential Moving Average Transformer with Energy-Based Modeling for Temporal Language Grounding","summaries":"Temporal Language Grounding seeks to localize video moments that semantically\ncorrespond to a natural language query. Recent advances employ the attention\nmechanism to learn the relations between video moments and the text query.\nHowever, naive attention might not be able to appropriately capture such\nrelations, resulting in ineffective distributions where target video moments\nare difficult to separate from the remaining ones. To resolve the issue, we\npropose an energy-based model framework to explicitly learn moment-query\ndistributions. Moreover, we propose DemaFormer, a novel Transformer-based\narchitecture that utilizes exponential moving average with a learnable damping\nfactor to effectively encode moment-query inputs. Comprehensive experiments on\nfour public temporal language grounding datasets showcase the superiority of\nour methods over the state-of-the-art baselines.","terms":["cs.CV","cs.CL"]},{"titles":"DeepInception: Hypnotize Large Language Model to Be Jailbreaker","summaries":"Despite remarkable success in various applications, large language models\n(LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails\nvoid. However, previous studies for jailbreaks usually resort to brute-force\noptimization or extrapolations of a high computation cost, which might not be\npractical or effective. In this paper, inspired by the Milgram experiment that\nindividuals can harm another person if they are told to do so by an\nauthoritative figure, we disclose a lightweight method, termed as\nDeepInception, which can easily hypnotize LLM to be a jailbreaker and unlock\nits misusing risks. Specifically, DeepInception leverages the personification\nability of LLM to construct a novel nested scene to behave, which realizes an\nadaptive way to escape the usage control in a normal scenario and provides the\npossibility for further direct jailbreaks. Empirically, we conduct\ncomprehensive experiments to show its efficacy. Our DeepInception can achieve\ncompetitive jailbreak success rates with previous counterparts and realize a\ncontinuous jailbreak in subsequent interactions, which reveals the critical\nweakness of self-losing on both open\/closed-source LLMs like Falcon, Vicuna,\nLlama-2, and GPT-3.5\/4\/4V. Our investigation appeals that people should pay\nmore attention to the safety aspects of LLMs and a stronger defense against\ntheir misuse risks. The code is publicly available at:\nhttps:\/\/github.com\/tmlr-group\/DeepInception.","terms":["cs.LG","cs.CR"]},{"titles":"GeNIe: Generative Hard Negative Images Through Diffusion","summaries":"Data augmentation is crucial in training deep models, preventing them from\noverfitting to limited data. Common data augmentation methods are effective,\nbut recent advancements in generative AI, such as diffusion models for image\ngeneration, enable more sophisticated augmentation techniques that produce data\nresembling natural images. We recognize that augmented samples closer to the\nideal decision boundary of a classifier are particularly effective and\nefficient in guiding the learning process. We introduce GeNIe which leverages a\ndiffusion model conditioned on a text prompt to merge contrasting data points\n(an image from the source category and a text prompt from the target category)\nto generate challenging samples for the target category. Inspired by recent\nimage editing methods, we limit the number of diffusion iterations and the\namount of noise. This ensures that the generated image retains low-level and\ncontextual features from the source image, potentially conflicting with the\ntarget category. Our extensive experiments, in few-shot and also long-tail\ndistribution settings, demonstrate the effectiveness of our novel augmentation\nmethod, especially benefiting categories with a limited number of examples.","terms":["cs.CV"]},{"titles":"Machine Vision Therapy: Multimodal Large Language Models Can Enhance Visual Robustness via Denoising In-Context Learning","summaries":"Although vision models such as Contrastive Language-Image Pre-Training (CLIP)\nshow impressive generalization performance, their zero-shot robustness is still\nlimited under Out-of-Distribution (OOD) scenarios without fine-tuning. Instead\nof undesirably providing human supervision as commonly done, it is possible to\ntake advantage of Multi-modal Large Language Models (MLLMs) that hold powerful\nvisual understanding abilities. However, MLLMs are shown to struggle with\nvision problems due to the incompatibility of tasks, thus hindering their\nutilization. In this paper, we propose to effectively leverage MLLMs to conduct\nMachine Vision Therapy which aims to rectify the noisy predictions from vision\nmodels. By fine-tuning with the denoised labels, the learning model performance\ncan be boosted in an unsupervised manner. To solve the incompatibility issue,\nwe propose a novel Denoising In-Context Learning (DICL) strategy to align\nvision tasks with MLLMs. Concretely, by estimating a transition matrix that\ncaptures the probability of one class being confused with another, an\ninstruction containing a correct exemplar and an erroneous one from the most\nprobable noisy class can be constructed. Such an instruction can help any MLLMs\nwith ICL ability to detect and rectify incorrect predictions of vision models.\nThrough extensive experiments on ImageNet, WILDS, DomainBed, and other OOD\ndatasets, we carefully validate the quantitative and qualitative effectiveness\nof our method. Our code is available at\nhttps:\/\/github.com\/tmllab\/Machine_Vision_Therapy.","terms":["cs.CV"]},{"titles":"Graph Information Bottleneck for Remote Sensing Segmentation","summaries":"Remote sensing segmentation has a wide range of applications in environmental\nprotection, and urban change detection, etc. Despite the success of deep\nlearning-based remote sensing segmentation methods (e.g., CNN and Transformer),\nthey are not flexible enough to model irregular objects. In addition, existing\ngraph contrastive learning methods usually adopt the way of maximizing mutual\ninformation to keep the node representations consistent between different graph\nviews, which may cause the model to learn task-independent redundant\ninformation. To tackle the above problems, this paper treats images as graph\nstructures and introduces a simple contrastive vision GNN (SC-ViG) architecture\nfor remote sensing segmentation. Specifically, we construct a node-masked and\nedge-masked graph view to obtain an optimal graph structure representation,\nwhich can adaptively learn whether to mask nodes and edges. Furthermore, this\npaper innovatively introduces information bottleneck theory into graph\ncontrastive learning to maximize task-related information while minimizing\ntask-independent redundant information. Finally, we replace the convolutional\nmodule in UNet with the SC-ViG module to complete the segmentation and\nclassification tasks of remote sensing images. Extensive experiments on\npublicly available real datasets demonstrate that our method outperforms\nstate-of-the-art remote sensing image segmentation methods.","terms":["cs.CV","cs.AI"]},{"titles":"Exploring Weight Balancing on Long-Tailed Recognition Problem","summaries":"Recognition problems in long-tailed data, in which the sample size per class\nis heavily skewed, have gained importance because the distribution of the\nsample size per class in a dataset is generally exponential unless the sample\nsize is intentionally adjusted. Various methods have been devised to address\nthese problems. Recently, weight balancing, which combines well-known classical\nregularization techniques with two-stage training, has been proposed. Despite\nits simplicity, it is known for its high performance compared with existing\nmethods devised in various ways. However, there is a lack of understanding as\nto why this method is effective for long-tailed data. In this study, we analyze\nweight balancing by focusing on neural collapse and the cone effect at each\ntraining stage and found that it can be decomposed into an increase in Fisher's\ndiscriminant ratio of the feature extractor caused by weight decay and cross\nentropy loss and implicit logit adjustment caused by weight decay and\nclass-balanced loss. Our analysis enables the training method to be further\nsimplified by reducing the number of training stages to one while increasing\naccuracy.","terms":["cs.LG"]},{"titles":"Continual Learning with Distributed Optimization: Does CoCoA Forget?","summaries":"We focus on the continual learning problem where the tasks arrive\nsequentially and the aim is to perform well on the newly arrived task without\nperformance degradation on the previously seen tasks. In contrast to the\ncontinual learning literature focusing on the centralized setting, we\ninvestigate the distributed estimation framework. We consider the\nwell-established distributed learning algorithm COCOA. We derive closed form\nexpressions for the iterations for the overparametrized case. We illustrate the\nconvergence and the error performance of the algorithm based on the\nover\/under-parameterization of the problem. Our results show that depending on\nthe problem dimensions and data generation assumptions, COCOA can perform\ncontinual learning over a sequence of tasks, i.e., it can learn a new task\nwithout forgetting previously learned tasks, with access only to one task at a\ntime.","terms":["stat.ML","cs.LG","eess.SP"]},{"titles":"PipeOptim: Ensuring Effective 1F1B Schedule with Optimizer-Dependent Weight Prediction","summaries":"Asynchronous pipeline model parallelism with a \"1F1B\" (one forward, one\nbackward) schedule generates little bubble overhead and always provides quite a\nhigh throughput. However, the \"1F1B\" schedule inevitably leads to weight\ninconsistency and weight staleness issues due to the cross-training of\ndifferent mini-batches across GPUs. To simultaneously address these two\nproblems, in this paper, we propose an optimizer-dependent weight prediction\nstrategy (a.k.a PipeOptim) for asynchronous pipeline training. The key insight\nof our proposal is that we employ a weight prediction strategy in the forward\npass to ensure that each mini-batch uses consistent and staleness-free weights\nto compute the forward pass. To be concrete, we first construct the weight\nprediction scheme based on the update rule of the used optimizer when training\nthe deep neural network models. Then throughout the \"1F1B\" pipelined training,\neach mini-batch is mandated to execute weight prediction ahead of the forward\npass, subsequently employing the predicted weights to perform the forward pass.\nAs a result, PipeOptim 1) inherits the advantage of the \"1F1B\" schedule and\ngenerates pretty high throughput, and 2) can ensure effective parameter\nlearning regardless of the type of the used optimizer. To verify the\neffectiveness of our proposal, we conducted extensive experimental evaluations\nusing eight different deep-learning models spanning three machine-learning\ntasks including image classification, sentiment analysis, and machine\ntranslation. The experiment results demonstrate that PipeOptim outperforms the\npopular pipelined approaches including GPipe, PipeDream, PipeDream-2BW, and\nSpecTrain. The code of PipeOptim can be accessible at\nhttps:\/\/github.com\/guanleics\/PipeOptim.","terms":["cs.LG","cs.AI"]},{"titles":"Slide-SAM: Medical SAM Meets Sliding Window","summaries":"The Segment Anything Model (SAM) has achieved a notable success in\ntwo-dimensional image segmentation in natural images. However, the substantial\ngap between medical and natural images hinders its direct application to\nmedical image segmentation tasks. Particularly in 3D medical images, SAM\nstruggles to learn contextual relationships between slices, limiting its\npractical applicability. Moreover, applying 2D SAM to 3D images requires\nprompting the entire volume, which is time- and label-consuming. To address\nthese problems, we propose Slide-SAM, which treats a stack of three adjacent\nslices as a prediction window. It firstly takes three slices from a 3D volume\nand point- or bounding box prompts on the central slice as inputs to predict\nsegmentation masks for all three slices. Subsequently, the masks of the top and\nbottom slices are then used to generate new prompts for adjacent slices.\nFinally, step-wise prediction can be achieved by sliding the prediction window\nforward or backward through the entire volume. Our model is trained on multiple\npublic and private medical datasets and demonstrates its effectiveness through\nextensive 3D segmetnation experiments, with the help of minimal prompts. Code\nis available at \\url{https:\/\/github.com\/Curli-quan\/Slide-SAM}.","terms":["cs.CV"]},{"titles":"Towards Open-set Gesture Recognition via Feature Activation Enhancement and Orthogonal Prototype Learning","summaries":"Gesture recognition is a foundational task in human-machine interaction\n(HMI). While there has been significant progress in gesture recognition based\non surface electromyography (sEMG), accurate recognition of predefined gestures\nonly within a closed set is still inadequate in practice. It is essential to\neffectively discern and reject unknown gestures of disinterest in a robust\nsystem. Numerous methods based on prototype learning (PL) have been proposed to\ntackle this open set recognition (OSR) problem. However, they do not fully\nexplore the inherent distinctions between known and unknown classes. In this\npaper, we propose a more effective PL method leveraging two novel and inherent\ndistinctions, feature activation level and projection inconsistency.\nSpecifically, the Feature Activation Enhancement Mechanism (FAEM) widens the\ngap in feature activation values between known and unknown classes.\nFurthermore, we introduce Orthogonal Prototype Learning (OPL) to construct\nmultiple perspectives. OPL acts to project a sample from orthogonal directions\nto maximize the distinction between its two projections, where unknown samples\nwill be projected near the clusters of different known classes while known\nsamples still maintain intra-class similarity. Our proposed method\nsimultaneously achieves accurate closed-set classification for predefined\ngestures and effective rejection for unknown gestures. Extensive experiments\ndemonstrate its efficacy and superiority in open-set gesture recognition based\non sEMG.","terms":["cs.CV"]},{"titles":"Qualitative Failures of Image Generation Models and Their Application in Detecting Deepfakes","summaries":"The ability of image and video generation models to create photorealistic\nimages has reached unprecedented heights, making it difficult to distinguish\nbetween real and fake images in many cases. However, despite this progress, a\ngap remains between the quality of generated images and those found in the real\nworld. To address this, we have reviewed a vast body of literature from both\nacademic publications and social media to identify qualitative shortcomings in\nimage generation models, which we have classified into five categories. By\nunderstanding these failures, we can identify areas where these models need\nimprovement, as well as develop strategies for detecting deep fakes. The\nprevalence of deep fakes in today's society is a serious concern, and our\nfindings can help mitigate their negative impact.","terms":["cs.CV","cs.AI"]},{"titles":"MEMTO: Memory-guided Transformer for Multivariate Time Series Anomaly Detection","summaries":"Detecting anomalies in real-world multivariate time series data is\nchallenging due to complex temporal dependencies and inter-variable\ncorrelations. Recently, reconstruction-based deep models have been widely used\nto solve the problem. However, these methods still suffer from an\nover-generalization issue and fail to deliver consistently high performance. To\naddress this issue, we propose the MEMTO, a memory-guided Transformer using a\nreconstruction-based approach. It is designed to incorporate a novel memory\nmodule that can learn the degree to which each memory item should be updated in\nresponse to the input data. To stabilize the training procedure, we use a\ntwo-phase training paradigm which involves using K-means clustering for\ninitializing memory items. Additionally, we introduce a bi-dimensional\ndeviation-based detection criterion that calculates anomaly scores considering\nboth input space and latent space. We evaluate our proposed method on five\nreal-world datasets from diverse domains, and it achieves an average anomaly\ndetection F1-score of 95.74%, significantly outperforming the previous\nstate-of-the-art methods. We also conduct extensive experiments to empirically\nvalidate the effectiveness of our proposed model's key components.","terms":["cs.LG","cs.AI"]},{"titles":"Towards Automatic Power Battery Detection: New Challenge, Benchmark Dataset and Baseline","summaries":"We conduct a comprehensive study on a new task named power battery detection\n(PBD), which aims to localize the dense cathode and anode plates endpoints from\nX-ray images to evaluate the quality of power batteries. Existing manufacturers\nusually rely on human eye observation to complete PBD, which makes it difficult\nto balance the accuracy and efficiency of detection. To address this issue and\ndrive more attention into this meaningful task, we first elaborately collect a\ndataset, called X-ray PBD, which has $1,500$ diverse X-ray images selected from\nthousands of power batteries of $5$ manufacturers, with $7$ different visual\ninterference. Then, we propose a novel segmentation-based solution for PBD,\ntermed multi-dimensional collaborative network (MDCNet). With the help of line\nand counting predictors, the representation of the point segmentation branch\ncan be improved at both semantic and detail aspects. Besides, we design an\neffective distance-adaptive mask generation strategy, which can alleviate the\nvisual challenge caused by the inconsistent distribution density of plates to\nprovide MDCNet with stable supervision. Without any bells and whistles, our\nsegmentation-based MDCNet consistently outperforms various other corner\ndetection, crowd counting and general\/tiny object detection-based solutions,\nmaking it a strong baseline that can help facilitate future research in PBD.\nFinally, we share some potential difficulties and works for future researches.\nThe source code and datasets will be publicly available at\n\\href{http:\/\/www.gy3000.company\/x3000%e5%bc%80%e6%94%be%e5%b9%b3%e5%8f%b0}{X-ray\nPBD}.","terms":["cs.CV"]},{"titles":"Finding Point with Image: A Simple and Efficient Method for UAV Self-Localization","summaries":"Image retrieval has emerged as a prominent solution for the self-localization\ntask of unmanned aerial vehicles (UAVs). However, this approach involves\ncomplicated pre-processing and post-processing operations, placing significant\ndemands on both computational and storage resources. To mitigate this issue,\nthis paper presents an end-to-end positioning framework, namely Finding Point\nwith Image (FPI), which aims to directly identify the corresponding location of\na UAV in satellite-view images via a UAV-view image. To validate the\npracticality of our framework, we construct a paired dataset, namely UL14, that\nconsists of UAV and satellite views. In addition, we establish two\ntransformer-based baseline models, Post Fusion and Mix Fusion, for end-to-end\ntraining and inference. Through experiments, we can conclude that fusion in the\nbackbone network can achieve better performance than later fusion. Furthermore,\nconsidering the singleness of paired images, Random Scale Crop (RSC) is\nproposed to enrich the diversity of the paired data. Also, the ratio and weight\nof positive and negative samples play a key role in model convergence.\nTherefore, we conducted experimental verification and proposed a Weight Balance\nLoss (WBL) to weigh the impact of positive and negative samples. Last, our\nproposed baseline based on Mix Fusion structure exhibits superior performance\nin time and storage efficiency, amounting to just 1\/24 and 1\/68, respectively,\nwhile delivering comparable or even superior performance compared to the image\nretrieval method. The dataset and code will be made publicly available.","terms":["cs.CV"]},{"titles":"Byzantine-Robust Distributed Online Learning: Taming Adversarial Participants in An Adversarial Environment","summaries":"This paper studies distributed online learning under Byzantine attacks. The\nperformance of an online learning algorithm is often characterized by\n(adversarial) regret, which evaluates the quality of one-step-ahead\ndecision-making when an environment provides adversarial losses, and a\nsublinear bound is preferred. But we prove that, even with a class of\nstate-of-the-art robust aggregation rules, in an adversarial environment and in\nthe presence of Byzantine participants, distributed online gradient descent can\nonly achieve a linear adversarial regret bound, which is tight. This is the\ninevitable consequence of Byzantine attacks, even though we can control the\nconstant of the linear adversarial regret to a reasonable level. Interestingly,\nwhen the environment is not fully adversarial so that the losses of the honest\nparticipants are i.i.d. (independent and identically distributed), we show that\nsublinear stochastic regret, in contrast to the aforementioned adversarial\nregret, is possible. We develop a Byzantine-robust distributed online momentum\nalgorithm to attain such a sublinear stochastic regret bound. Extensive\nnumerical experiments corroborate our theoretical analysis.","terms":["cs.LG","cs.DC"]},{"titles":"MASP: Scalable GNN-based Planning for Multi-Agent Navigation","summaries":"We investigate the problem of decentralized multi-agent navigation tasks,\nwhere multiple agents need to reach initially unassigned targets in a limited\ntime. Classical planning-based methods suffer from expensive computation\noverhead at each step and offer limited expressiveness for complex cooperation\nstrategies. In contrast, reinforcement learning (RL) has recently become a\npopular paradigm for addressing this issue. However, RL struggles with low data\nefficiency and cooperation when directly exploring (nearly) optimal policies in\nthe large search space, especially with an increased agent number (e.g., 10+\nagents) or in complex environments (e.g., 3D simulators). In this paper, we\npropose Multi-Agent Scalable GNN-based P lanner (MASP), a goal-conditioned\nhierarchical planner for navigation tasks with a substantial number of agents.\nMASP adopts a hierarchical framework to divide a large search space into\nmultiple smaller spaces, thereby reducing the space complexity and accelerating\ntraining convergence. We also leverage graph neural networks (GNN) to model the\ninteraction between agents and goals, improving goal achievement. Besides, to\nenhance generalization capabilities in scenarios with unseen team sizes, we\ndivide agents into multiple groups, each with a previously trained number of\nagents. The results demonstrate that MASP outperforms classical planning-based\ncompetitors and RL baselines, achieving a nearly 100% success rate with minimal\ntraining data in both multi-agent particle environments (MPE) with 50 agents\nand a quadrotor 3-dimensional environment (OmniDrones) with 20 agents.\nFurthermore, the learned policy showcases zero-shot generalization across\nunseen team sizes.","terms":["cs.LG","cs.AI","cs.RO"]},{"titles":"Retrieving Conditions from Reference Images for Diffusion Models","summaries":"Recent diffusion-based subject driven generative methods have enabled image\ngenerations with good fidelity for specific objects or human portraits.\nHowever, to achieve better versatility for applications, we argue that not only\nimproved datasets and evaluations are desired, but also more careful methods to\nretrieve only relevant information from conditional images are anticipated. To\nthis end, we propose an anime figures dataset RetriBooru-V1, with enhanced\nidentity and clothing labels. We state new tasks enabled by this dataset, and\nintroduce a new diversity metric to measure success in completing these tasks,\nquantifying the flexibility of image generations. We establish an RAG-inspired\nbaseline method, designed to retrieve precise conditional information from\nreference images. Then, we compare with current methods on existing task to\ndemonstrate the capability of the proposed method. Finally, we provide baseline\nexperiment results on new tasks, and conduct ablation studies on the possible\nstructural choices.","terms":["cs.CV","cs.AI"]},{"titles":"Towards More Unified In-context Visual Understanding","summaries":"The rapid advancement of large language models (LLMs) has accelerated the\nemergence of in-context learning (ICL) as a cutting-edge approach in the\nnatural language processing domain. Recently, ICL has been employed in visual\nunderstanding tasks, such as semantic segmentation and image captioning,\nyielding promising results. However, existing visual ICL framework can not\nenable producing content across multiple modalities, which limits their\npotential usage scenarios. To address this issue, we present a new ICL\nframework for visual understanding with multi-modal output enabled. First, we\nquantize and embed both text and visual prompt into a unified representational\nspace, structured as interleaved in-context sequences. Then a decoder-only\nsparse transformer architecture is employed to perform generative modeling on\nthem, facilitating in-context learning. Thanks to this design, the model is\ncapable of handling in-context vision understanding tasks with multimodal\noutput in a unified pipeline. Experimental results demonstrate that our model\nachieves competitive performance compared with specialized models and previous\nICL baselines. Overall, our research takes a further step toward unified\nmultimodal in-context learning.","terms":["cs.CV"]},{"titles":"Simplifying Neural Network Training Under Class Imbalance","summaries":"Real-world datasets are often highly class-imbalanced, which can adversely\nimpact the performance of deep learning models. The majority of research on\ntraining neural networks under class imbalance has focused on specialized loss\nfunctions, sampling techniques, or two-stage training procedures. Notably, we\ndemonstrate that simply tuning existing components of standard deep learning\npipelines, such as the batch size, data augmentation, optimizer, and label\nsmoothing, can achieve state-of-the-art performance without any such\nspecialized class imbalance methods. We also provide key prescriptions and\nconsiderations for training under class imbalance, and an understanding of why\nimbalance methods succeed or fail.","terms":["cs.LG","cs.AI"]},{"titles":"Deep Learning-Driven Enhancement of Welding Quality Control: Predicting Welding Depth and Pore Volume in Hairpin Welding","summaries":"To advance quality assurance in the welding process, this study presents a\nrobust deep learning model that enables the prediction of two critical welds\nKey Performance Characteristics (KPCs): welding depth and average pore volume.\nIn the proposed approach, a comprehensive range of laser welding Key Input\nCharacteristics (KICs) is utilized, including welding beam geometries, welding\nfeed rates, path repetitions for weld beam geometries, and bright light weld\nratios for all paths, all of which were obtained from hairpin welding\nexperiments. Two deep learning networks are employed with multiple hidden dense\nlayers and linear activation functions to showcase the capabilities of deep\nneural networks in capturing the intricate nonlinear connections inherent\nwithin welding KPCs and KICs. Applying deep learning networks to the small\nnumerical experimental hairpin welding dataset has shown promising results,\nachieving Mean Absolute Error (MAE) values as low as 0.1079 for predicting\nwelding depth and 0.0641 for average pore volume. Additionally, the validity\nverification demonstrates the reliability of the proposed method. This, in\nturn, promises significant advantages in controlling welding outcomes, moving\nbeyond the current trend of relying merely on monitoring for defect\nclassification.","terms":["cs.LG"]},{"titles":"ASPEN: High-Throughput LoRA Fine-Tuning of Large Language Models with a Single GPU","summaries":"Transformer-based large language models (LLMs) have demonstrated outstanding\nperformance across diverse domains, particularly when fine-turned for specific\ndomains. Recent studies suggest that the resources required for fine-tuning\nLLMs can be economized through parameter-efficient methods such as Low-Rank\nAdaptation (LoRA). While LoRA effectively reduces computational burdens and\nresource demands, it currently supports only a single-job fine-tuning setup.\n  In this paper, we present ASPEN, a high-throughput framework for fine-tuning\nLLMs. ASPEN efficiently trains multiple jobs on a single GPU using the LoRA\nmethod, leveraging shared pre-trained model and adaptive scheduling. ASPEN is\ncompatible with transformer-based language models like LLaMA and ChatGLM, etc.\nExperiments show that ASPEN saves 53% of GPU memory when training multiple\nLLaMA-7B models on NVIDIA A100 80GB GPU and boosts training throughput by about\n17% compared to existing methods when training with various pre-trained models\non different GPUs. The adaptive scheduling algorithm reduces turnaround time by\n24%, end-to-end training latency by 12%, prioritizing jobs and preventing\nout-of-memory issues.","terms":["cs.LG","cs.AI"]},{"titles":"Hulk: A Universal Knowledge Translator for Human-Centric Tasks","summaries":"Human-centric perception tasks, e.g., human mesh recovery, pedestrian\ndetection, skeleton-based action recognition, and pose estimation, have wide\nindustrial applications, such as metaverse and sports analysis. There is a\nrecent surge to develop human-centric foundation models that can benefit a\nbroad range of human-centric perception tasks. While many human-centric\nfoundation models have achieved success, most of them only excel in 2D vision\ntasks or require extensive fine-tuning for practical deployment in real-world\nscenarios. These limitations severely restrict their usability across various\ndownstream tasks and situations. To tackle these problems, we present Hulk, the\nfirst multimodal human-centric generalist model, capable of addressing most of\nthe mainstream tasks simultaneously without task-specific finetuning, covering\n2D vision, 3D vision, skeleton-based, and vision-language tasks. The key to\nachieving this is condensing various task-specific heads into two general\nheads, one for discrete representations, e.g., languages, and the other for\ncontinuous representations, e.g., location coordinates. The outputs of two\nheads can be further stacked into four distinct input and output modalities.\nThis uniform representation enables Hulk to treat human-centric tasks as\nmodality translation, integrating knowledge across a wide range of tasks. To\nvalidate the effectiveness of our proposed method, we conduct comprehensive\nexperiments on 11 benchmarks across 8 human-centric tasks. Experimental results\nsurpass previous methods substantially, demonstrating the superiority of our\nproposed method. The code will be available on\nhttps:\/\/github.com\/OpenGVLab\/HumanBench.","terms":["cs.CV","cs.AI"]},{"titles":"AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation","summaries":"This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech\nTranslation (AV2AV) framework, where the input and output of the system are\nmultimodal (i.e., audio and visual speech). With the proposed AV2AV, two key\nadvantages can be brought: 1) We can perform real-like conversations with\nindividuals worldwide in a virtual meeting by utilizing our own primary\nlanguages. In contrast to Speech-to-Speech Translation (A2A), which solely\ntranslates between audio modalities, the proposed AV2AV directly translates\nbetween audio-visual speech. This capability enhances the dialogue experience\nby presenting synchronized lip movements along with the translated speech. 2)\nWe can improve the robustness of the spoken language translation system. By\nemploying the complementary information of audio-visual speech, the system can\neffectively translate spoken language even in the presence of acoustic noise,\nshowcasing robust performance. To mitigate the problem of the absence of a\nparallel AV2AV translation dataset, we propose to train our spoken language\ntranslation system with the audio-only dataset of A2A. This is done by learning\nunified audio-visual speech representations through self-supervised learning in\nadvance to train the translation system. Moreover, we propose an AV-Renderer\nthat can generate raw audio and video in parallel. It is designed with\nzero-shot speaker modeling, thus the speaker in source audio-visual speech can\nbe maintained at the target translated audio-visual speech. The effectiveness\nof AV2AV is evaluated with extensive experiments in a many-to-many language\ntranslation setting. The demo page is available on\nhttps:\/\/choijeongsoo.github.io\/av2av.","terms":["cs.CV","cs.AI","cs.MM","eess.AS"]},{"titles":"ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance","summaries":"Understanding 3D scenes from multi-view inputs has been proven to alleviate\nthe view discrepancy issue in 3D visual grounding. However, existing methods\nnormally neglect the view cues embedded in the text modality and fail to weigh\nthe relative importance of different views. In this paper, we propose\nViewRefer, a multi-view framework for 3D visual grounding exploring how to\ngrasp the view knowledge from both text and 3D modalities. For the text branch,\nViewRefer leverages the diverse linguistic knowledge of large-scale language\nmodels, e.g., GPT, to expand a single grounding text to multiple\ngeometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer\nfusion module with inter-view attention is introduced to boost the interaction\nof objects across views. On top of that, we further present a set of learnable\nmulti-view prototypes, which memorize scene-agnostic knowledge for different\nviews, and enhance the framework from two perspectives: a view-guided attention\nmodule for more robust text features, and a view-guided scoring strategy during\nthe final prediction. With our designed paradigm, ViewRefer achieves superior\nperformance on three benchmarks and surpasses the second-best by +2.8%, +1.5%,\nand +1.35% on Sr3D, Nr3D, and ScanRefer. Code is released at\nhttps:\/\/github.com\/Ivan-Tang-3D\/ViewRefer3D.","terms":["cs.CV","cs.AI","cs.CL"]},{"titles":"GPT-Driver: Learning to Drive with GPT","summaries":"We present a simple yet effective approach that can transform the OpenAI\nGPT-3.5 model into a reliable motion planner for autonomous vehicles. Motion\nplanning is a core challenge in autonomous driving, aiming to plan a driving\ntrajectory that is safe and comfortable. Existing motion planners predominantly\nleverage heuristic methods to forecast driving trajectories, yet these\napproaches demonstrate insufficient generalization capabilities in the face of\nnovel and unseen driving scenarios. In this paper, we propose a novel approach\nto motion planning that capitalizes on the strong reasoning capabilities and\ngeneralization potential inherent to Large Language Models (LLMs). The\nfundamental insight of our approach is the reformulation of motion planning as\na language modeling problem, a perspective not previously explored.\nSpecifically, we represent the planner inputs and outputs as language tokens,\nand leverage the LLM to generate driving trajectories through a language\ndescription of coordinate positions. Furthermore, we propose a novel\nprompting-reasoning-finetuning strategy to stimulate the numerical reasoning\npotential of the LLM. With this strategy, the LLM can describe highly precise\ntrajectory coordinates and also its internal decision-making process in natural\nlanguage. We evaluate our approach on the large-scale nuScenes dataset, and\nextensive experiments substantiate the effectiveness, generalization ability,\nand interpretability of our GPT-based motion planner. Code is now available at\nhttps:\/\/github.com\/PointsCoder\/GPT-Driver.","terms":["cs.CV","cs.AI","cs.CL","cs.RO"]},{"titles":"Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion","summaries":"Distributional reinforcement learning algorithms have attempted to utilize\nestimated uncertainty for exploration, such as optimism in the face of\nuncertainty. However, using the estimated variance for optimistic exploration\nmay cause biased data collection and hinder convergence or performance. In this\npaper, we present a novel distributional reinforcement learning algorithm that\nselects actions by randomizing risk criterion to avoid one-sided tendency on\nrisk. We provide a perturbed distributional Bellman optimality operator by\ndistorting the risk measure and prove the convergence and optimality of the\nproposed method with the weaker contraction property. Our theoretical results\nsupport that the proposed method does not fall into biased exploration and is\nguaranteed to converge to an optimal return. Finally, we empirically show that\nour method outperforms other existing distribution-based algorithms in various\nenvironments including Atari 55 games.","terms":["cs.LG","cs.AI"]},{"titles":"SAVE: Protagonist Diversification with Structure Agnostic Video Editing","summaries":"Driven by the upsurge progress in text-to-image (T2I) generation models,\ntext-to-video (T2V) generation has experienced a significant advance as well.\nAccordingly, tasks such as modifying the object or changing the style in a\nvideo have been possible. However, previous works usually work well on trivial\nand consistent shapes, and easily collapse on a difficult target that has a\nlargely different body shape from the original one. In this paper, we spot the\nbias problem in the existing video editing method that restricts the range of\nchoices for the new protagonist and attempt to address this issue using the\nconventional image-level personalization method. We adopt motion\npersonalization that isolates the motion from a single source video and then\nmodifies the protagonist accordingly. To deal with the natural discrepancy\nbetween image and video, we propose a motion word with an inflated textual\nembedding to properly represent the motion in a source video. We also regulate\nthe motion word to attend to proper motion-related areas by introducing a novel\npseudo optical flow, efficiently computed from the pre-calculated attention\nmaps. Finally, we decouple the motion from the appearance of the source video\nwith an additional pseudo word. Extensive experiments demonstrate the editing\ncapability of our method, taking a step toward more diverse and extensive video\nediting.","terms":["cs.CV"]},{"titles":"Inspecting Model Fairness in Ultrasound Segmentation Tasks","summaries":"With the rapid expansion of machine learning and deep learning (DL),\nresearchers are increasingly employing learning-based algorithms to alleviate\ndiagnostic challenges across diverse medical tasks and applications. While\nadvancements in diagnostic precision are notable, some researchers have\nidentified a concerning trend: their models exhibit biased performance across\nsubgroups characterized by different sensitive attributes. This bias not only\ninfringes upon the rights of patients but also has the potential to lead to\nlife-altering consequences. In this paper, we inspect a series of DL\nsegmentation models using two ultrasound datasets, aiming to assess the\npresence of model unfairness in these specific tasks. Our findings reveal that\neven state-of-the-art DL algorithms demonstrate unfair behavior in ultrasound\nsegmentation tasks. These results serve as a crucial warning, underscoring the\nnecessity for careful model evaluation before their deployment in real-world\nscenarios. Such assessments are imperative to ensure ethical considerations and\nmitigate the risk of adverse impacts on patient outcomes.","terms":["cs.CV","cs.AI"]},{"titles":"Understanding Parameter Saliency via Extreme Value Theory","summaries":"Deep neural networks are being increasingly implemented throughout society in\nrecent years. It is useful to identify which parameters trigger\nmisclassification in diagnosing undesirable model behaviors. The concept of\nparameter saliency is proposed and used to diagnose convolutional neural\nnetworks (CNNs) by ranking convolution filters that may have caused\nmisclassification on the basis of parameter saliency. It is also shown that\nfine-tuning the top ranking salient filters efficiently corrects\nmisidentification on ImageNet. However, there is still a knowledge gap in terms\nof understanding why parameter saliency ranking can find the filters inducing\nmisidentification. In this work, we attempt to bridge the gap by analyzing\nparameter saliency ranking from a statistical viewpoint, namely, extreme value\ntheory. We first show that the existing work implicitly assumes that the\ngradient norm computed for each filter follows a normal distribution. Then, we\nclarify the relationship between parameter saliency and the score based on the\npeaks-over-threshold (POT) method, which is often used to model extreme values.\nFinally, we reformulate parameter saliency in terms of the POT method, where\nthis reformulation is regarded as statistical anomaly detection and does not\nrequire the implicit assumptions of the existing parameter-saliency\nformulation. Our experimental results demonstrate that our reformulation can\ndetect malicious filters as well. Furthermore, we show that the existing\nparameter saliency method exhibits a bias against the depth of layers in deep\nneural networks. In particular, this bias has the potential to inhibit the\ndiscovery of filters that cause misidentification in situations where domain\nshift occurs. In contrast, parameter saliency based on POT shows less of this\nbias.","terms":["cs.CV","cs.AI"]},{"titles":"Pseudo Replay-based Class Continual Learning for Online New Category Anomaly Detection in Additive Manufacturing","summaries":"The incorporation of advanced sensors and machine learning techniques has\nenabled modern manufacturing enterprises to perform data-driven in-situ quality\nmonitoring based on the sensor data collected in manufacturing processes.\nHowever, one critical challenge is that newly presented defect category may\nmanifest as the manufacturing process continues, resulting in monitoring\nperformance deterioration of previously trained machine learning models. Hence,\nthere is an increasing need for empowering machine learning model to learn\ncontinually. Among all continual learning methods, memory-based continual\nlearning has the best performance but faces the constraints of data storage\ncapacity. To address this issue, this paper develops a novel pseudo\nreplay-based continual learning by integrating class incremental learning and\noversampling-based data generation. Without storing all the data, the developed\nframework could generate high-quality data representing previous classes to\ntrain machine learning model incrementally when new category anomaly occurs. In\naddition, it could even enhance the monitoring performance since it also\neffectively improves the data quality. The effectiveness of the proposed\nframework is validated in an additive manufacturing process, which leverages\nsupervised classification problem for anomaly detection. The experimental\nresults show that the developed method is very promising in detecting novel\nanomaly while maintaining a good performance on the previous task and brings up\nmore flexibility in model architecture.","terms":["cs.LG","stat.ML"]},{"titles":"Constrained Twin Variational Auto-Encoder for Intrusion Detection in IoT Systems","summaries":"Intrusion detection systems (IDSs) play a critical role in protecting\nbillions of IoT devices from malicious attacks. However, the IDSs for IoT\ndevices face inherent challenges of IoT systems, including the heterogeneity of\nIoT data\/devices, the high dimensionality of training data, and the imbalanced\ndata. Moreover, the deployment of IDSs on IoT systems is challenging, and\nsometimes impossible, due to the limited resources such as memory\/storage and\ncomputing capability of typical IoT devices. To tackle these challenges, this\narticle proposes a novel deep neural network\/architecture called Constrained\nTwin Variational Auto-Encoder (CTVAE) that can feed classifiers of IDSs with\nmore separable\/distinguishable and lower-dimensional representation data.\nAdditionally, in comparison to the state-of-the-art neural networks used in\nIDSs, CTVAE requires less memory\/storage and computing power, hence making it\nmore suitable for IoT IDS systems. Extensive experiments with the 11 most\npopular IoT botnet datasets show that CTVAE can boost around 1% in terms of\naccuracy and Fscore in detection attack compared to the state-of-the-art\nmachine learning and representation learning methods, whilst the running time\nfor attack detection is lower than 2E-6 seconds and the model size is lower\nthan 1 MB. We also further investigate various characteristics of CTVAE in the\nlatent space and in the reconstruction representation to demonstrate its\nefficacy compared with current well-known methods.","terms":["cs.LG","cs.CR"]},{"titles":"EtC: Temporal Boundary Expand then Clarify for Weakly Supervised Video Grounding with Multimodal Large Language Model","summaries":"Early weakly supervised video grounding (WSVG) methods often struggle with\nincomplete boundary detection due to the absence of temporal boundary\nannotations. To bridge the gap between video-level and boundary-level\nannotation, explicit-supervision methods, i.e., generating pseudo-temporal\nboundaries for training, have achieved great success. However, data\naugmentations in these methods might disrupt critical temporal information,\nyielding poor pseudo boundaries. In this paper, we propose a new perspective\nthat maintains the integrity of the original temporal content while introducing\nmore valuable information for expanding the incomplete boundaries. To this end,\nwe propose EtC (Expand then Clarify), first use the additional information to\nexpand the initial incomplete pseudo boundaries, and subsequently refine these\nexpanded ones to achieve precise boundaries. Motivated by video continuity,\ni.e., visual similarity across adjacent frames, we use powerful multimodal\nlarge language models (MLLMs) to annotate each frame within initial pseudo\nboundaries, yielding more comprehensive descriptions for expanded boundaries.\nTo further clarify the noise of expanded boundaries, we combine mutual learning\nwith a tailored proposal-level contrastive objective to use a learnable\napproach to harmonize a balance between incomplete yet clean (initial) and\ncomprehensive yet noisy (expanded) boundaries for more precise ones.\nExperiments demonstrate the superiority of our method on two challenging WSVG\ndatasets.","terms":["cs.CV"]},{"titles":"Learning to Holistically Detect Bridges from Large-Size VHR Remote Sensing Imagery","summaries":"Bridge detection in remote sensing images (RSIs) plays a crucial role in\nvarious applications, but it poses unique challenges compared to the detection\nof other objects. In RSIs, bridges exhibit considerable variations in terms of\ntheir spatial scales and aspect ratios. Therefore, to ensure the visibility and\nintegrity of bridges, it is essential to perform holistic bridge detection in\nlarge-size very-high-resolution (VHR) RSIs. However, the lack of datasets with\nlarge-size VHR RSIs limits the deep learning algorithms' performance on bridge\ndetection. Due to the limitation of GPU memory in tackling large-size images,\ndeep learning-based object detection methods commonly adopt the cropping\nstrategy, which inevitably results in label fragmentation and discontinuous\nprediction. To ameliorate the scarcity of datasets, this paper proposes a\nlarge-scale dataset named GLH-Bridge comprising 6,000 VHR RSIs sampled from\ndiverse geographic locations across the globe. These images encompass a wide\nrange of sizes, varying from 2,048*2,048 to 16,38*16,384 pixels, and\ncollectively feature 59,737 bridges. Furthermore, we present an efficient\nnetwork for holistic bridge detection (HBD-Net) in large-size RSIs. The HBD-Net\npresents a separate detector-based feature fusion (SDFF) architecture and is\noptimized via a shape-sensitive sample re-weighting (SSRW) strategy. Based on\nthe proposed GLH-Bridge dataset, we establish a bridge detection benchmark\nincluding the OBB and HBB tasks, and validate the effectiveness of the proposed\nHBD-Net. Additionally, cross-dataset generalization experiments on two publicly\navailable datasets illustrate the strong generalization capability of the\nGLH-Bridge dataset.","terms":["cs.CV","cs.AI"]},{"titles":"Differentiable Point-based Inverse Rendering","summaries":"We present differentiable point-based inverse rendering, DPIR, an\nanalysis-by-synthesis method that processes images captured under diverse\nilluminations to estimate shape and spatially-varying BRDF. To this end, we\nadopt point-based rendering, eliminating the need for multiple samplings per\nray, typical of volumetric rendering, thus significantly enhancing the speed of\ninverse rendering. To realize this idea, we devise a hybrid point-volumetric\nrepresentation for geometry and a regularized basis-BRDF representation for\nreflectance. The hybrid geometric representation enables fast rendering through\npoint-based splatting while retaining the geometric details and stability\ninherent to SDF-based representations. The regularized basis-BRDF mitigates the\nill-posedness of inverse rendering stemming from limited light-view angular\nsamples. We also propose an efficient shadow detection method using point-based\nshadow map rendering. Our extensive evaluations demonstrate that DPIR\noutperforms prior works in terms of reconstruction accuracy, computational\nefficiency, and memory footprint. Furthermore, our explicit point-based\nrepresentation and rendering enables intuitive geometry and reflectance\nediting. The code will be publicly available.","terms":["cs.CV"]},{"titles":"NeutronStream: A Dynamic GNN Training Framework with Sliding Window for Graph Streams","summaries":"Existing Graph Neural Network (GNN) training frameworks have been designed to\nhelp developers easily create performant GNN implementations. However, most\nexisting GNN frameworks assume that the input graphs are static, but ignore\nthat most real-world graphs are constantly evolving. Though many dynamic GNN\nmodels have emerged to learn from evolving graphs, the training process of\nthese dynamic GNNs is dramatically different from traditional GNNs in that it\ncaptures both the spatial and temporal dependencies of graph updates. This\nposes new challenges for designing dynamic GNN training frameworks. First, the\ntraditional batched training method fails to capture real-time structural\nevolution information. Second, the time-dependent nature makes parallel\ntraining hard to design. Third, it lacks system supports for users to\nefficiently implement dynamic GNNs. In this paper, we present NeutronStream, a\nframework for training dynamic GNN models. NeutronStream abstracts the input\ndynamic graph into a chronologically updated stream of events and processes the\nstream with an optimized sliding window to incrementally capture the\nspatial-temporal dependencies of events. Furthermore, NeutronStream provides a\nparallel execution engine to tackle the sequential event processing challenge\nto achieve high performance. NeutronStream also integrates a built-in graph\nstorage structure that supports dynamic updates and provides a set of\neasy-to-use APIs that allow users to express their dynamic GNNs. Our\nexperimental results demonstrate that, compared to state-of-the-art dynamic GNN\nimplementations, NeutronStream achieves speedups ranging from 1.48X to 5.87X\nand an average accuracy improvement of 3.97%.","terms":["cs.LG","cs.DC"]},{"titles":"Signed Binary Weight Networks","summaries":"Efficient inference of Deep Neural Networks (DNNs) is essential to making AI\nubiquitous. Two important algorithmic techniques have shown promise for\nenabling efficient inference - sparsity and binarization. These techniques\ntranslate into weight sparsity and weight repetition at the hardware-software\nlevel enabling the deployment of DNNs with critically low power and latency\nrequirements. We propose a new method called signed-binary networks to improve\nefficiency further (by exploiting both weight sparsity and weight repetition\ntogether) while maintaining similar accuracy. Our method achieves comparable\naccuracy on ImageNet and CIFAR10 datasets with binary and can lead to 69%\nsparsity. We observe real speedup when deploying these models on\ngeneral-purpose devices and show that this high percentage of unstructured\nsparsity can lead to a further reduction in energy consumption on ASICs.","terms":["cs.CV","cs.DC","cs.PF"]},{"titles":"A Comprehensive Study of Vision Transformers in Image Classification Tasks","summaries":"Image Classification is a fundamental task in the field of computer vision\nthat frequently serves as a benchmark for gauging advancements in Computer\nVision. Over the past few years, significant progress has been made in image\nclassification due to the emergence of deep learning. However, challenges still\nexist, such as modeling fine-grained visual information, high computation\ncosts, the parallelism of the model, and inconsistent evaluation protocols\nacross datasets. In this paper, we conduct a comprehensive survey of existing\npapers on Vision Transformers for image classification. We first introduce the\npopular image classification datasets that influenced the design of models.\nThen, we present Vision Transformers models in chronological order, starting\nwith early attempts at adapting attention mechanism to vision tasks followed by\nthe adoption of vision transformers, as they have demonstrated success in\ncapturing intricate patterns and long-range dependencies within images.\nFinally, we discuss open problems and shed light on opportunities for image\nclassification to facilitate new research ideas.","terms":["cs.CV","cs.AI"]},{"titles":"Generator Born from Classifier","summaries":"In this paper, we make a bold attempt toward an ambitious task: given a\npre-trained classifier, we aim to reconstruct an image generator, without\nrelying on any data samples. From a black-box perspective, this challenge seems\nintractable, since it inevitably involves identifying the inverse function for\na classifier, which is, by nature, an information extraction process. As such,\nwe resort to leveraging the knowledge encapsulated within the parameters of the\nneural network. Grounded on the theory of Maximum-Margin Bias of gradient\ndescent, we propose a novel learning paradigm, in which the generator is\ntrained to ensure that the convergence conditions of the network parameters are\nsatisfied over the generated distribution of the samples. Empirical validation\nfrom various image generation tasks substantiates the efficacy of our strategy.","terms":["cs.LG","cs.CV"]},{"titles":"Learning Energy-based Model via Dual-MCMC Teaching","summaries":"This paper studies the fundamental learning problem of the energy-based model\n(EBM). Learning the EBM can be achieved using the maximum likelihood estimation\n(MLE), which typically involves the Markov Chain Monte Carlo (MCMC) sampling,\nsuch as the Langevin dynamics. However, the noise-initialized Langevin dynamics\ncan be challenging in practice and hard to mix. This motivates the exploration\nof joint training with the generator model where the generator model serves as\na complementary model to bypass MCMC sampling. However, such a method can be\nless accurate than the MCMC and result in biased EBM learning. While the\ngenerator can also serve as an initializer model for better MCMC sampling, its\nlearning can be biased since it only matches the EBM and has no access to\nempirical training examples. Such biased generator learning may limit the\npotential of learning the EBM. To address this issue, we present a joint\nlearning framework that interweaves the maximum likelihood learning algorithm\nfor both the EBM and the complementary generator model. In particular, the\ngenerator model is learned by MLE to match both the EBM and the empirical data\ndistribution, making it a more informative initializer for MCMC sampling of\nEBM. Learning generator with observed examples typically requires inference of\nthe generator posterior. To ensure accurate and efficient inference, we adopt\nthe MCMC posterior sampling and introduce a complementary inference model to\ninitialize such latent MCMC sampling. We show that three separate models can be\nseamlessly integrated into our joint framework through two (dual-) MCMC\nteaching, enabling effective and efficient EBM learning.","terms":["cs.LG","cs.CV","stat.CO"]},{"titles":"Free Lunch for Gait Recognition: A Novel Relation Descriptor","summaries":"Gait recognition is to seek correct matches for query individuals by their\nunique walking patterns. However, current methods focus solely on extracting\nindividual-specific features, overlooking ``interpersonal\" relationships. In\nthis paper, we propose a novel $\\textbf{Relation Descriptor}$ that captures not\nonly individual features but also relations between test gaits and pre-selected\ngait anchors. Specifically, we reinterpret classifier weights as gait anchors\nand compute similarity scores between test features and these anchors, which\nre-expresses individual gait features into a similarity relation distribution.\nIn essence, the relation descriptor offers a holistic perspective that\nleverages the collective knowledge stored within the classifier's weights,\nemphasizing meaningful patterns and enhancing robustness. Despite its\npotential, relation descriptor poses dimensionality challenges since its\ndimension depends on the training set's identity count. To address this, we\npropose Farthest gait-Anchor Selection to identify the most discriminative gait\nanchors and an Orthogonal Regularization Loss to increase diversity within gait\nanchors. Compared to individual-specific features extracted from the backbone,\nour relation descriptor can boost the performance nearly without any extra\ncosts. We evaluate the effectiveness of our method on the popular GREW, Gait3D,\nOU-MVLP, CASIA-B, and CCPG, showing that our method consistently outperforms\nthe baselines and achieves state-of-the-art performance.","terms":["cs.CV"]},{"titles":"SAM-Assisted Remote Sensing Imagery Semantic Segmentation with Object and Boundary Constraints","summaries":"Semantic segmentation of remote sensing imagery plays a pivotal role in\nextracting precise information for diverse down-stream applications. Recent\ndevelopment of the Segment Anything Model (SAM), an advanced general-purpose\nsegmentation model, has revolutionized this field, presenting new avenues for\naccurate and efficient segmentation. However, SAM is limited to generating\nsegmentation results without class information. Consequently, the utilization\nof such a powerful general vision model for semantic segmentation in remote\nsensing images has become a focal point of research. In this paper, we present\na streamlined framework aimed at leveraging the raw output of SAM by exploiting\ntwo novel concepts called SAM-Generated Object (SGO) and SAM-Generated Boundary\n(SGB). More specifically, we propose a novel object loss and further introduce\na boundary loss as augmentative components to aid in model optimization in a\ngeneral semantic segmentation framework. Taking into account the content\ncharacteristics of SGO, we introduce the concept of object consistency to\nleverage segmented regions lacking semantic information. By imposing\nconstraints on the consistency of predicted values within objects, the object\nloss aims to enhance semantic segmentation performance. Furthermore, the\nboundary loss capitalizes on the distinctive features of SGB by directing the\nmodel's attention to the boundary information of the object. Experimental\nresults on two well-known datasets, namely ISPRS Vaihingen and LoveDA Urban,\ndemonstrate the effectiveness of our proposed method. The source code for this\nwork will be accessible at https:\/\/github.com\/sstary\/SSRS.","terms":["cs.CV"]},{"titles":"Spatial-Temporal Enhanced Transformer Towards Multi-Frame 3D Object Detection","summaries":"The Detection Transformer (DETR) has revolutionized the design of CNN-based\nobject detection systems, showcasing impressive performance. However, its\npotential in the domain of multi-frame 3D object detection remains largely\nunexplored. In this paper, we present STEMD, a novel end-to-end framework for\nmulti-frame 3D object detection based on the DETR-like paradigm. STEMD treats\nmulti-frame 3D object detection as a sequence-to-sequence task and effectively\ncaptures spatial-temporal dependencies at both the feature and query levels.\nSpecifically, to model the inter-object spatial interaction and complex\ntemporal dependencies, we introduce the spatial-temporal graph attention\nnetwork, which represents queries as nodes in a graph and enables effective\nmodeling of object interactions within a social context. To solve the problem\nof missing hard cases in the proposed output of the encoder in the current\nframe, we incorporate the output of the previous frame to initialize the query\ninput of the decoder. Moreover, to mitigate the issue of redundant detection\nresults, where the model generates numerous overlapping boxes from similar\nqueries, we consider an IoU regularization term in the loss function, which can\ndistinguish between queries matched with the ground-truth box and queries that\nare similar but unmatched during the refinement process, leading to reduced\nredundancy and more accurate detections. Through extensive experiments, we\ndemonstrate the effectiveness of our approach in handling challenging\nscenarios, while incurring only a minor additional computational overhead. The\ncode is available at \\url{https:\/\/github.com\/Eaphan\/STEMD}.","terms":["cs.CV"]},{"titles":"Dimensionality Reduction and Dynamical Mode Recognition of Circular Arrays of Flame Oscillators Using Deep Neural Network","summaries":"Oscillatory combustion in aero engines and modern gas turbines often has\nsignificant adverse effects on their operation, and accurately recognizing\nvarious oscillation modes is the prerequisite for understanding and controlling\ncombustion instability. However, the high-dimensional spatial-temporal data of\na complex combustion system typically poses considerable challenges to the\ndynamical mode recognition. Based on a two-layer bidirectional long short-term\nmemory variational autoencoder (Bi-LSTM-VAE) dimensionality reduction model and\na two-dimensional Wasserstein distance-based classifier (WDC), this study\nproposes a promising method (Bi-LSTM-VAE-WDC) for recognizing dynamical modes\nin oscillatory combustion systems. Specifically, the Bi-LSTM-VAE dimension\nreduction model was introduced to reduce the high-dimensional spatial-temporal\ndata of the combustion system to a low-dimensional phase space; Gaussian kernel\ndensity estimates (GKDE) were computed based on the distribution of phase\npoints in a grid; two-dimensional WD values were calculated from the GKDE maps\nto recognize the oscillation modes. The time-series data used in this study\nwere obtained from numerical simulations of circular arrays of laminar flame\noscillators. The results show that the novel Bi-LSTM-VAE method can produce a\nnon-overlapping distribution of phase points, indicating an effective\nunsupervised mode recognition and classification. Furthermore, the present\nmethod exhibits a more prominent performance than VAE and PCA (principal\ncomponent analysis) for distinguishing dynamical modes in complex flame\nsystems, implying its potential in studying turbulent combustion.","terms":["cs.LG","physics.flu-dyn"]},{"titles":"GSDC Transformer: An Efficient and Effective Cue Fusion for Monocular Multi-Frame Depth Estimation","summaries":"Depth estimation provides an alternative approach for perceiving 3D\ninformation in autonomous driving. Monocular depth estimation, whether with\nsingle-frame or multi-frame inputs, has achieved significant success by\nlearning various types of cues and specializing in either static or dynamic\nscenes. Recently, these cues fusion becomes an attractive topic, aiming to\nenable the combined cues to perform well in both types of scenes. However,\nadaptive cue fusion relies on attention mechanisms, where the quadratic\ncomplexity limits the granularity of cue representation. Additionally, explicit\ncue fusion depends on precise segmentation, which imposes a heavy burden on\nmask prediction. To address these issues, we propose the GSDC Transformer, an\nefficient and effective component for cue fusion in monocular multi-frame depth\nestimation. We utilize deformable attention to learn cue relationships at a\nfine scale, while sparse attention reduces computational requirements when\ngranularity increases. To compensate for the precision drop in dynamic scenes,\nwe represent scene attributes in the form of super tokens without relying on\nprecise shapes. Within each super token attributed to dynamic scenes, we gather\nits relevant cues and learn local dense relationships to enhance cue fusion.\nOur method achieves state-of-the-art performance on the KITTI dataset with\nefficient fusion speed.","terms":["cs.CV"]},{"titles":"DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance","summaries":"Image-to-video generation, which aims to generate a video starting from a\ngiven reference image, has drawn great attention. Existing methods try to\nextend pre-trained text-guided image diffusion models to image-guided video\ngeneration models. Nevertheless, these methods often result in either low\nfidelity or flickering over time due to their limitation to shallow image\nguidance and poor temporal consistency. To tackle these problems, we propose a\nhigh-fidelity image-to-video generation method by devising a frame retention\nbranch on the basis of a pre-trained video diffusion model, named DreamVideo.\nInstead of integrating the reference image into the diffusion process in a\nsemantic level, our DreamVideo perceives the reference image via convolution\nlayers and concatenate the features with the noisy latents as model input. By\nthis means, the details of the reference image can be preserved to the greatest\nextent. In addition, by incorporating double-condition classifier-free\nguidance, a single image can be directed to videos of different actions by\nproviding varying prompt texts. This has significant implications for\ncontrollable video generation and holds broad application prospects. We conduct\ncomprehensive experiments on the public dataset, both quantitative and\nqualitative results indicate that our method outperforms the state-of-the-art\nmethod. Especially for fidelity, our model has powerful image retention ability\nand result in high FVD in UCF101 compared to other image-to-video models. Also,\nprecise control can be achieved by giving different text prompts. Further\ndetails and comprehensive results of our model will be presented in\nhttps:\/\/anonymous0769.github.io\/DreamVideo\/.","terms":["cs.CV"]},{"titles":"PG-VTON: A Novel Image-Based Virtual Try-On Method via Progressive Inference Paradigm","summaries":"Virtual try-on is a promising computer vision topic with a high commercial\nvalue wherein a new garment is visually worn on a person with a photo-realistic\neffect. Previous studies conduct their shape and content inference at one\nstage, employing a single-scale warping mechanism and a relatively\nunsophisticated content inference mechanism. These approaches have led to\nsuboptimal results in terms of garment warping and skin reservation under\nchallenging try-on scenarios. To address these limitations, we propose a novel\nvirtual try-on method via progressive inference paradigm (PGVTON) that\nleverages a top-down inference pipeline and a general garment try-on strategy.\nSpecifically, we propose a robust try-on parsing inference method by\ndisentangling semantic categories and introducing consistency. Exploiting the\ntry-on parsing as the shape guidance, we implement the garment try-on via\nwarping-mapping-composition. To facilitate adaptation to a wide range of try-on\nscenarios, we adopt a covering more and selecting one warping strategy and\nexplicitly distinguish tasks based on alignment. Additionally, we regulate\nStyleGAN2 to implement re-naked skin inpainting, conditioned on the target skin\nshape and spatial-agnostic skin features. Experiments demonstrate that our\nmethod has state-of-the-art performance under two challenging scenarios. The\ncode will be available at https:\/\/github.com\/NerdFNY\/PGVTON.","terms":["cs.CV"]},{"titles":"GIT-Net: Generalized Integral Transform for Operator Learning","summaries":"This article introduces GIT-Net, a deep neural network architecture for\napproximating Partial Differential Equation (PDE) operators, inspired by\nintegral transform operators. GIT-NET harnesses the fact that differential\noperators commonly used for defining PDEs can often be represented\nparsimoniously when expressed in specialized functional bases (e.g., Fourier\nbasis). Unlike rigid integral transforms, GIT-Net parametrizes adaptive\ngeneralized integral transforms with deep neural networks. When compared to\nseveral recently proposed alternatives, GIT-Net's computational and memory\nrequirements scale gracefully with mesh discretizations, facilitating its\napplication to PDE problems on complex geometries. Numerical experiments\ndemonstrate that GIT-Net is a competitive neural network operator, exhibiting\nsmall test errors and low evaluations across a range of PDE problems. This\nstands in contrast to existing neural network operators, which typically excel\nin just one of these areas.","terms":["stat.ML","cs.LG"]},{"titles":"Personalized Video Relighting With an At-Home Light Stage","summaries":"In this paper, we develop a personalized video relighting algorithm that\nproduces high-quality and temporally consistent relit videos under any pose,\nexpression, and lighting condition in real-time. Existing relighting algorithms\ntypically rely either on publicly available synthetic data, which yields poor\nrelighting results, or instead on light stage data which is difficult to\nobtain. We show that by just capturing video of a user watching YouTube videos\non a monitor we can train a personalized algorithm capable of performing\nhigh-quality relighting under any condition. Our key contribution is a novel\nneural relighting architecture that effectively separates the intrinsic\nappearance features - the geometry and reflectance of the face - from the\nsource lighting and then combines them with the target lighting to generate a\nrelit image. This neural network architecture enables smoothing of intrinsic\nappearance features leading to temporally stable video relighting. Both\nqualitative and quantitative evaluations show that our architecture improves\nportrait image relighting quality and temporal consistency over\nstate-of-the-art approaches on both casually captured `Light Stage at Your\nDesk' (LSYD) and light-stage-captured `One Light At a Time' (OLAT) datasets.","terms":["cs.CV","cs.GR"]},{"titles":"Revisiting Computer-Aided Tuberculosis Diagnosis","summaries":"Tuberculosis (TB) is a major global health threat, causing millions of deaths\nannually. Although early diagnosis and treatment can greatly improve the\nchances of survival, it remains a major challenge, especially in developing\ncountries. Recently, computer-aided tuberculosis diagnosis (CTD) using deep\nlearning has shown promise, but progress is hindered by limited training data.\nTo address this, we establish a large-scale dataset, namely the Tuberculosis\nX-ray (TBX11K) dataset, which contains 11,200 chest X-ray (CXR) images with\ncorresponding bounding box annotations for TB areas. This dataset enables the\ntraining of sophisticated detectors for high-quality CTD. Furthermore, we\npropose a strong baseline, SymFormer, for simultaneous CXR image classification\nand TB infection area detection. SymFormer incorporates Symmetric Search\nAttention (SymAttention) to tackle the bilateral symmetry property of CXR\nimages for learning discriminative features. Since CXR images may not strictly\nadhere to the bilateral symmetry property, we also propose Symmetric Positional\nEncoding (SPE) to facilitate SymAttention through feature recalibration. To\npromote future research on CTD, we build a benchmark by introducing evaluation\nmetrics, evaluating baseline models reformed from existing detectors, and\nrunning an online challenge. Experiments show that SymFormer achieves\nstate-of-the-art performance on the TBX11K dataset. The data, code, and models\nwill be released at https:\/\/github.com\/yun-liu\/Tuberculosis.","terms":["cs.CV"]},{"titles":"Adaptive Instrument Design for Indirect Experiments","summaries":"Indirect experiments provide a valuable framework for estimating treatment\neffects in situations where conducting randomized control trials (RCTs) is\nimpractical or unethical. Unlike RCTs, indirect experiments estimate treatment\neffects by leveraging (conditional) instrumental variables, enabling estimation\nthrough encouragement and recommendation rather than strict treatment\nassignment. However, the sample efficiency of such estimators depends not only\non the inherent variability in outcomes but also on the varying compliance\nlevels of users with the instrumental variables and the choice of estimator\nbeing used, especially when dealing with numerous instrumental variables. While\nadaptive experiment design has a rich literature for direct experiments, in\nthis paper we take the initial steps towards enhancing sample efficiency for\nindirect experiments by adaptively designing a data collection policy over\ninstrumental variables. Our main contribution is a practical computational\nprocedure that utilizes influence functions to search for an optimal data\ncollection policy, minimizing the mean-squared error of the desired\n(non-linear) estimator. Through experiments conducted in various domains\ninspired by real-world applications, we showcase how our method can\nsignificantly improve the sample efficiency of indirect experiments.","terms":["cs.LG"]},{"titles":"GDN: A Stacking Network Used for Skin Cancer Diagnosis","summaries":"Skin cancer, the primary type of cancer that can be identified by visual\nrecognition, requires an automatic identification system that can accurately\nclassify different types of lesions. This paper presents GoogLe-Dense Network\n(GDN), which is an image-classification model to identify two types of skin\ncancer, Basal Cell Carcinoma, and Melanoma. GDN uses stacking of different\nnetworks to enhance the model performance. Specifically, GDN consists of two\nsequential levels in its structure. The first level performs basic\nclassification tasks accomplished by GoogLeNet and DenseNet, which are trained\nin parallel to enhance efficiency. To avoid low accuracy and long training\ntime, the second level takes the output of the GoogLeNet and DenseNet as the\ninput for a logistic regression model. We compare our method with four baseline\nnetworks including ResNet, VGGNet, DenseNet, and GoogLeNet on the dataset, in\nwhich GoogLeNet and DenseNet significantly outperform ResNet and VGGNet. In the\nsecond level, different stacking methods such as perceptron, logistic\nregression, SVM, decision trees and K-neighbor are studied in which Logistic\nRegression shows the best prediction result among all. The results prove that\nGDN, compared to a single network structure, has higher accuracy in optimizing\nskin cancer detection.","terms":["cs.CV"]},{"titles":"FINER: Flexible spectral-bias tuning in Implicit NEural Representation by Variable-periodic Activation Functions","summaries":"Implicit Neural Representation (INR), which utilizes a neural network to map\ncoordinate inputs to corresponding attributes, is causing a revolution in the\nfield of signal processing. However, current INR techniques suffer from a\nrestricted capability to tune their supported frequency set, resulting in\nimperfect performance when representing complex signals with multiple\nfrequencies. We have identified that this frequency-related problem can be\ngreatly alleviated by introducing variable-periodic activation functions, for\nwhich we propose FINER. By initializing the bias of the neural network within\ndifferent ranges, sub-functions with various frequencies in the\nvariable-periodic function are selected for activation. Consequently, the\nsupported frequency set of FINER can be flexibly tuned, leading to improved\nperformance in signal representation. We demonstrate the capabilities of FINER\nin the contexts of 2D image fitting, 3D signed distance field representation,\nand 5D neural radiance fields optimization, and we show that it outperforms\nexisting INRs.","terms":["cs.CV"]},{"titles":"Lenna: Language Enhanced Reasoning Detection Assistant","summaries":"With the fast-paced development of multimodal large language models (MLLMs),\nwe can now converse with AI systems in natural languages to understand images.\nHowever, the reasoning power and world knowledge embedded in the large language\nmodels have been much less investigated and exploited for image perception\ntasks. In this paper, we propose Lenna, a language-enhanced reasoning detection\nassistant, which utilizes the robust multimodal feature representation of\nMLLMs, while preserving location information for detection. This is achieved by\nincorporating an additional <DET> token in the MLLM vocabulary that is free of\nexplicit semantic context but serves as a prompt for the detector to identify\nthe corresponding position. To evaluate the reasoning capability of Lenna, we\nconstruct a ReasonDet dataset to measure its performance on reasoning-based\ndetection. Remarkably, Lenna demonstrates outstanding performance on ReasonDet\nand comes with significantly low training costs. It also incurs minimal\ntransferring overhead when extended to other tasks. Our code and model will be\navailable at https:\/\/git.io\/Lenna.","terms":["cs.CV"]},{"titles":"Orthogonal Adaptation for Modular Customization of Diffusion Models","summaries":"Customization techniques for text-to-image models have paved the way for a\nwide range of previously unattainable applications, enabling the generation of\nspecific concepts across diverse contexts and styles. While existing methods\nfacilitate high-fidelity customization for individual concepts or a limited,\npre-defined set of them, they fall short of achieving scalability, where a\nsingle model can seamlessly render countless concepts. In this paper, we\naddress a new problem called Modular Customization, with the goal of\nefficiently merging customized models that were fine-tuned independently for\nindividual concepts. This allows the merged model to jointly synthesize\nconcepts in one image without compromising fidelity or incurring any additional\ncomputational costs.\n  To address this problem, we introduce Orthogonal Adaptation, a method\ndesigned to encourage the customized models, which do not have access to each\nother during fine-tuning, to have orthogonal residual weights. This ensures\nthat during inference time, the customized models can be summed with minimal\ninterference.\n  Our proposed method is both simple and versatile, applicable to nearly all\noptimizable weights in the model architecture. Through an extensive set of\nquantitative and qualitative evaluations, our method consistently outperforms\nrelevant baselines in terms of efficiency and identity preservation,\ndemonstrating a significant leap toward scalable customization of diffusion\nmodels.","terms":["cs.CV"]},{"titles":"FreestyleRet: Retrieving Images from Style-Diversified Queries","summaries":"Image Retrieval aims to retrieve corresponding images based on a given query.\nIn application scenarios, users intend to express their retrieval intent\nthrough various query styles. However, current retrieval tasks predominantly\nfocus on text-query retrieval exploration, leading to limited retrieval query\noptions and potential ambiguity or bias in user intention. In this paper, we\npropose the Style-Diversified Query-Based Image Retrieval task, which enables\nretrieval based on various query styles. To facilitate the novel setting, we\npropose the first Diverse-Style Retrieval dataset, encompassing diverse query\nstyles including text, sketch, low-resolution, and art. We also propose a\nlight-weighted style-diversified retrieval framework. For various query style\ninputs, we apply the Gram Matrix to extract the query's textural features and\ncluster them into a style space with style-specific bases. Then we employ the\nstyle-init prompt tuning module to enable the visual encoder to comprehend the\ntexture and style information of the query. Experiments demonstrate that our\nmodel, employing the style-init prompt tuning strategy, outperforms existing\nretrieval models on the style-diversified retrieval task. Moreover,\nstyle-diversified queries~(sketch+text, art+text, etc) can be simultaneously\nretrieved in our model. The auxiliary information from other queries enhances\nthe retrieval performance within the respective query.","terms":["cs.CV","cs.IR"]},{"titles":"CRAFT: Contextual Re-Activation of Filters for face recognition Training","summaries":"The first layer of a deep CNN backbone applies filters to an image to extract\nthe basic features available to later layers. During training, some filters may\ngo inactive, mean ing all weights in the filter approach zero. An inactive fil\nter in the final model represents a missed opportunity to extract a useful\nfeature. This phenomenon is especially prevalent in specialized CNNs such as\nfor face recogni tion (as opposed to, e.g., ImageNet). For example, in one the\nmost widely face recognition model (ArcFace), about half of the convolution\nfilters in the first layer are inactive. We propose a novel approach designed\nand tested specif ically for face recognition networks, known as \"CRAFT:\nContextual Re-Activation of Filters for Face Recognition Training\". CRAFT\nidentifies inactive filters during training and reinitializes them based on the\ncontext of strong filters at that stage in training. We show that CRAFT reduces\nfraction of inactive filters from 44% to 32% on average and discovers filter\npatterns not found by standard training. Compared to standard training without\nreactivation, CRAFT demonstrates enhanced model accuracy on standard\nface-recognition benchmark datasets including AgeDB-30, CPLFW, LFW, CALFW, and\nCFP-FP, as well as on more challenging datasets like IJBB and IJBC.","terms":["cs.CV"]},{"titles":"Real-Time Surface-to-Air Missile Engagement Zone Prediction Using Simulation and Machine Learning","summaries":"Surface-to-Air Missiles (SAMs) are crucial in modern air defense systems. A\ncritical aspect of their effectiveness is the Engagement Zone (EZ), the spatial\nregion within which a SAM can effectively engage and neutralize a target.\nNotably, the EZ is intrinsically related to the missile's maximum range; it\ndefines the furthest distance at which a missile can intercept a target. The\naccurate computation of this EZ is essential but challenging due to the dynamic\nand complex factors involved, which often lead to high computational costs and\nextended processing times when using conventional simulation methods. In light\nof these challenges, our study investigates the potential of machine learning\ntechniques, proposing an approach that integrates machine learning with a\ncustom-designed simulation tool to train supervised algorithms. We leverage a\ncomprehensive dataset of pre-computed SAM EZ simulations, enabling our model to\naccurately predict the SAM EZ for new input parameters. It accelerates SAM EZ\nsimulations, enhances air defense strategic planning, and provides real-time\ninsights, improving SAM system performance. The study also includes a\ncomparative analysis of machine learning algorithms, illuminating their\ncapabilities and performance metrics and suggesting areas for future research,\nhighlighting the transformative potential of machine learning in SAM EZ\nsimulations.","terms":["cs.LG","cs.RO"]},{"titles":"AI-driven emergence of frequency information non-uniform distribution via THz metasurface spectrum prediction","summaries":"Recently, artificial intelligence has been extensively deployed across\nvarious scientific disciplines, optimizing and guiding the progression of\nexperiments through the integration of abundant datasets, whilst continuously\nprobing the vast theoretical space encapsulated within the data. Particularly,\ndeep learning models, due to their end-to-end adaptive learning capabilities,\nare capable of autonomously learning intrinsic data features, thereby\ntranscending the limitations of traditional experience to a certain extent.\nHere, we unveil previously unreported information characteristics pertaining to\ndifferent frequencies emerged during our work on predicting the terahertz\nspectral modulation effects of metasurfaces based on AI-prediction. Moreover,\nwe have substantiated that our proposed methodology of simply adding\nsupplementary multi-frequency inputs to the existing dataset during the target\nspectral prediction process can significantly enhance the predictive accuracy\nof the network. This approach effectively optimizes the utilization of existing\ndatasets and paves the way for interdisciplinary research and applications in\nartificial intelligence, chemistry, composite material design, biomedicine, and\nother fields.","terms":["cs.LG","physics.optics"]},{"titles":"Towards Granularity-adjusted Pixel-level Semantic Annotation","summaries":"Recent advancements in computer vision predominantly rely on learning-based\nsystems, leveraging annotations as the driving force to develop specialized\nmodels. However, annotating pixel-level information, particularly in semantic\nsegmentation, presents a challenging and labor-intensive task, prompting the\nneed for autonomous processes. In this work, we propose GranSAM which\ndistinguishes itself by providing semantic segmentation at the user-defined\ngranularity level on unlabeled data without the need for any manual\nsupervision, offering a unique contribution in the realm of semantic mask\nannotation method. Specifically, we propose an approach to enable the Segment\nAnything Model (SAM) with semantic recognition capability to generate\npixel-level annotations for images without any manual supervision. For this, we\naccumulate semantic information from synthetic images generated by the Stable\nDiffusion model or web crawled images and employ this data to learn a mapping\nfunction between SAM mask embeddings and object class labels. As a result, SAM,\nenabled with granularity-adjusted mask recognition, can be used for pixel-level\nsemantic annotation purposes. We conducted experiments on the PASCAL VOC 2012\nand COCO-80 datasets and observed a +17.95% and +5.17% increase in mIoU,\nrespectively, compared to existing state-of-the-art methods when evaluated\nunder our problem setting.","terms":["cs.CV"]},{"titles":"PartSLIP++: Enhancing Low-Shot 3D Part Segmentation via Multi-View Instance Segmentation and Maximum Likelihood Estimation","summaries":"Open-world 3D part segmentation is pivotal in diverse applications such as\nrobotics and AR\/VR. Traditional supervised methods often grapple with limited\n3D data availability and struggle to generalize to unseen object categories.\nPartSLIP, a recent advancement, has made significant strides in zero- and\nfew-shot 3D part segmentation. This is achieved by harnessing the capabilities\nof the 2D open-vocabulary detection module, GLIP, and introducing a heuristic\nmethod for converting and lifting multi-view 2D bounding box predictions into\n3D segmentation masks. In this paper, we introduce PartSLIP++, an enhanced\nversion designed to overcome the limitations of its predecessor. Our approach\nincorporates two major improvements. First, we utilize a pre-trained 2D\nsegmentation model, SAM, to produce pixel-wise 2D segmentations, yielding more\nprecise and accurate annotations than the 2D bounding boxes used in PartSLIP.\nSecond, PartSLIP++ replaces the heuristic 3D conversion process with an\ninnovative modified Expectation-Maximization algorithm. This algorithm\nconceptualizes 3D instance segmentation as unobserved latent variables, and\nthen iteratively refines them through an alternating process of 2D-3D matching\nand optimization with gradient descent. Through extensive evaluations, we show\nthat PartSLIP++ demonstrates better performance over PartSLIP in both low-shot\n3D semantic and instance-based object part segmentation tasks. Code released at\nhttps:\/\/github.com\/zyc00\/PartSLIP2.","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"Towards Fast and Stable Federated Learning: Confronting Heterogeneity via Knowledge Anchor","summaries":"Federated learning encounters a critical challenge of data heterogeneity,\nadversely affecting the performance and convergence of the federated model.\nVarious approaches have been proposed to address this issue, yet their\neffectiveness is still limited. Recent studies have revealed that the federated\nmodel suffers severe forgetting in local training, leading to global forgetting\nand performance degradation. Although the analysis provides valuable insights,\na comprehensive understanding of the vulnerable classes and their impact\nfactors is yet to be established. In this paper, we aim to bridge this gap by\nsystematically analyzing the forgetting degree of each class during local\ntraining across different communication rounds. Our observations are: (1) Both\nmissing and non-dominant classes suffer similar severe forgetting during local\ntraining, while dominant classes show improvement in performance. (2) When\ndynamically reducing the sample size of a dominant class, catastrophic\nforgetting occurs abruptly when the proportion of its samples is below a\ncertain threshold, indicating that the local model struggles to leverage a few\nsamples of a specific class effectively to prevent forgetting. Motivated by\nthese findings, we propose a novel and straightforward algorithm called\nFederated Knowledge Anchor (FedKA). Assuming that all clients have a single\nshared sample for each class, the knowledge anchor is constructed before each\nlocal training stage by extracting shared samples for missing classes and\nrandomly selecting one sample per class for non-dominant classes. The knowledge\nanchor is then utilized to correct the gradient of each mini-batch towards the\ndirection of preserving the knowledge of the missing and non-dominant classes.\nExtensive experimental results demonstrate that our proposed FedKA achieves\nfast and stable convergence, significantly improving accuracy on popular\nbenchmarks.","terms":["cs.LG","cs.AI","68T99"]},{"titles":"Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey","summaries":"As artificial intelligence (AI) continues to rapidly evolve, the realm of\nEarth and atmospheric sciences is increasingly adopting data-driven models,\npowered by progressive developments in deep learning (DL). Specifically, DL\ntechniques are extensively utilized to decode the chaotic and nonlinear aspects\nof Earth systems, and to address climate challenges via understanding weather\nand climate data. Cutting-edge performance on specific tasks within narrower\nspatio-temporal scales has been achieved recently through DL. The rise of large\nmodels, specifically large language models (LLMs), has enabled fine-tuning\nprocesses that yield remarkable outcomes across various downstream tasks,\nthereby propelling the advancement of general AI. However, we are still\nnavigating the initial stages of crafting general AI for weather and climate.\nIn this survey, we offer an exhaustive, timely overview of state-of-the-art AI\nmethodologies specifically engineered for weather and climate data, with a\nspecial focus on time series and text data. Our primary coverage encompasses\nfour critical aspects: types of weather and climate data, principal model\narchitectures, model scopes and applications, and datasets for weather and\nclimate. Furthermore, in relation to the creation and application of foundation\nmodels for weather and climate data understanding, we delve into the field's\nprevailing challenges, offer crucial insights, and propose detailed avenues for\nfuture research. This comprehensive approach equips practitioners with the\nrequisite knowledge to make substantial progress in this domain. Our survey\nencapsulates the most recent breakthroughs in research on large, data-driven\nmodels for weather and climate data understanding, emphasizing robust\nfoundations, current advancements, practical applications, crucial resources,\nand prospective research opportunities.","terms":["cs.LG","cs.AI","cs.CV","physics.ao-ph"]},{"titles":"MGTR: Multi-Granular Transformer for Motion Prediction with LiDAR","summaries":"Motion prediction has been an essential component of autonomous driving\nsystems since it handles highly uncertain and complex scenarios involving\nmoving agents of different types. In this paper, we propose a Multi-Granular\nTRansformer (MGTR) framework, an encoder-decoder network that exploits context\nfeatures in different granularities for different kinds of traffic agents. To\nfurther enhance MGTR's capabilities, we leverage LiDAR point cloud data by\nincorporating LiDAR semantic features from an off-the-shelf LiDAR feature\nextractor. We evaluate MGTR on Waymo Open Dataset motion prediction benchmark\nand show that the proposed method achieved state-of-the-art performance,\nranking 1st on its leaderboard\n(https:\/\/waymo.com\/open\/challenges\/2023\/motion-prediction\/).","terms":["cs.CV","cs.RO"]},{"titles":"Robust Clustering using Hyperdimensional Computing","summaries":"This paper addresses the clustering of data in the hyperdimensional computing\n(HDC) domain. In prior work, an HDC-based clustering framework, referred to as\nHDCluster, has been proposed. However, the performance of the existing\nHDCluster is not robust. The performance of HDCluster is degraded as the\nhypervectors for the clusters are chosen at random during the initialization\nstep. To overcome this bottleneck, we assign the initial cluster hypervectors\nby exploring the similarity of the encoded data, referred to as \\textit{query}\nhypervectors. Intra-cluster hypervectors have a higher similarity than\ninter-cluster hypervectors. Harnessing the similarity results among query\nhypervectors, this paper proposes four HDC-based clustering algorithms:\nsimilarity-based k-means, equal bin-width histogram, equal bin-height\nhistogram, and similarity-based affinity propagation. Experimental results\nillustrate that: (i) Compared to the existing HDCluster, our proposed HDC-based\nclustering algorithms can achieve better accuracy, more robust performance,\nfewer iterations, and less execution time. Similarity-based affinity\npropagation outperforms the other three HDC-based clustering algorithms on\neight datasets by 2~38% in clustering accuracy. (ii) Even for one-pass\nclustering, i.e., without any iterative update of the cluster hypervectors, our\nproposed algorithms can provide more robust clustering accuracy than HDCluster.\n(iii) Over eight datasets, five out of eight can achieve higher or comparable\naccuracy when projected onto the hyperdimensional space. Traditional clustering\nis more desirable than HDC when the number of clusters, $k$, is large.","terms":["cs.LG","cs.DB","cs.SC"]},{"titles":"Neural Priming for Sample-Efficient Adaptation","summaries":"We propose Neural Priming, a technique for adapting large pretrained models\nto distribution shifts and downstream tasks given few or no labeled examples.\nPresented with class names or unlabeled test samples, Neural Priming enables\nthe model to recall and conditions its parameters on relevant data seen\nthroughout pretraining, thereby priming it for the test distribution. Neural\nPriming can be performed at test time, even for pretraining datasets as large\nas LAION-2B. Performing lightweight updates on the recalled data significantly\nimproves accuracy across a variety of distribution shift and transfer learning\nbenchmarks. Concretely, in the zero-shot setting, we see a 2.45% improvement in\naccuracy on ImageNet and 3.81% accuracy improvement on average across standard\ntransfer learning benchmarks. Further, using Neural Priming at inference to\nadapt to distribution shift, we see a 1.41% accuracy improvement on ImageNetV2.\nThese results demonstrate the effectiveness of Neural Priming in addressing the\nchallenge of limited labeled data and changing distributions. Code is available\nat github.com\/RAIVNLab\/neural-priming.","terms":["cs.LG","cs.AI","cs.CV"]},{"titles":"MOTOR: A Time-To-Event Foundation Model For Structured Medical Records","summaries":"We present a self-supervised, time-to-event (TTE) foundation model called\nMOTOR (Many Outcome Time Oriented Representations) which is pretrained on\ntimestamped sequences of events in electronic health records (EHR) and health\ninsurance claims. TTE models are used for estimating the probability\ndistribution of the time until a specific event occurs, which is an important\ntask in medical settings. TTE models provide many advantages over\nclassification using fixed time horizons, including naturally handling censored\nobservations, but are challenging to train with limited labeled data. MOTOR\naddresses this challenge by pretraining on up to 55M patient records (9B\nclinical events). We evaluate MOTOR's transfer learning performance on 19\ntasks, across 3 patient databases (a private EHR system, MIMIC-IV, and Merative\nclaims data). Task-specific models adapted from MOTOR improve time-dependent C\nstatistics by 4.6% over state-of-the-art, improve label efficiency by up to 95%\n,and are more robust to temporal distributional shifts. We further evaluate\ncross-site portability by adapting our MOTOR foundation model for six\nprediction tasks on the MIMIC-IV dataset, where it outperforms all baselines.\nMOTOR is the first foundation model for medical TTE predictions and we release\na 143M parameter pretrained model for research use at [redacted URL].","terms":["cs.LG"]},{"titles":"Harmonizing Global Voices: Culturally-Aware Models for Enhanced Content Moderation","summaries":"Content moderation at scale faces the challenge of considering local cultural\ndistinctions when assessing content. While global policies aim to maintain\ndecision-making consistency and prevent arbitrary rule enforcement, they often\noverlook regional variations in interpreting natural language as expressed in\ncontent. In this study, we are looking into how moderation systems can tackle\nthis issue by adapting to local comprehension nuances. We train large language\nmodels on extensive datasets of media news and articles to create culturally\nattuned models. The latter aim to capture the nuances of communication across\ngeographies with the goal of recognizing cultural and societal variations in\nwhat is considered offensive content. We further explore the capability of\nthese models to generate explanations for instances of content violation,\naiming to shed light on how policy guidelines are perceived when cultural and\nsocietal contexts change. We find that training on extensive media datasets\nsuccessfully induced cultural awareness and resulted in improvements in\nhandling content violations on a regional basis. Additionally, these\nadvancements include the ability to provide explanations that align with the\nspecific local norms and nuances as evidenced by the annotators' preference in\nour conducted study. This multifaceted success reinforces the critical role of\nan adaptable content moderation approach in keeping pace with the ever-evolving\nnature of the content it oversees.","terms":["stat.ML","cs.LG","cs.SI"]},{"titles":"Auto DP-SGD: Dual Improvements of Privacy and Accuracy via Automatic Clipping Threshold and Noise Multiplier Estimation","summaries":"DP-SGD has emerged as a popular method to protect personally identifiable\ninformation in deep learning applications. Unfortunately, DP-SGD's per-sample\ngradient clipping and uniform noise addition during training can significantly\ndegrade model utility. To enhance the model's utility, researchers proposed\nvarious adaptive DP-SGD methods. However, we examine and discover that these\ntechniques result in greater privacy leakage or lower accuracy than the\ntraditional DP-SGD method, or a lack of evaluation on a complex data set such\nas CIFAR100. To address these limitations, we propose an Auto DP-SGD. Our\nmethod automates clipping threshold estimation based on the DL model's gradient\nnorm and scales the gradients of each training sample without losing gradient\ninformation. This helps to improve the algorithm's utility while using a less\nprivacy budget. To further improve accuracy, we introduce automatic noise\nmultiplier decay mechanisms to decrease the noise multiplier after every epoch.\nFinally, we develop closed-form mathematical expressions using tCDP accountant\nfor automatic noise multiplier and automatic clipping threshold estimation.\nThrough extensive experimentation, we demonstrate that Auto DP-SGD outperforms\nexisting SOTA DP-SGD methods in privacy and accuracy on various benchmark\ndatasets. We also show that privacy can be improved by lowering the scale\nfactor and using learning rate schedulers without significantly reducing\naccuracy. Specifically, Auto DP-SGD, when used with a step noise multiplier,\nimproves accuracy by 3.20, 1.57, 6.73, and 1.42 for the MNIST, CIFAR10,\nCIFAR100, and AG News Corpus datasets, respectively. Furthermore, it obtains a\nsubstantial reduction in the privacy budget of 94.9, 79.16, 67.36, and 53.37\nfor the corresponding data sets.","terms":["cs.LG","cs.CR","26, 40"]},{"titles":"A Multi-In-Single-Out Network for Video Frame Interpolation without Optical Flow","summaries":"In general, deep learning-based video frame interpolation (VFI) methods have\npredominantly focused on estimating motion vectors between two input frames and\nwarping them to the target time. While this approach has shown impressive\nperformance for linear motion between two input frames, it exhibits limitations\nwhen dealing with occlusions and nonlinear movements. Recently, generative\nmodels have been applied to VFI to address these issues. However, as VFI is not\na task focused on generating plausible images, but rather on predicting\naccurate intermediate frames between two given frames, performance limitations\nstill persist. In this paper, we propose a multi-in-single-out (MISO) based VFI\nmethod that does not rely on motion vector estimation, allowing it to\neffectively model occlusions and nonlinear motion. Additionally, we introduce a\nnovel motion perceptual loss that enables MISO-VFI to better capture the\nspatio-temporal correlations within the video frames. Our MISO-VFI method\nachieves state-of-the-art results on VFI benchmarks Vimeo90K, Middlebury, and\nUCF101, with a significant performance gap compared to existing approaches.","terms":["cs.CV","cs.AI"]},{"titles":"Quantitative Analysis of Primary Attribution Explainable Artificial Intelligence Methods for Remote Sensing Image Classification","summaries":"We present a comprehensive analysis of quantitatively evaluating explainable\nartificial intelligence (XAI) techniques for remote sensing image\nclassification. Our approach leverages state-of-the-art machine learning\napproaches to perform remote sensing image classification across multiple\nmodalities. We investigate the results of the models qualitatively through XAI\nmethods. Additionally, we compare the XAI methods quantitatively through\nvarious categories of desired properties. Through our analysis, we offer\ninsights and recommendations for selecting the most appropriate XAI method(s)\nto gain a deeper understanding of the models' decision-making processes. The\ncode for this work is publicly available.","terms":["cs.LG","cs.AI","cs.CV"]},{"titles":"FaultFormer: Transformer-based Prediction of Bearing Faults","summaries":"The growth of deep learning in the past decade has motivated important\napplications to smart manufacturing and machine health monitoring. In\nparticular, vibration data offers a rich and reliable source to provide\nmeaningful insights into machine health and predictive maintenance. In this\nwork, we present a Transformer based framework for analyzing vibration signals\nto predict different types of bearing faults (FaultFormer). In particular, we\nprocess signal data using data augmentations and extract their Fourier modes to\ntrain a transformer encoder to achieve state of the art accuracies. The\nattention mechanism as well as model outputs were analyzed to confirm the\ntransformer's ability to automatically extract features within signals and\nlearn both global and local relationships to make classifications. Lastly, two\npretraining strategies were proposed to pave the way for large, generalizable\ntransformers that could adapt to new data, situations, or machinery on the\nproduction floor.","terms":["cs.LG","eess.SP"]},{"titles":"CityTFT: Temporal Fusion Transformer for Urban Building Energy Modeling","summaries":"Urban Building Energy Modeling (UBEM) is an emerging method to investigate\nurban design and energy systems against the increasing energy demand at urban\nand neighborhood levels. However, current UBEM methods are mostly physic-based\nand time-consuming in multiple climate change scenarios. This work proposes\nCityTFT, a data-driven UBEM framework, to accurately model the energy demands\nin urban environments. With the empowerment of the underlying TFT framework and\nan augmented loss function, CityTFT could predict heating and cooling triggers\nin unseen climate dynamics with an F1 score of 99.98 \\% while RMSE of loads of\n13.57 kWh.","terms":["stat.ML","cs.AI","cs.LG"]},{"titles":"A Multi-Task Perspective for Link Prediction with New Relation Types and Nodes","summaries":"The task of inductive link prediction in (discrete) attributed multigraphs\ninfers missing attributed links (relations) between nodes in new test\nmultigraphs. Traditional relational learning methods face the challenge of\nlimited generalization to test multigraphs containing both novel nodes and\nnovel relation types not seen in training. Recently, under the only assumption\nthat all relation types share the same structural predictive patterns (single\ntask), Gao et al. (2023) proposed a link prediction method using the\ntheoretical concept of double equivariance (equivariance for nodes & relation\ntypes), in contrast to the (single) equivariance (only for nodes) used to\ndesign Graph Neural Networks (GNNs). In this work we further extend the double\nequivariance concept to multi-task double equivariance, where we define link\nprediction in attributed multigraphs that can have distinct and potentially\nconflicting predictive patterns for different sets of relation types (multiple\ntasks). Our empirical results on real-world datasets demonstrate that our\napproach can effectively generalize to test graphs with multi-task structures\nwithout access to additional information.","terms":["cs.LG","cs.AI"]},{"titles":"Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks","summaries":"The integration of deep learning systems into the medical domain has been\nhindered by the resource-intensive process of data annotation and the inability\nof these systems to generalize to different data distributions. Foundation\nmodels, which are models pre-trained on large datasets, have emerged as a\nsolution to reduce reliance on annotated data and enhance model\ngeneralizability and robustness. DINOv2, an open-source foundation model\npre-trained with self-supervised learning on 142 million curated natural\nimages, excels in extracting general-purpose visual representations, exhibiting\npromising capabilities across various vision tasks. Nevertheless, a critical\nquestion remains unanswered regarding DINOv2's adaptability to radiological\nimaging, and the clarity on whether its features are sufficiently general to\nbenefit radiology image analysis is yet to be established. Therefore, this\nstudy comprehensively evaluates DINOv2 for radiology, conducting over 100\nexperiments across diverse modalities (X-ray, CT, and MRI). Tasks include\ndisease classification and organ segmentation on both 2D and 3D images,\nevaluated under different settings like kNN, few-shot learning, linear-probing,\nend-to-end fine-tuning, and parameter-efficient fine-tuning, to measure the\neffectiveness and generalizability of the DINOv2 feature embeddings.\nComparative analyses with established medical image analysis models, U-Net and\nTransUnet for segmentation, and CNN and ViT models pre-trained via supervised,\nweakly supervised, and self-supervised learning for classification, reveal\nDINOv2's superior performance in segmentation tasks and competitive results in\ndisease classification. The findings contribute insights to potential avenues\nfor optimizing pre-training strategies for medical imaging and enhancing the\nbroader understanding of DINOv2's role in bridging the gap between natural and\nradiological image analysis.","terms":["cs.CV","cs.AI"]},{"titles":"Class-Discriminative Attention Maps for Vision Transformers","summaries":"Interpretability methods are critical components for examining and exploring\ndeep neural networks (DNN), as well as increasing our understanding of and\ntrust in them. Vision transformers (ViT), which can be trained to\nstate-of-the-art performance with a self-supervised learning (SSL) training\nmethod, provide built-in attention maps (AM). While AMs can provide\nhigh-quality semantic segmentation of input images, they do not account for any\nsignal coming from a downstream classifier. We introduce class-discriminative\nattention maps (CDAM), a novel post-hoc explanation method that is highly\nsensitive to the target class. Our method essentially scales attention scores\nby how relevant the corresponding tokens are for the predictions of a\nclassifier head. Alternative to classifier outputs, CDAM can also explain a\nuser-defined concept by targeting similarity measures in the latent space of\nthe ViT. This allows for explanations of arbitrary concepts, defined by the\nuser through a few sample images. We investigate the operating characteristics\nof CDAM in comparison with relevance propagation (RP) and token ablation maps\n(TAM), an alternative to pixel occlusion methods. CDAM is highly\nclass-discriminative and semantically relevant, while providing implicit\nregularization of relevance scores.\n  PyTorch implementation: \\url{https:\/\/github.com\/lenbrocki\/CDAM}\n  Web live demo: \\url{https:\/\/cdam.informatism.com\/}","terms":["cs.CV","cs.AI","cs.LG","stat.ML"]},{"titles":"PointNeRF++: A multi-scale, point-based Neural Radiance Field","summaries":"Point clouds offer an attractive source of information to complement images\nin neural scene representations, especially when few images are available.\nNeural rendering methods based on point clouds do exist, but they do not\nperform well when the point cloud quality is low -- e.g., sparse or incomplete,\nwhich is often the case with real-world data. We overcome these problems with a\nsimple representation that aggregates point clouds at multiple scale levels\nwith sparse voxel grids at different resolutions. To deal with point cloud\nsparsity, we average across multiple scale levels -- but only among those that\nare valid, i.e., that have enough neighboring points in proximity to the ray of\na pixel. To help model areas without points, we add a global voxel at the\ncoarsest scale, thus unifying \"classical\" and point-based NeRF formulations. We\nvalidate our method on the NeRF Synthetic, ScanNet, and KITTI-360 datasets,\noutperforming the state of the art by a significant margin.","terms":["cs.CV","cs.GR"]},{"titles":"When is Offline Policy Selection Sample Efficient for Reinforcement Learning?","summaries":"Offline reinforcement learning algorithms often require careful\nhyperparameter tuning. Consequently, before deployment, we need to select\namongst a set of candidate policies. As yet, however, there is little\nunderstanding about the fundamental limits of this offline policy selection\n(OPS) problem. In this work we aim to provide clarity on when sample efficient\nOPS is possible, primarily by connecting OPS to off-policy policy evaluation\n(OPE) and Bellman error (BE) estimation. We first show a hardness result, that\nin the worst case, OPS is just as hard as OPE, by proving a reduction of OPE to\nOPS. As a result, no OPS method can be more sample efficient than OPE in the\nworst case. We then propose a BE method for OPS, called Identifiable BE\nSelection (IBES), that has a straightforward method for selecting its own\nhyperparameters. We highlight that using IBES for OPS generally has more\nrequirements than OPE methods, but if satisfied, can be more sample efficient.\nWe conclude with an empirical study comparing OPE and IBES, and by showing the\ndifficulty of OPS on an offline Atari benchmark dataset.","terms":["cs.LG","cs.AI"]},{"titles":"U-TILISE: A Sequence-to-sequence Model for Cloud Removal in Optical Satellite Time Series","summaries":"Satellite image time series in the optical and infrared spectrum suffer from\nfrequent data gaps due to cloud cover, cloud shadows, and temporary sensor\noutages. It has been a long-standing problem of remote sensing research how to\nbest reconstruct the missing pixel values and obtain complete, cloud-free image\nsequences. We approach that problem from the perspective of representation\nlearning and develop U-TILISE, an efficient neural model that is able to\nimplicitly capture spatio-temporal patterns of the spectral intensities, and\nthat can therefore be trained to map a cloud-masked input sequence to a\ncloud-free output sequence. The model consists of a convolutional spatial\nencoder that maps each individual frame of the input sequence to a latent\nencoding; an attention-based temporal encoder that captures dependencies\nbetween those per-frame encodings and lets them exchange information along the\ntime dimension; and a convolutional spatial decoder that decodes the latent\nembeddings back into multi-spectral images. We experimentally evaluate the\nproposed model on EarthNet2021, a dataset of Sentinel-2 time series acquired\nall over Europe, and demonstrate its superior ability to reconstruct the\nmissing pixels. Compared to a standard interpolation baseline, it increases the\nPSNR by 1.8 dB at previously seen locations and by 1.3 dB at unseen locations.","terms":["cs.CV","eess.IV"]},{"titles":"Calibrated Uncertainties for Neural Radiance Fields","summaries":"Neural Radiance Fields have achieved remarkable results for novel view\nsynthesis but still lack a crucial component: precise measurement of\nuncertainty in their predictions. Probabilistic NeRF methods have tried to\naddress this, but their output probabilities are not typically accurately\ncalibrated, and therefore do not capture the true confidence levels of the\nmodel. Calibration is a particularly challenging problem in the sparse-view\nsetting, where additional held-out data is unavailable for fitting a calibrator\nthat generalizes to the test distribution. In this paper, we introduce the\nfirst method for obtaining calibrated uncertainties from NeRF models. Our\nmethod is based on a robust and efficient metric to calculate per-pixel\nuncertainties from the predictive posterior distribution. We propose two\ntechniques that eliminate the need for held-out data. The first, based on patch\nsampling, involves training two NeRF models for each scene. The second is a\nnovel meta-calibrator that only requires the training of one NeRF model. Our\nproposed approach for obtaining calibrated uncertainties achieves\nstate-of-the-art uncertainty in the sparse-view setting while maintaining image\nquality. We further demonstrate our method's effectiveness in applications such\nas view enhancement and next-best view selection.","terms":["cs.CV"]},{"titles":"Applying Bayesian Ridge Regression AI Modeling in Virus Severity Prediction","summaries":"Artificial intelligence (AI) is a powerful tool for reshaping healthcare\nsystems. In healthcare, AI is invaluable for its capacity to manage vast\namounts of data, which can lead to more accurate and speedy diagnoses,\nultimately easing the workload on healthcare professionals. As a result, AI has\nproven itself to be a power tool across various industries, simplifying complex\ntasks and pattern recognition that would otherwise be overwhelming for humans\nor traditional computer algorithms. In this paper, we review the strengths and\nweaknesses of Bayesian Ridge Regression, an AI model that can be used to bring\ncutting edge virus analysis to healthcare professionals around the world. The\nmodel's accuracy assessment revealed promising results, with room for\nimprovement primarily related to data organization. In addition, the severity\nindex serves as a valuable tool to gain a broad overview of patient care needs,\naligning with healthcare professionals' preference for broader categorizations.","terms":["cs.LG"]},{"titles":"CLIPDrawX: Primitive-based Explanations for Text Guided Sketch Synthesis","summaries":"With the goal of understanding the visual concepts that CLIP associates with\ntext prompts, we show that the latent space of CLIP can be visualized solely in\nterms of linear transformations on simple geometric primitives like circles and\nstraight lines. Although existing approaches achieve this by\nsketch-synthesis-through-optimization, they do so on the space of B\\'ezier\ncurves, which exhibit a wastefully large set of structures that they can evolve\ninto, as most of them are non-essential for generating meaningful sketches. We\npresent CLIPDrawX, an algorithm that provides significantly better\nvisualizations for CLIP text embeddings, using only simple primitive shapes\nlike straight lines and circles. This constrains the set of possible outputs to\nlinear transformations on these primitives, thereby exhibiting an inherently\nsimpler mathematical form. The synthesis process of CLIPDrawX can be tracked\nend-to-end, with each visual concept being explained exclusively in terms of\nprimitives. Implementation will be released upon acceptance. Project Page:\n$\\href{https:\/\/clipdrawx.github.io\/}{\\text{https:\/\/clipdrawx.github.io\/}}$.","terms":["cs.CV"]},{"titles":"Fedstellar: A Platform for Decentralized Federated Learning","summaries":"In 2016, Google proposed Federated Learning (FL) as a novel paradigm to train\nMachine Learning (ML) models across the participants of a federation while\npreserving data privacy. Since its birth, Centralized FL (CFL) has been the\nmost used approach, where a central entity aggregates participants' models to\ncreate a global one. However, CFL presents limitations such as communication\nbottlenecks, single point of failure, and reliance on a central server.\nDecentralized Federated Learning (DFL) addresses these issues by enabling\ndecentralized model aggregation and minimizing dependency on a central entity.\nDespite these advances, current platforms training DFL models struggle with key\nissues such as managing heterogeneous federation network topologies. To\novercome these challenges, this paper presents Fedstellar, a novel platform\ndesigned to train FL models in a decentralized, semi-decentralized, and\ncentralized fashion across diverse federations of physical or virtualized\ndevices. The Fedstellar implementation encompasses a web application with an\ninteractive graphical interface, a controller for deploying federations of\nnodes using physical or virtual devices, and a core deployed on each device\nwhich provides the logic needed to train, aggregate, and communicate in the\nnetwork. The effectiveness of the platform has been demonstrated in two\nscenarios: a physical deployment involving single-board devices such as\nRaspberry Pis for detecting cyberattacks, and a virtualized deployment\ncomparing various FL approaches in a controlled environment using MNIST and\nCIFAR-10 datasets. In both scenarios, Fedstellar demonstrated consistent\nperformance and adaptability, achieving F1 scores of 91%, 98%, and 91.2% using\nDFL for detecting cyberattacks and classifying MNIST and CIFAR-10,\nrespectively, reducing training time by 32% compared to centralized approaches.","terms":["cs.LG","cs.AI","cs.DC","cs.NI"]},{"titles":"Grassmann Manifold Flows for Stable Shape Generation","summaries":"Recently, studies on machine learning have focused on methods that use\nsymmetry implicit in a specific manifold as an inductive bias. Grassmann\nmanifolds provide the ability to handle fundamental shapes represented as shape\nspaces, enabling stable shape analysis. In this paper, we present a novel\napproach in which we establish the theoretical foundations for learning\ndistributions on the Grassmann manifold via continuous normalization flows,\nwith the explicit goal of generating stable shapes. Our approach facilitates\nmore robust generation by effectively eliminating the influence of extraneous\ntransformations, such as rotations and inversions, through learning and\ngenerating within a Grassmann manifold designed to accommodate the essential\nshape information of the object. The experimental results indicated that the\nproposed method could generate high-quality samples by capturing the data\nstructure. Furthermore, the proposed method significantly outperformed\nstate-of-the-art methods in terms of the log-likelihood or evidence lower\nbound. The results obtained are expected to stimulate further research in this\nfield, leading to advances for stable shape generation and analysis.","terms":["cs.LG","math.DG","stat.ML"]},{"titles":"STEREOFOG -- Computational DeFogging via Image-to-Image Translation on a real-world Dataset","summaries":"Image-to-Image translation (I2I) is a subtype of Machine Learning (ML) that\nhas tremendous potential in applications where two domains of images and the\nneed for translation between the two exist, such as the removal of fog. For\nexample, this could be useful for autonomous vehicles, which currently struggle\nwith adverse weather conditions like fog. However, datasets for I2I tasks are\nnot abundant and typically hard to acquire. Here, we introduce STEREOFOG, a\ndataset comprised of $10,067$ paired fogged and clear images, captured using a\ncustom-built device, with the purpose of exploring I2I's potential in this\ndomain. It is the only real-world dataset of this kind to the best of our\nknowledge. Furthermore, we apply and optimize the pix2pix I2I ML framework to\nthis dataset. With the final model achieving an average Complex\nWavelet-Structural Similarity (CW-SSIM) score of $0.76$, we prove the\ntechnique's suitability for the problem.","terms":["cs.CV","cs.LG"]},{"titles":"Can SAM recognize crops? Quantifying the zero-shot performance of a semantic segmentation foundation model on generating crop-type maps using satellite imagery for precision agriculture","summaries":"Climate change is increasingly disrupting worldwide agriculture, making\nglobal food production less reliable. To tackle the growing challenges in\nfeeding the planet, cutting-edge management strategies, such as precision\nagriculture, empower farmers and decision-makers with rich and actionable\ninformation to increase the efficiency and sustainability of their farming\npractices. Crop-type maps are key information for decision-support tools but\nare challenging and costly to generate. We investigate the capabilities of Meta\nAI's Segment Anything Model (SAM) for crop-map prediction task, acknowledging\nits recent successes at zero-shot image segmentation. However, SAM being\nlimited to up-to 3 channel inputs and its zero-shot usage being class-agnostic\nin nature pose unique challenges in using it directly for crop-type mapping. We\npropose using clustering consensus metrics to assess SAM's zero-shot\nperformance in segmenting satellite imagery and producing crop-type maps.\nAlthough direct crop-type mapping is challenging using SAM in zero-shot\nsetting, experiments reveal SAM's potential for swiftly and accurately\noutlining fields in satellite images, serving as a foundation for subsequent\ncrop classification. This paper attempts to highlight a use-case of\nstate-of-the-art image segmentation models like SAM for crop-type mapping and\nrelated specific needs of the agriculture industry, offering a potential avenue\nfor automatic, efficient, and cost-effective data products for precision\nagriculture practices.","terms":["cs.CV"]},{"titles":"InfiMM-Eval: Complex Open-Ended Reasoning Evaluation For Multi-Modal Large Language Models","summaries":"Multi-modal Large Language Models (MLLMs) are increasingly prominent in the\nfield of artificial intelligence. These models not only excel in traditional\nvision-language tasks but also demonstrate impressive performance in\ncontemporary multi-modal benchmarks. Although many of these benchmarks attempt\nto holistically evaluate MLLMs, they typically concentrate on basic reasoning\ntasks, often yielding only simple yes\/no or multi-choice responses. These\nmethods naturally lead to confusion and difficulties in conclusively\ndetermining the reasoning capabilities of MLLMs. To mitigate this issue, we\nmanually curate a benchmark dataset specifically designed for MLLMs, with a\nfocus on complex reasoning tasks. Our benchmark comprises three key reasoning\ncategories: deductive, abductive, and analogical reasoning. The queries in our\ndataset are intentionally constructed to engage the reasoning capabilities of\nMLLMs in the process of generating answers. For a fair comparison across\nvarious MLLMs, we incorporate intermediate reasoning steps into our evaluation\ncriteria. In instances where an MLLM is unable to produce a definitive answer,\nits reasoning ability is evaluated by requesting intermediate reasoning steps.\nIf these steps align with our manual annotations, appropriate scores are\nassigned. This evaluation scheme resembles methods commonly used in human\nassessments, such as exams or assignments, and represents what we consider a\nmore effective assessment technique compared with existing benchmarks. We\nevaluate a selection of representative MLLMs using this rigorously developed\nopen-ended multi-step elaborate reasoning benchmark, designed to challenge and\naccurately measure their reasoning capabilities. The code and data will be\nreleased at https:\/\/infimm.github.io\/InfiMM-Eval\/","terms":["cs.CV"]},{"titles":"Expressive Sign Equivariant Networks for Spectral Geometric Learning","summaries":"Recent work has shown the utility of developing machine learning models that\nrespect the structure and symmetries of eigenvectors. These works promote sign\ninvariance, since for any eigenvector v the negation -v is also an eigenvector.\nHowever, we show that sign invariance is theoretically limited for tasks such\nas building orthogonally equivariant models and learning node positional\nencodings for link prediction in graphs. In this work, we demonstrate the\nbenefits of sign equivariance for these tasks. To obtain these benefits, we\ndevelop novel sign equivariant neural network architectures. Our models are\nbased on a new analytic characterization of sign equivariant polynomials and\nthus inherit provable expressiveness properties. Controlled synthetic\nexperiments show that our networks can achieve the theoretically predicted\nbenefits of sign equivariant models. Code is available at\nhttps:\/\/github.com\/cptq\/Sign-Equivariant-Nets.","terms":["cs.LG","cs.AI","stat.ML"]},{"titles":"A Contrastive Compositional Benchmark for Text-to-Image Synthesis: A Study with Unified Text-to-Image Fidelity Metrics","summaries":"Text-to-image (T2I) synthesis has recently achieved significant advancements.\nHowever, challenges remain in the model's compositionality, which is the\nability to create new combinations from known components. We introduce\nWinoground-T2I, a benchmark designed to evaluate the compositionality of T2I\nmodels. This benchmark includes 11K complex, high-quality contrastive sentence\npairs spanning 20 categories. These contrastive sentence pairs with subtle\ndifferences enable fine-grained evaluations of T2I synthesis models.\nAdditionally, to address the inconsistency across different metrics, we propose\na strategy that evaluates the reliability of various metrics by using\ncomparative sentence pairs. We use Winoground-T2I with a dual objective: to\nevaluate the performance of T2I models and the metrics used for their\nevaluation. Finally, we provide insights into the strengths and weaknesses of\nthese metrics and the capabilities of current T2I models in tackling challenges\nacross a range of complex compositional categories. Our benchmark is publicly\navailable at https:\/\/github.com\/zhuxiangru\/Winoground-T2I .","terms":["cs.CV","cs.AI","cs.MM"]},{"titles":"High-probability Convergence Bounds for Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise","summaries":"Several recent works have studied the convergence \\textit{in high\nprobability} of stochastic gradient descent (SGD) and its clipped variant.\nCompared to vanilla SGD, clipped SGD is practically more stable and has the\nadditional theoretical benefit of logarithmic dependence on the failure\nprobability. However, the convergence of other practical nonlinear variants of\nSGD, e.g., sign SGD, quantized SGD and normalized SGD, that achieve improved\ncommunication efficiency or accelerated convergence is much less understood. In\nthis work, we study the convergence bounds \\textit{in high probability} of a\nbroad class of nonlinear SGD methods. For strongly convex loss functions with\nLipschitz continuous gradients, we prove a logarithmic dependence on the\nfailure probability, even when the noise is heavy-tailed. Strictly more general\nthan the results for clipped SGD, our results hold for any nonlinearity with\nbounded (component-wise or joint) outputs, such as clipping, normalization, and\nquantization. Further, existing results with heavy-tailed noise assume bounded\n$\\eta$-th central moments, with $\\eta \\in (1,2]$. In contrast, our refined\nanalysis works even for $\\eta=1$, strictly relaxing the noise moment\nassumptions in the literature.","terms":["cs.LG","math.OC","math.ST","stat.ML","stat.TH"]},{"titles":"InstructBooth: Instruction-following Personalized Text-to-Image Generation","summaries":"Personalizing text-to-image models using a limited set of images for a\nspecific object has been explored in subject-specific image generation.\nHowever, existing methods often encounter challenges in aligning with text\nprompts due to overfitting to the limited training images. In this work, we\nintroduce InstructBooth, a novel method designed to enhance image-text\nalignment in personalized text-to-image models. Our approach first personalizes\ntext-to-image models with a small number of subject-specific images using a\nunique identifier. After personalization, we fine-tune personalized\ntext-to-image models using reinforcement learning to maximize a reward that\nquantifies image-text alignment. Additionally, we propose complementary\ntechniques to increase the synergy between these two processes. Our method\ndemonstrates superior image-text alignment compared to baselines while\nmaintaining personalization ability. In human evaluations, InstructBooth\noutperforms DreamBooth when considering all comprehensive factors.","terms":["cs.CV","cs.AI"]},{"titles":"FLea: Improving federated learning on scarce and label-skewed data via privacy-preserving feature augmentation","summaries":"Learning a global model by abstracting the knowledge, distributed across\nmultiple clients, without aggregating the raw data is the primary goal of\nFederated Learning (FL). Typically, this works in rounds alternating between\nparallel local training at several clients, followed by model aggregation at a\nserver. We found that existing FL methods under-perform when local datasets are\nsmall and present severe label skew as these lead to over-fitting and local\nmodel bias. This is a realistic setting in many real-world applications. To\naddress the problem, we propose \\textit{FLea}, a unified framework that tackles\nover-fitting and local bias by encouraging clients to exchange\nprivacy-protected features to aid local training. The features refer to\nactivations from an intermediate layer of the model, which are obfuscated\nbefore being shared with other clients to protect sensitive information in the\ndata. \\textit{FLea} leverages a novel way of combining local and shared\nfeatures as augmentations to enhance local model learning. Our extensive\nexperiments demonstrate that \\textit{FLea} outperforms the start-of-the-art FL\nmethods, sharing only model parameters, by up to $17.6\\%$, and FL methods that\nshare data augmentations by up to $6.3\\%$, while reducing the privacy\nvulnerability associated with shared data augmentations.","terms":["cs.LG","cs.CR","cs.DC"]},{"titles":"Towards out-of-distribution generalizable predictions of chemical kinetics properties","summaries":"Machine Learning (ML) techniques have found applications in estimating\nchemical kinetic properties. With the accumulated drug molecules identified\nthrough \"AI4drug discovery\", the next imperative lies in AI-driven design for\nhigh-throughput chemical synthesis processes, with the estimation of properties\nof unseen reactions with unexplored molecules. To this end, the existing ML\napproaches for kinetics property prediction are required to be\nOut-Of-Distribution (OOD) generalizable. In this paper, we categorize the OOD\nkinetic property prediction into three levels (structure, condition, and\nmechanism), revealing unique aspects of such problems. Under this framework, we\ncreate comprehensive datasets to benchmark (1) the state-of-the-art ML\napproaches for reaction prediction in the OOD setting and (2) the\nstate-of-the-art graph OOD methods in kinetics property prediction problems.\nOur results demonstrated the challenges and opportunities in OOD kinetics\nproperty prediction. Our datasets and benchmarks can further support research\nin this direction.","terms":["cs.LG"]},{"titles":"Cable Slack Detection for Arresting Gear Application using Machine Vision","summaries":"The cable-based arrestment systems are integral to the launch and recovery of\naircraft onboard carriers and on expeditionary land-based installations. These\nmodern arrestment systems rely on various mechanisms to absorb energy from an\naircraft during an arrestment cycle to bring the aircraft to a full stop. One\nof the primary components of this system is the cable interface to the engine.\nThe formation of slack in the cable at this interface can result in reduced\nefficiency and drives maintenance efforts to remove the slack prior to\ncontinued operations. In this paper, a machine vision based slack detection\nsystem is presented. A situational awareness camera is utilized to collect\nvideo data of the cable interface region, machine vision algorithms are applied\nto reduce noise, remove background clutter, focus on regions of interest, and\ndetect changes in the image representative of slack formations. Some algorithms\nemployed in this system include bilateral image filters, least squares\npolynomial fit, Canny Edge Detection, K-Means clustering, Gaussian\nMixture-based Background\/Foreground Segmentation for background subtraction,\nHough Circle Transforms, and Hough line Transforms. The resulting detections\nare filtered and highlighted to create an indication to the shipboard operator\nof the presence of slack and a need for a maintenance action. A user interface\nwas designed to provide operators with an easy method to redefine regions of\ninterest and adjust the methods to specific locations. The algorithms were\nvalidated on shipboard footage and were able to accurately identify slack with\nminimal false positives.","terms":["cs.CV"]},{"titles":"Domain Generalization via Nuclear Norm Regularization","summaries":"The ability to generalize to unseen domains is crucial for machine learning\nsystems deployed in the real world, especially when we only have data from\nlimited training domains. In this paper, we propose a simple and effective\nregularization method based on the nuclear norm of the learned features for\ndomain generalization. Intuitively, the proposed regularizer mitigates the\nimpacts of environmental features and encourages learning domain-invariant\nfeatures. Theoretically, we provide insights into why nuclear norm\nregularization is more effective compared to ERM and alternative regularization\nmethods. Empirically, we conduct extensive experiments on both synthetic and\nreal datasets. We show nuclear norm regularization achieves strong performance\ncompared to baselines in a wide range of domain generalization tasks. Moreover,\nour regularizer is broadly applicable with various methods such as ERM and SWAD\nwith consistently improved performance, e.g., 1.7% and 0.9% test accuracy\nimprovements respectively on the DomainBed benchmark.","terms":["cs.LG","cs.CV"]},{"titles":"Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games","summaries":"Video games have served as useful benchmarks for the decision making\ncommunity, but going beyond Atari games towards training agents in modern games\nhas been prohibitively expensive for the vast majority of the research\ncommunity. Recent progress in the research, development and open release of\nlarge vision models has the potential to amortize some of these costs across\nthe community. However, it is currently unclear which of these models have\nlearnt representations that retain information critical for sequential decision\nmaking. Towards enabling wider participation in the research of gameplaying\nagents in modern games, we present a systematic study of imitation learning\nwith publicly available visual encoders compared to the typical, task-specific,\nend-to-end training approach in Minecraft, Minecraft Dungeons and\nCounter-Strike: Global Offensive.","terms":["cs.LG","cs.AI","cs.CV"]},{"titles":"Source Free Unsupervised Graph Domain Adaptation","summaries":"Graph Neural Networks (GNNs) have achieved great success on a variety of\ntasks with graph-structural data, among which node classification is an\nessential one. Unsupervised Graph Domain Adaptation (UGDA) shows its practical\nvalue of reducing the labeling cost for node classification. It leverages\nknowledge from a labeled graph (i.e., source domain) to tackle the same task on\nanother unlabeled graph (i.e., target domain). Most existing UGDA methods\nheavily rely on the labeled graph in the source domain. They utilize labels\nfrom the source domain as the supervision signal and are jointly trained on\nboth the source graph and the target graph. However, in some real-world\nscenarios, the source graph is inaccessible because of privacy issues.\nTherefore, we propose a novel scenario named Source Free Unsupervised Graph\nDomain Adaptation (SFUGDA). In this scenario, the only information we can\nleverage from the source domain is the well-trained source model, without any\nexposure to the source graph and its labels. As a result, existing UGDA methods\nare not feasible anymore. To address the non-trivial adaptation challenges in\nthis practical scenario, we propose a model-agnostic algorithm called SOGA for\ndomain adaptation to fully exploit the discriminative ability of the source\nmodel while preserving the consistency of structural proximity on the target\ngraph. We prove the effectiveness of the proposed algorithm both theoretically\nand empirically. The experimental results on four cross-domain tasks show\nconsistent improvements in the Macro-F1 score and Macro-AUC.","terms":["cs.LG","cs.AI"]},{"titles":"VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding","summaries":"Recent advancements in language-model-based video understanding have been\nprogressing at a remarkable pace, spurred by the introduction of Large Language\nModels (LLMs). However, the focus of prior research has been predominantly on\ndevising a projection layer that maps video features to tokens, an approach\nthat is both rudimentary and inefficient. In our study, we introduce a\ncutting-edge framework, VaQuitA, designed to refine the synergy between video\nand textual information. At the data level, instead of sampling frames\nuniformly, we implement a sampling method guided by CLIP-score rankings, which\nenables a more aligned selection of frames with the given question. At the\nfeature level, we integrate a trainable Video Perceiver alongside a\nVisual-Query Transformer (abbreviated as VQ-Former), which bolsters the\ninterplay between the input question and the video features. We also discover\nthat incorporating a simple prompt, \"Please be critical\", into the LLM input\ncan substantially enhance its video comprehension capabilities. Our\nexperimental results indicate that VaQuitA consistently sets a new benchmark\nfor zero-shot video question-answering tasks and is adept at producing\nhigh-quality, multi-turn video dialogues with users.","terms":["cs.CV","cs.AI","cs.CL","cs.LG"]},{"titles":"AdsorbRL: Deep Multi-Objective Reinforcement Learning for Inverse Catalysts Design","summaries":"A central challenge of the clean energy transition is the development of\ncatalysts for low-emissions technologies. Recent advances in Machine Learning\nfor quantum chemistry drastically accelerate the computation of catalytic\nactivity descriptors such as adsorption energies. Here we introduce AdsorbRL, a\nDeep Reinforcement Learning agent aiming to identify potential catalysts given\na multi-objective binding energy target, trained using offline learning on the\nOpen Catalyst 2020 and Materials Project data sets. We experiment with Deep\nQ-Network agents to traverse the space of all ~160,000 possible unary, binary\nand ternary compounds of 55 chemical elements, with very sparse rewards based\non adsorption energy known for only between 2,000 and 3,000 catalysts per\nadsorbate. To constrain the actions space, we introduce Random Edge Traversal\nand train a single-objective DQN agent on the known states subgraph, which we\nfind strengthens target binding energy by an average of 4.1 eV. We extend this\napproach to multi-objective, goal-conditioned learning, and train a DQN agent\nto identify materials with the highest (respectively lowest) adsorption\nenergies for multiple simultaneous target adsorbates. We experiment with\nObjective Sub-Sampling, a novel training scheme aimed at encouraging\nexploration in the multi-objective setup, and demonstrate simultaneous\nadsorption energy improvement across all target adsorbates, by an average of\n0.8 eV. Overall, our results suggest strong potential for Deep Reinforcement\nLearning applied to the inverse catalysts design problem.","terms":["cs.LG","cs.AI","physics.chem-ph"]},{"titles":"Reconsideration on evaluation of machine learning models in continuous monitoring using wearables","summaries":"This paper explores the challenges in evaluating machine learning (ML) models\nfor continuous health monitoring using wearable devices beyond conventional\nmetrics. We state the complexities posed by real-world variability, disease\ndynamics, user-specific characteristics, and the prevalence of false\nnotifications, necessitating novel evaluation strategies. Drawing insights from\nlarge-scale heart studies, the paper offers a comprehensive guideline for\nrobust ML model evaluation on continuous health monitoring.","terms":["cs.LG","eess.SP"]},{"titles":"Cotton Yield Prediction Using Random Forest","summaries":"The cotton industry in the United States is committed to sustainable\nproduction practices that minimize water, land, and energy use while improving\nsoil health and cotton output. Climate-smart agricultural technologies are\nbeing developed to boost yields while decreasing operating expenses. Crop yield\nprediction, on the other hand, is difficult because of the complex and\nnonlinear impacts of cultivar, soil type, management, pest and disease,\nclimate, and weather patterns on crops. To solve this issue, we employ machine\nlearning (ML) to forecast production while considering climate change, soil\ndiversity, cultivar, and inorganic nitrogen levels. From the 1980s to the\n1990s, field data were gathered across the southern cotton belt of the United\nStates. To capture the most current effects of climate change over the previous\nsix years, a second data source was produced using the process-based crop\nmodel, GOSSYM. We concentrated our efforts on three distinct areas inside each\nof the three southern states: Texas, Mississippi, and Georgia. To simplify the\namount of computations, accumulated heat units (AHU) for each set of\nexperimental data were employed as an analogy to use time-series weather data.\nThe Random Forest Regressor yielded a 97.75% accuracy rate, with a root mean\nsquare error of 55.05 kg\/ha and an R2 of around 0.98. These findings\ndemonstrate how an ML technique may be developed and applied as a reliable and\neasy-to-use model to support the cotton climate-smart initiative.","terms":["cs.LG","cs.CY","stat.AP"]},{"titles":"You Can Run but not Hide: Improving Gait Recognition with Intrinsic Occlusion Type Awareness","summaries":"While gait recognition has seen many advances in recent years, the occlusion\nproblem has largely been ignored. This problem is especially important for gait\nrecognition from uncontrolled outdoor sequences at range - since any small\nobstruction can affect the recognition system. Most current methods assume the\navailability of complete body information while extracting the gait features.\nWhen parts of the body are occluded, these methods may hallucinate and output a\ncorrupted gait signature as they try to look for body parts which are not\npresent in the input at all. To address this, we exploit the learned occlusion\ntype while extracting identity features from videos. Thus, in this work, we\npropose an occlusion aware gait recognition method which can be used to model\nintrinsic occlusion awareness into potentially any state-of-the-art gait\nrecognition method. Our experiments on the challenging GREW and BRIAR datasets\nshow that networks enhanced with this occlusion awareness perform better at\nrecognition tasks than their counterparts trained on similar occlusions.","terms":["cs.CV"]},{"titles":"PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation","summaries":"Single image depth estimation is a foundational task in computer vision and\ngenerative modeling. However, prevailing depth estimation models grapple with\naccommodating the increasing resolutions commonplace in today's consumer\ncameras and devices. Existing high-resolution strategies show promise, but they\noften face limitations, ranging from error propagation to the loss of\nhigh-frequency details. We present PatchFusion, a novel tile-based framework\nwith three key components to improve the current state of the art: (1) A\npatch-wise fusion network that fuses a globally-consistent coarse prediction\nwith finer, inconsistent tiled predictions via high-level feature guidance, (2)\nA Global-to-Local (G2L) module that adds vital context to the fusion network,\ndiscarding the need for patch selection heuristics, and (3) A Consistency-Aware\nTraining (CAT) and Inference (CAI) approach, emphasizing patch overlap\nconsistency and thereby eradicating the necessity for post-processing.\nExperiments on UnrealStereo4K, MVS-Synth, and Middleburry 2014 demonstrate that\nour framework can generate high-resolution depth maps with intricate details.\nPatchFusion is independent of the base model for depth estimation. Notably, our\nframework built on top of SOTA ZoeDepth brings improvements for a total of\n17.3% and 29.4% in terms of the root mean squared error (RMSE) on\nUnrealStereo4K and MVS-Synth, respectively.","terms":["cs.CV"]},{"titles":"PaSCo: Urban 3D Panoptic Scene Completion with Uncertainty Awareness","summaries":"We propose the task of Panoptic Scene Completion (PSC) which extends the\nrecently popular Semantic Scene Completion (SSC) task with instance-level\ninformation to produce a richer understanding of the 3D scene. Our PSC proposal\nutilizes a hybrid mask-based technique on the non-empty voxels from sparse\nmulti-scale completions. Whereas the SSC literature overlooks uncertainty which\nis critical for robotics applications, we instead propose an efficient\nensembling to estimate both voxel-wise and instance-wise uncertainties along\nPSC. This is achieved by building on a multi-input multi-output (MIMO)\nstrategy, while improving performance and yielding better uncertainty for\nlittle additional compute. Additionally, we introduce a technique to aggregate\npermutation-invariant mask predictions. Our experiments demonstrate that our\nmethod surpasses all baselines in both Panoptic Scene Completion and\nuncertainty estimation on three large-scale autonomous driving datasets. Our\ncode and data are available at https:\/\/astra-vision.github.io\/PaSCo .","terms":["cs.CV","cs.AI"]},{"titles":"Mesh-Guided Neural Implicit Field Editing","summaries":"Neural implicit fields have emerged as a powerful 3D representation for\nreconstructing and rendering photo-realistic views, yet they possess limited\neditability. Conversely, explicit 3D representations, such as polygonal meshes,\noffer ease of editing but may not be as suitable for rendering high-quality\nnovel views. To harness the strengths of both representations, we propose a new\napproach that employs a mesh as a guiding mechanism in editing the neural\nradiance field. We first introduce a differentiable method using marching\ntetrahedra for polygonal mesh extraction from the neural implicit field and\nthen design a differentiable color extractor to assign colors obtained from the\nvolume renderings to this extracted mesh. This differentiable colored mesh\nallows gradient back-propagation from the explicit mesh to the implicit fields,\nempowering users to easily manipulate the geometry and color of neural implicit\nfields. To enhance user control from coarse-grained to fine-grained levels, we\nintroduce an octree-based structure into its optimization. This structure\nprioritizes the edited regions and the surface part, making our method achieve\nfine-grained edits to the neural implicit field and accommodate various user\nmodifications, including object additions, component removals, specific area\ndeformations, and adjustments to local and global colors. Through extensive\nexperiments involving diverse scenes and editing operations, we have\ndemonstrated the capabilities and effectiveness of our method. Our project page\nis: \\url{https:\/\/cassiepython.github.io\/MNeuEdit\/}","terms":["cs.CV"]},{"titles":"GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis","summaries":"We present a new approach, termed GPS-Gaussian, for synthesizing novel views\nof a character in a real-time manner. The proposed method enables 2K-resolution\nrendering under a sparse-view camera setting. Unlike the original Gaussian\nSplatting or neural implicit rendering methods that necessitate per-subject\noptimizations, we introduce Gaussian parameter maps defined on the source views\nand regress directly Gaussian Splatting properties for instant novel view\nsynthesis without any fine-tuning or optimization. To this end, we train our\nGaussian parameter regression module on a large amount of human scan data,\njointly with a depth estimation module to lift 2D parameter maps to 3D space.\nThe proposed framework is fully differentiable and experiments on several\ndatasets demonstrate that our method outperforms state-of-the-art methods while\nachieving an exceeding rendering speed.","terms":["cs.CV"]},{"titles":"Latent Feature-Guided Diffusion Models for Shadow Removal","summaries":"Recovering textures under shadows has remained a challenging problem due to\nthe difficulty of inferring shadow-free scenes from shadow images. In this\npaper, we propose the use of diffusion models as they offer a promising\napproach to gradually refine the details of shadow regions during the diffusion\nprocess. Our method improves this process by conditioning on a learned latent\nfeature space that inherits the characteristics of shadow-free images, thus\navoiding the limitation of conventional methods that condition on degraded\nimages only. Additionally, we propose to alleviate potential local optima\nduring training by fusing noise features with the diffusion network. We\ndemonstrate the effectiveness of our approach which outperforms the previous\nbest method by 13% in terms of RMSE on the AISTD dataset. Further, we explore\ninstance-level shadow removal, where our model outperforms the previous best\nmethod by 82% in terms of RMSE on the DESOBA dataset.","terms":["cs.CV","cs.AI"]},{"titles":"Aligning and Prompting Everything All at Once for Universal Visual Perception","summaries":"Vision foundation models have been explored recently to build general-purpose\nvision systems. However, predominant paradigms, driven by casting\ninstance-level tasks as an object-word alignment, bring heavy cross-modality\ninteraction, which is not effective in prompting object detection and visual\ngrounding. Another line of work that focuses on pixel-level tasks often\nencounters a large annotation gap of things and stuff, and suffers from mutual\ninterference between foreground-object and background-class segmentation. In\nstark contrast to the prevailing methods, we present APE, a universal visual\nperception model for aligning and prompting everything all at once in an image\nto perform diverse tasks, i.e., detection, segmentation, and grounding, as an\ninstance-level sentence-object matching paradigm. Specifically, APE advances\nthe convergence of detection and grounding by reformulating language-guided\ngrounding as open-vocabulary detection, which efficiently scales up model\nprompting to thousands of category vocabularies and region descriptions while\nmaintaining the effectiveness of cross-modality fusion. To bridge the\ngranularity gap of different pixel-level tasks, APE equalizes semantic and\npanoptic segmentation to proxy instance learning by considering any isolated\nregions as individual instances. APE aligns vision and language representation\non broad data with natural and challenging characteristics all at once without\ntask-specific fine-tuning. The extensive experiments on over 160 datasets\ndemonstrate that, with only one-suit of weights, APE outperforms (or is on par\nwith) the state-of-the-art models, proving that an effective yet universal\nperception for anything aligning and prompting is indeed feasible. Codes and\ntrained models are released at https:\/\/github.com\/shenyunhang\/APE.","terms":["cs.CV"]},{"titles":"Steerers: A framework for rotation equivariant keypoint descriptors","summaries":"Image keypoint descriptions that are discriminative and matchable over large\nchanges in viewpoint are vital for 3D reconstruction. However, descriptions\noutput by learned descriptors are typically not robust to camera rotation.\nWhile they can be made more robust by, e.g., data augmentation, this degrades\nperformance on upright images. Another approach is test-time augmentation,\nwhich incurs a significant increase in runtime. We instead learn a linear\ntransform in description space that encodes rotations of the input image. We\ncall this linear transform a steerer since it allows us to transform the\ndescriptions as if the image was rotated. From representation theory we know\nall possible steerers for the rotation group. Steerers can be optimized (A)\ngiven a fixed descriptor, (B) jointly with a descriptor or (C) we can optimize\na descriptor given a fixed steerer. We perform experiments in all of these\nthree settings and obtain state-of-the-art results on the rotation invariant\nimage matching benchmarks AIMS and Roto-360. We publish code and model weights\nat github.com\/georg-bn\/rotation-steerers.","terms":["cs.CV"]},{"titles":"Guarding Barlow Twins Against Overfitting with Mixed Samples","summaries":"Self-supervised Learning (SSL) aims to learn transferable feature\nrepresentations for downstream applications without relying on labeled data.\nThe Barlow Twins algorithm, renowned for its widespread adoption and\nstraightforward implementation compared to its counterparts like contrastive\nlearning methods, minimizes feature redundancy while maximizing invariance to\ncommon corruptions. Optimizing for the above objective forces the network to\nlearn useful representations, while avoiding noisy or constant features,\nresulting in improved downstream task performance with limited adaptation.\nDespite Barlow Twins' proven effectiveness in pre-training, the underlying SSL\nobjective can inadvertently cause feature overfitting due to the lack of strong\ninteraction between the samples unlike the contrastive learning approaches.\nFrom our experiments, we observe that optimizing for the Barlow Twins objective\ndoesn't necessarily guarantee sustained improvements in representation quality\nbeyond a certain pre-training phase, and can potentially degrade downstream\nperformance on some datasets. To address this challenge, we introduce Mixed\nBarlow Twins, which aims to improve sample interaction during Barlow Twins\ntraining via linearly interpolated samples. This results in an additional\nregularization term to the original Barlow Twins objective, assuming linear\ninterpolation in the input space translates to linearly interpolated features\nin the feature space. Pre-training with this regularization effectively\nmitigates feature overfitting and further enhances the downstream performance\non CIFAR-10, CIFAR-100, TinyImageNet, STL-10, and ImageNet datasets. The code\nand checkpoints are available at: https:\/\/github.com\/wgcban\/mix-bt.git","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"Readout Guidance: Learning Control from Diffusion Features","summaries":"We present Readout Guidance, a method for controlling text-to-image diffusion\nmodels with learned signals. Readout Guidance uses readout heads, lightweight\nnetworks trained to extract signals from the features of a pre-trained, frozen\ndiffusion model at every timestep. These readouts can encode single-image\nproperties, such as pose, depth, and edges; or higher-order properties that\nrelate multiple images, such as correspondence and appearance similarity.\nFurthermore, by comparing the readout estimates to a user-defined target, and\nback-propagating the gradient through the readout head, these estimates can be\nused to guide the sampling process. Compared to prior methods for conditional\ngeneration, Readout Guidance requires significantly fewer added parameters and\ntraining samples, and offers a convenient and simple recipe for reproducing\ndifferent forms of conditional control under a single framework, with a single\narchitecture and sampling procedure. We showcase these benefits in the\napplications of drag-based manipulation, identity-consistent generation, and\nspatially aligned control. Project page: https:\/\/readout-guidance.github.io.","terms":["cs.CV"]},{"titles":"Generative Powers of Ten","summaries":"We present a method that uses a text-to-image model to generate consistent\ncontent across multiple image scales, enabling extreme semantic zooms into a\nscene, e.g., ranging from a wide-angle landscape view of a forest to a macro\nshot of an insect sitting on one of the tree branches. We achieve this through\na joint multi-scale diffusion sampling approach that encourages consistency\nacross different scales while preserving the integrity of each individual\nsampling process. Since each generated scale is guided by a different text\nprompt, our method enables deeper levels of zoom than traditional\nsuper-resolution methods that may struggle to create new contextual structure\nat vastly different scales. We compare our method qualitatively with\nalternative techniques in image super-resolution and outpainting, and show that\nour method is most effective at generating consistent multi-scale content.","terms":["cs.CV","cs.AI","cs.CL","cs.GR"]},{"titles":"Rejuvenating image-GPT as Strong Visual Representation Learners","summaries":"This paper enhances image-GPT (iGPT), one of the pioneering works that\nintroduce autoregressive pretraining to predict next pixels for visual\nrepresentation learning. Two simple yet essential changes are made. First, we\nshift the prediction target from raw pixels to semantic tokens, enabling a\nhigher-level understanding of visual content. Second, we supplement the\nautoregressive modeling by instructing the model to predict not only the next\ntokens but also the visible tokens. This pipeline is particularly effective\nwhen semantic tokens are encoded by discriminatively trained models, such as\nCLIP. We introduce this novel approach as D-iGPT. Extensive experiments\nshowcase that D-iGPT excels as a strong learner of visual representations: A\nnotable achievement of D-iGPT is its compelling performance on the ImageNet-1K\ndataset -- by training on publicly available datasets, D-iGPT achieves 89.5\\%\ntop-1 accuracy with a vanilla ViT-Large model. This model also shows strong\ngeneralization on the downstream task and robustness on out-of-distribution\nsamples. Code is avaiable at\n\\href{https:\/\/github.com\/OliverRensu\/D-iGPT}{https:\/\/github.com\/OliverRensu\/D-iGPT}.","terms":["cs.CV"]},{"titles":"Learning Polynomial Problems with $SL(2,\\mathbb{R})$ Equivariance","summaries":"Optimizing and certifying the positivity of polynomials are fundamental\nprimitives across mathematics and engineering applications, from dynamical\nsystems to operations research. However, solving these problems in practice\nrequires large semidefinite programs, with poor scaling in dimension and\ndegree. In this work, we demonstrate for the first time that neural networks\ncan effectively solve such problems in a data-driven fashion, achieving tenfold\nspeedups while retaining high accuracy. Moreover, we observe that these\npolynomial learning problems are equivariant to the non-compact group\n$SL(2,\\mathbb{R})$, which consists of area-preserving linear transformations.\nWe therefore adapt our learning pipelines to accommodate this structure,\nincluding data augmentation, a new $SL(2,\\mathbb{R})$-equivariant architecture,\nand an architecture equivariant with respect to its maximal compact subgroup,\n$SO(2, \\mathbb{R})$. Surprisingly, the most successful approaches in practice\ndo not enforce equivariance to the entire group, which we prove arises from an\nunusual lack of architecture universality for $SL(2,\\mathbb{R})$ in particular.\nA consequence of this result, which is of independent interest, is that there\nexists an equivariant function for which there is no sequence of equivariant\npolynomials multiplied by arbitrary invariants that approximates the original\nfunction. This is a rare example of a symmetric problem where data augmentation\noutperforms a fully equivariant architecture, and provides interesting lessons\nin both theory and practice for other problems with non-compact symmetries.","terms":["cs.LG"]},{"titles":"Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation","summaries":"Monocular depth estimation is a fundamental computer vision task. Recovering\n3D depth from a single image is geometrically ill-posed and requires scene\nunderstanding, so it is not surprising that the rise of deep learning has led\nto a breakthrough. The impressive progress of monocular depth estimators has\nmirrored the growth in model capacity, from relatively modest CNNs to large\nTransformer architectures. Still, monocular depth estimators tend to struggle\nwhen presented with images with unfamiliar content and layout, since their\nknowledge of the visual world is restricted by the data seen during training,\nand challenged by zero-shot generalization to new domains. This motivates us to\nexplore whether the extensive priors captured in recent generative diffusion\nmodels can enable better, more generalizable depth estimation. We introduce\nMarigold, a method for affine-invariant monocular depth estimation that is\nderived from Stable Diffusion and retains its rich prior knowledge. The\nestimator can be fine-tuned in a couple of days on a single GPU using only\nsynthetic training data. It delivers state-of-the-art performance across a wide\nrange of datasets, including over 20% performance gains in specific cases.\nProject page: https:\/\/marigoldmonodepth.github.io.","terms":["cs.CV"]},{"titles":"Optimizing Camera Configurations for Multi-View Pedestrian Detection","summaries":"Jointly considering multiple camera views (multi-view) is very effective for\npedestrian detection under occlusion. For such multi-view systems, it is\ncritical to have well-designed camera configurations, including camera\nlocations, directions, and fields-of-view (FoVs). Usually, these configurations\nare crafted based on human experience or heuristics. In this work, we present a\nnovel solution that features a transformer-based camera configuration\ngenerator. Using reinforcement learning, this generator autonomously explores\nvast combinations within the action space and searches for configurations that\ngive the highest detection accuracy according to the training dataset. The\ngenerator learns advanced techniques like maximizing coverage, minimizing\nocclusion, and promoting collaboration. Across multiple simulation scenarios,\nthe configurations generated by our transformer-based model consistently\noutperform random search, heuristic-based methods, and configurations designed\nby human experts, shedding light on future camera layout optimization.","terms":["cs.CV"]},{"titles":"Object Recognition as Next Token Prediction","summaries":"We present an approach to pose object recognition as next token prediction.\nThe idea is to apply a language decoder that auto-regressively predicts the\ntext tokens from image embeddings to form labels. To ground this prediction\nprocess in auto-regression, we customize a non-causal attention mask for the\ndecoder, incorporating two key features: modeling tokens from different labels\nto be independent, and treating image tokens as a prefix. This masking\nmechanism inspires an efficient method - one-shot sampling - to simultaneously\nsample tokens of multiple labels in parallel and rank generated labels by their\nprobabilities during inference. To further enhance the efficiency, we propose a\nsimple strategy to construct a compact decoder by simply discarding the\nintermediate blocks of a pretrained language model. This approach yields a\ndecoder that matches the full model's performance while being notably more\nefficient. The code is available at https:\/\/github.com\/kaiyuyue\/nxtp","terms":["cs.CV"]},{"titles":"EMDM: Efficient Motion Diffusion Model for Fast, High-Quality Motion Generation","summaries":"We introduce Efficient Motion Diffusion Model (EMDM) for fast and\nhigh-quality human motion generation. Although previous motion diffusion models\nhave shown impressive results, they struggle to achieve fast generation while\nmaintaining high-quality human motions. Motion latent diffusion has been\nproposed for efficient motion generation. However, effectively learning a\nlatent space can be non-trivial in such a two-stage manner. Meanwhile,\naccelerating motion sampling by increasing the step size, e.g., DDIM, typically\nleads to a decline in motion quality due to the inapproximation of complex data\ndistributions when naively increasing the step size. In this paper, we propose\nEMDM that allows for much fewer sample steps for fast motion generation by\nmodeling the complex denoising distribution during multiple sampling steps.\nSpecifically, we develop a Conditional Denoising Diffusion GAN to capture\nmultimodal data distributions conditioned on both control signals, i.e.,\ntextual description and denoising time step. By modeling the complex data\ndistribution, a larger sampling step size and fewer steps are achieved during\nmotion synthesis, significantly accelerating the generation process. To\neffectively capture the human dynamics and reduce undesired artifacts, we\nemploy motion geometric loss during network training, which improves the motion\nquality and training efficiency. As a result, EMDM achieves a remarkable\nspeed-up at the generation stage while maintaining high-quality motion\ngeneration in terms of fidelity and diversity.","terms":["cs.CV","cs.AI","cs.GR"]},{"titles":"iMatching: Imperative Correspondence Learning","summaries":"Learning feature correspondence is a foundational task in computer vision,\nholding immense importance for downstream applications such as visual odometry\nand 3D reconstruction. Despite recent progress in data-driven models, feature\ncorrespondence learning is still limited by the lack of accurate per-pixel\ncorrespondence labels. To overcome this difficulty, we introduce a new\nself-supervised scheme, imperative learning (IL), for training feature\ncorrespondence. It enables correspondence learning on arbitrary uninterrupted\nvideos without any camera pose or depth labels, heralding a new era for\nself-supervised correspondence learning. Specifically, we formulated the\nproblem of correspondence learning as a bilevel optimization, which takes the\nreprojection error from bundle adjustment as a supervisory signal for the\nmodel. To avoid large memory and computation overhead, we leverage the\nstationary point to effectively back-propagate the implicit gradients through\nbundle adjustment. Through extensive experiments, we demonstrate superior\nperformance on tasks including feature matching and pose estimation, in which\nwe obtained an average of 30% accuracy gain over the state-of-the-art matching\nmodels.","terms":["cs.CV"]},{"titles":"DiffiT: Diffusion Vision Transformers for Image Generation","summaries":"Diffusion models with their powerful expressivity and high sample quality\nhave enabled many new applications and use-cases in various domains. For sample\ngeneration, these models rely on a denoising neural network that generates\nimages by iterative denoising. Yet, the role of denoising network architecture\nis not well-studied with most efforts relying on convolutional residual U-Nets.\nIn this paper, we study the effectiveness of vision transformers in\ndiffusion-based generative learning. Specifically, we propose a new model,\ndenoted as Diffusion Vision Transformers (DiffiT), which consists of a hybrid\nhierarchical architecture with a U-shaped encoder and decoder. We introduce a\nnovel time-dependent self-attention module that allows attention layers to\nadapt their behavior at different stages of the denoising process in an\nefficient manner. We also introduce latent DiffiT which consists of transformer\nmodel with the proposed self-attention layers, for high-resolution image\ngeneration. Our results show that DiffiT is surprisingly effective in\ngenerating high-fidelity images, and it achieves state-of-the-art (SOTA)\nbenchmarks on a variety of class-conditional and unconditional synthesis tasks.\nIn the latent space, DiffiT achieves a new SOTA FID score of 1.73 on\nImageNet-256 dataset. Repository: https:\/\/github.com\/NVlabs\/DiffiT","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"MANUS: Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians","summaries":"Understanding how we grasp objects with our hands has important applications\nin areas like robotics and mixed reality. However, this challenging problem\nrequires accurate modeling of the contact between hands and objects. To capture\ngrasps, existing methods use skeletons, meshes, or parametric models that can\ncause misalignments resulting in inaccurate contacts. We present MANUS, a\nmethod for Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians.\nWe build a novel articulated 3D Gaussians representation that extends 3D\nGaussian splatting for high-fidelity representation of articulating hands.\nSince our representation uses Gaussian primitives, it enables us to efficiently\nand accurately estimate contacts between the hand and the object. For the most\naccurate results, our method requires tens of camera views that current\ndatasets do not provide. We therefore build MANUS-Grasps, a new dataset that\ncontains hand-object grasps viewed from 53 cameras across 30+ scenes, 3\nsubjects, and comprising over 7M frames. In addition to extensive qualitative\nresults, we also show that our method outperforms others on a quantitative\ncontact evaluation method that uses paint transfer from the object to the hand.","terms":["cs.CV"]},{"titles":"BerfScene: Bev-conditioned Equivariant Radiance Fields for Infinite 3D Scene Generation","summaries":"Generating large-scale 3D scenes cannot simply apply existing 3D object\nsynthesis technique since 3D scenes usually hold complex spatial configurations\nand consist of a number of objects at varying scales. We thus propose a\npractical and efficient 3D representation that incorporates an equivariant\nradiance field with the guidance of a bird's-eye view (BEV) map. Concretely,\nobjects of synthesized 3D scenes could be easily manipulated through steering\nthe corresponding BEV maps. Moreover, by adequately incorporating positional\nencoding and low-pass filters into the generator, the representation becomes\nequivariant to the given BEV map. Such equivariance allows us to produce\nlarge-scale, even infinite-scale, 3D scenes via synthesizing local scenes and\nthen stitching them with smooth consistency. Extensive experiments on 3D scene\ndatasets demonstrate the effectiveness of our approach. Our project website is\nat https:\/\/zqh0253.github.io\/BerfScene\/.","terms":["cs.CV"]},{"titles":"Re-Nerfing: Enforcing Geometric Constraints on Neural Radiance Fields through Novel Views Synthesis","summaries":"Neural Radiance Fields (NeRFs) have shown remarkable novel view synthesis\ncapabilities even in large-scale, unbounded scenes, albeit requiring hundreds\nof views or introducing artifacts in sparser settings. Their optimization\nsuffers from shape-radiance ambiguities wherever only a small visual overlap is\navailable. This leads to erroneous scene geometry and artifacts. In this paper,\nwe propose Re-Nerfing, a simple and general multi-stage approach that leverages\nNeRF's own view synthesis to address these limitations. With Re-Nerfing, we\nincrease the scene's coverage and enhance the geometric consistency of novel\nviews as follows: First, we train a NeRF with the available views. Then, we use\nthe optimized NeRF to synthesize pseudo-views next to the original ones to\nsimulate a stereo or trifocal setup. Finally, we train a second NeRF with both\noriginal and pseudo views while enforcing structural, epipolar constraints via\nthe newly synthesized images. Extensive experiments on the mip-NeRF 360 dataset\nshow the effectiveness of Re-Nerfing across denser and sparser input scenarios,\nbringing improvements to the state-of-the-art Zip-NeRF, even when trained with\nall views.","terms":["cs.CV","cs.GR","cs.LG"]},{"titles":"Fast View Synthesis of Casual Videos","summaries":"Novel view synthesis from an in-the-wild video is difficult due to challenges\nlike scene dynamics and lack of parallax. While existing methods have shown\npromising results with implicit neural radiance fields, they are slow to train\nand render. This paper revisits explicit video representations to synthesize\nhigh-quality novel views from a monocular video efficiently. We treat static\nand dynamic video content separately. Specifically, we build a global static\nscene model using an extended plane-based scene representation to synthesize\ntemporally coherent novel video. Our plane-based scene representation is\naugmented with spherical harmonics and displacement maps to capture\nview-dependent effects and model non-planar complex surface geometry. We opt to\nrepresent the dynamic content as per-frame point clouds for efficiency. While\nsuch representations are inconsistency-prone, minor temporal inconsistencies\nare perceptually masked due to motion. We develop a method to quickly estimate\nsuch a hybrid video representation and render novel views in real time. Our\nexperiments show that our method can render high-quality novel views from an\nin-the-wild video with comparable quality to state-of-the-art methods while\nbeing 100x faster in training and enabling real-time rendering.","terms":["cs.CV"]},{"titles":"GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians","summaries":"We present GaussianAvatar, an efficient approach to creating realistic human\navatars with dynamic 3D appearances from a single video. We start by\nintroducing animatable 3D Gaussians to explicitly represent humans in various\nposes and clothing styles. Such an explicit and animatable representation can\nfuse 3D appearances more efficiently and consistently from 2D observations. Our\nrepresentation is further augmented with dynamic properties to support\npose-dependent appearance modeling, where a dynamic appearance network along\nwith an optimizable feature tensor is designed to learn the\nmotion-to-appearance mapping. Moreover, by leveraging the differentiable motion\ncondition, our method enables a joint optimization of motions and appearances\nduring avatar modeling, which helps to tackle the long-standing issue of\ninaccurate motion estimation in monocular settings. The efficacy of\nGaussianAvatar is validated on both the public dataset and our collected\ndataset, demonstrating its superior performances in terms of appearance quality\nand rendering efficiency.","terms":["cs.CV"]},{"titles":"Style Aligned Image Generation via Shared Attention","summaries":"Large-scale Text-to-Image (T2I) models have rapidly gained prominence across\ncreative fields, generating visually compelling outputs from textual prompts.\nHowever, controlling these models to ensure consistent style remains\nchallenging, with existing methods necessitating fine-tuning and manual\nintervention to disentangle content and style. In this paper, we introduce\nStyleAligned, a novel technique designed to establish style alignment among a\nseries of generated images. By employing minimal `attention sharing' during the\ndiffusion process, our method maintains style consistency across images within\nT2I models. This approach allows for the creation of style-consistent images\nusing a reference style through a straightforward inversion operation. Our\nmethod's evaluation across diverse styles and text prompts demonstrates\nhigh-quality synthesis and fidelity, underscoring its efficacy in achieving\nconsistent style across various inputs.","terms":["cs.CV","cs.GR","cs.LG"]},{"titles":"Hot PATE: Private Aggregation of Distributions for Diverse Task","summaries":"The Private Aggregation of Teacher Ensembles (PATE)\nframework~\\cite{PapernotAEGT:ICLR2017} is a versatile approach to\nprivacy-preserving machine learning. In PATE, teacher models are trained on\ndistinct portions of sensitive data, and their predictions are privately\naggregated to label new training examples for a student model.\n  Until now, PATE has primarily been explored with classification-like tasks,\nwhere each example possesses a ground-truth label, and knowledge is transferred\nto the student by labeling public examples. Generative AI models, however,\nexcel in open ended \\emph{diverse} tasks with multiple valid responses and\nscenarios that may not align with traditional labeled examples. Furthermore,\nthe knowledge of models is often encapsulated in the response distribution\nitself and may be transferred from teachers to student in a more fluid way. We\npropose \\emph{hot PATE}, tailored for the diverse setting. In hot PATE, each\nteacher model produces a response distribution and the aggregation method must\npreserve both privacy and diversity of responses. We demonstrate, analytically\nand empirically, that hot PATE achieves privacy-utility tradeoffs that are\ncomparable to, and in diverse settings, significantly surpass, the baseline\n``cold'' PATE.","terms":["cs.LG","cs.AI","cs.CR","cs.DS"]},{"titles":"Can we truly transfer an actor's genuine happiness to avatars? An investigation into virtual, real, posed and spontaneous faces","summaries":"A look is worth a thousand words is a popular phrase. And why is a simple\nlook enough to portray our feelings about something or someone? Behind this\nquestion are the theoretical foundations of the field of psychology regarding\nsocial cognition and the studies of psychologist Paul Ekman. Facial\nexpressions, as a form of non-verbal communication, are the primary way to\ntransmit emotions between human beings. The set of movements and expressions of\nfacial muscles that convey some emotional state of the individual to their\nobservers are targets of studies in many areas. Our research aims to evaluate\nEkman's action units in datasets of real human faces, posed and spontaneous,\nand virtual human faces resulting from transferring real faces into Computer\nGraphics faces. In addition, we also conducted a case study with specific movie\ncharacters, such as SheHulk and Genius. We intend to find differences and\nsimilarities in facial expressions between real and CG datasets, posed and\nspontaneous faces, and also to consider the actors' genders in the videos. This\ninvestigation can help several areas of knowledge, whether using real or\nvirtual human beings, in education, health, entertainment, games, security, and\neven legal matters. Our results indicate that AU intensities are greater for\nposed than spontaneous datasets, regardless of gender. Furthermore, there is a\nsmoothing of intensity up to 80 percent for AU6 and 45 percent for AU12 when a\nreal face is transformed into CG.","terms":["cs.CV"]},{"titles":"SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM","summaries":"Dense simultaneous localization and mapping (SLAM) is pivotal for embodied\nscene understanding. Recent work has shown that 3D Gaussians enable\nhigh-quality reconstruction and real-time rendering of scenes using multiple\nposed cameras. In this light, we show for the first time that representing a\nscene by 3D Gaussians can enable dense SLAM using a single unposed monocular\nRGB-D camera. Our method, SplaTAM, addresses the limitations of prior radiance\nfield-based representations, including fast rendering and optimization, the\nability to determine if areas have been previously mapped, and structured map\nexpansion by adding more Gaussians. We employ an online tracking and mapping\npipeline while tailoring it to specifically use an underlying Gaussian\nrepresentation and silhouette-guided optimization via differentiable rendering.\nExtensive experiments show that SplaTAM achieves up to 2X state-of-the-art\nperformance in camera pose estimation, map construction, and novel-view\nsynthesis, demonstrating its superiority over existing approaches, while\nallowing real-time rendering of a high-resolution dense 3D map.","terms":["cs.CV","cs.AI","cs.RO"]},{"titles":"AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents","summaries":"We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses\nsequence models to tackle the challenges of generalization, long-term memory,\nand meta-learning. Recent works have shown that off-policy learning can make\nin-context RL with recurrent policies viable. Nonetheless, these approaches\nrequire extensive tuning and limit scalability by creating key bottlenecks in\nagents' memory capacity, planning horizon, and model size. AMAGO revisits and\nredesigns the off-policy in-context approach to successfully train\nlong-sequence Transformers over entire rollouts in parallel with end-to-end RL.\nOur agent is uniquely scalable and applicable to a wide range of problems. We\ndemonstrate its strong performance empirically in meta-RL and long-term memory\ndomains. AMAGO's focus on sparse rewards and off-policy data also allows\nin-context learning to extend to goal-conditioned problems with challenging\nexploration. When combined with a novel hindsight relabeling scheme, AMAGO can\nsolve a previously difficult category of open-world domains, where agents\ncomplete many possible instructions in procedurally generated environments. We\nevaluate our agent on three goal-conditioned domains and study how its\nindividual improvements connect to create a generalist policy.","terms":["cs.LG"]},{"titles":"VerA: Versatile Anonymization Fit for Clinical Facial Images","summaries":"The escalating legislative demand for data privacy in facial image\ndissemination has underscored the significance of image anonymization. Recent\nadvancements in the field surpass traditional pixelation or blur methods, yet\nthey predominantly address regular single images. This leaves clinical image\nanonymization -- a necessity for illustrating medical interventions -- largely\nunaddressed. We present VerA, a versatile facial image anonymization that is\nfit for clinical facial images where: (1) certain semantic areas must be\npreserved to show medical intervention results, and (2) anonymizing image pairs\nis crucial for showing before-and-after results. VerA outperforms or is on par\nwith state-of-the-art methods in de-identification and photorealism for regular\nimages. In addition, we validate our results on paired anonymization, and on\nthe anonymization of both single and paired clinical images with extensive\nquantitative and qualitative evaluation.","terms":["cs.CV","cs.GR","cs.LG"]},{"titles":"Tree of Attacks: Jailbreaking Black-Box LLMs Automatically","summaries":"While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an LLM to\niteratively refine candidate (attack) prompts using tree-of-thoughts reasoning\nuntil one of the generated prompts jailbreaks the target. Crucially, before\nsending prompts to the target, TAP assesses them and prunes the ones unlikely\nto result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate\na large search space of prompts and pruning reduces the total number of queries\nsent to the target. In empirical evaluations, we observe that TAP generates\nprompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo)\nfor more than 80% of the prompts using only a small number of queries. This\nsignificantly improves upon the previous state-of-the-art black-box method for\ngenerating jailbreaks.","terms":["cs.LG","cs.AI","cs.CL","cs.CR","stat.ML"]},{"titles":"GIVT: Generative Infinite-Vocabulary Transformers","summaries":"We introduce generative infinite-vocabulary transformers (GIVT) which\ngenerate vector sequences with real-valued entries, instead of discrete tokens\nfrom a finite vocabulary. To this end, we propose two surprisingly simple\nmodifications to decoder-only transformers: 1) at the input, we replace the\nfinite-vocabulary lookup table with a linear projection of the input vectors;\nand 2) at the output, we replace the logits prediction (usually mapped to a\ncategorical distribution) with the parameters of a multivariate Gaussian\nmixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT,\nwhere transformers are used to model the discrete latent sequences of a VQ-VAE,\nwe use GIVT to model the unquantized real-valued latent sequences of a VAE.\nWhen applying GIVT to class-conditional image generation with iterative masked\nmodeling, we show competitive results with MaskGIT, while our approach\noutperforms both VQ-GAN and MaskGIT when using it for causal modeling. Finally,\nwe obtain competitive results outside of image generation when applying our\napproach to panoptic segmentation and depth estimation with a VAE-based variant\nof the UViM framework.","terms":["cs.CV"]},{"titles":"Innovations in Agricultural Forecasting: A Multivariate Regression Study on Global Crop Yield Prediction","summaries":"The prediction of crop yields internationally is a crucial objective in\nagricultural research. Thus, this study implements 6 regression models (Linear,\nTree, Gradient Descent, Gradient Boosting, K- Nearest Neighbors, and Random\nForest) to predict crop yields in 196 countries. Given 4 key training\nparameters, pesticides (tonnes), rainfall (mm), temperature (Celsius), and\nyield (hg\/ha), it was found that our Random Forest Regression model achieved a\ndetermination coefficient (r^2) of 0.94, with a margin of error (ME) of .03.\nThe models were trained and tested using the Food and Agricultural Organization\nof the United Nations data, along with the World Bank Climate Change Data\nCatalog. Furthermore, each parameter was analyzed to understand how varying\nfactors could impact overall yield. We used unconventional models, contrary to\ngenerally used Deep Learning (DL) and Machine Learning (ML) models, combined\nwith recently collected data to implement a unique approach in our research.\nExisting scholarship would benefit from understanding the most optimal model\nfor agricultural research, specifically using the United Nations data.","terms":["cs.LG","cs.AI","68W03"]},{"titles":"SCOPE-RL: A Python Library for Offline Reinforcement Learning and Off-Policy Evaluation","summaries":"This paper introduces SCOPE-RL, a comprehensive open-source Python software\ndesigned for offline reinforcement learning (offline RL), off-policy evaluation\n(OPE), and selection (OPS). Unlike most existing libraries that focus solely on\neither policy learning or evaluation, SCOPE-RL seamlessly integrates these two\nkey aspects, facilitating flexible and complete implementations of both offline\nRL and OPE processes. SCOPE-RL put particular emphasis on its OPE modules,\noffering a range of OPE estimators and robust evaluation-of-OPE protocols. This\napproach enables more in-depth and reliable OPE compared to other packages. For\ninstance, SCOPE-RL enhances OPE by estimating the entire reward distribution\nunder a policy rather than its mere point-wise expected value. Additionally,\nSCOPE-RL provides a more thorough evaluation-of-OPE by presenting the\nrisk-return tradeoff in OPE results, extending beyond mere accuracy evaluations\nin existing OPE literature. SCOPE-RL is designed with user accessibility in\nmind. Its user-friendly APIs, comprehensive documentation, and a variety of\neasy-to-follow examples assist researchers and practitioners in efficiently\nimplementing and experimenting with various offline RL methods and OPE\nestimators, tailored to their specific problem contexts. The documentation of\nSCOPE-RL is available at https:\/\/scope-rl.readthedocs.io\/en\/latest\/.","terms":["cs.LG","cs.AI"]},{"titles":"A new sampling methodology for defining heterogeneous subsets of samples for training image segmentation algorithms","summaries":"Creating a dataset for training supervised machine learning algorithms can be\na demanding task. This is especially true for medical image segmentation since\none or more specialists are usually required for image annotation, and creating\nground truth labels for just a single image can take up to several hours. In\naddition, it is paramount that the annotated samples represent well the\ndifferent conditions that might affect the imaged tissues as well as possible\nchanges in the image acquisition process. This can only be achieved by\nconsidering samples that are typical in the dataset as well as atypical, or\neven outlier, samples. We introduce a new sampling methodology for selecting\nrelevant images from a large dataset in a way that evenly considers both\nprototypical as well as atypical samples. The methodology involves the\ngeneration of a uniform grid from a feature space representing the samples,\nwhich is then used for randomly drawing relevant images. The selected images\nprovide a uniform covering of the original dataset, and thus define a\nheterogeneous set of images that can be annotated and used for training\nsupervised segmentation algorithms. We provide a case example by creating a\ndataset containing a representative set of blood vessel microscopy images\nselected from a larger dataset containing thousands of images. The dataset,\nwhich we call VessMAP, is being made available online to aid the development of\nnew blood vessel segmentation algorithms.","terms":["cs.CV"]},{"titles":"ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder and Explicit Adaptation","summaries":"This work introduces ArtAdapter, a transformative text-to-image (T2I) style\ntransfer framework that transcends traditional limitations of color,\nbrushstrokes, and object shape, capturing high-level style elements such as\ncomposition and distinctive artistic expression. The integration of a\nmulti-level style encoder with our proposed explicit adaptation mechanism\nenables ArtAdapte to achieve unprecedented fidelity in style transfer, ensuring\nclose alignment with textual descriptions. Additionally, the incorporation of\nan Auxiliary Content Adapter (ACA) effectively separates content from style,\nalleviating the borrowing of content from style references. Moreover, our novel\nfast finetuning approach could further enhance zero-shot style representation\nwhile mitigating the risk of overfitting. Comprehensive evaluations confirm\nthat ArtAdapter surpasses current state-of-the-art methods.","terms":["cs.CV"]},{"titles":"Towards Assessing and Benchmarking Risk-Return Tradeoff of Off-Policy Evaluation","summaries":"Off-Policy Evaluation (OPE) aims to assess the effectiveness of\ncounterfactual policies using only offline logged data and is often used to\nidentify the top-k promising policies for deployment in online A\/B tests.\nExisting evaluation metrics for OPE estimators primarily focus on the\n\"accuracy\" of OPE or that of downstream policy selection, neglecting\nrisk-return tradeoff in the subsequent online policy deployment. To address\nthis issue, we draw inspiration from portfolio evaluation in finance and\ndevelop a new metric, called SharpeRatio@k, which measures the risk-return\ntradeoff of policy portfolios formed by an OPE estimator under varying online\nevaluation budgets (k). We validate our metric in two example scenarios,\ndemonstrating its ability to effectively distinguish between low-risk and\nhigh-risk estimators and to accurately identify the most efficient estimator.\nThis efficient estimator is characterized by its capability to form the most\nadvantageous policy portfolios, maximizing returns while minimizing risks\nduring online deployment, a nuance that existing metrics typically overlook. To\nfacilitate a quick, accurate, and consistent evaluation of OPE via\nSharpeRatio@k, we have also integrated this metric into an open-source\nsoftware, SCOPE-RL. Employing SharpeRatio@k and SCOPE-RL, we conduct\ncomprehensive benchmarking experiments on various estimators and RL tasks,\nfocusing on their risk-return tradeoff. These experiments offer several\ninteresting directions and suggestions for future OPE research.","terms":["cs.LG","cs.AI"]},{"titles":"Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with Synthetic Images","summaries":"Recent advances in generative deep learning have enabled the creation of\nhigh-quality synthetic images in text-to-image generation. Prior work shows\nthat fine-tuning a pretrained diffusion model on ImageNet and generating\nsynthetic training images from the finetuned model can enhance an ImageNet\nclassifier's performance. However, performance degrades as synthetic images\noutnumber real ones. In this paper, we explore whether generative fine-tuning\nis essential for this improvement and whether it is possible to further scale\nup training using more synthetic data. We present a new framework leveraging\noff-the-shelf generative models to generate synthetic training images,\naddressing multiple challenges: class name ambiguity, lack of diversity in\nnaive prompts, and domain shifts. Specifically, we leverage large language\nmodels (LLMs) and CLIP to resolve class name ambiguity. To diversify images, we\npropose contextualized diversification (CD) and stylized diversification (SD)\nmethods, also prompted by LLMs. Finally, to mitigate domain shifts, we leverage\ndomain adaptation techniques with auxiliary batch normalization for synthetic\nimages. Our framework consistently enhances recognition model performance with\nmore synthetic data, up to 6x of original ImageNet size showcasing the\npotential of synthetic data for improved recognition models and strong\nout-of-domain generalization.","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"Learning Pseudo-Labeler beyond Noun Concepts for Open-Vocabulary Object Detection","summaries":"Open-vocabulary object detection (OVOD) has recently gained significant\nattention as a crucial step toward achieving human-like visual intelligence.\nExisting OVOD methods extend target vocabulary from pre-defined categories to\nopen-world by transferring knowledge of arbitrary concepts from vision-language\npre-training models to the detectors. While previous methods have shown\nremarkable successes, they suffer from indirect supervision or limited\ntransferable concepts. In this paper, we propose a simple yet effective method\nto directly learn region-text alignment for arbitrary concepts. Specifically,\nthe proposed method aims to learn arbitrary image-to-text mapping for\npseudo-labeling of arbitrary concepts, named Pseudo-Labeling for Arbitrary\nConcepts (PLAC). The proposed method shows competitive performance on the\nstandard OVOD benchmark for noun concepts and a large improvement on referring\nexpression comprehension benchmark for arbitrary concepts.","terms":["cs.CV"]},{"titles":"Mitigating Data Injection Attacks on Federated Learning","summaries":"Federated learning is a technique that allows multiple entities to\ncollaboratively train models using their data without compromising data\nprivacy. However, despite its advantages, federated learning can be susceptible\nto false data injection attacks. In these scenarios, a malicious entity with\ncontrol over specific agents in the network can manipulate the learning\nprocess, leading to a suboptimal model. Consequently, addressing these data\ninjection attacks presents a significant research challenge in federated\nlearning systems. In this paper, we propose a novel technique to detect and\nmitigate data injection attacks on federated learning systems. Our mitigation\nmethod is a local scheme, performed during a single instance of training by the\ncoordinating node, allowing the mitigation during the convergence of the\nalgorithm. Whenever an agent is suspected to be an attacker, its data will be\nignored for a certain period, this decision will often be re-evaluated. We\nprove that with probability 1, after a finite time, all attackers will be\nignored while the probability of ignoring a trustful agent becomes 0, provided\nthat there is a majority of truthful agents. Simulations show that when the\ncoordinating node detects and isolates all the attackers, the model recovers\nand converges to the truthful model.","terms":["cs.LG","cs.CR","eess.SP"]},{"titles":"Accuracy Improvement in Differentially Private Logistic Regression: A Pre-training Approach","summaries":"Machine learning (ML) models can memorize training datasets. As a result,\ntraining ML models over private datasets can lead to the violation of\nindividuals' privacy. Differential privacy (DP) is a rigorous privacy notion to\npreserve the privacy of underlying training datasets. Yet, training ML models\nin a DP framework usually degrades the accuracy of ML models. This paper aims\nto boost the accuracy of a DP logistic regression (LR) via a pre-training\nmodule. In more detail, we initially pre-train our LR model on a public\ntraining dataset that there is no privacy concern about it. Then, we fine-tune\nour DP-LR model with the private dataset. In the numerical results, we show\nthat adding a pre-training module significantly improves the accuracy of the\nDP-LR model.","terms":["cs.LG","cs.CR"]},{"titles":"A Nonstochastic Control Approach to Optimization","summaries":"Selecting the best hyperparameters for a particular optimization instance,\nsuch as the learning rate and momentum, is an important but nonconvex problem.\nAs a result, iterative optimization methods such as hypergradient descent lack\nglobal optimality guarantees in general.\n  We propose an online nonstochastic control methodology for mathematical\noptimization. First, we formalize the setting of meta-optimization, an online\nlearning formulation of learning the best optimization algorithm from a class\nof methods. The meta-optimization problem over gradient-based methods can be\nframed as a feedback control problem over the choice of hyperparameters,\nincluding the learning rate, momentum, and the preconditioner.\n  Although the original optimal control problem is nonconvex, we show how\nrecent methods from online nonstochastic control using convex relaxations can\nbe used to overcome the challenge of nonconvexity, and obtain regret guarantees\nagainst the best offline solution. This guarantees that in meta-optimization,\ngiven a sequence of optimization problems, we can learn a method that attains\nconvergence comparable to that of the best optimization method in hindsight\nfrom a class of methods.","terms":["cs.LG"]},{"titles":"Large Language Models as Consistent Story Visualizers","summaries":"Recent generative models have demonstrated impressive capabilities in\ngenerating realistic and visually pleasing images grounded on textual prompts.\nNevertheless, a significant challenge remains in applying these models for the\nmore intricate task of story visualization. Since it requires resolving\npronouns (he, she, they) in the frame descriptions, i.e., anaphora resolution,\nand ensuring consistent characters and background synthesis across frames. Yet,\nthe emerging Large Language Model (LLM) showcases robust reasoning abilities to\nnavigate through ambiguous references and process extensive sequences.\nTherefore, we introduce \\textbf{StoryGPT-V}, which leverages the merits of the\nlatent diffusion (LDM) and LLM to produce images with consistent and\nhigh-quality characters grounded on given story descriptions. First, we train a\ncharacter-aware LDM, which takes character-augmented semantic embedding as\ninput and includes the supervision of the cross-attention map using character\nsegmentation masks, aiming to enhance character generation accuracy and\nfaithfulness. In the second stage, we enable an alignment between the output of\nLLM and the character-augmented embedding residing in the input space of the\nfirst-stage model. This harnesses the reasoning ability of LLM to address\nambiguous references and the comprehension capability to memorize the context.\nWe conduct comprehensive experiments on two visual story visualization\nbenchmarks. Our model reports superior quantitative results and consistently\ngenerates accurate characters of remarkable quality with low memory\nconsumption. Our code will be made publicly available.","terms":["cs.CV"]},{"titles":"Single-sample versus case-control sampling scheme for Positive Unlabeled data: the story of two scenarios","summaries":"In the paper we argue that performance of the classifiers based on Empirical\nRisk Minimization (ERM) for positive unlabeled data, which are designed for\ncase-control sampling scheme may significantly deteriorate when applied to a\nsingle-sample scenario. We reveal why their behavior depends, in all but very\nspecific cases, on the scenario. Also, we introduce a single-sample case\nanalogue of the popular non-negative risk classifier designed for case-control\ndata and compare its performance with the original proposal. We show that the\nsignificant differences occur between them, especiall when half or more\npositive of observations are labeled. The opposite case when ERM minimizer\ndesigned for the case-control case is applied for single-sample data is also\nconsidered and similar conclusions are drawn. Taking into account difference of\nscenarios requires a sole, but crucial, change in the definition of the\nEmpirical Risk.","terms":["cs.LG"]},{"titles":"I-AI: A Controllable & Interpretable AI System for Decoding Radiologists' Intense Focus for Accurate CXR Diagnoses","summaries":"In the field of chest X-ray (CXR) diagnosis, existing works often focus\nsolely on determining where a radiologist looks, typically through tasks such\nas detection, segmentation, or classification. However, these approaches are\noften designed as black-box models, lacking interpretability. In this paper, we\nintroduce Interpretable Artificial Intelligence (I-AI) a novel and unified\ncontrollable interpretable pipeline for decoding the intense focus of\nradiologists in CXR diagnosis. Our I-AI addresses three key questions: where a\nradiologist looks, how long they focus on specific areas, and what findings\nthey diagnose. By capturing the intensity of the radiologist's gaze, we provide\na unified solution that offers insights into the cognitive process underlying\nradiological interpretation. Unlike current methods that rely on black-box\nmachine learning models, which can be prone to extracting erroneous information\nfrom the entire input image during the diagnosis process, we tackle this issue\nby effectively masking out irrelevant information. Our proposed I-AI leverages\na vision-language model, allowing for precise control over the interpretation\nprocess while ensuring the exclusion of irrelevant features. To train our I-AI\nmodel, we utilize an eye gaze dataset to extract anatomical gaze information\nand generate ground truth heatmaps. Through extensive experimentation, we\ndemonstrate the efficacy of our method. We showcase that the attention\nheatmaps, designed to mimic radiologists' focus, encode sufficient and relevant\ninformation, enabling accurate classification tasks using only a portion of\nCXR.","terms":["cs.CV","cs.AI"]},{"titles":"Integrating AI into CCTV Systems: A Comprehensive Evaluation of Smart Video Surveillance in Community Space","summaries":"This article presents an AI-enabled Smart Video Surveillance (SVS) designed\nto enhance safety in community spaces such as educational and recreational\nareas, and small businesses. The proposed system innovatively integrates with\nexisting CCTV and wired camera networks, simplifying its adoption across\nvarious community cases to leverage recent AI advancements. Our SVS system,\nfocusing on privacy, uses metadata instead of pixel data for activity\nrecognition, aligning with ethical standards. It features cloud-based\ninfrastructure and a mobile app for real-time, privacy-conscious alerts in\ncommunities.\n  This article notably pioneers a comprehensive real-world evaluation of the\nSVS system, covering AI-driven visual processing, statistical analysis,\ndatabase management, cloud communication, and user notifications. It's also the\nfirst to assess an end-to-end anomaly detection system's performance, vital for\nidentifying potential public safety incidents.\n  For our evaluation, we implemented the system in a community college, serving\nas an ideal model to exemplify the proposed system's capabilities. Our findings\nin this setting demonstrate the system's robustness, with throughput, latency,\nand scalability effectively managing 16 CCTV cameras. The system maintained a\nconsistent 16.5 frames per second (FPS) over a 21-hour operation. The average\nend-to-end latency for detecting behavioral anomalies and alerting users was\n26.76 seconds.","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"Direct Unsupervised Denoising","summaries":"Traditional supervised denoisers are trained using pairs of noisy input and\nclean target images. They learn to predict a central tendency of the posterior\ndistribution over possible clean images. When, e.g., trained with the popular\nquadratic loss function, the network's output will correspond to the minimum\nmean square error (MMSE) estimate. Unsupervised denoisers based on Variational\nAutoEncoders (VAEs) have succeeded in achieving state-of-the-art results while\nrequiring only unpaired noisy data as training input. In contrast to the\ntraditional supervised approach, unsupervised denoisers do not directly produce\na single prediction, such as the MMSE estimate, but allow us to draw samples\nfrom the posterior distribution of clean solutions corresponding to the noisy\ninput. To approximate the MMSE estimate during inference, unsupervised methods\nhave to create and draw a large number of samples - a computationally expensive\nprocess - rendering the approach inapplicable in many situations. Here, we\npresent an alternative approach that trains a deterministic network alongside\nthe VAE to directly predict a central tendency. Our method achieves results\nthat surpass the results achieved by the unsupervised method at a fraction of\nthe computational cost.","terms":["cs.CV","eess.IV"]},{"titles":"Improving Intrinsic Exploration by Creating Stationary Objectives","summaries":"Exploration bonuses in reinforcement learning guide long-horizon exploration\nby defining custom intrinsic objectives. Several exploration objectives like\ncount-based bonuses, pseudo-counts, and state-entropy maximization are\nnon-stationary and hence are difficult to optimize for the agent. While this\nissue is generally known, it is usually omitted and solutions remain\nunder-explored. The key contribution of our work lies in transforming the\noriginal non-stationary rewards into stationary rewards through an augmented\nstate representation. For this purpose, we introduce the Stationary Objectives\nFor Exploration (SOFE) framework. SOFE requires identifying sufficient\nstatistics for different exploration bonuses and finding an efficient encoding\nof these statistics to use as input to a deep network. SOFE is based on\nproposing state augmentations that expand the state space but hold the promise\nof simplifying the optimization of the agent's objective. We show that SOFE\nimproves the performance of several exploration objectives, including\ncount-based bonuses, pseudo-counts, and state-entropy maximization. Moreover,\nSOFE outperforms prior methods that attempt to stabilize the optimization of\nintrinsic objectives. We demonstrate the efficacy of SOFE in hard-exploration\nproblems, including sparse-reward tasks, pixel-based observations, 3D\nnavigation, and procedurally generated environments.","terms":["cs.LG","cs.AI"]},{"titles":"GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians","summaries":"We introduce GaussianAvatars, a new method to create photorealistic head\navatars that are fully controllable in terms of expression, pose, and\nviewpoint. The core idea is a dynamic 3D representation based on 3D Gaussian\nsplats that are rigged to a parametric morphable face model. This combination\nfacilitates photorealistic rendering while allowing for precise animation\ncontrol via the underlying parametric model, e.g., through expression transfer\nfrom a driving sequence or by manually changing the morphable model parameters.\nWe parameterize each splat by a local coordinate frame of a triangle and\noptimize for explicit displacement offset to obtain a more accurate geometric\nrepresentation. During avatar reconstruction, we jointly optimize for the\nmorphable model parameters and Gaussian splat parameters in an end-to-end\nfashion. We demonstrate the animation capabilities of our photorealistic avatar\nin several challenging scenarios. For instance, we show reenactments from a\ndriving video, where our method outperforms existing works by a significant\nmargin.","terms":["cs.CV"]},{"titles":"Recursive Visual Programming","summaries":"Visual Programming (VP) has emerged as a powerful framework for Visual\nQuestion Answering (VQA). By generating and executing bespoke code for each\nquestion, these methods demonstrate impressive compositional and reasoning\ncapabilities, especially in few-shot and zero-shot scenarios. However, existing\nVP methods generate all code in a single function, resulting in code that is\nsuboptimal in terms of both accuracy and interpretability. Inspired by human\ncoding practices, we propose Recursive Visual Programming (RVP), which\nsimplifies generated routines, provides more efficient problem solving, and can\nmanage more complex data structures. RVP is inspired by human coding practices\nand approaches VQA tasks with an iterative recursive code generation approach,\nallowing decomposition of complicated problems into smaller parts. Notably, RVP\nis capable of dynamic type assignment, i.e., as the system recursively\ngenerates a new piece of code, it autonomously determines the appropriate\nreturn type and crafts the requisite code to generate that output. We show\nRVP's efficacy through extensive experiments on benchmarks including VSR, COVR,\nGQA, and NextQA, underscoring the value of adopting human-like recursive and\nmodular programming techniques for solving VQA tasks through coding.","terms":["cs.CV","cs.CL"]},{"titles":"DUCK: Distance-based Unlearning via Centroid Kinematics","summaries":"Machine Unlearning is rising as a new field, driven by the pressing necessity\nof ensuring privacy in modern artificial intelligence models. This technique\nprimarily aims to eradicate any residual influence of a specific subset of data\nfrom the knowledge acquired by a neural model during its training. This work\nintroduces a novel unlearning algorithm, denoted as Distance-based Unlearning\nvia Centroid Kinematics (DUCK), which employs metric learning to guide the\nremoval of samples matching the nearest incorrect centroid in the embedding\nspace. Evaluation of the algorithm's performance is conducted across various\nbenchmark datasets in two distinct scenarios, class removal, and homogeneous\nsampling removal, obtaining state-of-the-art performance. We introduce a novel\nmetric, called Adaptive Unlearning Score (AUS), encompassing not only the\nefficacy of the unlearning process in forgetting target data but also\nquantifying the performance loss relative to the original model. Moreover, we\npropose a novel membership inference attack to assess the algorithm's capacity\nto erase previously acquired knowledge, designed to be adaptable to future\nmethodologies.","terms":["cs.CV","cs.LG"]},{"titles":"TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding","summaries":"This work proposes TimeChat, a time-sensitive multimodal large language model\nspecifically designed for long video understanding. Our model incorporates two\nkey architectural contributions: (1) a timestamp-aware frame encoder that binds\nvisual content with the timestamp of each frame, and (2) a sliding video\nQ-Former that produces a video token sequence of varying lengths to accommodate\nvideos of various durations. Additionally, we construct an instruction-tuning\ndataset, encompassing 6 tasks and a total of 125K instances, to further enhance\nTimeChat's instruction-following performance. Experiment results across various\nvideo understanding tasks, such as dense captioning, temporal grounding, and\nhighlight detection, demonstrate TimeChat's strong zero-shot temporal\nlocalization and reasoning capabilities. For example, it achieves +9.2 F1 score\nand +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5)\non Charades-STA, compared to state-of-the-art video large language models,\nholding the potential to serve as a versatile video assistant for long-form\nvideo comprehension tasks and satisfy realistic user requirements.","terms":["cs.CV","cs.AI","cs.CL"]},{"titles":"How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model","summaries":"Deep learning algorithms demonstrate a surprising ability to learn\nhigh-dimensional tasks from limited examples. This is commonly attributed to\nthe depth of neural networks, enabling them to build a hierarchy of abstract,\nlow-dimensional data representations. However, how many training examples are\nrequired to learn such representations remains unknown. To quantitatively study\nthis question, we introduce the Random Hierarchy Model: a family of synthetic\ntasks inspired by the hierarchical structure of language and images. The model\nis a classification task where each class corresponds to a group of high-level\nfeatures, chosen among several equivalent groups associated with the same\nclass. In turn, each feature corresponds to a group of sub-features chosen\namong several equivalent ones and so on, following a hierarchy of composition\nrules. We find that deep networks learn the task by developing internal\nrepresentations invariant to exchanging equivalent groups. Moreover, the number\nof data required corresponds to the point where correlations between low-level\nfeatures and classes become detectable. Overall, our results indicate how deep\nnetworks overcome the curse of dimensionality by building invariant\nrepresentations, and provide an estimate of the number of data required to\nlearn a hierarchical task.","terms":["cs.LG","cs.CV","stat.ML"]},{"titles":"Generating Realistic Counterfactuals for Retinal Fundus and OCT Images using Diffusion Models","summaries":"Counterfactual reasoning is often used in clinical settings to explain\ndecisions or weigh alternatives. Therefore, for imaging based specialties such\nas ophthalmology, it would be beneficial to be able to create counterfactual\nimages, illustrating answers to questions like \"If the subject had had diabetic\nretinopathy, how would the fundus image have looked?\". Here, we demonstrate\nthat using a diffusion model in combination with an adversarially robust\nclassifier trained on retinal disease classification tasks enables the\ngeneration of highly realistic counterfactuals of retinal fundus images and\noptical coherence tomography (OCT) B-scans. The key to the realism of\ncounterfactuals is that these classifiers encode salient features indicative\nfor each disease class and can steer the diffusion model to depict disease\nsigns or remove disease-related lesions in a realistic way. In a user study,\ndomain experts also found the counterfactuals generated using our method\nsignificantly more realistic than counterfactuals generated from a previous\nmethod, and even indistinguishable from real images.","terms":["cs.CV","cs.LG"]},{"titles":"Interpreting and Improving Diffusion Models Using the Euclidean Distance Function","summaries":"Denoising is intuitively related to projection. Indeed, under the manifold\nhypothesis, adding random noise is approximately equivalent to orthogonal\nperturbation. Hence, learning to denoise is approximately learning to project.\nIn this paper, we use this observation to reinterpret denoising diffusion\nmodels as approximate gradient descent applied to the Euclidean distance\nfunction. We then provide straight-forward convergence analysis of the DDIM\nsampler under simple assumptions on the projection-error of the denoiser.\nFinally, we propose a new sampler based on two simple modifications to DDIM\nusing insights from our theoretical results. In as few as 5-10 function\nevaluations, our sampler achieves state-of-the-art FID scores on pretrained\nCIFAR-10 and CelebA models and can generate high quality samples on latent\ndiffusion models.","terms":["cs.LG","cs.CV","math.OC","stat.ML"]},{"titles":"GFS: Graph-based Feature Synthesis for Prediction over Relational Databases","summaries":"Relational databases are extensively utilized in a variety of modern\ninformation system applications, and they always carry valuable data patterns.\nThere are a huge number of data mining or machine learning tasks conducted on\nrelational databases. However, it is worth noting that there are limited\nmachine learning models specifically designed for relational databases, as most\nmodels are primarily tailored for single table settings. Consequently, the\nprevalent approach for training machine learning models on data stored in\nrelational databases involves performing feature engineering to merge the data\nfrom multiple tables into a single table and subsequently applying single table\nmodels. This approach not only requires significant effort in feature\nengineering but also destroys the inherent relational structure present in the\ndata. To address these challenges, we propose a novel framework called\nGraph-based Feature Synthesis (GFS). GFS formulates the relational database as\na heterogeneous graph, thereby preserving the relational structure within the\ndata. By leveraging the inductive bias from single table models, GFS\neffectively captures the intricate relationships inherent in each table.\nAdditionally, the whole framework eliminates the need for manual feature\nengineering. In the extensive experiment over four real-world multi-table\nrelational databases, GFS outperforms previous methods designed for relational\ndatabases, demonstrating its superior performance.","terms":["cs.LG","cs.DB"]},{"titles":"Implicit Learning of Scene Geometry from Poses for Global Localization","summaries":"Global visual localization estimates the absolute pose of a camera using a\nsingle image, in a previously mapped area. Obtaining the pose from a single\nimage enables many robotics and augmented\/virtual reality applications.\nInspired by latest advances in deep learning, many existing approaches directly\nlearn and regress 6 DoF pose from an input image. However, these methods do not\nfully utilize the underlying scene geometry for pose regression. The challenge\nin monocular relocalization is the minimal availability of supervised training\ndata, which is just the corresponding 6 DoF poses of the images. In this paper,\nwe propose to utilize these minimal available labels (.i.e, poses) to learn the\nunderlying 3D geometry of the scene and use the geometry to estimate the 6 DoF\ncamera pose. We present a learning method that uses these pose labels and rigid\nalignment to learn two 3D geometric representations (\\textit{X, Y, Z\ncoordinates}) of the scene, one in camera coordinate frame and the other in\nglobal coordinate frame. Given a single image, it estimates these two 3D scene\nrepresentations, which are then aligned to estimate a pose that matches the\npose label. This formulation allows for the active inclusion of additional\nlearning constraints to minimize 3D alignment errors between the two 3D scene\nrepresentations, and 2D re-projection errors between the 3D global scene\nrepresentation and 2D image pixels, resulting in improved localization\naccuracy. During inference, our model estimates the 3D scene geometry in camera\nand global frames and aligns them rigidly to obtain pose in real-time. We\nevaluate our work on three common visual localization datasets, conduct\nablation studies, and show that our method exceeds state-of-the-art regression\nmethods' pose accuracy on all datasets.","terms":["cs.CV","cs.RO"]},{"titles":"AgentAvatar: Disentangling Planning, Driving and Rendering for Photorealistic Avatar Agents","summaries":"In this study, our goal is to create interactive avatar agents that can\nautonomously plan and animate nuanced facial movements realistically, from both\nvisual and behavioral perspectives. Given high-level inputs about the\nenvironment and agent profile, our framework harnesses LLMs to produce a series\nof detailed text descriptions of the avatar agents' facial motions. These\ndescriptions are then processed by our task-agnostic driving engine into motion\ntoken sequences, which are subsequently converted into continuous motion\nembeddings that are further consumed by our standalone neural-based renderer to\ngenerate the final photorealistic avatar animations. These streamlined\nprocesses allow our framework to adapt to a variety of non-verbal avatar\ninteractions, both monadic and dyadic. Our extensive study, which includes\nexperiments on both newly compiled and existing datasets featuring two types of\nagents -- one capable of monadic interaction with the environment, and the\nother designed for dyadic conversation -- validates the effectiveness and\nversatility of our approach. To our knowledge, we advanced a leap step by\ncombining LLMs and neural rendering for generalized non-verbal prediction and\nphoto-realistic rendering of avatar agents.","terms":["cs.CV"]},{"titles":"VLTSeg: Simple Transfer of CLIP-Based Vision-Language Representations for Domain Generalized Semantic Segmentation","summaries":"Domain generalization (DG) remains a significant challenge for perception\nbased on deep neural networks (DNN), where domain shifts occur due to lighting,\nweather, or geolocation changes. In this work, we propose VLTSeg to enhance\ndomain generalization in semantic segmentation, where the network is solely\ntrained on the source domain and evaluated on unseen target domains. Our method\nleverages the inherent semantic robustness of vision-language models. First, by\nsubstituting traditional vision-only backbones with pre-trained encoders from\nCLIP and EVA-CLIP as transfer learning setting we find that in the field of DG,\nvision-language pre-training significantly outperforms supervised and\nself-supervised vision pre-training. We thus propose a new vision-language\napproach for domain generalized segmentation, which improves the domain\ngeneralization SOTA by 7.6% mIoU when training on the synthetic GTA5 dataset.\nWe further show the superior generalization capabilities of vision-language\nsegmentation models by reaching 76.48% mIoU on the popular Cityscapes-to-ACDC\nbenchmark, outperforming the previous SOTA approach by 6.9% mIoU on the test\nset at the time of writing. Additionally, our approach shows strong in-domain\ngeneralization capabilities indicated by 86.1% mIoU on the Cityscapes test set,\nresulting in a shared first place with the previous SOTA on the current\nleaderboard at the time of submission.","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"Space-Time Attention with Shifted Non-Local Search","summaries":"Efficiently computing attention maps for videos is challenging due to the\nmotion of objects between frames. While a standard non-local search is\nhigh-quality for a window surrounding each query point, the window's small size\ncannot accommodate motion. Methods for long-range motion use an auxiliary\nnetwork to predict the most similar key coordinates as offsets from each query\nlocation. However, accurately predicting this flow field of offsets remains\nchallenging, even for large-scale networks. Small spatial inaccuracies\nsignificantly impact the attention module's quality. This paper proposes a\nsearch strategy that combines the quality of a non-local search with the range\nof predicted offsets. The method, named Shifted Non-Local Search, executes a\nsmall grid search surrounding the predicted offsets to correct small spatial\nerrors. Our method's in-place computation consumes 10 times less memory and is\nover 3 times faster than previous work. Experimentally, correcting the small\nspatial errors improves the video frame alignment quality by over 3 dB PSNR.\nOur search upgrades existing space-time attention modules, which improves video\ndenoising results by 0.30 dB PSNR for a 7.5% increase in overall runtime. We\nintegrate our space-time attention module into a UNet-like architecture to\nachieve state-of-the-art results on video denoising.","terms":["cs.CV","cs.LG"]},{"titles":"Action Inference by Maximising Evidence: Zero-Shot Imitation from Observation with World Models","summaries":"Unlike most reinforcement learning agents which require an unrealistic amount\nof environment interactions to learn a new behaviour, humans excel at learning\nquickly by merely observing and imitating others. This ability highly depends\non the fact that humans have a model of their own embodiment that allows them\nto infer the most likely actions that led to the observed behaviour. In this\npaper, we propose Action Inference by Maximising Evidence (AIME) to replicate\nthis behaviour using world models. AIME consists of two distinct phases. In the\nfirst phase, the agent learns a world model from its past experience to\nunderstand its own body by maximising the ELBO. While in the second phase, the\nagent is given some observation-only demonstrations of an expert performing a\nnovel task and tries to imitate the expert's behaviour. AIME achieves this by\ndefining a policy as an inference model and maximising the evidence of the\ndemonstration under the policy and world model. Our method is \"zero-shot\" in\nthe sense that it does not require further training for the world model or\nonline interactions with the environment after given the demonstration. We\nempirically validate the zero-shot imitation performance of our method on the\nWalker and Cheetah embodiment of the DeepMind Control Suite and find it\noutperforms the state-of-the-art baselines. Code is available at:\nhttps:\/\/github.com\/argmax-ai\/aime.","terms":["cs.LG","cs.AI"]},{"titles":"Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images","summaries":"In this paper, we present a method to optimize Gaussian splatting with a\nlimited number of images while avoiding overfitting. Representing a 3D scene by\ncombining numerous Gaussian splats has yielded outstanding visual quality.\nHowever, it tends to overfit the training views when only a small number of\nimages are available. To address this issue, we introduce a dense depth map as\na geometry guide to mitigate overfitting. We obtained the depth map using a\npre-trained monocular depth estimation model and aligning the scale and offset\nusing sparse COLMAP feature points. The adjusted depth aids in the color-based\noptimization of 3D Gaussian splatting, mitigating floating artifacts, and\nensuring adherence to geometric constraints. We verify the proposed method on\nthe NeRF-LLFF dataset with varying numbers of few images. Our approach\ndemonstrates robust geometry compared to the original method that relies solely\non images. Project page: robot0321.github.io\/DepthRegGS","terms":["cs.CV","cs.GR"]},{"titles":"ColonNeRF: Neural Radiance Fields for High-Fidelity Long-Sequence Colonoscopy Reconstruction","summaries":"Colonoscopy reconstruction is pivotal for diagnosing colorectal cancer.\nHowever, accurate long-sequence colonoscopy reconstruction faces three major\nchallenges: (1) dissimilarity among segments of the colon due to its meandering\nand convoluted shape; (2) co-existence of simple and intricately folded\ngeometry structures; (3) sparse viewpoints due to constrained camera\ntrajectories. To tackle these challenges, we introduce a new reconstruction\nframework based on neural radiance field (NeRF), named ColonNeRF, which\nleverages neural rendering for novel view synthesis of long-sequence\ncolonoscopy. Specifically, to reconstruct the entire colon in a piecewise\nmanner, our ColonNeRF introduces a region division and integration module,\neffectively reducing shape dissimilarity and ensuring geometric consistency in\neach segment. To learn both the simple and complex geometry in a unified\nframework, our ColonNeRF incorporates a multi-level fusion module that\nprogressively models the colon regions from easy to hard. Additionally, to\novercome the challenges from sparse views, we devise a DensiNet module for\ndensifying camera poses under the guidance of semantic consistency. We conduct\nextensive experiments on both synthetic and real-world datasets to evaluate our\nColonNeRF. Quantitatively, our ColonNeRF outperforms existing methods on two\nbenchmarks over four evaluation metrics. Notably, our LPIPS-ALEX scores exhibit\na substantial increase of about 67%-85% on the SimCol-to-3D dataset.\nQualitatively, our reconstruction visualizations show much clearer textures and\nmore accurate geometric details. These sufficiently demonstrate our superior\nperformance over the state-of-the-art methods.","terms":["cs.CV"]},{"titles":"Optimal Data Generation in Multi-Dimensional Parameter Spaces, using Bayesian Optimization","summaries":"Acquiring a substantial number of data points for training accurate machine\nlearning (ML) models is a big challenge in scientific fields where data\ncollection is resource-intensive. Here, we propose a novel approach for\nconstructing a minimal yet highly informative database for training ML models\nin complex multi-dimensional parameter spaces. To achieve this, we mimic the\nunderlying relation between the output and input parameters using Gaussian\nprocess regression (GPR). Using a set of known data, GPR provides predictive\nmeans and standard deviation for the unknown data. Given the predicted standard\ndeviation by GPR, we select data points using Bayesian optimization to obtain\nan efficient database for training ML models. We compare the performance of ML\nmodels trained on databases obtained through this method, with databases\nobtained using traditional approaches. Our results demonstrate that the ML\nmodels trained on the database obtained using Bayesian optimization approach\nconsistently outperform the other two databases, achieving high accuracy with a\nsignificantly smaller number of data points. Our work contributes to the\nresource-efficient collection of data in high-dimensional complex parameter\nspaces, to achieve high precision machine learning predictions.","terms":["cs.LG","physics.app-ph","physics.comp-ph"]},{"titles":"SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and Mixed Variables Gaussian Processes","summaries":"The Surrogate Modeling Toolbox (SMT) is an open-source Python package that\noffers a collection of surrogate modeling methods, sampling techniques, and a\nset of sample problems. This paper presents SMT 2.0, a major new release of SMT\nthat introduces significant upgrades and new features to the toolbox. This\nrelease adds the capability to handle mixed-variable surrogate models and\nhierarchical variables. These types of variables are becoming increasingly\nimportant in several surrogate modeling applications. SMT 2.0 also improves SMT\nby extending sampling methods, adding new surrogate models, and computing\nvariance and kernel derivatives for Kriging. This release also includes new\nfunctions to handle noisy and use multifidelity data. To the best of our\nknowledge, SMT 2.0 is the first open-source surrogate library to propose\nsurrogate models for hierarchical and mixed inputs. This open-source software\nis distributed under the New BSD license.","terms":["cs.LG","math.OC","stat.CO"]},{"titles":"FaceDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and Relighting with Diffusion Models","summaries":"The ability to create high-quality 3D faces from a single image has become\nincreasingly important with wide applications in video conferencing, AR\/VR, and\nadvanced video editing in movie industries. In this paper, we propose Face\nDiffusion NeRF (FaceDNeRF), a new generative method to reconstruct high-quality\nFace NeRFs from single images, complete with semantic editing and relighting\ncapabilities. FaceDNeRF utilizes high-resolution 3D GAN inversion and expertly\ntrained 2D latent-diffusion model, allowing users to manipulate and construct\nFace NeRFs in zero-shot learning without the need for explicit 3D data. With\ncarefully designed illumination and identity preserving loss, as well as\nmulti-modal pre-training, FaceDNeRF offers users unparalleled control over the\nediting process enabling them to create and edit face NeRFs using just\nsingle-view images, text prompts, and explicit target lighting. The advanced\nfeatures of FaceDNeRF have been designed to produce more impressive results\nthan existing 2D editing approaches that rely on 2D segmentation maps for\neditable attributes. Experiments show that our FaceDNeRF achieves exceptionally\nrealistic results and unprecedented flexibility in editing compared with\nstate-of-the-art 3D face reconstruction and editing methods. Our code will be\navailable at https:\/\/github.com\/BillyXYB\/FaceDNeRF.","terms":["cs.CV"]},{"titles":"Language-only Efficient Training of Zero-shot Composed Image Retrieval","summaries":"Composed image retrieval (CIR) task takes a composed query of image and text,\naiming to search relative images for both conditions. Conventional CIR\napproaches need a training dataset composed of triplets of query image, query\ntext, and target image, which is very expensive to collect. Several recent\nworks have worked on the zero-shot (ZS) CIR paradigm to tackle the issue\nwithout using pre-collected triplets. However, the existing ZS-CIR methods show\nlimited backbone scalability and generalizability due to the lack of diversity\nof the input texts during training. We propose a novel CIR framework, only\nusing language for its training. Our LinCIR (Language-only training for CIR)\ncan be trained only with text datasets by a novel self-supervision named\nself-masking projection (SMP). We project the text latent embedding to the\ntoken embedding space and construct a new text by replacing the keyword tokens\nof the original text. Then, we let the new and original texts have the same\nlatent embedding vector. With this simple strategy, LinCIR is surprisingly\nefficient and highly effective; LinCIR with CLIP ViT-G backbone is trained in\n48 minutes and shows the best ZS-CIR performances on four different CIR\nbenchmarks, CIRCO, GeneCIS, FashionIQ, and CIRR, even outperforming supervised\nmethod on FashionIQ. Code is available at https:\/\/github.com\/navervision\/lincir","terms":["cs.CV","cs.IR"]},{"titles":"A Generative Self-Supervised Framework using Functional Connectivity in fMRI Data","summaries":"Deep neural networks trained on Functional Connectivity (FC) networks\nextracted from functional Magnetic Resonance Imaging (fMRI) data have gained\npopularity due to the increasing availability of data and advances in model\narchitectures, including Graph Neural Network (GNN). Recent research on the\napplication of GNN to FC suggests that exploiting the time-varying properties\nof the FC could significantly improve the accuracy and interpretability of the\nmodel prediction. However, the high cost of acquiring high-quality fMRI data\nand corresponding phenotypic labels poses a hurdle to their application in\nreal-world settings, such that a model na\\\"ively trained in a supervised\nfashion can suffer from insufficient performance or a lack of generalization on\na small number of data. In addition, most Self-Supervised Learning (SSL)\napproaches for GNNs to date adopt a contrastive strategy, which tends to lose\nappropriate semantic information when the graph structure is perturbed or does\nnot leverage both spatial and temporal information simultaneously. In light of\nthese challenges, we propose a generative SSL approach that is tailored to\neffectively harness spatio-temporal information within dynamic FC. Our\nempirical results, experimented with large-scale (>50,000) fMRI datasets,\ndemonstrate that our approach learns valuable representations and enables the\nconstruction of accurate and robust models when fine-tuned for downstream\ntasks.","terms":["cs.LG","cs.CV","eess.IV","q-bio.NC"]},{"titles":"BioCLIP: A Vision Foundation Model for the Tree of Life","summaries":"Images of the natural world, collected by a variety of cameras, from drones\nto individual phones, are increasingly abundant sources of biological\ninformation. There is an explosion of computational methods and tools,\nparticularly computer vision, for extracting biologically relevant information\nfrom images for science and conservation. Yet most of these are bespoke\napproaches designed for a specific task and are not easily adaptable or\nextendable to new questions, contexts, and datasets. A vision model for general\norganismal biology questions on images is of timely need. To approach this, we\ncurate and release TreeOfLife-10M, the largest and most diverse ML-ready\ndataset of biology images. We then develop BioCLIP, a foundation model for the\ntree of life, leveraging the unique properties of biology captured by\nTreeOfLife-10M, namely the abundance and variety of images of plants, animals,\nand fungi, together with the availability of rich structured biological\nknowledge. We rigorously benchmark our approach on diverse fine-grained biology\nclassification tasks, and find that BioCLIP consistently and substantially\noutperforms existing baselines (by 17% to 20% absolute). Intrinsic evaluation\nreveals that BioCLIP has learned a hierarchical representation conforming to\nthe tree of life, shedding light on its strong generalizability. Our code,\nmodels and data will be made available at\nhttps:\/\/github.com\/Imageomics\/bioclip.","terms":["cs.CV","cs.CL","cs.LG"]},{"titles":"Information Modified K-Nearest Neighbor","summaries":"In this research paper, we introduce a novel classification method aimed at\nimproving the performance of the K-Nearest Neighbors (KNN) algorithm. Our\napproach leverages Mutual Information (MI) to enhance the significance of\nweights and draw inspiration from Shapley values, a concept originating from\ncooperative game theory, to refine value allocation. The fundamental concept\nunderlying KNN is the classification of samples based on the majority thorough\ntheir k-nearest neighbors. While both the distances and labels of these\nneighbors are crucial, traditional KNN assigns equal weight to all samples and\nprevance considering the varying importance of each neighbor based on their\ndistances and labels.\n  In the proposed method, known as Information-Modified KNN (IMKNN), we address\nthis issue by introducing a straightforward algorithm. To evaluate the\neffectiveness of our approach, it is compared with 7 contemporary variants of\nKNN, as well as the traditional KNN. Each of these variants exhibits its unique\nadvantages and limitations. We conduct experiments on 12 widely-used datasets,\nassessing the methods' performance in terms of accuracy, precision and recall.\n  Our study demonstrates that IMKNN consistently outperforms other methods\nacross different datasets and criteria by highlighting its superior performance\nin various classification tasks. These findings underscore the potential of\nIMKNN as a valuable tool for enhancing the capabilities of the KNN algorithm in\ndiverse applications.","terms":["cs.LG","cs.IT","math.IT"]},{"titles":"Bootstrapping SparseFormers from Vision Foundation Models","summaries":"The recently proposed SparseFormer architecture provides an alternative\napproach to visual understanding by utilizing a significantly lower number of\nvisual tokens via adjusting RoIs, greatly reducing computational costs while\nstill achieving promising performance. However, training SparseFormers from\nscratch is still expensive, and scaling up the number of parameters can be\nchallenging. In this paper, we propose to bootstrap SparseFormers from\nViT-based vision foundation models in a simple and efficient way. Since the\nmajority of SparseFormer blocks are the standard transformer ones, we can\ninherit weights from large-scale pre-trained vision transformers and freeze\nthem as much as possible. Therefore, we only need to train the\nSparseFormer-specific lightweight focusing transformer to adjust token RoIs and\nfine-tune a few early pre-trained blocks to align the final token\nrepresentation. In such a way, we can bootstrap SparseFormer architectures from\nvarious large-scale pre-trained models (e.g., IN-21K pre-trained AugRegs or\nCLIPs) using a rather smaller amount of training samples (e.g., IN-1K) and\nwithout labels or captions within just a few hours. As a result, the\nbootstrapped unimodal SparseFormer (from AugReg-ViT-L\/16-384) can reach 84.9%\naccuracy on IN-1K with only 49 tokens, and the multimodal SparseFormer from\nCLIPs also demonstrates notable zero-shot performance with highly reduced\ncomputational cost without seeing any caption during the bootstrapping\nprocedure. In addition, CLIP-bootstrapped SparseFormers, which align the output\nspace with language without seeing a word, can serve as efficient vision\nencoders in multimodal large language models. Code will be publicly available\nat https:\/\/github.com\/showlab\/sparseformer","terms":["cs.CV"]},{"titles":"Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors","summaries":"Modeling long-range dependencies across sequences is a longstanding goal in\nmachine learning and has led to architectures, such as state space models, that\ndramatically outperform Transformers on long sequences. However, these\nimpressive empirical gains have been by and large demonstrated on benchmarks\n(e.g. Long Range Arena), where models are randomly initialized and trained to\npredict a target label from an input sequence. In this work, we show that\nrandom initialization leads to gross overestimation of the differences between\narchitectures and that pretraining with standard denoising objectives, using\n$\\textit{only the downstream task data}$, leads to dramatic gains across\nmultiple architectures and to very small gaps between Transformers and state\nspace models (SSMs). In stark contrast to prior works, we find vanilla\nTransformers to match the performance of S4 on Long Range Arena when properly\npretrained, and we improve the best reported results of SSMs on the PathX-256\ntask by 20 absolute points. Subsequently, we analyze the utility of\npreviously-proposed structured parameterizations for SSMs and show they become\nmostly redundant in the presence of data-driven initialization obtained through\npretraining. Our work shows that, when evaluating different architectures on\nsupervised tasks, incorporation of data-driven priors via pretraining is\nessential for reliable performance estimation, and can be done efficiently.","terms":["cs.LG","cs.CL"]},{"titles":"UniGS: Unified Representation for Image Generation and Segmentation","summaries":"This paper introduces a novel unified representation of diffusion models for\nimage generation and segmentation. Specifically, we use a colormap to represent\nentity-level masks, addressing the challenge of varying entity numbers while\naligning the representation closely with the image RGB domain. Two novel\nmodules, including the location-aware color palette and progressive dichotomy\nmodule, are proposed to support our mask representation. On the one hand, a\nlocation-aware palette guarantees the colors' consistency to entities'\nlocations. On the other hand, the progressive dichotomy module can efficiently\ndecode the synthesized colormap to high-quality entity-level masks in a\ndepth-first binary search without knowing the cluster numbers. To tackle the\nissue of lacking large-scale segmentation training data, we employ an\ninpainting pipeline and then improve the flexibility of diffusion models across\nvarious tasks, including inpainting, image synthesis, referring segmentation,\nand entity segmentation. Comprehensive experiments validate the efficiency of\nour approach, demonstrating comparable segmentation mask quality to\nstate-of-the-art and adaptability to multiple tasks. The code will be released\nat \\href{https:\/\/github.com\/qqlu\/Entity}{https:\/\/github.com\/qqlu\/Entity}.","terms":["cs.CV"]},{"titles":"Optimization dependent generalization bound for ReLU networks based on sensitivity in the tangent bundle","summaries":"Recent advances in deep learning have given us some very promising results on\nthe generalization ability of deep neural networks, however literature still\nlacks a comprehensive theory explaining why heavily over-parametrized models\nare able to generalize well while fitting the training data. In this paper we\npropose a PAC type bound on the generalization error of feedforward ReLU\nnetworks via estimating the Rademacher complexity of the set of networks\navailable from an initial parameter vector via gradient descent. The key idea\nis to bound the sensitivity of the network's gradient to perturbation of the\ninput data along the optimization trajectory. The obtained bound does not\nexplicitly depend on the depth of the network. Our results are experimentally\nverified on the MNIST and CIFAR-10 datasets.","terms":["cs.LG","cs.AI","68","I.2.6"]},{"titles":"On skip connections and normalisation layers in deep optimisation","summaries":"We introduce a general theoretical framework, designed for the study of\ngradient optimisation of deep neural networks, that encompasses ubiquitous\narchitecture choices including batch normalisation, weight normalisation and\nskip connections. Our framework determines the curvature and regularity\nproperties of multilayer loss landscapes in terms of their constituent layers,\nthereby elucidating the roles played by normalisation layers and skip\nconnections in globalising these properties. We then demonstrate the utility of\nthis framework in two respects. First, we give the only proof of which we are\naware that a class of deep neural networks can be trained using gradient\ndescent to global optima even when such optima only exist at infinity, as is\nthe case for the cross-entropy cost. Second, we identify a novel causal\nmechanism by which skip connections accelerate training, which we verify\npredictively with ResNets on MNIST, CIFAR10, CIFAR100 and ImageNet.","terms":["cs.LG"]},{"titles":"Trading-off price for data quality to achieve fair online allocation","summaries":"We consider the problem of online allocation subject to a long-term fairness\npenalty. Contrary to existing works, however, we do not assume that the\ndecision-maker observes the protected attributes -- which is often unrealistic\nin practice. Instead they can purchase data that help estimate them from\nsources of different quality; and hence reduce the fairness penalty at some\ncost. We model this problem as a multi-armed bandit problem where each arm\ncorresponds to the choice of a data source, coupled with the online allocation\nproblem. We propose an algorithm that jointly solves both problems and show\nthat it has a regret bounded by $\\mathcal{O}(\\sqrt{T})$. A key difficulty is\nthat the rewards received by selecting a source are correlated by the fairness\npenalty, which leads to a need for randomization (despite a stochastic\nsetting). Our algorithm takes into account contextual information available\nbefore the source selection, and can adapt to many different fairness notions.\nWe also show that in some instances, the estimates used can be learned on the\nfly.","terms":["cs.LG"]},{"titles":"Semantics-aware Motion Retargeting with Vision-Language Models","summaries":"Capturing and preserving motion semantics is essential to motion retargeting\nbetween animation characters. However, most of the previous works neglect the\nsemantic information or rely on human-designed joint-level representations.\nHere, we present a novel Semantics-aware Motion reTargeting (SMT) method with\nthe advantage of vision-language models to extract and maintain meaningful\nmotion semantics. We utilize a differentiable module to render 3D motions. Then\nthe high-level motion semantics are incorporated into the motion retargeting\nprocess by feeding the vision-language model with the rendered images and\naligning the extracted semantic embeddings. To ensure the preservation of\nfine-grained motion details and high-level semantics, we adopt a two-stage\npipeline consisting of skeleton-aware pre-training and fine-tuning with\nsemantics and geometry constraints. Experimental results show the effectiveness\nof the proposed method in producing high-quality motion retargeting results\nwhile accurately preserving motion semantics. Project page can be found at\nhttps:\/\/sites.google.com\/view\/smtnet.","terms":["cs.CV","cs.GR"]},{"titles":"Student Classroom Behavior Detection based on Spatio-Temporal Network and Multi-Model Fusion","summaries":"Using deep learning methods to detect students' classroom behavior\nautomatically is a promising approach for analyzing their class performance and\nimproving teaching effectiveness. However, the lack of publicly available\nspatio-temporal datasets on student behavior, as well as the high cost of\nmanually labeling such datasets, pose significant challenges for researchers in\nthis field. To address this issue, we proposed a method for extending the\nspatio-temporal behavior dataset in Student Classroom Scenarios\n(SCB-ST-Dataset4) through image dataset. Our SCB-ST-Dataset4 comprises 757265\nimages with 25810 labels, focusing on 3 behaviors: hand-raising, reading,\nwriting. Our proposed method can rapidly generate spatio-temporal behavior\ndatasets without requiring extra manual labeling. Furthermore, we proposed a\nBehavior Similarity Index (BSI) to explore the similarity of behaviors. We\nevaluated the dataset using the YOLOv5, YOLOv7, YOLOv8, and SlowFast\nalgorithms, achieving a mean average precision (map) of up to 82.3%. Last, we\nfused multiple models to generate student behavior-related data from various\nperspectives. The experiment further demonstrates the effectiveness of our\nmethod. And SCB-ST-Dataset4 provides a robust foundation for future research in\nstudent behavior detection, potentially contributing to advancements in this\nfield. The SCB-ST-Dataset4 is available for download at:\nhttps:\/\/github.com\/Whiffe\/SCB-dataset.","terms":["cs.CV"]},{"titles":"High-Fidelity Zero-Shot Texture Anomaly Localization Using Feature Correspondence Analysis","summaries":"We propose a novel method for Zero-Shot Anomaly Localization on textures. The\ntask refers to identifying abnormal regions in an otherwise homogeneous image.\nTo obtain a high-fidelity localization, we leverage a bijective mapping derived\nfrom the 1-dimensional Wasserstein Distance. As opposed to using holistic\ndistances between distributions, the proposed approach allows pinpointing the\nnon-conformity of a pixel in a local context with increased precision. By\naggregating the contribution of the pixel to the errors of all nearby patches\nwe obtain a reliable anomaly score estimate. We validate our solution on\nseveral datasets and obtain more than a 40% reduction in error over the\nprevious state of the art on the MVTec AD dataset in a zero-shot setting. Also\nsee https:\/\/reality.tf.fau.de\/pub\/ardelean2024highfidelity.html.","terms":["cs.CV"]},{"titles":"Instance-guided Cartoon Editing with a Large-scale Dataset","summaries":"Cartoon editing, appreciated by both professional illustrators and hobbyists,\nallows extensive creative freedom and the development of original narratives\nwithin the cartoon domain. However, the existing literature on cartoon editing\nis complex and leans heavily on manual operations, owing to the challenge of\nautomatic identification of individual character instances. Therefore, an\nautomated segmentation of these elements becomes imperative to facilitate a\nvariety of cartoon editing applications such as visual style editing, motion\ndecomposition and transfer, and the computation of stereoscopic depths for an\nenriched visual experience. Unfortunately, most current segmentation methods\nare designed for natural photographs, failing to recognize from the intricate\naesthetics of cartoon subjects, thus lowering segmentation quality. The major\nchallenge stems from two key shortcomings: the rarity of high-quality cartoon\ndedicated datasets and the absence of competent models for high-resolution\ninstance extraction on cartoons. To address this, we introduce a high-quality\ndataset of over 100k paired high-resolution cartoon images and their instance\nlabeling masks. We also present an instance-aware image segmentation model that\ncan generate accurate, high-resolution segmentation masks for characters in\ncartoon images. We present that the proposed approach enables a range of\nsegmentation-dependent cartoon editing applications like 3D Ken Burns parallax\neffects, text-guided cartoon style editing, and puppet animation from\nillustrations and manga.","terms":["cs.CV","cs.GR","I.4.6; I.3.3; I.3.8"]},{"titles":"Foundations for Transfer in Reinforcement Learning: A Taxonomy of Knowledge Modalities","summaries":"Contemporary artificial intelligence systems exhibit rapidly growing\nabilities accompanied by the growth of required resources, expansive datasets\nand corresponding investments into computing infrastructure. Although earlier\nsuccesses predominantly focus on constrained settings, recent strides in\nfundamental research and applications aspire to create increasingly general\nsystems. This evolving landscape presents a dual panorama of opportunities and\nchallenges in refining the generalisation and transfer of knowledge - the\nextraction from existing sources and adaptation as a comprehensive foundation\nfor tackling new problems. Within the domain of reinforcement learning (RL),\nthe representation of knowledge manifests through various modalities, including\ndynamics and reward models, value functions, policies, and the original data.\nThis taxonomy systematically targets these modalities and frames its discussion\nbased on their inherent properties and alignment with different objectives and\nmechanisms for transfer. Where possible, we aim to provide coarse guidance\ndelineating approaches which address requirements such as limiting environment\ninteractions, maximising computational efficiency, and enhancing generalisation\nacross varying axes of change. Finally, we analyse reasons contributing to the\nprevalence or scarcity of specific forms of transfer, the inherent potential\nbehind pushing these frontiers, and underscore the significance of\ntransitioning from designed to learned transfer.","terms":["cs.LG","cs.AI","cs.RO","stat.ML"]},{"titles":"Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks","summaries":"We study building multi-task agents in open-world environments. Without human\ndemonstrations, learning to accomplish long-horizon tasks in a large open-world\nenvironment with reinforcement learning (RL) is extremely inefficient. To\ntackle this challenge, we convert the multi-task learning problem into learning\nbasic skills and planning over the skills. Using the popular open-world game\nMinecraft as the testbed, we propose three types of fine-grained basic skills,\nand use RL with intrinsic rewards to acquire skills. A novel Finding-skill that\nperforms exploration to find diverse items provides better initialization for\nother skills, improving the sample efficiency for skill learning. In skill\nplanning, we leverage the prior knowledge in Large Language Models to find the\nrelationships between skills and build a skill graph. When the agent is solving\na task, our skill search algorithm walks on the skill graph and generates the\nproper skill plans for the agent. In experiments, our method accomplishes 40\ndiverse Minecraft tasks, where many tasks require sequentially executing for\nmore than 10 skills. Our method outperforms baselines by a large margin and is\nthe most sample-efficient demonstration-free RL method to solve Minecraft Tech\nTree tasks. The project's website and code can be found at\nhttps:\/\/sites.google.com\/view\/plan4mc.","terms":["cs.LG","cs.AI"]},{"titles":"Continual Learning with Dynamic Sparse Training: Exploring Algorithms for Effective Model Updates","summaries":"Continual learning (CL) refers to the ability of an intelligent system to\nsequentially acquire and retain knowledge from a stream of data with as little\ncomputational overhead as possible. To this end; regularization, replay,\narchitecture, and parameter isolation approaches were introduced to the\nliterature. Parameter isolation using a sparse network which enables to\nallocate distinct parts of the neural network to different tasks and also\nallows to share of parameters between tasks if they are similar. Dynamic Sparse\nTraining (DST) is a prominent way to find these sparse networks and isolate\nthem for each task. This paper is the first empirical study investigating the\neffect of different DST components under the CL paradigm to fill a critical\nresearch gap and shed light on the optimal configuration of DST for CL if it\nexists. Therefore, we perform a comprehensive study in which we investigate\nvarious DST components to find the best topology per task on well-known\nCIFAR100 and miniImageNet benchmarks in a task-incremental CL setup since our\nprimary focus is to evaluate the performance of various DST criteria, rather\nthan the process of mask selection. We found that, at a low sparsity level,\nErdos-R\\'enyi Kernel (ERK) initialization utilizes the backbone more\nefficiently and allows to effectively learn increments of tasks. At a high\nsparsity level, unless it is extreme, uniform initialization demonstrates a\nmore reliable and robust performance. In terms of growth strategy; performance\nis dependent on the defined initialization strategy and the extent of sparsity.\nFinally, adaptivity within DST components is a promising way for better\ncontinual learners.","terms":["cs.LG","cs.CV"]},{"titles":"Federated Active Learning for Target Domain Generalisation","summaries":"In this paper, we introduce Active Learning framework in Federated Learning\nfor Target Domain Generalisation, harnessing the strength from both learning\nparadigms. Our framework, FEDALV, composed of Active Learning (AL) and\nFederated Domain Generalisation (FDG), enables generalisation of an image\nclassification model trained from limited source domain client's data without\nsharing images to an unseen target domain. To this end, our FDG, FEDA, consists\nof two optimisation updates during training, one at the client and another at\nthe server level. For the client, the introduced losses aim to reduce feature\ncomplexity and condition alignment, while in the server, the regularisation\nlimits free energy biases between source and target obtained by the global\nmodel. The remaining component of FEDAL is AL with variable budgets, which\nqueries the server to retrieve and sample the most informative local data for\nthe targeted client. We performed multiple experiments on FDG w\/ and w\/o AL and\ncompared with both conventional FDG baselines and Federated Active Learning\nbaselines. Our extensive quantitative experiments demonstrate the superiority\nof our method in accuracy and efficiency compared to the multiple contemporary\nmethods. FEDALV manages to obtain the performance of the full training target\naccuracy while sampling as little as 5% of the source client's data.","terms":["cs.LG","cs.AI","cs.CV"]},{"titles":"Conditional Variational Diffusion Models","summaries":"Inverse problems aim to determine parameters from observations, a crucial\ntask in engineering and science. Lately, generative models, especially\ndiffusion models, have gained popularity in this area for their ability to\nproduce realistic solutions and their good mathematical properties. Despite\ntheir success, an important drawback of diffusion models is their sensitivity\nto the choice of variance schedule, which controls the dynamics of the\ndiffusion process. Fine-tuning this schedule for specific applications is\ncrucial but time-costly and does not guarantee an optimal result. We propose a\nnovel approach for learning the schedule as part of the training process. Our\nmethod supports probabilistic conditioning on data, provides high-quality\nsolutions, and is flexible, proving able to adapt to different applications\nwith minimum overhead. This approach is tested in two unrelated inverse\nproblems: super-resolution microscopy and quantitative phase imaging, yielding\ncomparable or superior results to previous methods and fine-tuned diffusion\nmodels. We conclude that fine-tuning the schedule by experimentation should be\navoided because it can be learned during training in a stable way that yields\nbetter results.","terms":["cs.CV","cs.AI","cs.LG","stat.ML","I.2.6"]},{"titles":"Analysis and mining of low-carbon and energy-saving tourism data characteristics based on machine learning algorithm","summaries":"In order to study the formation mechanism of residents' low-carbon awareness\nand provide an important basis for traffic managers to guide urban residents to\nchoose low-carbon travel mode, this paper proposes a low-carbon energy-saving\ntravel data feature analysis and mining based on machine learning algorithm.\nThis paper uses data mining technology to analyze the data of low-carbon travel\nquestionnaire, and regards the 15-dimensional problem under the framework of\nplanned behavior theory as the internal cause variable that characterizes\nresidents' low-carbon travel willingness. The author uses K-means clustering\nalgorithm to classify the intensity of residents' low-carbon travel\nwillingness, and applies the results as the explanatory variables to the random\nforest model to explore the mechanism of residents' social attribute\ncharacteristics, travel characteristics, etc. on their low-carbon travel\nwillingness. The experimental results show that based on the Silhouette index\ntest and t-SNE dimensionality reduction, residents' low-carbon travel\nwillingness can be divided into three categories: strong, neutral, and not\nstrong; Based on the importance index, the four most significant factors are\nthe occupation, residence, family composition and commuting time of residents.\nConclusion: This method provides policy recommendations for the development and\nmanagement of urban traffic low-carbon from multiple perspectives.","terms":["cs.LG"]},{"titles":"FedEmb: A Vertical and Hybrid Federated Learning Algorithm using Network And Feature Embedding Aggregation","summaries":"Federated learning (FL) is an emerging paradigm for decentralized training of\nmachine learning models on distributed clients, without revealing the data to\nthe central server. The learning scheme may be horizontal, vertical or hybrid\n(both vertical and horizontal). Most existing research work with deep neural\nnetwork (DNN) modelling is focused on horizontal data distributions, while\nvertical and hybrid schemes are much less studied. In this paper, we propose a\ngeneralized algorithm FedEmb, for modelling vertical and hybrid DNN-based\nlearning. The idea of our algorithm is characterised by higher inference\naccuracy, stronger privacy-preserving properties, and lower client-server\ncommunication bandwidth demands as compared with existing work. The\nexperimental results show that FedEmb is an effective method to tackle both\nsplit feature & subject space decentralized problems, shows 0.3% to 4.2%\ninference accuracy improvement with limited privacy revealing for datasets\nstored in local clients, and reduces 88.9 % time complexity over vertical\nbaseline method.","terms":["cs.LG","cs.AI"]},{"titles":"COTR: Compact Occupancy TRansformer for Vision-based 3D Occupancy Prediction","summaries":"The autonomous driving community has shown significant interest in 3D\noccupancy prediction, driven by its exceptional geometric perception and\ngeneral object recognition capabilities. To achieve this, current works try to\nconstruct a Tri-Perspective View (TPV) or Occupancy (OCC) representation\nextending from the Bird-Eye-View perception. However, compressed views like TPV\nrepresentation lose 3D geometry information while raw and sparse OCC\nrepresentation requires heavy but reducant computational costs. To address the\nabove limitations, we propose Compact Occupancy TRansformer (COTR), with a\ngeometry-aware occupancy encoder and a semantic-aware group decoder to\nreconstruct a compact 3D OCC representation. The occupancy encoder first\ngenerates a compact geometrical OCC feature through efficient explicit-implicit\nview transformation. Then, the occupancy decoder further enhances the semantic\ndiscriminability of the compact OCC representation by a coarse-to-fine semantic\ngrouping strategy. Empirical experiments show that there are evident\nperformance gains across multiple baselines, e.g., COTR outperforms baselines\nwith a relative improvement of 8%-15%, demonstrating the superiority of our\nmethod.","terms":["cs.CV"]},{"titles":"SVDinsTN: A Tensor Network Paradigm for Efficient Structure Search from Regularized Modeling Perspective","summaries":"Tensor network (TN) representation is a powerful technique for computer\nvision and machine learning. TN structure search (TN-SS) aims to search for a\ncustomized structure to achieve a compact representation, which is a\nchallenging NP-hard problem. Recent \"sampling-evaluation-based\" methods require\nsampling an extensive collection of structures and evaluating them one by one,\nresulting in prohibitively high computational costs. To address this issue, we\npropose a novel TN paradigm, named SVD-inspired TN decomposition (SVDinsTN),\nwhich allows us to efficiently solve the TN-SS problem from a regularized\nmodeling perspective, eliminating the repeated structure evaluations. To be\nspecific, by inserting a diagonal factor for each edge of the fully-connected\nTN, SVDinsTN allows us to calculate TN cores and diagonal factors\nsimultaneously, with the factor sparsity revealing a compact TN structure. In\ntheory, we prove a convergence guarantee for the proposed method. Experimental\nresults demonstrate that the proposed method achieves approximately 100 to 1000\ntimes acceleration compared to the state-of-the-art TN-SS methods while\nmaintaining a comparable representation ability.","terms":["cs.LG"]},{"titles":"A Reliable Representation with Bidirectional Transition Model for Visual Reinforcement Learning Generalization","summaries":"Visual reinforcement learning has proven effective in solving control tasks\nwith high-dimensional observations. However, extracting reliable and\ngeneralizable representations from vision-based observations remains a central\nchallenge. Inspired by the human thought process, when the representation\nextracted from the observation can predict the future and trace history, the\nrepresentation is reliable and accurate in comprehending the environment. Based\non this concept, we introduce a Bidirectional Transition (BiT) model, which\nleverages the ability to bidirectionally predict environmental transitions both\nforward and backward to extract reliable representations. Our model\ndemonstrates competitive generalization performance and sample efficiency on\ntwo settings of the DeepMind Control suite. Additionally, we utilize robotic\nmanipulation and CARLA simulators to demonstrate the wide applicability of our\nmethod.","terms":["cs.CV"]},{"titles":"Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis","summaries":"Driven by the large foundation models, the development of artificial\nintelligence has witnessed tremendous progress lately, leading to a surge of\ngeneral interest from the public. In this study, we aim to assess the\nperformance of OpenAI's newest model, GPT-4V(ision), specifically in the realm\nof multimodal medical diagnosis. Our evaluation encompasses 17 human body\nsystems, including Central Nervous System, Head and Neck, Cardiac, Chest,\nHematology, Hepatobiliary, Gastrointestinal, Urogenital, Gynecology,\nObstetrics, Breast, Musculoskeletal, Spine, Vascular, Oncology, Trauma,\nPediatrics, with images taken from 8 modalities used in daily clinic routine,\ne.g., X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI),\nPositron Emission Tomography (PET), Digital Subtraction Angiography (DSA),\nMammography, Ultrasound, and Pathology. We probe the GPT-4V's ability on\nmultiple clinical tasks with or without patent history provided, including\nimaging modality and anatomy recognition, disease diagnosis, report generation,\ndisease localisation.\n  Our observation shows that, while GPT-4V demonstrates proficiency in\ndistinguishing between medical image modalities and anatomy, it faces\nsignificant challenges in disease diagnosis and generating comprehensive\nreports. These findings underscore that while large multimodal models have made\nsignificant advancements in computer vision and natural language processing, it\nremains far from being used to effectively support real-world medical\napplications and clinical decision-making.\n  All images used in this report can be found in\nhttps:\/\/github.com\/chaoyi-wu\/GPT-4V_Medical_Evaluation.","terms":["cs.CV","cs.CL"]},{"titles":"Unsupervised Anomaly Detection using Aggregated Normative Diffusion","summaries":"Early detection of anomalies in medical images such as brain MRI is highly\nrelevant for diagnosis and treatment of many conditions. Supervised machine\nlearning methods are limited to a small number of pathologies where there is\ngood availability of labeled data. In contrast, unsupervised anomaly detection\n(UAD) has the potential to identify a broader spectrum of anomalies by spotting\ndeviations from normal patterns. Our research demonstrates that existing\nstate-of-the-art UAD approaches do not generalise well to diverse types of\nanomalies in realistic multi-modal MR data. To overcome this, we introduce a\nnew UAD method named Aggregated Normative Diffusion (ANDi). ANDi operates by\naggregating differences between predicted denoising steps and ground truth\nbackwards transitions in Denoising Diffusion Probabilistic Models (DDPMs) that\nhave been trained on pyramidal Gaussian noise. We validate ANDi against three\nrecent UAD baselines, and across three diverse brain MRI datasets. We show that\nANDi, in some cases, substantially surpasses these baselines and shows\nincreased robustness to varying types of anomalies. Particularly in detecting\nmultiple sclerosis (MS) lesions, ANDi achieves improvements of up to 178% in\nterms of AUPRC.","terms":["cs.CV","cs.LG","eess.IV"]},{"titles":"Adapting Short-Term Transformers for Action Detection in Untrimmed Videos","summaries":"Vision transformer (ViT) has shown high potential in video recognition, owing\nto its flexible design, adaptable self-attention mechanisms, and the efficacy\nof masked pre-training. Yet, it still remains unclear how to adapt these\npre-trained short-term ViTs for temporal action detection (TAD) in untrimmed\nvideos. The existing works treat them as off-the-shelf feature extractors for\neach short trimmed snippet without capturing the fine-grained relation among\ndifferent snippets in a broader temporal context. To mitigate this issue, this\npaper focuses on designing a new mechanism for adapting these pre-trained ViT\nmodels as a unified long-form video transformer to fully unleash its modeling\npower in capturing inter-snippet relation, while still keeping low computation\noverhead and memory consumption for efficient TAD. To this end, we design\neffective cross-snippet propagation modules to gradually exchange short-term\nvideo information among different snippets from two levels. For inner-backbone\ninformation propagation, we introduce a cross-snippet propagation strategy to\nenable multi-snippet temporal feature interaction inside the backbone. For\npost-backbone information propagation, we propose temporal transformer layers\nfor further clip-level modeling. With the plain ViT-B pre-trained with\nVideoMAE, our end-to-end temporal action detector (ViT-TAD) yields a very\ncompetitive performance to previous temporal action detectors, riching up to\n69.0 average mAP on THUMOS14, 37.12 average mAP on ActivityNet-1.3 and 17.20\naverage mAP on FineAction.","terms":["cs.CV"]},{"titles":"Data-efficient operator learning for solving high Mach number fluid flow problems","summaries":"We consider the problem of using SciML to predict solutions of high Mach\nfluid flows over irregular geometries. In this setting, data is limited, and so\nit is desirable for models to perform well in the low-data setting. We show\nthat Neural Basis Functions (NBF), which learns a basis of behavior modes from\nthe data and then uses this basis to make predictions, is more effective than a\nbasis-unaware baseline model. In addition, we identify continuing challenges in\nthe space of predicting solutions for this type of problem.","terms":["cs.LG","cs.NA","math.NA","physics.flu-dyn"]},{"titles":"Deep Reinforcement Learning for Community Battery Scheduling under Uncertainties of Load, PV Generation, and Energy Prices","summaries":"In response to the growing uptake of distributed energy resources (DERs),\ncommunity batteries have emerged as a promising solution to support renewable\nenergy integration, reduce peak load, and enhance grid reliability. This paper\npresents a deep reinforcement learning (RL) strategy, centered around the soft\nactor-critic (SAC) algorithm, to schedule a community battery system in the\npresence of uncertainties, such as solar photovoltaic (PV) generation, local\ndemand, and real-time energy prices. We position the community battery to play\na versatile role, in integrating local PV energy, reducing peak load, and\nexploiting energy price fluctuations for arbitrage, thereby minimizing the\nsystem cost. To improve exploration and convergence during RL training, we\nutilize the noisy network technique. This paper conducts a comparative study of\ndifferent RL algorithms, including proximal policy optimization (PPO) and deep\ndeterministic policy gradient (DDPG) algorithms, to evaluate their\neffectiveness in the community battery scheduling problem. The results\ndemonstrate the potential of RL in addressing community battery scheduling\nchallenges and show that the SAC algorithm achieves the best performance\ncompared to RL and optimization benchmarks.","terms":["cs.LG","cs.AI","math.OC"]},{"titles":"Non-Intrusive Load Monitoring for Feeder-Level EV Charging Detection: Sliding Window-based Approaches to Offline and Online Detection","summaries":"Understanding electric vehicle (EV) charging on the distribution network is\nkey to effective EV charging management and aiding decarbonization across the\nenergy and transport sectors. Advanced metering infrastructure has allowed\ndistribution system operators and utility companies to collect high-resolution\nload data from their networks. These advancements enable the non-intrusive load\nmonitoring (NILM) technique to detect EV charging using load measurement data.\nWhile existing studies primarily focused on NILM for EV charging detection in\nindividual households, there is a research gap on EV charging detection at the\nfeeder level, presenting unique challenges due to the combined load measurement\nfrom multiple households. In this paper, we develop a novel and effective\napproach for EV detection at the feeder level, involving sliding-window feature\nextraction and classical machine learning techniques, specifically models like\nXGBoost and Random Forest. Our developed method offers a lightweight and\nefficient solution, capable of quick training. Moreover, our developed method\nis versatile, supporting both offline and online EV charging detection. Our\nexperimental results demonstrate high-accuracy EV charging detection at the\nfeeder level, achieving an F-Score of 98.88% in offline detection and 93.01% in\nonline detection.","terms":["cs.LG","eess.SP"]},{"titles":"InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models","summaries":"Large vision-language models (LVLMs) have demonstrated their incredible\ncapability in image understanding and response generation. However, this rich\nvisual interaction also makes LVLMs vulnerable to adversarial examples. In this\npaper, we formulate a novel and practical gray-box attack scenario that the\nadversary can only access the visual encoder of the victim LVLM, without the\nknowledge of its prompts (which are often proprietary for service providers and\nnot publicly available) and its underlying large language model (LLM). This\npractical setting poses challenges to the cross-prompt and cross-model\ntransferability of targeted adversarial attack, which aims to confuse the LVLM\nto output a response that is semantically similar to the attacker's chosen\ntarget text. To this end, we propose an instruction-tuned targeted attack\n(dubbed InstructTA) to deliver the targeted adversarial attack on LVLMs with\nhigh transferability. Initially, we utilize a public text-to-image generative\nmodel to \"reverse\" the target response into a target image, and employ GPT-4 to\ninfer a reasonable instruction $\\boldsymbol{p}^\\prime$ from the target\nresponse. We then form a local surrogate model (sharing the same visual encoder\nwith the victim LVLM) to extract instruction-aware features of an adversarial\nimage example and the target image, and minimize the distance between these two\nfeatures to optimize the adversarial example. To further improve the\ntransferability, we augment the instruction $\\boldsymbol{p}^\\prime$ with\ninstructions paraphrased from an LLM. Extensive experiments demonstrate the\nsuperiority of our proposed method in targeted attack performance and\ntransferability.","terms":["cs.CV"]},{"titles":"Towards Surveillance Video-and-Language Understanding: New Dataset, Baselines, and Challenges","summaries":"Surveillance videos are an essential component of daily life with various\ncritical applications, particularly in public security. However, current\nsurveillance video tasks mainly focus on classifying and localizing anomalous\nevents. Existing methods are limited to detecting and classifying the\npredefined events with unsatisfactory semantic understanding, although they\nhave obtained considerable performance. To address this issue, we propose a new\nresearch direction of surveillance video-and-language understanding, and\nconstruct the first multimodal surveillance video dataset. We manually annotate\nthe real-world surveillance dataset UCF-Crime with fine-grained event content\nand timing. Our newly annotated dataset, UCA (UCF-Crime Annotation), contains\n23,542 sentences, with an average length of 20 words, and its annotated videos\nare as long as 110.7 hours. Furthermore, we benchmark SOTA models for four\nmultimodal tasks on this newly created dataset, which serve as new baselines\nfor surveillance video-and-language understanding. Through our experiments, we\nfind that mainstream models used in previously publicly available datasets\nperform poorly on surveillance video, which demonstrates the new challenges in\nsurveillance video-and-language understanding. To validate the effectiveness of\nour UCA, we conducted experiments on multimodal anomaly detection. The results\ndemonstrate that our multimodal surveillance learning can improve the\nperformance of conventional anomaly detection tasks. All the experiments\nhighlight the necessity of constructing this dataset to advance surveillance\nAI. The link to our dataset is provided at:\nhttps:\/\/xuange923.github.io\/Surveillance-Video-Understanding.","terms":["cs.CV","cs.AI"]},{"titles":"Correlation and Unintended Biases on Univariate and Multivariate Decision Trees","summaries":"Decision Trees are accessible, interpretable, and well-performing\nclassification models. A plethora of variants with increasing expressiveness\nhas been proposed in the last forty years. We contrast the two families of\nunivariate DTs, whose split functions partition data through axis-parallel\nhyperplanes, and multivariate DTs, whose splits instead partition data through\noblique hyperplanes. The latter include the former, hence multivariate DTs are\nin principle more powerful. Surprisingly enough, however, univariate DTs\nconsistently show comparable performances in the literature. We analyze the\nreasons behind this, both with synthetic and real-world benchmark datasets. Our\nresearch questions test whether the pre-processing phase of removing\ncorrelation among features in datasets has an impact on the relative\nperformances of univariate vs multivariate DTs. We find that existing benchmark\ndatasets are likely biased towards favoring univariate DTs.","terms":["cs.LG","cs.AI"]},{"titles":"Unleashing the Potential of Large Language Model: Zero-shot VQA for Flood Disaster Scenario","summaries":"Visual question answering (VQA) is a fundamental and essential AI task, and\nVQA-based disaster scenario understanding is a hot research topic. For\ninstance, we can ask questions about a disaster image by the VQA model and the\nanswer can help identify whether anyone or anything is affected by the\ndisaster. However, previous VQA models for disaster damage assessment have some\nshortcomings, such as limited candidate answer space, monotonous question\ntypes, and limited answering capability of existing models. In this paper, we\npropose a zero-shot VQA model named Zero-shot VQA for Flood Disaster Damage\nAssessment (ZFDDA). It is a VQA model for damage assessment without\npre-training. Also, with flood disaster as the main research object, we build a\nFreestyle Flood Disaster Image Question Answering dataset (FFD-IQA) to evaluate\nour VQA model. This new dataset expands the question types to include\nfree-form, multiple-choice, and yes-no questions. At the same time, we expand\nthe size of the previous dataset to contain a total of 2,058 images and 22,422\nquestion-meta ground truth pairs. Most importantly, our model uses\nwell-designed chain of thought (CoT) demonstrations to unlock the potential of\nthe large language model, allowing zero-shot VQA to show better performance in\ndisaster scenarios. The experimental results show that the accuracy in\nanswering complex questions is greatly improved with CoT prompts. Our study\nprovides a research basis for subsequent research of VQA for other disaster\nscenarios.","terms":["cs.CV","cs.AI"]},{"titles":"Benchpress: A Scalable and Versatile Workflow for Benchmarking Structure Learning Algorithms","summaries":"Describing the relationship between the variables in a study domain and\nmodelling the data generating mechanism is a fundamental problem in many\nempirical sciences. Probabilistic graphical models are one common approach to\ntackle the problem. Learning the graphical structure for such models is\ncomputationally challenging and a fervent area of current research with a\nplethora of algorithms being developed. To facilitate the benchmarking of\ndifferent methods, we present a novel Snakemake workflow, called Benchpress for\nproducing scalable, reproducible, and platform-independent benchmarks of\nstructure learning algorithms for probabilistic graphical models. Benchpress is\ninterfaced via a simple JSON-file, which makes it accessible for all users,\nwhile the code is designed in a fully modular fashion to enable researchers to\ncontribute additional methodologies. Benchpress currently provides an interface\nto a large number of state-of-the-art algorithms from libraries such as\nBDgraph, BiDAG, bnlearn, causal-learn, gCastle, GOBNILP, pcalg, r.blip,\nscikit-learn, TETRAD, and trilearn as well as a variety of methods for data\ngenerating models and performance evaluation. Alongside user-defined models and\nrandomly generated datasets, the workflow also includes a number of standard\ndatasets and graphical models from the literature, which may be included in a\nbenchmarking study. We demonstrate the applicability of this workflow for\nlearning Bayesian networks in five typical data scenarios. The source code and\ndocumentation is publicly available from http:\/\/benchpressdocs.readthedocs.io.","terms":["stat.ML","cs.LG","stat.AP","stat.CO"]},{"titles":"BatteryML:An Open-source platform for Machine Learning on Battery Degradation","summaries":"Battery degradation remains a pivotal concern in the energy storage domain,\nwith machine learning emerging as a potent tool to drive forward insights and\nsolutions. However, this intersection of electrochemical science and machine\nlearning poses complex challenges. Machine learning experts often grapple with\nthe intricacies of battery science, while battery researchers face hurdles in\nadapting intricate models tailored to specific datasets. Beyond this, a\ncohesive standard for battery degradation modeling, inclusive of data formats\nand evaluative benchmarks, is conspicuously absent. Recognizing these\nimpediments, we present BatteryML - a one-step, all-encompass, and open-source\nplatform designed to unify data preprocessing, feature extraction, and the\nimplementation of both traditional and state-of-the-art models. This\nstreamlined approach promises to enhance the practicality and efficiency of\nresearch applications. BatteryML seeks to fill this void, fostering an\nenvironment where experts from diverse specializations can collaboratively\ncontribute, thus elevating the collective understanding and advancement of\nbattery research.The code for our project is publicly available on GitHub at\nhttps:\/\/github.com\/microsoft\/BatteryML.","terms":["cs.LG","cs.AI","68T05"]},{"titles":"HGPROMPT: Bridging Homogeneous and Heterogeneous Graphs for Few-shot Prompt Learning","summaries":"Graph neural networks (GNNs) and heterogeneous graph neural networks (HGNNs)\nare prominent techniques for homogeneous and heterogeneous graph representation\nlearning, yet their performance in an end-to-end supervised framework greatly\ndepends on the availability of task-specific supervision. To reduce the\nlabeling cost, pre-training on self-supervised pretext tasks has become a\npopular paradigm,but there is often a gap between the pre-trained model and\ndownstream tasks, stemming from the divergence in their objectives. To bridge\nthe gap, prompt learning has risen as a promising direction especially in\nfew-shot settings, without the need to fully fine-tune the pre-trained model.\nWhile there has been some early exploration of prompt-based learning on graphs,\nthey primarily deal with homogeneous graphs, ignoring the heterogeneous graphs\nthat are prevalent in downstream applications. In this paper, we propose\nHGPROMPT, a novel pre-training and prompting framework to unify not only\npre-training and downstream tasks but also homogeneous and heterogeneous graphs\nvia a dual-template design. Moreover, we propose dual-prompt in HGPROMPT to\nassist a downstream task in locating the most relevant prior to bridge the gaps\ncaused by not only feature variations but also heterogeneity differences across\ntasks. Finally, we thoroughly evaluate and analyze HGPROMPT through extensive\nexperiments on three public datasets.","terms":["cs.LG"]},{"titles":"FeaInfNet: Diagnosis in Medical Image with Feature-Driven Inference and Visual Explanations","summaries":"Interpretable deep learning models have received widespread attention in the\nfield of image recognition. Due to the unique multi-instance learning of\nmedical images and the difficulty in identifying decision-making regions, many\ninterpretability models that have been proposed still have problems of\ninsufficient accuracy and interpretability in medical image disease diagnosis.\nTo solve these problems, we propose feature-driven inference network\n(FeaInfNet). Our first key innovation involves proposing a feature-based\nnetwork reasoning structure, which is applied to FeaInfNet. The network of this\nstructure compares the similarity of each sub-region image patch with the\ndisease templates and normal templates that may appear in the region, and\nfinally combines the comparison of each sub-region to make the final diagnosis.\nIt simulates the diagnosis process of doctors to make the model interpretable\nin the reasoning process, while avoiding the misleading caused by the\nparticipation of normal areas in reasoning. Secondly, we propose local feature\nmasks (LFM) to extract feature vectors in order to provide global information\nfor these vectors, thus enhancing the expressive ability of the FeaInfNet.\nFinally, we propose adaptive dynamic masks (Adaptive-DM) to interpret feature\nvectors and prototypes into human-understandable image patches to provide\naccurate visual interpretation. We conducted qualitative and quantitative\nexperiments on multiple publicly available medical datasets, including RSNA,\niChallenge-PM, Covid-19, ChinaCXRSet, and MontgomerySet. The results of our\nexperiments validate that our method achieves state-of-the-art performance in\nterms of classification accuracy and interpretability compared to baseline\nmethods in medical image diagnosis. Additional ablation studies verify the\neffectiveness of each of our proposed components.","terms":["cs.CV"]},{"titles":"Generalization by Adaptation: Diffusion-Based Domain Extension for Domain-Generalized Semantic Segmentation","summaries":"When models, e.g., for semantic segmentation, are applied to images that are\nvastly different from training data, the performance will drop significantly.\nDomain adaptation methods try to overcome this issue, but need samples from the\ntarget domain. However, this might not always be feasible for various reasons\nand therefore domain generalization methods are useful as they do not require\nany target data. We present a new diffusion-based domain extension (DIDEX)\nmethod and employ a diffusion model to generate a pseudo-target domain with\ndiverse text prompts. In contrast to existing methods, this allows to control\nthe style and content of the generated images and to introduce a high\ndiversity. In a second step, we train a generalizing model by adapting towards\nthis pseudo-target domain. We outperform previous approaches by a large margin\nacross various datasets and architectures without using any real data. For the\ngeneralization from GTA5, we improve state-of-the-art mIoU performance by 3.8%\nabsolute on average and for SYNTHIA by 11.8% absolute, marking a big step for\nthe generalization performance on these benchmarks. Code is available at\nhttps:\/\/github.com\/JNiemeijer\/DIDEX","terms":["cs.CV","cs.LG"]},{"titles":"Geometrically-driven Aggregation for Zero-shot 3D Point Cloud Understanding","summaries":"Zero-shot 3D point cloud understanding can be achieved via 2D Vision-Language\nModels (VLMs). Existing strategies directly map Vision-Language Models from 2D\npixels of rendered or captured views to 3D points, overlooking the inherent and\nexpressible point cloud geometric structure. Geometrically similar or close\nregions can be exploited for bolstering point cloud understanding as they are\nlikely to share semantic information. To this end, we introduce the first\ntraining-free aggregation technique that leverages the point cloud's 3D\ngeometric structure to improve the quality of the transferred Vision-Language\nModels. Our approach operates iteratively, performing local-to-global\naggregation based on geometric and semantic point-level reasoning. We benchmark\nour approach on three downstream tasks, including classification, part\nsegmentation, and semantic segmentation, with a variety of datasets\nrepresenting both synthetic\/real-world, and indoor\/outdoor scenarios. Our\napproach achieves new state-of-the-art results in all benchmarks. We will\nrelease the source code publicly.","terms":["cs.CV"]},{"titles":"Few Clicks Suffice: Active Test-Time Adaptation for Semantic Segmentation","summaries":"Test-time adaptation (TTA) adapts the pre-trained models during inference\nusing unlabeled test data and has received a lot of research attention due to\nits potential practical value. Unfortunately, without any label supervision,\nexisting TTA methods rely heavily on heuristic or empirical studies. Where to\nupdate the model always falls into suboptimal or brings more computational\nresource consumption. Meanwhile, there is still a significant performance gap\nbetween the TTA approaches and their supervised counterparts. Motivated by\nactive learning, in this work, we propose the active test-time adaptation for\nsemantic segmentation setup. Specifically, we introduce the human-in-the-loop\npattern during the testing phase, which queries very few labels to facilitate\npredictions and model updates in an online manner. To do so, we propose a\nsimple but effective ATASeg framework, which consists of two parts, i.e., model\nadapter and label annotator. Extensive experiments demonstrate that ATASeg\nbridges the performance gap between TTA methods and their supervised\ncounterparts with only extremely few annotations, even one click for labeling\nsurpasses known SOTA TTA methods by 2.6% average mIoU on ACDC benchmark.\nEmpirical results imply that progress in either the model adapter or the label\nannotator will bring improvements to the ATASeg framework, giving it large\nresearch and reality potential.","terms":["cs.CV"]},{"titles":"FlowHON: Representing Flow Fields Using Higher-Order Networks","summaries":"Flow fields are often partitioned into data blocks for massively parallel\ncomputation and analysis based on blockwise relationships. However, most of the\nprevious techniques only consider the first-order dependencies among blocks,\nwhich is insufficient in describing complex flow patterns. In this work, we\npresent FlowHON, an approach to construct higher-order networks (HONs) from\nflow fields. FlowHON captures the inherent higher-order dependencies in flow\nfields as nodes and estimates the transitions among them as edges. We formulate\nthe HON construction as an optimization problem with three linear\ntransformations. The first two layers correspond to the node generation and the\nthird one corresponds to edge estimation. Our formulation allows the node\ngeneration and edge estimation to be solved in a unified framework. With\nFlowHON, the rich set of traditional graph algorithms can be applied without\nany modification to analyze flow fields, while leveraging the higher-order\ninformation to understand the inherent structure and manage flow data for\nefficiency. We demonstrate the effectiveness of FlowHON using a series of\ndownstream tasks, including estimating the density of particles during tracing,\npartitioning flow fields for data management, and understanding flow fields\nusing the node-link diagram representation of networks.","terms":["cs.LG"]},{"titles":"Class Symbolic Regression: Gotta Fit 'Em All","summaries":"We introduce \"Class Symbolic Regression\" a first framework for automatically\nfinding a single analytical functional form that accurately fits multiple\ndatasets - each governed by its own (possibly) unique set of fitting\nparameters. This hierarchical framework leverages the common constraint that\nall the members of a single class of physical phenomena follow a common\ngoverning law. Our approach extends the capabilities of our earlier Physical\nSymbolic Optimization ($\\Phi$-SO) framework for Symbolic Regression, which\nintegrates dimensional analysis constraints and deep reinforcement learning for\nsymbolic analytical function discovery from data. We demonstrate the efficacy\nof this novel approach by applying it to a panel of synthetic toy case datasets\nand showcase its practical utility for astrophysics by successfully extracting\nan analytic galaxy potential from a set of simulated orbits approximating\nstellar streams.","terms":["cs.LG","astro-ph.GA","astro-ph.IM","physics.comp-ph"]},{"titles":"Energy-based Potential Games for Joint Motion Forecasting and Control","summaries":"This work uses game theory as a mathematical framework to address interaction\nmodeling in multi-agent motion forecasting and control. Despite its\ninterpretability, applying game theory to real-world robotics, like automated\ndriving, faces challenges such as unknown game parameters. To tackle these, we\nestablish a connection between differential games, optimal control, and\nenergy-based models, demonstrating how existing approaches can be unified under\nour proposed Energy-based Potential Game formulation. Building upon this, we\nintroduce a new end-to-end learning application that combines neural networks\nfor game-parameter inference with a differentiable game-theoretic optimization\nlayer, acting as an inductive bias. The analysis provides empirical evidence\nthat the game-theoretic layer adds interpretability and improves the predictive\nperformance of various neural network backbones using two simulations and two\nreal-world driving datasets.","terms":["cs.LG","cs.AI","cs.GT","cs.MA","cs.RO"]},{"titles":"BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge","summaries":"Pre-trained large language models have significantly improved code\ngeneration. As these models scale up, there is an increasing need for the\noutput to handle more intricate tasks and to be appropriately specialized to\nparticular domains. Here, we target bioinformatics due to the amount of\nspecialized domain knowledge, algorithms, and data operations this discipline\nrequires. We present BioCoder, a benchmark developed to evaluate large language\nmodels (LLMs) in generating bioinformatics-specific code. BioCoder spans a\nbroad spectrum of the field and covers cross-file dependencies, class\ndeclarations, and global variables. It incorporates 1026 Python functions and\n1243 Java methods extracted from GitHub, along with 253 examples from the\nRosalind Project, all pertaining to bioinformatics. Using topic modeling we\nshow that overall coverage of the included code is representative of the full\nspectrum of bioinformatics calculations. BioCoder incorporates a fuzz-testing\nframework for evaluation. We have applied it to evaluate many models including\nInCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+,\nGPT-3.5, and GPT-4. Furthermore, we finetuned StarCoder, demonstrating how our\ndataset can effectively enhance the performance of LLMs on our benchmark (by\n>15% in terms of Pass@K in certain prompt configurations and always >3%). The\nresults highlight two key aspects of successful models: (1) Successful models\naccommodate a long prompt (> ~2600 tokens) with full context, for functional\ndependencies. (2) They contain specific domain knowledge of bioinformatics,\nbeyond just general coding knowledge. This is evident from the performance gain\nof GPT-3.5\/4 compared to the smaller models on the benchmark (50% vs up to\n~25%). Our dataset, benchmark, Docker images, and scripts required for testing\nare all available at https:\/\/github.com\/gersteinlab\/biocoder.","terms":["cs.LG","cs.AI","cs.CL"]},{"titles":"Collaborative Neural Painting","summaries":"The process of painting fosters creativity and rational planning. However,\nexisting generative AI mostly focuses on producing visually pleasant artworks,\nwithout emphasizing the painting process. We introduce a novel task,\nCollaborative Neural Painting (CNP), to facilitate collaborative art painting\ngeneration between humans and machines. Given any number of user-input\nbrushstrokes as the context or just the desired object class, CNP should\nproduce a sequence of strokes supporting the completion of a coherent painting.\nImportantly, the process can be gradual and iterative, so allowing users'\nmodifications at any phase until the completion. Moreover, we propose to solve\nthis task using a painting representation based on a sequence of parametrized\nstrokes, which makes it easy both editing and composition operations. These\nparametrized strokes are processed by a Transformer-based architecture with a\nnovel attention mechanism to model the relationship between the input strokes\nand the strokes to complete. We also propose a new masking scheme to reflect\nthe interactive nature of CNP and adopt diffusion models as the basic learning\nprocess for its effectiveness and diversity in the generative field. Finally,\nto develop and validate methods on the novel task, we introduce a new dataset\nof painted objects and an evaluation protocol to benchmark CNP both\nquantitatively and qualitatively. We demonstrate the effectiveness of our\napproach and the potential of the CNP task as a promising avenue for future\nresearch.","terms":["cs.CV"]},{"titles":"Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation and Human Feedback","summaries":"Risk-sensitive reinforcement learning (RL) aims to optimize policies that\nbalance the expected reward and risk. In this paper, we present a novel\nrisk-sensitive RL framework that employs an Iterated Conditional Value-at-Risk\n(CVaR) objective under both linear and general function approximations,\nenriched by human feedback. These new formulations provide a principled way to\nguarantee safety in each decision making step throughout the control process.\nMoreover, integrating human feedback into risk-sensitive RL framework bridges\nthe gap between algorithmic decision-making and human participation, allowing\nus to also guarantee safety for human-in-the-loop systems. We propose provably\nsample-efficient algorithms for this Iterated CVaR RL and provide rigorous\ntheoretical analysis. Furthermore, we establish a matching lower bound to\ncorroborate the optimality of our algorithms in a linear context.","terms":["cs.LG"]},{"titles":"GS-IR: 3D Gaussian Splatting for Inverse Rendering","summaries":"We propose GS-IR, a novel inverse rendering approach based on 3D Gaussian\nSplatting (GS) that leverages forward mapping volume rendering to achieve\nphotorealistic novel view synthesis and relighting results. Unlike previous\nworks that use implicit neural representations and volume rendering (e.g.\nNeRF), which suffer from low expressive power and high computational\ncomplexity, we extend GS, a top-performance representation for novel view\nsynthesis, to estimate scene geometry, surface material, and environment\nillumination from multi-view images captured under unknown lighting conditions.\nThere are two main problems when introducing GS to inverse rendering: 1) GS\ndoes not support producing plausible normal natively; 2) forward mapping (e.g.\nrasterization and splatting) cannot trace the occlusion like backward mapping\n(e.g. ray tracing). To address these challenges, our GS-IR proposes an\nefficient optimization scheme that incorporates a depth-derivation-based\nregularization for normal estimation and a baking-based occlusion to model\nindirect lighting. The flexible and expressive GS representation allows us to\nachieve fast and compact geometry reconstruction, photorealistic novel view\nsynthesis, and effective physically-based rendering. We demonstrate the\nsuperiority of our method over baseline methods through qualitative and\nquantitative evaluations on various challenging scenes.","terms":["cs.CV"]},{"titles":"Distributed Continual Learning with CoCoA in High-dimensional Linear Regression","summaries":"We consider estimation under scenarios where the signals of interest exhibit\nchange of characteristics over time. In particular, we consider the continual\nlearning problem where different tasks, e.g., data with different\ndistributions, arrive sequentially and the aim is to perform well on the newly\narrived task without performance degradation on the previously seen tasks. In\ncontrast to the continual learning literature focusing on the centralized\nsetting, we investigate the problem from a distributed estimation perspective.\nWe consider the well-established distributed learning algorithm COCOA, which\ndistributes the model parameters and the corresponding features over the\nnetwork. We provide exact analytical characterization for the generalization\nerror of COCOA under continual learning for linear regression in a range of\nscenarios, where overparameterization is of particular interest. These\nanalytical results characterize how the generalization error depends on the\nnetwork structure, the task similarity and the number of tasks, and show how\nthese dependencies are intertwined. In particular, our results show that the\ngeneralization error can be significantly reduced by adjusting the network\nsize, where the most favorable network size depends on task similarity and the\nnumber of tasks. We present numerical results verifying the theoretical\nanalysis and illustrate the continual learning performance of COCOA with a\ndigit classification task.","terms":["cs.LG","eess.SP"]},{"titles":"Visualizing key features in X-ray images of epoxy resins for improved material classification using singular value decomposition of deep learning features","summaries":"Although the process variables of epoxy resins alter their mechanical\nproperties, the visual identification of the characteristic features of X-ray\nimages of samples of these materials is challenging. To facilitate the\nidentification, we approximate the magnitude of the gradient of the intensity\nfield of the X-ray images of different kinds of epoxy resins and then we use\ndeep learning to discover the most representative features of the transformed\nimages. In this solution of the inverse problem to finding characteristic\nfeatures to discriminate samples of heterogeneous materials, we use the\neigenvectors obtained from the singular value decomposition of all the channels\nof the feature maps of the early layers in a convolutional neural network.\nWhile the strongest activated channel gives a visual representation of the\ncharacteristic features, often these are not robust enough in some practical\nsettings. On the other hand, the left singular vectors of the matrix\ndecomposition of the feature maps, barely change when variables such as the\ncapacity of the network or network architecture change. High classification\naccuracy and robustness of characteristic features are presented in this work.","terms":["cs.CV"]},{"titles":"Wild-Tab: A Benchmark For Out-Of-Distribution Generalization In Tabular Regression","summaries":"Out-of-Distribution (OOD) generalization, a cornerstone for building robust\nmachine learning models capable of handling data diverging from the training\nset's distribution, is an ongoing challenge in deep learning. While significant\nprogress has been observed in computer vision and natural language processing,\nits exploration in tabular data, ubiquitous in many industrial applications,\nremains nascent. To bridge this gap, we present Wild-Tab, a large-scale\nbenchmark tailored for OOD generalization in tabular regression tasks. The\nbenchmark incorporates 3 industrial datasets sourced from fields like weather\nprediction and power consumption estimation, providing a challenging testbed\nfor evaluating OOD performance under real-world conditions. Our extensive\nexperiments, evaluating 10 distinct OOD generalization methods on Wild-Tab,\nreveal nuanced insights. We observe that many of these methods often struggle\nto maintain high-performance levels on unseen data, with OOD performance\nshowing a marked drop compared to in-distribution performance. At the same\ntime, Empirical Risk Minimization (ERM), despite its simplicity, delivers\nrobust performance across all evaluations, rivaling the results of\nstate-of-the-art methods. Looking forward, we hope that the release of Wild-Tab\nwill facilitate further research on OOD generalization and aid in the\ndeployment of machine learning models in various real-world contexts where\nhandling distribution shifts is a crucial requirement.","terms":["cs.LG"]},{"titles":"Contrastive Learning-Based Spectral Knowledge Distillation for Multi-Modality and Missing Modality Scenarios in Semantic Segmentation","summaries":"Improving the performance of semantic segmentation models using multispectral\ninformation is crucial, especially for environments with low-light and adverse\nconditions. Multi-modal fusion techniques pursue either the learning of\ncross-modality features to generate a fused image or engage in knowledge\ndistillation but address multimodal and missing modality scenarios as distinct\nissues, which is not an optimal approach for multi-sensor models. To address\nthis, a novel multi-modal fusion approach called CSK-Net is proposed, which\nuses a contrastive learning-based spectral knowledge distillation technique\nalong with an automatic mixed feature exchange mechanism for semantic\nsegmentation in optical (EO) and infrared (IR) images. The distillation scheme\nextracts detailed textures from the optical images and distills them into the\noptical branch of CSK-Net. The model encoder consists of shared convolution\nweights with separate batch norm (BN) layers for both modalities, to capture\nthe multi-spectral information from different modalities of the same objects. A\nNovel Gated Spectral Unit (GSU) and mixed feature exchange strategy are\nproposed to increase the correlation of modality-shared information and\ndecrease the modality-specific information during the distillation process.\nComprehensive experiments show that CSK-Net surpasses state-of-the-art models\nin multi-modal tasks and for missing modalities when exclusively utilizing IR\ndata for inference across three public benchmarking datasets. For missing\nmodality scenarios, the performance increase is achieved without additional\ncomputational costs compared to the baseline segmentation models.","terms":["cs.CV","cs.AI"]},{"titles":"Exploring Multi-Modal Fusion for Image Manipulation Detection and Localization","summaries":"Recent image manipulation localization and detection techniques usually\nleverage forensic artifacts and traces that are produced by a noise-sensitive\nfilter, such as SRM and Bayar convolution. In this paper, we showcase that\ndifferent filters commonly used in such approaches excel at unveiling different\ntypes of manipulations and provide complementary forensic traces. Thus, we\nexplore ways of merging the outputs of such filters and aim to leverage the\ncomplementary nature of the artifacts produced to perform image manipulation\nlocalization and detection (IMLD). We propose two distinct methods: one that\nproduces independent features from each forensic filter and then fuses them\n(this is referred to as late fusion) and one that performs early mixing of\ndifferent modal outputs and produces early combined features (this is referred\nto as early fusion). We demonstrate that both approaches achieve competitive\nperformance for both image manipulation localization and detection,\noutperforming state-of-the-art models across several datasets.","terms":["cs.CV"]},{"titles":"Two-stage optimized unified adversarial patch for attacking visible-infrared cross-modal detectors in the physical world","summaries":"Currently, many studies have addressed security concerns related to visible\nand infrared detectors independently. In practical scenarios, utilizing\ncross-modal detectors for tasks proves more reliable than relying on\nsingle-modal detectors. Despite this, there is a lack of comprehensive security\nevaluations for cross-modal detectors. While existing research has explored the\nfeasibility of attacks against cross-modal detectors, the implementation of a\nrobust attack remains unaddressed. This work introduces the Two-stage Optimized\nUnified Adversarial Patch (TOUAP) designed for performing attacks against\nvisible-infrared cross-modal detectors in real-world, black-box settings. The\nTOUAP employs a two-stage optimization process: firstly, PSO optimizes an\nirregular polygonal infrared patch to attack the infrared detector; secondly,\nthe color QR code is optimized, and the shape information of the infrared patch\nfrom the first stage is used as a mask. The resulting irregular polygon visible\nmodal patch executes an attack on the visible detector. Through extensive\nexperiments conducted in both digital and physical environments, we validate\nthe effectiveness and robustness of the proposed method. As the TOUAP surpasses\nbaseline performance, we advocate for its widespread attention.","terms":["cs.CV"]},{"titles":"Multi Time Scale World Models","summaries":"Intelligent agents use internal world models to reason and make predictions\nabout different courses of their actions at many scales. Devising learning\nparadigms and architectures that allow machines to learn world models that\noperate at multiple levels of temporal abstractions while dealing with complex\nuncertainty predictions is a major technical hurdle. In this work, we propose a\nprobabilistic formalism to learn multi-time scale world models which we call\nthe Multi Time Scale State Space (MTS3) model. Our model uses a computationally\nefficient inference scheme on multiple time scales for highly accurate\nlong-horizon predictions and uncertainty estimates over several seconds into\nthe future. Our experiments, which focus on action conditional long horizon\nfuture predictions, show that MTS3 outperforms recent methods on several system\nidentification benchmarks including complex simulated and real-world dynamical\nsystems. Code is available at this repository: https:\/\/github.com\/ALRhub\/MTS3.","terms":["cs.LG","cs.AI"]},{"titles":"Label-Efficient Deep Learning in Medical Image Analysis: Challenges and Future Directions","summaries":"Deep learning has seen rapid growth in recent years and achieved\nstate-of-the-art performance in a wide range of applications. However, training\nmodels typically requires expensive and time-consuming collection of large\nquantities of labeled data. This is particularly true within the scope of\nmedical imaging analysis (MIA), where data are limited and labels are expensive\nto be acquired. Thus, label-efficient deep learning methods are developed to\nmake comprehensive use of the labeled data as well as the abundance of\nunlabeled and weak-labeled data. In this survey, we extensively investigated\nover 300 recent papers to provide a comprehensive overview of recent progress\non label-efficient learning strategies in MIA. We first present the background\nof label-efficient learning and categorize the approaches into different\nschemes. Next, we examine the current state-of-the-art methods in detail\nthrough each scheme. Specifically, we provide an in-depth investigation,\ncovering not only canonical semi-supervised, self-supervised, and\nmulti-instance learning schemes, but also recently emerged active and\nannotation-efficient learning strategies. Moreover, as a comprehensive\ncontribution to the field, this survey not only elucidates the commonalities\nand unique features of the surveyed methods but also presents a detailed\nanalysis of the current challenges in the field and suggests potential avenues\nfor future research.","terms":["cs.CV","cs.AI"]},{"titles":"Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation","summaries":"Recently, machine learning (ML) has become a popular approach to support\nself-adaptation. ML has been used to deal with several problems in\nself-adaptation, such as maintaining an up-to-date runtime model under\nuncertainty and scalable decision-making. Yet, exploiting ML comes with\ninherent challenges. In this paper, we focus on a particularly important\nchallenge for learning-based self-adaptive systems: drift in adaptation spaces.\nWith adaptation space we refer to the set of adaptation options a self-adaptive\nsystem can select from at a given time to adapt based on the estimated quality\nproperties of the adaptation options. Drift of adaptation spaces originates\nfrom uncertainties, affecting the quality properties of the adaptation options.\nSuch drift may imply that eventually no adaptation option can satisfy the\ninitial set of the adaptation goals, deteriorating the quality of the system,\nor adaptation options may emerge that allow enhancing the adaptation goals. In\nML, such shift corresponds to novel class appearance, a type of concept drift\nin target data that common ML techniques have problems dealing with. To tackle\nthis problem, we present a novel approach to self-adaptation that enhances\nlearning-based self-adaptive systems with a lifelong ML layer. We refer to this\napproach as lifelong self-adaptation. The lifelong ML layer tracks the system\nand its environment, associates this knowledge with the current tasks,\nidentifies new tasks based on differences, and updates the learning models of\nthe self-adaptive system accordingly. A human stakeholder may be involved to\nsupport the learning process and adjust the learning and goal models. We\npresent a general architecture for lifelong self-adaptation and apply it to the\ncase of drift of adaptation spaces that affects the decision-making in\nself-adaptation. We validate the approach for a series of scenarios using the\nDeltaIoT exemplar.","terms":["cs.LG","cs.AI","cs.NE","cs.SE"]},{"titles":"Minimal Random Code Learning with Mean-KL Parameterization","summaries":"This paper studies the qualitative behavior and robustness of two variants of\nMinimal Random Code Learning (MIRACLE) used to compress variational Bayesian\nneural networks. MIRACLE implements a powerful, conditionally Gaussian\nvariational approximation for the weight posterior $Q_{\\mathbf{w}}$ and uses\nrelative entropy coding to compress a weight sample from the posterior using a\nGaussian coding distribution $P_{\\mathbf{w}}$. To achieve the desired\ncompression rate, $D_{\\mathrm{KL}}[Q_{\\mathbf{w}} \\Vert P_{\\mathbf{w}}]$ must\nbe constrained, which requires a computationally expensive annealing procedure\nunder the conventional mean-variance (Mean-Var) parameterization for\n$Q_{\\mathbf{w}}$. Instead, we parameterize $Q_{\\mathbf{w}}$ by its mean and KL\ndivergence from $P_{\\mathbf{w}}$ to constrain the compression cost to the\ndesired value by construction. We demonstrate that variational training with\nMean-KL parameterization converges twice as fast and maintains predictive\nperformance after compression. Furthermore, we show that Mean-KL leads to more\nmeaningful variational distributions with heavier tails and compressed weight\nsamples which are more robust to pruning.","terms":["cs.LG","stat.ML"]},{"titles":"CNN Feature Map Augmentation for Single-Source Domain Generalization","summaries":"In search of robust and generalizable machine learning models, Domain\nGeneralization (DG) has gained significant traction during the past few years.\nThe goal in DG is to produce models which continue to perform well when\npresented with data distributions different from the ones available during\ntraining. While deep convolutional neural networks (CNN) have been able to\nachieve outstanding performance on downstream computer vision tasks, they still\noften fail to generalize on previously unseen data Domains. Therefore, in this\nwork we focus on producing a model which is able to remain robust under data\ndistribution shift and propose an alternative regularization technique for\nconvolutional neural network architectures in the single-source DG image\nclassification setting. To mitigate the problem caused by domain shift between\nsource and target data, we propose augmenting intermediate feature maps of\nCNNs. Specifically, we pass them through a novel Augmentation Layer} to prevent\nmodels from overfitting on the training set and improve their cross-domain\ngeneralization. To the best of our knowledge, this is the first paper proposing\nsuch a setup for the DG image classification setting. Experiments on the DG\nbenchmark datasets of PACS, VLCS, Office-Home and TerraIncognita validate the\neffectiveness of our method, in which our model surpasses state-of-the-art\nalgorithms in most cases.","terms":["cs.CV"]},{"titles":"IMProv: Inpainting-based Multimodal Prompting for Computer Vision Tasks","summaries":"In-context learning allows adapting a model to new tasks given a task\ndescription at test time. In this paper, we present IMProv - a generative model\nthat is able to in-context learn visual tasks from multimodal prompts. Given a\ntextual description of a visual task (e.g. \"Left: input image, Right:\nforeground segmentation\"), a few input-output visual examples, or both, the\nmodel in-context learns to solve it for a new test input. We train a masked\ngenerative transformer on a new dataset of figures from computer vision papers\nand their associated captions, together with a captioned large-scale image-text\ndataset. During inference time, we prompt the model with text and\/or image task\nexample(s) and have the model inpaint the corresponding output. We show that\ntraining our model with text conditioning and scaling the dataset size improves\nin-context learning for computer vision tasks by over +10\\% AP for Foreground\nSegmentation, over +5\\% gains in AP for Single Object Detection, and almost\n20\\% lower LPIPS in Colorization. Our empirical results suggest that vision and\nlanguage prompts are complementary and it is advantageous to use both to\nachieve better in-context learning performance. Project page is available at\nhttps:\/\/jerryxu.net\/IMProv .","terms":["cs.CV"]},{"titles":"Few-Shot Anomaly Detection with Adversarial Loss for Robust Feature Representations","summaries":"Anomaly detection is a critical and challenging task that aims to identify\ndata points deviating from normal patterns and distributions within a dataset.\nVarious methods have been proposed using a one-class-one-model approach, but\nthese techniques often face practical problems such as memory inefficiency and\nthe requirement of sufficient data for training. In particular, few-shot\nanomaly detection presents significant challenges in industrial applications,\nwhere limited samples are available before mass production. In this paper, we\npropose a few-shot anomaly detection method that integrates adversarial\ntraining loss to obtain more robust and generalized feature representations. We\nutilize the adversarial loss previously employed in domain adaptation to align\nfeature distributions between source and target domains, to enhance feature\nrobustness and generalization in few-shot anomaly detection tasks. We\nhypothesize that adversarial loss is effective when applied to features that\nshould have similar characteristics, such as those from the same layer in a\nSiamese network's parallel branches or input-output pairs of\nreconstruction-based methods. Experimental results demonstrate that the\nproposed method generally achieves better performance when utilizing the\nadversarial loss.","terms":["cs.LG","cs.CV"]},{"titles":"Localizing and Assessing Node Significance in Default Mode Network using Sub-Community Detection in Mild Cognitive Impairment","summaries":"Our study aims to utilize fMRI to identify the affected brain regions within\nthe Default Mode Network (DMN) in subjects with Mild Cognitive Impairment\n(MCI), using a novel Node Significance Score (NSS). We construct\nsubject-specific DMN graphs by employing partial correlation of Regions of\nInterest (ROIs) that make-up the DMN. For the DMN graph, ROIs are the nodes and\nedges are determined based on partial correlation. Four popular community\ndetection algorithms (Clique Percolation Method (CPM), Louvain algorithm,\nGreedy Modularity and Leading Eigenvectors) are applied to determine the\nlargest sub-community. NSS ratings are derived for each node, considering (I)\nfrequency in the largest sub-community within a class across all subjects and\n(II) occurrence in the largest sub-community according to all four methods.\nAfter computing the NSS of each ROI in both healthy and MCI subjects, we\nquantify the score disparity to identify nodes most impacted by MCI. The\nresults reveal a disparity exceeding 20% for 10 DMN nodes, maximally for PCC\nand Fusiform, showing 45.69% and 43.08% disparity. This aligns with existing\nmedical literature, additionally providing a quantitative measure that enables\nthe ordering of the affected ROIs. These findings offer valuable insights and\ncould lead to treatment strategies aggressively targeting the affected nodes.","terms":["cs.CV"]},{"titles":"Dynamic Erasing Network Based on Multi-Scale Temporal Features for Weakly Supervised Video Anomaly Detection","summaries":"The goal of weakly supervised video anomaly detection is to learn a detection\nmodel using only video-level labeled data. However, prior studies typically\ndivide videos into fixed-length segments without considering the complexity or\nduration of anomalies. Moreover, these studies usually just detect the most\nabnormal segments, potentially overlooking the completeness of anomalies. To\naddress these limitations, we propose a Dynamic Erasing Network (DE-Net) for\nweakly supervised video anomaly detection, which learns multi-scale temporal\nfeatures. Specifically, to handle duration variations of abnormal events, we\nfirst propose a multi-scale temporal modeling module, capable of extracting\nfeatures from segments of varying lengths and capturing both local and global\nvisual information across different temporal scales. Then, we design a dynamic\nerasing strategy, which dynamically assesses the completeness of the detected\nanomalies and erases prominent abnormal segments in order to encourage the\nmodel to discover gentle abnormal segments in a video. The proposed method\nobtains favorable performance compared to several state-of-the-art approaches\non three datasets: XD-Violence, TAD, and UCF-Crime. Code will be made available\nat https:\/\/github.com\/ArielZc\/DE-Net.","terms":["cs.CV"]},{"titles":"CZL-CIAE: CLIP-driven Zero-shot Learning for Correcting Inverse Age Estimation","summaries":"Zero-shot age estimation aims to learn feature information about age from\ninput images and make inferences about a given person's image or video frame\nwithout specific sample data. The development of zero-shot age estimation can\nimprove the efficiency and accuracy of various applications (e.g., age\nverification and secure access control, etc.), while also promoting research on\nmulti-modal and zero-shot learning in the social media field. For example,\nzero-sample age estimation can be used to create social networks focused on\nspecific age groups. However, existing methods mainly focus on supervised,\nlabeled age estimation learning, and the prediction effect of zero-shot\nlearning is very poor. To tackle the above issues, we propose a novel\nCLIP-driven Zero-shot Learning for Correcting Inverse Age Estimation\n(CZL-CIAE). Specifically, we first introduce the CLIP model to extract image\nfeatures and text semantic information respectively, and map them into a highly\nsemantically aligned high-dimensional feature space. Next, we designed a new\nTransformer architecture (i.e., FourierFormer) to achieve channel evolution and\nspatial interaction of images, and to fuse image and text semantic information.\nFinally, we introduce reversible age estimation, which uses end-to-end error\nfeedback to reduce the error rate of age predictions. Through extensive\nexperiments on multiple data sets, CZL-CIAE has achieved better age prediction\nresults.","terms":["cs.CV","cs.AI"]},{"titles":"A Comprehensive Literature Review on Sweet Orange Leaf Diseases","summaries":"Sweet orange leaf diseases are significant to agricultural productivity. Leaf\ndiseases impact fruit quality in the citrus industry. The apparition of machine\nlearning makes the development of disease finder. Early detection and diagnosis\nare necessary for leaf management. Sweet orange leaf disease-predicting\nautomated systems have already been developed using different image-processing\ntechniques. This comprehensive literature review is systematically based on\nleaf disease and machine learning methodologies applied to the detection of\ndamaged leaves via image classification. The benefits and limitations of\ndifferent machine learning models, including Vision Transformer (ViT), Neural\nNetwork (CNN), CNN with SoftMax and RBF SVM, Hybrid CNN-SVM, HLB-ConvMLP,\nEfficientNet-b0, YOLOv5, YOLOv7, Convolutional, Deep CNN. These machine\nlearning models tested on various datasets and detected the disease. This\ncomprehensive review study related to leaf disease compares the performance of\nthe models; those models' accuracy, precision, recall, etc., were used in the\nsubsisting studies","terms":["cs.CV","cs.AI","cs.HC"]},{"titles":"3D Point Cloud Registration with Learning-based Matching Algorithm","summaries":"We present a novel differential matching algorithm for 3D point cloud\nregistration. Instead of only optimizing the feature extractor for a matching\nalgorithm, we propose a learning-based matching module optimized to the\njointly-trained feature extractor. We focused on edge-wise feature-forwarding\narchitectures, which are memory-consuming but can avoid the over-smoothing\neffect that GNNs suffer. We improve its memory efficiency to scale it for point\ncloud registration while investigating the best way of connecting it to the\nfeature extractor. Experimental results show our matching module's significant\nimpact on performance improvement in rigid\/non-rigid and whole\/partial point\ncloud registration datasets with multiple contemporary feature extractors. For\nexample, our module boosted the current SOTA method, RoITr, by +5.4%, and +7.2%\nin the NFMR metric and +6.1% and +8.5% in the IR metric on the 4DMatch and\n4DLoMatch datasets, respectively.","terms":["cs.CV"]},{"titles":"Long-Tail Learning with Rebalanced Contrastive Loss","summaries":"Integrating supervised contrastive loss to cross entropy-based communication\nhas recently been proposed as a solution to address the long-tail learning\nproblem. However, when the class imbalance ratio is high, it requires adjusting\nthe supervised contrastive loss to support the tail classes, as the\nconventional contrastive learning is biased towards head classes by default. To\nthis end, we present Rebalanced Contrastive Learning (RCL), an efficient means\nto increase the long tail classification accuracy by addressing three main\naspects: 1. Feature space balancedness - Equal division of the feature space\namong all the classes, 2. Intra-Class compactness - Reducing the distance\nbetween same-class embeddings, 3. Regularization - Enforcing larger margins for\ntail classes to reduce overfitting. RCL adopts class frequency-based SoftMax\nloss balancing to supervised contrastive learning loss and exploits scalar\nmultiplied features fed to the contrastive learning loss to enforce\ncompactness. We implement RCL on the Balanced Contrastive Learning (BCL)\nFramework, which has the SOTA performance. Our experiments on three benchmark\ndatasets demonstrate the richness of the learnt embeddings and increased top-1\nbalanced accuracy RCL provides to the BCL framework. We further demonstrate\nthat the performance of RCL as a standalone loss also achieves state-of-the-art\nlevel accuracy.","terms":["cs.LG","cs.CV"]},{"titles":"Open-DDVM: A Reproduction and Extension of Diffusion Model for Optical Flow Estimation","summaries":"Recently, Google proposes DDVM which for the first time demonstrates that a\ngeneral diffusion model for image-to-image translation task works impressively\nwell on optical flow estimation task without any specific designs like RAFT.\nHowever, DDVM is still a closed-source model with the expensive and private\nPalette-style pretraining. In this technical report, we present the first\nopen-source DDVM by reproducing it. We study several design choices and find\nthose important ones. By training on 40k public data with 4 GPUs, our\nreproduction achieves comparable performance to the closed-source DDVM. The\ncode and model have been released in\nhttps:\/\/github.com\/DQiaole\/FlowDiffusion_pytorch.","terms":["cs.CV"]},{"titles":"Cross-Modal Adaptive Dual Association for Text-to-Image Person Retrieval","summaries":"Text-to-image person re-identification (ReID) aims to retrieve images of a\nperson based on a given textual description. The key challenge is to learn the\nrelations between detailed information from visual and textual modalities.\nExisting works focus on learning a latent space to narrow the modality gap and\nfurther build local correspondences between two modalities. However, these\nmethods assume that image-to-text and text-to-image associations are\nmodality-agnostic, resulting in suboptimal associations. In this work, we show\nthe discrepancy between image-to-text association and text-to-image association\nand propose CADA: Cross-Modal Adaptive Dual Association that finely builds\nbidirectional image-text detailed associations. Our approach features a\ndecoder-based adaptive dual association module that enables full interaction\nbetween visual and textual modalities, allowing for bidirectional and adaptive\ncross-modal correspondence associations. Specifically, the paper proposes a\nbidirectional association mechanism: Association of text Tokens to image\nPatches (ATP) and Association of image Regions to text Attributes (ARA). We\nadaptively model the ATP based on the fact that aggregating cross-modal\nfeatures based on mistaken associations will lead to feature distortion. For\nmodeling the ARA, since the attributes are typically the first distinguishing\ncues of a person, we propose to explore the attribute-level association by\npredicting the masked text phrase using the related image region. Finally, we\nlearn the dual associations between texts and images, and the experimental\nresults demonstrate the superiority of our dual formulation. Codes will be made\npublicly available.","terms":["cs.CV"]},{"titles":"Singular Regularization with Information Bottleneck Improves Model's Adversarial Robustness","summaries":"Adversarial examples are one of the most severe threats to deep learning\nmodels. Numerous works have been proposed to study and defend adversarial\nexamples. However, these works lack analysis of adversarial information or\nperturbation, which cannot reveal the mystery of adversarial examples and lose\nproper interpretation. In this paper, we aim to fill this gap by studying\nadversarial information as unstructured noise, which does not have a clear\npattern. Specifically, we provide some empirical studies with singular value\ndecomposition, by decomposing images into several matrices, to analyze\nadversarial information for different attacks. Based on the analysis, we\npropose a new module to regularize adversarial information and combine\ninformation bottleneck theory, which is proposed to theoretically restrict\nintermediate representations. Therefore, our method is interpretable. Moreover,\nthe fashion of our design is a novel principle that is general and unified.\nEquipped with our new module, we evaluate two popular model structures on two\nmainstream datasets with various adversarial attacks. The results indicate that\nthe improvement in robust accuracy is significant. On the other hand, we prove\nthat our method is efficient with only a few additional parameters and able to\nbe explained under regional faithfulness analysis.","terms":["cs.CV"]},{"titles":"Fully Spiking Denoising Diffusion Implicit Models","summaries":"Spiking neural networks (SNNs) have garnered considerable attention owing to\ntheir ability to run on neuromorphic devices with super-high speeds and\nremarkable energy efficiencies. SNNs can be used in conventional neural\nnetwork-based time- and energy-consuming applications. However, research on\ngenerative models within SNNs remains limited, despite their advantages. In\nparticular, diffusion models are a powerful class of generative models, whose\nimage generation quality surpass that of the other generative models, such as\nGANs. However, diffusion models are characterized by high computational costs\nand long inference times owing to their iterative denoising feature. Therefore,\nwe propose a novel approach fully spiking denoising diffusion implicit model\n(FSDDIM) to construct a diffusion model within SNNs and leverage the high speed\nand low energy consumption features of SNNs via synaptic current learning\n(SCL). SCL fills the gap in that diffusion models use a neural network to\nestimate real-valued parameters of a predefined probabilistic distribution,\nwhereas SNNs output binary spike trains. The SCL enables us to complete the\nentire generative process of diffusion models exclusively using SNNs. We\ndemonstrate that the proposed method outperforms the state-of-the-art fully\nspiking generative model.","terms":["cs.CV"]},{"titles":"SRSNetwork: Siamese Reconstruction-Segmentation Networks based on Dynamic-Parameter Convolution","summaries":"In this paper, we present a high-performance deep neural network for weak\ntarget image segmentation, including medical image segmentation and infrared\nimage segmentation. To this end, this work analyzes the existing dynamic\nconvolutions and proposes dynamic parameter convolution (DPConv). Furthermore,\nit reevaluates the relationship between reconstruction tasks and segmentation\ntasks from the perspective of DPConv, leading to the proposal of a dual-network\nmodel called the Siamese Reconstruction-Segmentation Network (SRSNet). The\nproposed model is not only a universal network but also enhances the\nsegmentation performance without altering its structure, leveraging the\nreconstruction task. Additionally, as the amount of training data for the\nreconstruction network increases, the performance of the segmentation network\nalso improves synchronously. On seven datasets including five medical datasets\nand two infrared image datasets, our SRSNet consistently achieves the best\nsegmentation results. The code is released at https:\/\/github.com\/fidshu\/SRSNet.","terms":["cs.CV","I.4.6"]},{"titles":"Divide-and-Conquer Strategy for Large-Scale Dynamic Bayesian Network Structure Learning","summaries":"Dynamic Bayesian Networks (DBNs), renowned for their interpretability, have\nbecome increasingly vital in representing complex stochastic processes in\nvarious domains such as gene expression analysis, healthcare, and traffic\nprediction. Structure learning of DBNs from data is challenging, particularly\nfor datasets with thousands of variables. Most current algorithms for DBN\nstructure learning are adaptations from those used in static Bayesian Networks\n(BNs), and are typically focused on small-scale problems. In order to solve\nlarge-scale problems while taking full advantage of existing algorithms, this\npaper introduces a novel divide-and-conquer strategy, originally developed for\nstatic BNs, and adapts it for large-scale DBN structure learning. In this work,\nwe specifically concentrate on 2 Time-sliced Bayesian Networks (2-TBNs), a\nspecial class of DBNs. Furthermore, we leverage the prior knowledge of 2-TBNs\nto enhance the performance of the strategy we introduce. Our approach\nsignificantly improves the scalability and accuracy of 2-TBN structure\nlearning. Experimental results demonstrate the effectiveness of our method,\nshowing substantial improvements over existing algorithms in both computational\nefficiency and structure learning accuracy. On problem instances with more than\n1,000 variables, our approach improves two accuracy metrics by 74.45% and\n110.94% on average , respectively, while reducing runtime by 93.65% on average.","terms":["cs.LG","cs.AI"]},{"titles":"RL4CO: a Unified Reinforcement Learning for Combinatorial Optimization Library","summaries":"Deep reinforcement learning offers notable benefits in addressing\ncombinatorial problems over traditional solvers, reducing the reliance on\ndomain-specific knowledge and expert solutions, and improving computational\nefficiency. Despite the recent surge in interest in neural combinatorial\noptimization, practitioners often do not have access to a standardized code\nbase. Moreover, different algorithms are frequently based on fragmentized\nimplementations that hinder reproducibility and fair comparison. To address\nthese challenges, we introduce RL4CO, a unified Reinforcement Learning (RL) for\nCombinatorial Optimization (CO) library. We employ state-of-the-art software\nand best practices in implementation, such as modularity and configuration\nmanagement, to be flexible, easily modifiable, and extensible by researchers.\nThanks to our unified codebase, we benchmark baseline RL solvers with different\nevaluation schemes on zero-shot performance, generalization, and adaptability\non diverse tasks. Notably, we find that some recent methods may fall behind\ntheir predecessors depending on the evaluation settings. We hope RL4CO will\nencourage the exploration of novel solutions to complex real-world tasks,\nallowing the community to compare with existing methods through a unified\nframework that decouples the science from software engineering. We open-source\nour library at https:\/\/github.com\/ai4co\/rl4co.","terms":["cs.LG","cs.AI"]},{"titles":"Omnipotent Adversarial Training in the Wild","summaries":"Adversarial training is an important topic in robust deep learning, but the\ncommunity lacks attention to its practical usage. In this paper, we aim to\nresolve a real-world challenge, i.e., training a model on an imbalanced and\nnoisy dataset to achieve high clean accuracy and adversarial robustness, with\nour proposed Omnipotent Adversarial Training (OAT) strategy. OAT consists of\ntwo innovative methodologies to address the imperfection in the training set.\nWe first introduce an oracle into the adversarial training process to help the\nmodel learn a correct data-label conditional distribution. This\ncarefully-designed oracle can provide correct label annotations for adversarial\ntraining. We further propose logits adjustment adversarial training to overcome\nthe data imbalance issue, which can help the model learn a Bayes-optimal\ndistribution. Our comprehensive evaluation results show that OAT outperforms\nother baselines by more than 20% clean accuracy improvement and 10% robust\naccuracy improvement under complex combinations of data imbalance and label\nnoise scenarios. The code can be found in https:\/\/github.com\/GuanlinLee\/OAT.","terms":["cs.LG","cs.CR","cs.CV"]},{"titles":"Effective Adapter for Face Recognition in the Wild","summaries":"In this paper, we tackle the challenge of face recognition in the wild, where\nimages often suffer from low quality and real-world distortions. Traditional\nheuristic approaches-either training models directly on these degraded images\nor their enhanced counterparts using face restoration techniques-have proven\nineffective, primarily due to the degradation of facial features and the\ndiscrepancy in image domains. To overcome these issues, we propose an effective\nadapter for augmenting existing face recognition models trained on high-quality\nfacial datasets. The key of our adapter is to process both the unrefined and\nthe enhanced images by two similar structures where one is fixed and the other\ntrainable. Such design can confer two benefits. First, the dual-input system\nminimizes the domain gap while providing varied perspectives for the face\nrecognition model, where the enhanced image can be regarded as a complex\nnon-linear transformation of the original one by the restoration model. Second,\nboth two similar structures can be initialized by the pre-trained models\nwithout dropping the past knowledge. The extensive experiments in zero-shot\nsettings show the effectiveness of our method by surpassing baselines of about\n3%, 4%, and 7% in three datasets. Our code will be publicly available at\nhttps:\/\/github.com\/liuyunhaozz\/FaceAdapter\/.","terms":["cs.CV"]},{"titles":"Likelihood-Aware Semantic Alignment for Full-Spectrum Out-of-Distribution Detection","summaries":"Full-spectrum out-of-distribution (F-OOD) detection aims to accurately\nrecognize in-distribution (ID) samples while encountering semantic and\ncovariate shifts simultaneously. However, existing out-of-distribution (OOD)\ndetectors tend to overfit the covariance information and ignore intrinsic\nsemantic correlation, inadequate for adapting to complex domain\ntransformations. To address this issue, we propose a Likelihood-Aware Semantic\nAlignment (LSA) framework to promote the image-text correspondence into\nsemantically high-likelihood regions. LSA consists of an offline Gaussian\nsampling strategy which efficiently samples semantic-relevant visual embeddings\nfrom the class-conditional Gaussian distribution, and a bidirectional prompt\ncustomization mechanism that adjusts both ID-related and negative context for\ndiscriminative ID\/OOD boundary. Extensive experiments demonstrate the\nremarkable OOD detection performance of our proposed LSA especially on the\nintractable Near-OOD setting, surpassing existing methods by a margin of\n$15.26\\%$ and $18.88\\%$ on two F-OOD benchmarks, respectively.","terms":["cs.CV"]},{"titles":"Alleviating the Effect of Data Imbalance on Adversarial Training","summaries":"In this paper, we study adversarial training on datasets that obey the\nlong-tailed distribution, which is practical but rarely explored in previous\nworks. Compared with conventional adversarial training on balanced datasets,\nthis process falls into the dilemma of generating uneven adversarial examples\n(AEs) and an unbalanced feature embedding space, causing the resulting model to\nexhibit low robustness and accuracy on tail data. To combat that, we\ntheoretically analyze the lower bound of the robust risk to train a model on a\nlong-tailed dataset to obtain the key challenges in addressing the\naforementioned dilemmas. Based on it, we propose a new adversarial training\nframework -- Re-balancing Adversarial Training (REAT). This framework consists\nof two components: (1) a new training strategy inspired by the effective number\nto guide the model to generate more balanced and informative AEs; (2) a\ncarefully constructed penalty function to force a satisfactory feature space.\nEvaluation results on different datasets and model structures prove that REAT\ncan effectively enhance the model's robustness and preserve the model's clean\naccuracy. The code can be found in https:\/\/github.com\/GuanlinLee\/REAT.","terms":["cs.LG","cs.CR","cs.CV"]},{"titles":"Learning Structure-from-Motion with Graph Attention Networks","summaries":"In this paper we tackle the problem of learning Structure-from-Motion (SfM)\nthrough the use of graph attention networks. SfM is a classic computer vision\nproblem that is solved though iterative minimization of reprojection errors,\nreferred to as Bundle Adjustment (BA), starting from a good initialization. In\norder to obtain a good enough initialization to BA, conventional methods rely\non a sequence of sub-problems (such as pairwise pose estimation, pose averaging\nor triangulation) which provides an initial solution that can then be refined\nusing BA. In this work we replace these sub-problems by learning a model that\ntakes as input the 2D keypoints detected across multiple views, and outputs the\ncorresponding camera poses and 3D keypoint coordinates. Our model takes\nadvantage of graph neural networks to learn SfM-specific primitives, and we\nshow that it can be used for fast inference of the reconstruction for new and\nunseen sequences. The experimental results show that the proposed model\noutperforms competing learning-based methods, and challenges COLMAP while\nhaving lower runtime.","terms":["cs.CV","cs.LG"]},{"titles":"EdgeConvFormer: Dynamic Graph CNN and Transformer based Anomaly Detection in Multivariate Time Series","summaries":"Transformer-based models for anomaly detection in multivariate time series\ncan benefit from the self-attention mechanism due to its advantage in modeling\nlong-term dependencies. However, Transformer-based anomaly detection models\nhave problems such as a large amount of data being required for training,\nstandard positional encoding is not suitable for multivariate time series data,\nand the interdependence between time series is not considered. To address these\nlimitations, we propose a novel anomaly detection method, named EdgeConvFormer,\nwhich integrates Time2vec embedding, stacked dynamic graph CNN, and Transformer\nto extract global and local spatial-time information. This design of\nEdgeConvFormer empowers it with decomposition capacities for complex time\nseries, progressive spatiotemporal correlation discovery between time series,\nand representation aggregation of multi-scale features. Experiments demonstrate\nthat EdgeConvFormer can learn the spatial-temporal correlations from\nmultivariate time series data and achieve better anomaly detection performance\nthan the state-of-the-art approaches on many real-world datasets of different\nscales.","terms":["cs.LG"]},{"titles":"ImputeFormer: Graph Transformers for Generalizable Spatiotemporal Imputation","summaries":"This paper focuses on the multivariate time series imputation problem using\ndeep neural architectures. The ubiquitous issue of missing data in both\nscientific and engineering tasks necessitates the development of an effective\nand general imputation model. Leveraging the wisdom and expertise garnered from\nlow-rank imputation methods, we power the canonical Transformers with three key\nknowledge-driven enhancements, including projected temporal attention, global\nadaptive graph convolution, and Fourier imputation loss. These task-agnostic\ninductive biases exploit the inherent structures of incomplete time series, and\nthus make our model versatile for a variety of imputation problems. We\ndemonstrate its superiority in terms of accuracy, efficiency, and flexibility\non heterogeneous datasets, including traffic speed, traffic volume, solar\nenergy, smart metering, and air quality. Comprehensive case studies are\nperformed to further strengthen the interpretability. Promising empirical\nresults provide strong conviction that incorporating time series primitives,\nsuch as low-rank properties, can substantially facilitate the development of a\ngeneralizable model to approach a wide range of spatiotemporal imputation\nproblems.","terms":["cs.LG"]},{"titles":"De Novo Drug Design with Joint Transformers","summaries":"De novo drug design requires simultaneously generating novel molecules\noutside of training data and predicting their target properties, making it a\nhard task for generative models. To address this, we propose Joint Transformer\nthat combines a Transformer decoder, Transformer encoder, and a predictor in a\njoint generative model with shared weights. We formulate a probabilistic\nblack-box optimization algorithm that employs Joint Transformer to generate\nnovel molecules with improved target properties and outperforms other\nSMILES-based optimization methods in de novo drug design.","terms":["cs.LG","cs.AI"]},{"titles":"MineGAN++: Mining Generative Models for Efficient Knowledge Transfer to Limited Data Domains","summaries":"GANs largely increases the potential impact of generative models. Therefore,\nwe propose a novel knowledge transfer method for generative models based on\nmining the knowledge that is most beneficial to a specific target domain,\neither from a single or multiple pretrained GANs. This is done using a miner\nnetwork that identifies which part of the generative distribution of each\npretrained GAN outputs samples closest to the target domain. Mining effectively\nsteers GAN sampling towards suitable regions of the latent space, which\nfacilitates the posterior finetuning and avoids pathologies of other methods,\nsuch as mode collapse and lack of flexibility. Furthermore, to prevent\noverfitting on small target domains, we introduce sparse subnetwork selection,\nthat restricts the set of trainable neurons to those that are relevant for the\ntarget dataset. We perform comprehensive experiments on several challenging\ndatasets using various GAN architectures (BigGAN, Progressive GAN, and\nStyleGAN) and show that the proposed method, called MineGAN, effectively\ntransfers knowledge to domains with few target images, outperforming existing\nmethods. In addition, MineGAN can successfully transfer knowledge from multiple\npretrained GANs.","terms":["cs.CV"]},{"titles":"Warfare:Breaking the Watermark Protection of AI-Generated Content","summaries":"AI-Generated Content (AIGC) is gaining great popularity, with many emerging\ncommercial services and applications. These services leverage advanced\ngenerative models, such as latent diffusion models and large language models,\nto generate creative content (e.g., realistic images and fluent sentences) for\nusers. The usage of such generated content needs to be highly regulated, as the\nservice providers need to ensure the users do not violate the usage policies\n(e.g., abuse for commercialization, generating and distributing unsafe\ncontent). A promising solution to achieve this goal is watermarking, which adds\nunique and imperceptible watermarks on the content for service verification and\nattribution. Numerous watermarking approaches have been proposed recently.\nHowever, in this paper, we show that an adversary can easily break these\nwatermarking mechanisms. Specifically, we consider two possible attacks. (1)\nWatermark removal: the adversary can easily erase the embedded watermark from\nthe generated content and then use it freely bypassing the regulation of the\nservice provider. (2) Watermark forging: the adversary can create illegal\ncontent with forged watermarks from another user, causing the service provider\nto make wrong attributions. We propose Warfare, a unified methodology to\nachieve both attacks in a holistic way. The key idea is to leverage a\npre-trained diffusion model for content processing and a generative adversarial\nnetwork for watermark removal or forging. We evaluate Warfare on different\ndatasets and embedding setups. The results prove that it can achieve high\nsuccess rates while maintaining the quality of the generated content. Compared\nto existing diffusion model-based attacks, Warfare is 5,050~11,000x faster.","terms":["cs.CV","cs.AI"]},{"titles":"StableVITON: Learning Semantic Correspondence with Latent Diffusion Model for Virtual Try-On","summaries":"Given a clothing image and a person image, an image-based virtual try-on aims\nto generate a customized image that appears natural and accurately reflects the\ncharacteristics of the clothing image. In this work, we aim to expand the\napplicability of the pre-trained diffusion model so that it can be utilized\nindependently for the virtual try-on task.The main challenge is to preserve the\nclothing details while effectively utilizing the robust generative capability\nof the pre-trained model. In order to tackle these issues, we propose\nStableVITON, learning the semantic correspondence between the clothing and the\nhuman body within the latent space of the pre-trained diffusion model in an\nend-to-end manner. Our proposed zero cross-attention blocks not only preserve\nthe clothing details by learning the semantic correspondence but also generate\nhigh-fidelity images by utilizing the inherent knowledge of the pre-trained\nmodel in the warping process. Through our proposed novel attention total\nvariation loss and applying augmentation, we achieve the sharp attention map,\nresulting in a more precise representation of clothing details. StableVITON\noutperforms the baselines in qualitative and quantitative evaluation, showing\npromising quality in arbitrary person images. Our code is available at\nhttps:\/\/github.com\/rlawjdghek\/StableVITON.","terms":["cs.CV"]},{"titles":"The Self-Loop Paradox: Investigating the Impact of Self-Loops on Graph Neural Networks","summaries":"Many Graph Neural Networks (GNNs) add self-loops to a graph to include\nfeature information about a node itself at each layer. However, if the GNN\nconsists of more than one layer, this information can return to its origin via\ncycles in the graph topology. Intuition suggests that this \"backflow\" of\ninformation should be larger in graphs with self-loops compared to graphs\nwithout. In this work, we counter this intuition and show that for certain GNN\narchitectures, the information a node gains from itself can be smaller in\ngraphs with self-loops compared to the same graphs without. We adopt an\nanalytical approach for the study of statistical graph ensembles with a given\ndegree sequence and show that this phenomenon, which we call the self-loop\nparadox, can depend both on the number of GNN layers $k$ and whether $k$ is\neven or odd. We experimentally validate our theoretical findings in a synthetic\nnode classification task and investigate its practical relevance in 23\nreal-world graphs.","terms":["cs.LG"]},{"titles":"Rethinking Adversarial Training with Neural Tangent Kernel","summaries":"Adversarial training (AT) is an important and attractive topic in deep\nlearning security, exhibiting mysteries and odd properties. Recent studies of\nneural network training dynamics based on Neural Tangent Kernel (NTK) make it\npossible to reacquaint AT and deeply analyze its properties. In this paper, we\nperform an in-depth investigation of AT process and properties with NTK, such\nas NTK evolution. We uncover three new findings that are missed in previous\nworks. First, we disclose the impact of data normalization on AT and the\nimportance of unbiased estimators in batch normalization layers. Second, we\nexperimentally explore the kernel dynamics and propose more time-saving AT\nmethods. Third, we study the spectrum feature inside the kernel to address the\ncatastrophic overfitting problem. To the best of our knowledge, it is the first\nwork leveraging the observations of kernel dynamics to improve existing AT\nmethods.","terms":["cs.LG","cs.AI"]},{"titles":"Disentangled Interaction Representation for One-Stage Human-Object Interaction Detection","summaries":"Human-Object Interaction (HOI) detection is a core task for human-centric\nimage understanding. Recent one-stage methods adopt a transformer decoder to\ncollect image-wide cues that are useful for interaction prediction; however,\nthe interaction representations obtained using this method are entangled and\nlack interpretability. In contrast, traditional two-stage methods benefit\nsignificantly from their ability to compose interaction features in a\ndisentangled and explainable manner. In this paper, we improve the performance\nof one-stage methods by enabling them to extract disentangled interaction\nrepresentations. First, we propose Shunted Cross-Attention (SCA) to extract\nhuman appearance, object appearance, and global context features using\ndifferent cross-attention heads. This is achieved by imposing different masks\non the cross-attention maps produced by the different heads. Second, we\nintroduce the Interaction-aware Pose Estimation (IPE) task to learn\ninteraction-relevant human pose features using a disentangled decoder. This is\nachieved with a novel attention module that accurately captures the human\nkeypoints relevant to the current interaction category. Finally, our approach\nfuses the appearance feature and pose feature via element-wise addition to form\nthe interaction representation. Experimental results show that our approach can\nbe readily applied to existing one-stage HOI detectors. Moreover, we achieve\nstate-of-the-art performance on two benchmarks: HICO-DET and V-COCO.","terms":["cs.CV"]},{"titles":"Typhoon Intensity Prediction with Vision Transformer","summaries":"Predicting typhoon intensity accurately across space and time is crucial for\nissuing timely disaster warnings and facilitating emergency response. This has\nvast potential for minimizing life losses and property damages as well as\nreducing economic and environmental impacts. Leveraging satellite imagery for\nscenario analysis is effective but also introduces additional challenges due to\nthe complex relations among clouds and the highly dynamic context. Existing\ndeep learning methods in this domain rely on convolutional neural networks\n(CNNs), which suffer from limited per-layer receptive fields. This limitation\nhinders their ability to capture long-range dependencies and global contextual\nknowledge during inference. In response, we introduce a novel approach, namely\n\"Typhoon Intensity Transformer\" (Tint), which leverages self-attention\nmechanisms with global receptive fields per layer. Tint adopts a\nsequence-to-sequence feature representation learning perspective. It begins by\ncutting a given satellite image into a sequence of patches and recursively\nemploys self-attention operations to extract both local and global contextual\nrelations between all patch pairs simultaneously, thereby enhancing per-patch\nfeature representation learning. Extensive experiments on a publicly available\ntyphoon benchmark validate the efficacy of Tint in comparison with both\nstate-of-the-art deep learning and conventional meteorological methods. Our\ncode is available at https:\/\/github.com\/chen-huanxin\/Tint.","terms":["cs.CV","cs.AI"]},{"titles":"Regressor-Segmenter Mutual Prompt Learning for Crowd Counting","summaries":"Crowd counting has achieved significant progress by training regressors to\npredict instance positions. In heavily crowded scenarios, however, regressors\nare challenged by uncontrollable annotation variance, which causes density map\nbias and context information inaccuracy. In this study, we propose mutual\nprompt learning (mPrompt), which leverages a regressor and a segmenter as\nguidance for each other, solving bias and inaccuracy caused by annotation\nvariance while distinguishing foreground from background. In specific, mPrompt\nleverages point annotations to tune the segmenter and predict pseudo head masks\nin a way of point prompt learning. It then uses the predicted segmentation\nmasks, which serve as spatial constraint, to rectify biased point annotations\nas context prompt learning. mPrompt defines a way of mutual information\nmaximization from prompt learning, mitigating the impact of annotation variance\nwhile improving model accuracy. Experiments show that mPrompt significantly\nreduces the Mean Average Error (MAE), demonstrating the potential to be general\nframework for down-stream vision tasks.","terms":["cs.CV"]},{"titles":"GenEM: Physics-Informed Generative Cryo-Electron Microscopy","summaries":"In the past decade, deep conditional generative models have revolutionized\nthe generation of realistic images, extending their application from\nentertainment to scientific domains. Single-particle cryo-electron microscopy\n(cryo-EM) is crucial in resolving near-atomic resolution 3D structures of\nproteins, such as the SARS-COV-2 spike protein. To achieve high-resolution\nreconstruction, AI models for particle picking and pose estimation have been\nadopted. However, their performance is still limited as they lack high-quality\nannotated datasets. To address this, we introduce physics-informed generative\ncryo-electron microscopy (GenEM), which for the first time integrates\nphysical-based cryo-EM simulation with a generative unpaired noise translation\nto generate physically correct synthetic cryo-EM datasets with realistic\nnoises. Initially, GenEM simulates the cryo-EM imaging process based on a\nvirtual specimen. To generate realistic noises, we leverage an unpaired noise\ntranslation via contrastive learning with a novel mask-guided sampling scheme.\nExtensive experiments show that GenEM is capable of generating realistic\ncryo-EM images. The generated dataset can further enhance particle picking and\npose estimation models, eventually improving the reconstruction resolution. We\nwill release our code and annotated synthetic datasets.","terms":["cs.CV"]},{"titles":"Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites","summaries":"Large language models (LLMs) have shown remarkable performance in natural\nlanguage processing (NLP) tasks. To comprehend and execute diverse human\ninstructions over image data, instruction-tuned large vision-language models\n(LVLMs) have been introduced. However, LVLMs may suffer from different types of\nobject hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained\nobject hallucinations only (i.e., generated objects non-existent in the input\nimage). The fine-grained object attributes and behaviors non-existent in the\nimage may still be generated but not measured by the current evaluation\nmethods. In this paper, we thus focus on reducing fine-grained hallucinations\nof LVLMs. We propose \\textit{ReCaption}, a framework that consists of two\ncomponents: rewriting captions using ChatGPT and fine-tuning the\ninstruction-tuned LVLMs on the rewritten captions. We also propose a\nfine-grained probing-based evaluation method named \\textit{Fine-Grained Object\nHallucination Evaluation} (\\textit{FGHE}). Our experiment results demonstrate\nthat ReCaption effectively reduces fine-grained object hallucination for\ndifferent LVLM options and improves their text generation quality. The code can\nbe found at https:\/\/github.com\/Anonymousanoy\/FOHE.","terms":["cs.CV","cs.CL"]},{"titles":"Rethinking Urban Mobility Prediction: A Super-Multivariate Time Series Forecasting Approach","summaries":"Long-term urban mobility predictions play a crucial role in the effective\nmanagement of urban facilities and services. Conventionally, urban mobility\ndata has been structured as spatiotemporal videos, treating longitude and\nlatitude grids as fundamental pixels. Consequently, video prediction methods,\nrelying on Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs),\nhave been instrumental in this domain. In our research, we introduce a fresh\nperspective on urban mobility prediction. Instead of oversimplifying urban\nmobility data as traditional video data, we regard it as a complex multivariate\ntime series. This perspective involves treating the time-varying values of each\ngrid in each channel as individual time series, necessitating a thorough\nexamination of temporal dynamics, cross-variable correlations, and\nfrequency-domain insights for precise and reliable predictions. To address this\nchallenge, we present the Super-Multivariate Urban Mobility Transformer\n(SUMformer), which utilizes a specially designed attention mechanism to\ncalculate temporal and cross-variable correlations and reduce computational\ncosts stemming from a large number of time series. SUMformer also employs\nlow-frequency filters to extract essential information for long-term\npredictions. Furthermore, SUMformer is structured with a temporal patch merge\nmechanism, forming a hierarchical framework that enables the capture of\nmulti-scale correlations. Consequently, it excels in urban mobility pattern\nmodeling and long-term prediction, outperforming current state-of-the-art\nmethods across three real-world datasets.","terms":["cs.LG","cs.AI"]},{"titles":"BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection","summaries":"Recently, the rise of query-based Transformer decoders is reshaping\ncamera-based 3D object detection. These query-based decoders are surpassing the\ntraditional dense BEV (Bird's Eye View)-based methods. However, we argue that\ndense BEV frameworks remain important due to their outstanding abilities in\ndepth estimation and object localization, depicting 3D scenes accurately and\ncomprehensively. This paper aims to address the drawbacks of the existing dense\nBEV-based 3D object detectors by introducing our proposed enhanced components,\nincluding a CRF-modulated depth estimation module enforcing object-level\nconsistencies, a long-term temporal aggregation module with extended receptive\nfields, and a two-stage object decoder combining perspective techniques with\nCRF-modulated depth embedding. These enhancements lead to a \"modernized\" dense\nBEV framework dubbed BEVNeXt. On the nuScenes benchmark, BEVNeXt outperforms\nboth BEV-based and query-based frameworks under various settings, achieving a\nstate-of-the-art result of 64.2 NDS on the nuScenes test set.","terms":["cs.CV"]},{"titles":"LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS","summaries":"Recent advancements in real-time neural rendering using point-based\ntechniques have paved the way for the widespread adoption of 3D\nrepresentations. However, foundational approaches like 3D Gaussian Splatting\ncome with a substantial storage overhead caused by growing the SfM points to\nmillions, often demanding gigabyte-level disk space for a single unbounded\nscene, posing significant scalability challenges and hindering the splatting\nefficiency.\n  To address this challenge, we introduce LightGaussian, a novel method\ndesigned to transform 3D Gaussians into a more efficient and compact format.\nDrawing inspiration from the concept of Network Pruning, LightGaussian\nidentifies Gaussians that are insignificant in contributing to the scene\nreconstruction and adopts a pruning and recovery process, effectively reducing\nredundancy in Gaussian counts while preserving visual effects. Additionally,\nLightGaussian employs distillation and pseudo-view augmentation to distill\nspherical harmonics to a lower degree, allowing knowledge transfer to more\ncompact representations while maintaining reflectance. Furthermore, we propose\na hybrid scheme, VecTree Quantization, to quantize all attributes, resulting in\nlower bitwidth representations with minimal accuracy losses.\n  In summary, LightGaussian achieves an averaged compression rate over 15x\nwhile boosting the FPS from 139 to 215, enabling an efficient representation of\ncomplex scenes on Mip-NeRF 360, Tank and Temple datasets.\n  Project website: https:\/\/lightgaussian.github.io\/","terms":["cs.CV"]},{"titles":"Risk-Controlling Model Selection via Guided Bayesian Optimization","summaries":"Adjustable hyperparameters of machine learning models typically impact\nvarious key trade-offs such as accuracy, fairness, robustness, or inference\ncost. Our goal in this paper is to find a configuration that adheres to\nuser-specified limits on certain risks while being useful with respect to other\nconflicting metrics. We solve this by combining Bayesian Optimization (BO) with\nrigorous risk-controlling procedures, where our core idea is to steer BO\ntowards an efficient testing strategy. Our BO method identifies a set of Pareto\noptimal configurations residing in a designated region of interest. The\nresulting candidates are statistically verified and the best-performing\nconfiguration is selected with guaranteed risk levels. We demonstrate the\neffectiveness of our approach on a range of tasks with multiple desiderata,\nincluding low error rates, equitable predictions, handling spurious\ncorrelations, managing rate and distortion in generative models, and reducing\ncomputational costs.","terms":["cs.LG","cs.AI","stat.ME","stat.ML"]},{"titles":"Optimizing Bus Travel: A Novel Approach to Feature Mining with P-KMEANS and P-LDA Algorithms","summaries":"Customizing services for bus travel can bolster its attractiveness, optimize\nusage, alleviate traffic congestion, and diminish carbon emissions. This\npotential is realized by harnessing recent advancements in positioning\ncommunication facilities, the Internet of Things, and artificial intelligence\nfor feature mining in public transportation. However, the inherent complexities\nof disorganized and unstructured public transportation data introduce\nsubstantial challenges to travel feature extraction. This study presents a bus\ntravel feature extraction method rooted in Point of Interest (POI) data,\nemploying enhanced P-KMENAS and P-LDA algorithms to overcome these limitations.\nWhile the KMEANS algorithm adeptly segments passenger travel paths into\ndistinct clusters, its outcomes can be influenced by the initial K value. On\nthe other hand, Latent Dirichlet Allocation (LDA) excels at feature\nidentification and probabilistic interpretations yet encounters difficulties\nwith feature intermingling and nuanced sub-feature interactions. Incorporating\nthe POI dimension enhances our understanding of travel behavior, aligning it\nmore closely with passenger attributes and facilitating easier data analysis.\nBy incorporating POI data, our refined P-KMENAS and P-LDA algorithms grant a\nholistic insight into travel behaviors and attributes, effectively mitigating\nthe limitations above. Consequently, this POI-centric algorithm effectively\namalgamates diverse POI attributes, delineates varied travel contexts, and\nimparts probabilistic metrics to feature properties. Our method successfully\nmines the diverse aspects of bus travel, such as age, occupation, gender,\nsports, cost, safety, and personality traits. It effectively calculates\nrelationships between individual travel behaviors and assigns explanatory and\nevaluative probabilities to POI labels, thereby enhancing bus travel\noptimization.","terms":["cs.LG"]},{"titles":"ResEnsemble-DDPM: Residual Denoising Diffusion Probabilistic Models for Ensemble Learning","summaries":"Nowadays, denoising diffusion probabilistic models have been adapted for many\nimage segmentation tasks. However, existing end-to-end models have already\ndemonstrated remarkable capabilities. Rather than using denoising diffusion\nprobabilistic models alone, integrating the abilities of both denoising\ndiffusion probabilistic models and existing end-to-end models can better\nimprove the performance of image segmentation. Based on this, we implicitly\nintroduce residual term into the diffusion process and propose\nResEnsemble-DDPM, which seamlessly integrates the diffusion model and the\nend-to-end model through ensemble learning. The output distributions of these\ntwo models are strictly symmetric with respect to the ground truth\ndistribution, allowing us to integrate the two models by reducing the residual\nterm. Experimental results demonstrate that our ResEnsemble-DDPM can further\nimprove the capabilities of existing models. Furthermore, its ensemble learning\nstrategy can be generalized to other downstream tasks in image generation and\nget strong competitiveness.","terms":["cs.CV","cs.AI"]},{"titles":"3D Face Reconstruction with the Geometric Guidance of Facial Part Segmentation","summaries":"3D Morphable Models (3DMMs) provide promising 3D face reconstructions in\nvarious applications. However, existing methods struggle to reconstruct faces\nwith extreme expressions due to deficiencies in supervisory signals, such as\nsparse or inaccurate landmarks. Segmentation information contains effective\ngeometric contexts for face reconstruction. Certain attempts intuitively depend\non differentiable renderers to compare the rendered silhouettes of\nreconstruction with segmentation, which is prone to issues like local optima\nand gradient instability. In this paper, we fully utilize the facial part\nsegmentation geometry by introducing Part Re-projection Distance Loss (PRDL).\nSpecifically, PRDL transforms facial part segmentation into 2D points and\nre-projects the reconstruction onto the image plane. Subsequently, by\nintroducing grid anchors and computing different statistical distances from\nthese anchors to the point sets, PRDL establishes geometry descriptors to\noptimize the distribution of the point sets for face reconstruction. PRDL\nexhibits a clear gradient compared to the renderer-based methods and presents\nstate-of-the-art reconstruction performance in extensive quantitative and\nqualitative experiments. The project will be publicly available.","terms":["cs.CV"]},{"titles":"Applications of Large Scale Foundation Models for Autonomous Driving","summaries":"Since DARPA Grand Challenges (rural) in 2004\/05 and Urban Challenges in 2007,\nautonomous driving has been the most active field of AI applications. Recently\npowered by large language models (LLMs), chat systems, such as chatGPT and\nPaLM, emerge and rapidly become a promising direction to achieve artificial\ngeneral intelligence (AGI) in natural language processing (NLP). There comes a\nnatural thinking that we could employ these abilities to reformulate autonomous\ndriving. By combining LLM with foundation models, it is possible to utilize the\nhuman knowledge, commonsense and reasoning to rebuild autonomous driving\nsystems from the current long-tailed AI dilemma. In this paper, we investigate\nthe techniques of foundation models and LLMs applied for autonomous driving,\ncategorized as simulation, world model, data annotation and planning or E2E\nsolutions etc.","terms":["cs.CV"]},{"titles":"EDALearn: A Comprehensive RTL-to-Signoff EDA Benchmark for Democratized and Reproducible ML for EDA Research","summaries":"The application of Machine Learning (ML) in Electronic Design Automation\n(EDA) for Very Large-Scale Integration (VLSI) design has garnered significant\nresearch attention. Despite the requirement for extensive datasets to build\neffective ML models, most studies are limited to smaller, internally generated\ndatasets due to the lack of comprehensive public resources. In response, we\nintroduce EDALearn, the first holistic, open-source benchmark suite\nspecifically for ML tasks in EDA. This benchmark suite presents an end-to-end\nflow from synthesis to physical implementation, enriching data collection\nacross various stages. It fosters reproducibility and promotes research into ML\ntransferability across different technology nodes. Accommodating a wide range\nof VLSI design instances and sizes, our benchmark aptly represents the\ncomplexity of contemporary VLSI designs. Additionally, we provide an in-depth\ndata analysis, enabling users to fully comprehend the attributes and\ndistribution of our data, which is essential for creating efficient ML models.\nOur contributions aim to encourage further advances in the ML-EDA domain.","terms":["cs.LG"]},{"titles":"MedXChat: Bridging CXR Modalities with a Unified Multimodal Large Model","summaries":"Despite the success of Large Language Models (LLMs) in general image tasks, a\ngap persists in the medical field for a multimodal large model adept at\nhandling the nuanced diversity of medical images. Addressing this, we propose\nMedXChat, a unified multimodal large model designed for seamless interactions\nbetween medical assistants and users. MedXChat encompasses three key\nfunctionalities: CXR(Chest X-ray)-to-Report generation, CXR-based visual\nquestion-answering (VQA), and Text-to-CXR synthesis. Our contributions are as\nfollows. Firstly, our model showcases exceptional cross-task adaptability,\ndisplaying adeptness across all three defined tasks and outperforming the\nbenchmark models on the MIMIC dataset in medical multimodal applications.\nSecondly, we introduce an innovative Text-to-CXR synthesis approach that\nutilizes instruction-following capabilities within the Stable Diffusion (SD)\narchitecture. This technique integrates smoothly with the existing model\nframework, requiring no extra parameters, thereby maintaining the SD's\ngenerative strength while also bestowing upon it the capacity to render\nfine-grained medical images with high fidelity. Comprehensive experiments\nvalidate MedXChat's synergistic enhancement across all tasks. Our instruction\ndata and model will be open-sourced.","terms":["cs.CV"]},{"titles":"Multimodality-guided Image Style Transfer using Cross-modal GAN Inversion","summaries":"Image Style Transfer (IST) is an interdisciplinary topic of computer vision\nand art that continuously attracts researchers' interests. Different from\ntraditional Image-guided Image Style Transfer (IIST) methods that require a\nstyle reference image as input to define the desired style, recent works start\nto tackle the problem in a text-guided manner, i.e., Text-guided Image Style\nTransfer (TIST). Compared to IIST, such approaches provide more flexibility\nwith text-specified styles, which are useful in scenarios where the style is\nhard to define with reference images. Unfortunately, many TIST approaches\nproduce undesirable artifacts in the transferred images. To address this issue,\nwe present a novel method to achieve much improved style transfer based on text\nguidance. Meanwhile, to offer more flexibility than IIST and TIST, our method\nallows style inputs from multiple sources and modalities, enabling\nMultiModality-guided Image Style Transfer (MMIST). Specifically, we realize\nMMIST with a novel cross-modal GAN inversion method, which generates style\nrepresentations consistent with specified styles. Such style representations\nfacilitate style transfer and in principle generalize any IIST methods to\nMMIST. Large-scale experiments and user studies demonstrate that our method\nachieves state-of-the-art performance on TIST task. Furthermore, comprehensive\nqualitative results confirm the effectiveness of our method on MMIST task and\ncross-modal style interpolation.","terms":["cs.CV"]},{"titles":"HumanNeRF-SE: A Simple yet Effective Approach to Animate HumanNeRF with Diverse Poses","summaries":"We present HumanNeRF-SE, which can synthesize diverse novel pose images with\nsimple input. Previous HumanNeRF studies require large neural networks to fit\nthe human appearance and prior knowledge. Subsequent methods build upon this\napproach with some improvements. Instead, we reconstruct this approach,\ncombining explicit and implicit human representations with both general and\nspecific mapping processes. Our key insight is that explicit shape can filter\nthe information used to fit implicit representation, and frozen general mapping\ncombined with point-specific mapping can effectively avoid overfitting and\nimprove pose generalization performance. Our explicit and implicit human\nrepresent combination architecture is extremely effective. This is reflected in\nour model's ability to synthesize images under arbitrary poses with few-shot\ninput and increase the speed of synthesizing images by 15 times through a\nreduction in computational complexity without using any existing acceleration\nmodules. Compared to the state-of-the-art HumanNeRF studies, HumanNeRF-SE\nachieves better performance with fewer learnable parameters and less training\ntime (see Figure 1).","terms":["cs.CV"]},{"titles":"Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training","summaries":"In this paper, we target the adaptive source driven 3D scene editing task by\nproposing a CustomNeRF model that unifies a text description or a reference\nimage as the editing prompt. However, obtaining desired editing results\nconformed with the editing prompt is nontrivial since there exist two\nsignificant challenges, including accurate editing of only foreground regions\nand multi-view consistency given a single-view reference image. To tackle the\nfirst challenge, we propose a Local-Global Iterative Editing (LGIE) training\nscheme that alternates between foreground region editing and full-image\nediting, aimed at foreground-only manipulation while preserving the background.\nFor the second challenge, we also design a class-guided regularization that\nexploits class priors within the generation model to alleviate the\ninconsistency problem among different views in image-driven editing. Extensive\nexperiments show that our CustomNeRF produces precise editing results under\nvarious real scenes for both text- and image-driven settings.","terms":["cs.CV","cs.AI"]},{"titles":"Function-constrained Program Synthesis","summaries":"This work introduces (1) a technique that allows large language models (LLMs)\nto leverage user-provided code when solving programming tasks and (2) a method\nto iteratively generate modular sub-functions that can aid future code\ngeneration attempts when the initial code generated by the LLM is inadequate.\nGenerating computer programs in general-purpose programming languages like\nPython poses a challenge for LLMs when instructed to use code provided in the\nprompt. Code-specific LLMs (e.g., GitHub Copilot, CodeLlama2) can generate code\ncompletions in real-time by drawing on all code available in a development\nenvironment. However, restricting code-specific LLMs to use only in-context\ncode is not straightforward, as the model is not explicitly instructed to use\nthe user-provided code and users cannot highlight precisely which snippets of\ncode the model should incorporate into its context. Moreover, current systems\nlack effective recovery methods, forcing users to iteratively re-prompt the\nmodel with modified prompts until a sufficient solution is reached. Our method\ndiffers from traditional LLM-powered code-generation by constraining\ncode-generation to an explicit function set and enabling recovery from failed\nattempts through automatically generated sub-functions. When the LLM cannot\nproduce working code, we generate modular sub-functions to aid subsequent\nattempts at generating functional code. A by-product of our method is a library\nof reusable sub-functions that can solve related tasks, imitating a software\nteam where efficiency scales with experience. We also introduce a new\n\"half-shot\" evaluation paradigm that provides tighter estimates of LLMs' coding\nabilities compared to traditional zero-shot evaluation. Our proposed evaluation\nmethod encourages models to output solutions in a structured format, decreasing\nsyntax errors that can be mistaken for poor coding ability.","terms":["cs.LG","cs.CL","cs.PL"]},{"titles":"RiskBench: A Scenario-based Benchmark for Risk Identification","summaries":"Intelligent driving systems aim to achieve a zero-collision mobility\nexperience, requiring interdisciplinary efforts to enhance safety performance.\nThis work focuses on risk identification, the process of identifying and\nanalyzing risks stemming from dynamic traffic participants and unexpected\nevents. While significant advances have been made in the community, the current\nevaluation of different risk identification algorithms uses independent\ndatasets, leading to difficulty in direct comparison and hindering collective\nprogress toward safety performance enhancement. To address this limitation, we\nintroduce \\textbf{RiskBench}, a large-scale scenario-based benchmark for risk\nidentification. We design a scenario taxonomy and augmentation pipeline to\nenable a systematic collection of ground truth risks under diverse scenarios.\nWe assess the ability of ten algorithms to (1) detect and locate risks, (2)\nanticipate risks, and (3) facilitate decision-making. We conduct extensive\nexperiments and summarize future research on risk identification. Our aim is to\nencourage collaborative endeavors in achieving a society with zero collisions.\nWe have made our dataset and benchmark toolkit publicly on the project page:\nhttps:\/\/hcis-lab.github.io\/RiskBench\/","terms":["cs.CV","cs.LG","cs.RO"]},{"titles":"AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix","summaries":"Adaptive optimizers, such as Adam, have achieved remarkable success in deep\nlearning. A key component of these optimizers is the so-called preconditioning\nmatrix, providing enhanced gradient information and regulating the step size of\neach gradient direction. In this paper, we propose a novel approach to\ndesigning the preconditioning matrix by utilizing the gradient difference\nbetween two successive steps as the diagonal elements. These diagonal elements\nare closely related to the Hessian and can be perceived as an approximation of\nthe inner product between the Hessian row vectors and difference of the\nadjacent parameter vectors. Additionally, we introduce an auto-switching\nfunction that enables the preconditioning matrix to switch dynamically between\nStochastic Gradient Descent (SGD) and the adaptive optimizer. Based on these\ntwo techniques, we develop a new optimizer named AGD that enhances the\ngeneralization performance. We evaluate AGD on public datasets of Natural\nLanguage Processing (NLP), Computer Vision (CV), and Recommendation Systems\n(RecSys). Our experimental results demonstrate that AGD outperforms the\nstate-of-the-art (SOTA) optimizers, achieving highly competitive or\nsignificantly better predictive performance. Furthermore, we analyze how AGD is\nable to switch automatically between SGD and the adaptive optimizer and its\nactual effects on various scenarios. The code is available at\nhttps:\/\/github.com\/intelligent-machine-learning\/dlrover\/tree\/master\/atorch\/atorch\/optimizers.","terms":["cs.LG","cs.DC","math.OC"]},{"titles":"On Tuning Neural ODE for Stability, Consistency and Faster Convergence","summaries":"Neural-ODE parameterize a differential equation using continuous depth neural\nnetwork and solve it using numerical ODE-integrator. These models offer a\nconstant memory cost compared to models with discrete sequence of hidden layers\nin which memory cost increases linearly with the number of layers. In addition\nto memory efficiency, other benefits of neural-ode include adaptability of\nevaluation approach to input, and flexibility to choose numerical precision or\nfast training. However, despite having all these benefits, it still has some\nlimitations. We identify the ODE-integrator (also called ODE-solver) as the\nweakest link in the chain as it may have stability, consistency and convergence\n(CCS) issues and may suffer from slower convergence or may not converge at all.\nWe propose a first-order Nesterov's accelerated gradient (NAG) based ODE-solver\nwhich is proven to be tuned vis-a-vis CCS conditions. We empirically\ndemonstrate the efficacy of our approach by training faster, while achieving\nbetter or comparable performance against neural-ode employing other fixed-step\nexplicit ODE-solvers as well discrete depth models such as ResNet in three\ndifferent tasks including supervised classification, density estimation, and\ntime-series modelling.","terms":["cs.LG","cs.AI","stat.ML"]},{"titles":"An End-to-End Network Pruning Pipeline with Sparsity Enforcement","summaries":"Neural networks have emerged as a powerful tool for solving complex tasks\nacross various domains, but their increasing size and computational\nrequirements have posed significant challenges in deploying them on\nresource-constrained devices. Neural network sparsification, and in particular\npruning, has emerged as an effective technique to alleviate these challenges by\nreducing model size, computational complexity, and memory footprint while\nmaintaining competitive performance. However, many pruning pipelines modify the\nstandard training pipeline at only a single stage, if at all. In this work, we\nlook to develop an end-to-end training pipeline that befits neural network\npruning and sparsification at all stages of training. To do so, we make use of\nnonstandard model parameter initialization, pre-pruning training methodologies,\nand post-pruning training optimizations. We conduct experiments utilizing\ncombinations of these methods, in addition to different techniques used in the\npruning step, and find that our combined pipeline can achieve significant gains\nover current state of the art approaches to neural network sparsification.","terms":["cs.LG"]},{"titles":"AnimateAnything: Fine-Grained Open Domain Image Animation with Motion Guidance","summaries":"Image animation is a key task in computer vision which aims to generate\ndynamic visual content from static image. Recent image animation methods employ\nneural based rendering technique to generate realistic animations. Despite\nthese advancements, achieving fine-grained and controllable image animation\nguided by text remains challenging, particularly for open-domain images\ncaptured in diverse real environments. In this paper, we introduce an open\ndomain image animation method that leverages the motion prior of video\ndiffusion model. Our approach introduces targeted motion area guidance and\nmotion strength guidance, enabling precise control the movable area and its\nmotion speed. This results in enhanced alignment between the animated visual\nelements and the prompting text, thereby facilitating a fine-grained and\ninteractive animation generation process for intricate motion sequences. We\nvalidate the effectiveness of our method through rigorous experiments on an\nopen-domain dataset, with the results showcasing its superior performance.\nProject page can be found at https:\/\/animationai.github.io\/AnimateAnything.","terms":["cs.CV"]},{"titles":"SequencePAR: Understanding Pedestrian Attributes via A Sequence Generation Paradigm","summaries":"Current pedestrian attribute recognition (PAR) algorithms are developed based\non multi-label or multi-task learning frameworks, which aim to discriminate the\nattributes using specific classification heads. However, these discriminative\nmodels are easily influenced by imbalanced data or noisy samples. Inspired by\nthe success of generative models, we rethink the pedestrian attribute\nrecognition scheme and believe the generative models may perform better on\nmodeling dependencies and complexity between human attributes. In this paper,\nwe propose a novel sequence generation paradigm for pedestrian attribute\nrecognition, termed SequencePAR. It extracts the pedestrian features using a\npre-trained CLIP model and embeds the attribute set into query tokens under the\nguidance of text prompts. Then, a Transformer decoder is proposed to generate\nthe human attributes by incorporating the visual features and attribute query\ntokens. The masked multi-head attention layer is introduced into the decoder\nmodule to prevent the model from remembering the next attribute while making\nattribute predictions during training. Extensive experiments on multiple widely\nused pedestrian attribute recognition datasets fully validated the\neffectiveness of our proposed SequencePAR. The source code and pre-trained\nmodels will be released at https:\/\/github.com\/Event-AHU\/OpenPAR.","terms":["cs.CV","cs.MM"]},{"titles":"Binary Radiance Fields","summaries":"In this paper, we propose \\textit{binary radiance fields} (BiRF), a\nstorage-efficient radiance field representation employing binary feature\nencoding that encodes local features using binary encoding parameters in a\nformat of either $+1$ or $-1$. This binarization strategy lets us represent the\nfeature grid with highly compact feature encoding and a dramatic reduction in\nstorage size. Furthermore, our 2D-3D hybrid feature grid design enhances the\ncompactness of feature encoding as the 3D grid includes main components while\n2D grids capture details. In our experiments, binary radiance field\nrepresentation successfully outperforms the reconstruction performance of\nstate-of-the-art (SOTA) efficient radiance field models with lower storage\nallocation. In particular, our model achieves impressive results in static\nscene reconstruction, with a PSNR of 32.03 dB for Synthetic-NeRF scenes, 34.48\ndB for Synthetic-NSVF scenes, 28.20 dB for Tanks and Temples scenes while only\nutilizing 0.5 MB of storage space, respectively. We hope the proposed binary\nradiance field representation will make radiance fields more accessible without\na storage bottleneck.","terms":["cs.CV"]},{"titles":"Robust Streaming, Sampling, and a Perspective on Online Learning","summaries":"In this work we present an overview of statistical learning, followed by a\nsurvey of robust streaming techniques and challenges, culminating in several\nrigorous results proving the relationship that we motivate and hint at\nthroughout the journey. Furthermore, we unify often disjoint theorems in a\nshared framework and notation to clarify the deep connections that are\ndiscovered. We hope that by approaching these results from a shared\nperspective, already aware of the technical connections that exist, we can\nenlighten the study of both fields and perhaps motivate new and previously\nunconsidered directions of research.","terms":["cs.LG","cs.GT","stat.ML"]},{"titles":"GaussianHead: Impressive 3D Gaussian-based Head Avatars with Dynamic Hybrid Neural Field","summaries":"Previous head avatar methods have mostly relied on fixed explicit primitives\n(mesh, point) or implicit surfaces (Sign Distance Function) and volumetric\nneural radiance field, it challenging to strike a balance among high fidelity,\ntraining speed, and resource consumption. The recent popularity of hybrid field\nhas brought novel representation, but is limited by relying on parameterization\nfactors obtained through fixed mappings. We propose GaussianHead: an head\navatar algorithm based on anisotropic 3D gaussian primitives. We leverage\ncanonical gaussians to represent dynamic scenes. Using explicit \"dynamic\"\ntri-plane as an efficient container for parameterized head geometry, aligned\nwell with factors in the underlying geometry and tri-plane, we obtain aligned\ncanonical factors for the canonical gaussians. With a tiny MLP, factors are\ndecoded into opacity and spherical harmonic coefficients of 3D gaussian\nprimitives. Finally, we use efficient differentiable gaussian rasterizer for\nrendering. Our approach benefits significantly from our novel representation\nbased on 3D gaussians, and the proper alignment transformation of underlying\ngeometry structures and factors in tri-plane eliminates biases introduced by\nfixed mappings. Compared to state-of-the-art techniques, we achieve optimal\nvisual results in tasks such as self-reconstruction, novel view synthesis, and\ncross-identity reenactment while maintaining high rendering efficiency (0.12s\nper frame). Even the pores around the nose are clearly visible in some cases.\nCode and additional video can be found on the project homepage.","terms":["cs.CV"]},{"titles":"CLAMP: Contrastive LAnguage Model Prompt-tuning","summaries":"Large language models (LLMs) have emerged as powerful general-purpose\ninterfaces for many machine learning problems. Recent work has adapted LLMs to\ngenerative visual tasks like image captioning, visual question answering, and\nvisual chat, using a relatively small amount of instruction-tuning data. In\nthis paper, we explore whether modern LLMs can also be adapted to classifying\nan image into a set of categories. First, we evaluate multimodal LLMs that are\ntuned for generative tasks on zero-shot image classification and find that\ntheir performance is far below that of specialized models like CLIP. We then\npropose an approach for light fine-tuning of LLMs using the same contrastive\nimage-caption matching objective as CLIP. Our results show that LLMs can,\nindeed, achieve good image classification performance when adapted this way.\nOur approach beats state-of-the-art mLLMs by 13% and slightly outperforms\ncontrastive learning with a custom text model, while also retaining the LLM's\ngenerative abilities. LLM initialization appears to particularly help\nclassification in domains under-represented in the visual pre-training data.","terms":["cs.CV"]},{"titles":"Learning Triangular Distribution in Visual World","summaries":"Convolution neural network is successful in pervasive vision tasks, including\nlabel distribution learning, which usually takes the form of learning an\ninjection from the non-linear visual features to the well-defined labels.\nHowever, how the discrepancy between features is mapped to the label\ndiscrepancy is ambient, and its correctness is not guaranteed. To address these\nproblems, we study the mathematical connection between feature and its label,\npresenting a general and simple framework for label distribution learning. We\npropose a so-called Triangular Distribution Transform (TDT) to build an\ninjective function between feature and label, guaranteeing that any symmetric\nfeature discrepancy linearly reflects the difference between labels. The\nproposed TDT can be used as a plug-in in mainstream backbone networks to\naddress different label distribution learning tasks. Experiments on Facial Age\nRecognition, Illumination Chromaticity Estimation, and Aesthetics assessment\nshow that TDT achieves on-par or better results than the prior arts.","terms":["cs.CV"]},{"titles":"GVFs in the Real World: Making Predictions Online for Water Treatment","summaries":"In this paper we investigate the use of reinforcement-learning based\nprediction approaches for a real drinking-water treatment plant. Developing\nsuch a prediction system is a critical step on the path to optimizing and\nautomating water treatment. Before that, there are many questions to answer\nabout the predictability of the data, suitable neural network architectures,\nhow to overcome partial observability and more. We first describe this dataset,\nand highlight challenges with seasonality, nonstationarity, partial\nobservability, and heterogeneity across sensors and operation modes of the\nplant. We then describe General Value Function (GVF) predictions -- discounted\ncumulative sums of observations -- and highlight why they might be preferable\nto classical n-step predictions common in time series prediction. We discuss\nhow to use offline data to appropriately pre-train our temporal difference\nlearning (TD) agents that learn these GVF predictions, including how to select\nhyperparameters for online fine-tuning in deployment. We find that the\nTD-prediction agent obtains an overall lower normalized mean-squared error than\nthe n-step prediction agent. Finally, we show the importance of learning in\ndeployment, by comparing a TD agent trained purely offline with no online\nupdating to a TD agent that learns online. This final result is one of the\nfirst to motivate the importance of adapting predictions in real-time, for\nnon-stationary high-volume systems in the real world.","terms":["cs.LG","cs.AI"]},{"titles":"Knowledge Diffusion for Distillation","summaries":"The representation gap between teacher and student is an emerging topic in\nknowledge distillation (KD). To reduce the gap and improve the performance,\ncurrent methods often resort to complicated training schemes, loss functions,\nand feature alignments, which are task-specific and feature-specific. In this\npaper, we state that the essence of these methods is to discard the noisy\ninformation and distill the valuable information in the feature, and propose a\nnovel KD method dubbed DiffKD, to explicitly denoise and match features using\ndiffusion models. Our approach is based on the observation that student\nfeatures typically contain more noises than teacher features due to the smaller\ncapacity of student model. To address this, we propose to denoise student\nfeatures using a diffusion model trained by teacher features. This allows us to\nperform better distillation between the refined clean feature and teacher\nfeature. Additionally, we introduce a light-weight diffusion model with a\nlinear autoencoder to reduce the computation cost and an adaptive noise\nmatching module to improve the denoising performance. Extensive experiments\ndemonstrate that DiffKD is effective across various types of features and\nachieves state-of-the-art performance consistently on image classification,\nobject detection, and semantic segmentation tasks. Code is available at\nhttps:\/\/github.com\/hunto\/DiffKD.","terms":["cs.CV","cs.AI"]},{"titles":"Efficient Multimodal Semantic Segmentation via Dual-Prompt Learning","summaries":"Multimodal (e.g., RGB-Depth\/RGB-Thermal) fusion has shown great potential for\nimproving semantic segmentation in complex scenes (e.g., indoor\/low-light\nconditions). Existing approaches often fully fine-tune a dual-branch\nencoder-decoder framework with a complicated feature fusion strategy for\nachieving multimodal semantic segmentation, which is training-costly due to the\nmassive parameter updates in feature extraction and fusion. To address this\nissue, we propose a surprisingly simple yet effective dual-prompt learning\nnetwork (dubbed DPLNet) for training-efficient multimodal (e.g., RGB-D\/T)\nsemantic segmentation. The core of DPLNet is to directly adapt a frozen\npre-trained RGB model to multimodal semantic segmentation, reducing parameter\nupdates. For this purpose, we present two prompt learning modules, comprising\nmultimodal prompt generator (MPG) and multimodal feature adapter (MFA). MPG\nworks to fuse the features from different modalities in a compact manner and is\ninserted from shadow to deep stages to generate the multi-level multimodal\nprompts that are injected into the frozen backbone, while MPG adapts prompted\nmultimodal features in the frozen backbone for better multimodal semantic\nsegmentation. Since both the MPG and MFA are lightweight, only a few trainable\nparameters (3.88M, 4.4% of the pre-trained backbone parameters) are introduced\nfor multimodal feature fusion and learning. Using a simple decoder (3.27M\nparameters), DPLNet achieves new state-of-the-art performance or is on a par\nwith other complex approaches on four RGB-D\/T semantic segmentation datasets\nwhile satisfying parameter efficiency. Moreover, we show that DPLNet is general\nand applicable to other multimodal tasks such as salient object detection and\nvideo semantic segmentation. Without special design, DPLNet outperforms many\ncomplicated models. Our code will be available at\ngithub.com\/ShaohuaDong2021\/DPLNet.","terms":["cs.CV"]},{"titles":"SchurVINS: Schur Complement-Based Lightweight Visual Inertial Navigation System","summaries":"Accuracy and computational efficiency are the most important metrics to\nVisual Inertial Navigation System (VINS). The existing VINS algorithms with\neither high accuracy or low computational complexity, are difficult to provide\nthe high precision localization in resource-constrained devices. To this end,\nwe propose a novel filter-based VINS framework named SchurVINS, which could\nguarantee both high accuracy by building a complete residual model and low\ncomputational complexity with Schur complement. Technically, we first formulate\nthe full residual model where Gradient, Hessian and observation covariance are\nexplicitly modeled. Then Schur complement is employed to decompose the full\nmodel into ego-motion residual model and landmark residual model. Finally,\nExtended Kalman Filter (EKF) update is implemented in these two models with\nhigh efficiency. Experiments on EuRoC and TUM-VI datasets show that our method\nnotably outperforms state-of-the-art (SOTA) methods in both accuracy and\ncomputational complexity. We will open source our experimental code to benefit\nthe community.","terms":["cs.CV","cs.RO"]},{"titles":"xNeuSM: Explainable Neural Subgraph Matching with Graph Learnable Multi-hop Attention Networks","summaries":"Subgraph matching is a challenging problem with a wide range of applications\nin database systems, biochemistry, and cognitive science. It involves\ndetermining whether a given query graph is present within a larger target\ngraph. Traditional graph-matching algorithms provide precise results but face\nchallenges in large graph instances due to the NP-complete problem, limiting\ntheir practical applicability. In contrast, recent neural network-based\napproximations offer more scalable solutions, but often lack interpretable node\ncorrespondences. To address these limitations, this article presents xNeuSM:\nExplainable Neural Subgraph Matching which introduces Graph Learnable Multi-hop\nAttention Networks (GLeMA) that adaptively learns the parameters governing the\nattention factor decay for each node across hops rather than relying on fixed\nhyperparameters. We provide a theoretical analysis establishing error bounds\nfor GLeMA's approximation of multi-hop attention as a function of the number of\nhops. Additionally, we prove that learning distinct attention decay factors for\neach node leads to a correct approximation of multi-hop attention. Empirical\nevaluation on real-world datasets shows that xNeuSM achieves substantial\nimprovements in prediction accuracy of up to 34% compared to approximate\nbaselines and, notably, at least a seven-fold faster query time than exact\nalgorithms. The source code of our implementation is available at\nhttps:\/\/github.com\/martinakaduc\/xNeuSM.","terms":["cs.LG","cs.AI"]},{"titles":"Graph Generation with $K^2$-trees","summaries":"Generating graphs from a target distribution is a significant challenge\nacross many domains, including drug discovery and social network analysis. In\nthis work, we introduce a novel graph generation method leveraging $K^2$-tree\nrepresentation, originally designed for lossless graph compression. The\n$K^2$-tree representation {encompasses inherent hierarchy while enabling\ncompact graph generation}. In addition, we make contributions by (1) presenting\na sequential $K^2$-treerepresentation that incorporates pruning, flattening,\nand tokenization processes and (2) introducing a Transformer-based architecture\ndesigned to generate the sequence by incorporating a specialized tree\npositional encoding scheme. Finally, we extensively evaluate our algorithm on\nfour general and two molecular graph datasets to confirm its superiority for\ngraph generation.","terms":["cs.LG","cs.AI","cs.SI"]},{"titles":"A Simple and Scalable Representation for Graph Generation","summaries":"Recently, there has been a surge of interest in employing neural networks for\ngraph generation, a fundamental statistical learning problem with critical\napplications like molecule design and community analysis. However, most\napproaches encounter significant limitations when generating large-scale\ngraphs. This is due to their requirement to output the full adjacency matrices\nwhose size grows quadratically with the number of nodes. In response to this\nchallenge, we introduce a new, simple, and scalable graph representation named\ngap encoded edge list (GEEL) that has a small representation size that aligns\nwith the number of edges. In addition, GEEL significantly reduces the\nvocabulary size by incorporating the gap encoding and bandwidth restriction\nschemes. GEEL can be autoregressively generated with the incorporation of node\npositional encoding, and we further extend GEEL to deal with attributed graphs\nby designing a new grammar. Our findings reveal that the adoption of this\ncompact representation not only enhances scalability but also bolsters\nperformance by simplifying the graph generation process. We conduct a\ncomprehensive evaluation across ten non-attributed and two molecular graph\ngeneration tasks, demonstrating the effectiveness of GEEL.","terms":["cs.LG","cs.AI"]},{"titles":"TextAug: Test time Text Augmentation for Multimodal Person Re-identification","summaries":"Multimodal Person Reidentification is gaining popularity in the research\ncommunity due to its effectiveness compared to counter-part unimodal\nframeworks. However, the bottleneck for multimodal deep learning is the need\nfor a large volume of multimodal training examples. Data augmentation\ntechniques such as cropping, flipping, rotation, etc. are often employed in the\nimage domain to improve the generalization of deep learning models. Augmenting\nin other modalities than images, such as text, is challenging and requires\nsignificant computational resources and external data sources. In this study,\nwe investigate the effectiveness of two computer vision data augmentation\ntechniques: cutout and cutmix, for text augmentation in multi-modal person\nre-identification. Our approach merges these two augmentation strategies into\none strategy called CutMixOut which involves randomly removing words or\nsub-phrases from a sentence (Cutout) and blending parts of two or more\nsentences to create diverse examples (CutMix) with a certain probability\nassigned to each operation. This augmentation was implemented at inference time\nwithout any prior training. Our results demonstrate that the proposed technique\nis simple and effective in improving the performance on multiple multimodal\nperson re-identification benchmarks.","terms":["cs.CV","cs.LG"]},{"titles":"Simple Transferability Estimation for Regression Tasks","summaries":"We consider transferability estimation, the problem of estimating how well\ndeep learning models transfer from a source to a target task. We focus on\nregression tasks, which received little previous attention, and propose two\nsimple and computationally efficient approaches that estimate transferability\nbased on the negative regularized mean squared error of a linear regression\nmodel. We prove novel theoretical results connecting our approaches to the\nactual transferability of the optimal target models obtained from the transfer\nlearning process. Despite their simplicity, our approaches significantly\noutperform existing state-of-the-art regression transferability estimators in\nboth accuracy and efficiency. On two large-scale keypoint regression\nbenchmarks, our approaches yield 12% to 36% better results on average while\nbeing at least 27% faster than previous state-of-the-art methods.","terms":["cs.LG","cs.AI","stat.ML"]},{"titles":"Good Questions Help Zero-Shot Image Reasoning","summaries":"Aligning the recent large language models (LLMs) with computer vision models\nleads to large vision-language models (LVLMs), which have paved the way for\nzero-shot image reasoning tasks. However, LVLMs are usually trained on short\nhigh-level captions only referring to sparse focus regions in images. Such a\n``tunnel vision'' limits LVLMs to exploring other relevant contexts in complex\nscenes. To address this challenge, we introduce Question-Driven Visual\nExploration (QVix), a novel prompting strategy that enhances the exploratory\ncapabilities of LVLMs in zero-shot reasoning tasks. QVix leverages LLMs' strong\nlanguage prior to generate input-exploratory questions with more details than\nthe original query, guiding LVLMs to explore visual content more\ncomprehensively and uncover subtle or peripheral details. QVix enables a wider\nexploration of visual scenes, improving the LVLMs' reasoning accuracy and depth\nin tasks such as visual question answering and visual entailment. Our\nevaluations on various challenging zero-shot vision-language benchmarks,\nincluding ScienceQA and fine-grained visual classification, demonstrate that\nQVix significantly outperforms existing methods, highlighting its effectiveness\nin bridging the gap between complex visual data and LVLMs' exploratory\nabilities.","terms":["cs.CV"]},{"titles":"SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference","summaries":"Recent advances in contrastive language-image pretraining (CLIP) have\ndemonstrated strong capabilities in zero-shot classification by aligning visual\nrepresentations with target text embeddings in an image level. However, in\ndense prediction tasks, CLIP often struggles to localize visual features within\nan image and fails to give accurate pixel-level predictions, which prevents it\nfrom functioning as a generalized visual foundation model. In this work, we aim\nto enhance CLIP's potential for semantic segmentation with minimal\nmodifications to its pretrained models. By rethinking self-attention, we\nsurprisingly find that CLIP can adapt to dense prediction tasks by simply\nintroducing a novel Correlative Self-Attention (CSA) mechanism. Specifically,\nwe replace the traditional self-attention block of CLIP vision encoder's last\nlayer by our CSA module and reuse its pretrained projection matrices of query,\nkey, and value, leading to a training-free adaptation approach for CLIP's\nzero-shot semantic segmentation. Extensive experiments show the advantage of\nCSA: we obtain a 38.2% average zero-shot mIoU across eight semantic\nsegmentation benchmarks highlighted in this paper, significantly outperforming\nthe existing SoTA's 33.9% and the vanilla CLIP's 14.1%.","terms":["cs.CV"]},{"titles":"Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation","summaries":"Domain Adaptation (DA) is always challenged by the spurious correlation\nbetween domain-invariant features (e.g., class identity) and domain-specific\nfeatures (e.g., environment) that does not generalize to the target domain.\nUnfortunately, even enriched with additional unsupervised target domains,\nexisting Unsupervised DA (UDA) methods still suffer from it. This is because\nthe source domain supervision only considers the target domain samples as\nauxiliary data (e.g., by pseudo-labeling), yet the inherent distribution in the\ntarget domain -- where the valuable de-correlation clues hide -- is\ndisregarded. We propose to make the U in UDA matter by giving equal status to\nthe two domains. Specifically, we learn an invariant classifier whose\nprediction is simultaneously consistent with the labels in the source domain\nand clusters in the target domain, hence the spurious correlation inconsistent\nin the target domain is removed. We dub our approach \"Invariant CONsistency\nlearning\" (ICON). Extensive experiments show that ICON achieves the\nstate-of-the-art performance on the classic UDA benchmarks: Office-Home and\nVisDA-2017, and outperforms all the conventional methods on the challenging\nWILDS 2.0 benchmark. Codes are in https:\/\/github.com\/yue-zhongqi\/ICON.","terms":["cs.LG"]},{"titles":"PixelLM: Pixel Reasoning with Large Multimodal Model","summaries":"While large multimodal models (LMMs) have achieved remarkable progress,\ngenerating pixel-level masks for image reasoning tasks involving multiple\nopen-world targets remains a challenge. To bridge this gap, we introduce\nPixelLM, an effective and efficient LMM for pixel-level reasoning and\nunderstanding. Central to PixelLM is a novel, lightweight pixel decoder and a\ncomprehensive segmentation codebook. The decoder efficiently produces masks\nfrom the hidden embeddings of the codebook tokens, which encode detailed\ntarget-relevant information. With this design, PixelLM harmonizes with the\nstructure of popular LMMs and avoids the need for additional costly\nsegmentation models. Furthermore, we propose a target refinement loss to\nenhance the model's ability to differentiate between multiple targets, leading\nto substantially improved mask quality. To advance research in this area, we\nconstruct MUSE, a high-quality multi-target reasoning segmentation benchmark.\nPixelLM excels across various pixel-level image reasoning and understanding\ntasks, outperforming well-established methods in multiple benchmarks, including\nMUSE, single- and multi-referring segmentation. Comprehensive ablations confirm\nthe efficacy of each proposed component. All code, models, and datasets will be\npublicly available.","terms":["cs.CV"]},{"titles":"Open-Set Object Detection Using Classification-free Object Proposal and Instance-level Contrastive Learning","summaries":"Detecting both known and unknown objects is a fundamental skill for robot\nmanipulation in unstructured environments. Open-set object detection (OSOD) is\na promising direction to handle the problem consisting of two subtasks: objects\nand background separation, and open-set object classification. In this paper,\nwe present Openset RCNN to address the challenging OSOD. To disambiguate\nunknown objects and background in the first subtask, we propose to use\nclassification-free region proposal network (CF-RPN) which estimates the\nobjectness score of each region purely using cues from object's location and\nshape preventing overfitting to the training categories. To identify unknown\nobjects in the second subtask, we propose to represent them using the\ncomplementary region of known categories in a latent space which is\naccomplished by a prototype learning network (PLN). PLN performs instance-level\ncontrastive learning to encode proposals to a latent space and builds a compact\nregion centering with a prototype for each known category. Further, we note\nthat the detection performance of unknown objects can not be unbiasedly\nevaluated on the situation that commonly used object detection datasets are not\nfully annotated. Thus, a new benchmark is introduced by reorganizing\nGraspNet-1billion, a robotic grasp pose detection dataset with complete\nannotation. Extensive experiments demonstrate the merits of our method. We\nfinally show that our Openset RCNN can endow the robot with an open-set\nperception ability to support robotic rearrangement tasks in cluttered\nenvironments. More details can be found in\nhttps:\/\/sites.google.com\/view\/openset-rcnn\/","terms":["cs.CV","cs.RO"]},{"titles":"Improving Multimodal Sentiment Analysis: Supervised Angular Margin-based Contrastive Learning for Enhanced Fusion Representation","summaries":"The effectiveness of a model is heavily reliant on the quality of the fusion\nrepresentation of multiple modalities in multimodal sentiment analysis.\nMoreover, each modality is extracted from raw input and integrated with the\nrest to construct a multimodal representation. Although previous methods have\nproposed multimodal representations and achieved promising results, most of\nthem focus on forming positive and negative pairs, neglecting the variation in\nsentiment scores within the same class. Additionally, they fail to capture the\nsignificance of unimodal representations in the fusion vector. To address these\nlimitations, we introduce a framework called Supervised Angular-based\nContrastive Learning for Multimodal Sentiment Analysis. This framework aims to\nenhance discrimination and generalizability of the multimodal representation\nand overcome biases in the fusion vector's modality. Our experimental results,\nalong with visualizations on two widely used datasets, demonstrate the\neffectiveness of our approach.","terms":["cs.LG","cs.CL"]},{"titles":"OCGEC: One-class Graph Embedding Classification for DNN Backdoor Detection","summaries":"Deep neural networks (DNNs) have been found vulnerable to backdoor attacks,\nraising security concerns about their deployment in mission-critical\napplications. There are various approaches to detect backdoor attacks, however\nthey all make certain assumptions about the target attack to be detected and\nrequire equal and huge numbers of clean and backdoor samples for training,\nwhich renders these detection methods quite limiting in real-world\ncircumstances.\n  This study proposes a novel one-class classification framework called\nOne-class Graph Embedding Classification (OCGEC) that uses GNNs for model-level\nbackdoor detection with only a little amount of clean data. First, we train\nthousands of tiny models as raw datasets from a small number of clean datasets.\nFollowing that, we design a ingenious model-to-graph method for converting the\nmodel's structural details and weight features into graph data. We then\npre-train a generative self-supervised graph autoencoder (GAE) to better learn\nthe features of benign models in order to detect backdoor models without\nknowing the attack strategy. After that, we dynamically combine the GAE and\none-class classifier optimization goals to form classification boundaries that\ndistinguish backdoor models from benign models.\n  Our OCGEC combines the powerful representation capabilities of graph neural\nnetworks with the utility of one-class classification techniques in the field\nof anomaly detection. In comparison to other baselines, it achieves AUC scores\nof more than 98% on a number of tasks, which far exceeds existing methods for\ndetection even when they rely on a huge number of positive and negative\nsamples. Our pioneering application of graphic scenarios for generic backdoor\ndetection can provide new insights that can be used to improve other backdoor\ndefense tasks. Code is available at https:\/\/github.com\/jhy549\/OCGEC.","terms":["cs.LG","cs.AI","cs.CR"]},{"titles":"Signed Binarization: Unlocking Efficiency Through Repetition-Sparsity Trade-Off","summaries":"Efficient inference of Deep Neural Networks (DNNs) on resource-constrained\nedge devices is essential. Quantization and sparsity are key algorithmic\ntechniques that translate to repetition and sparsity within tensors at the\nhardware-software interface. This paper introduces the concept of\nrepetition-sparsity trade-off that helps explain computational efficiency\nduring inference. We propose Signed Binarization, a unified co-design framework\nthat synergistically integrates hardware-software systems, quantization\nfunctions, and representation learning techniques to address this trade-off.\nOur results demonstrate that Signed Binarization is more accurate than\nbinarization with the same number of non-zero weights. Detailed analysis\nindicates that signed binarization generates a smaller distribution of\neffectual (non-zero) parameters nested within a larger distribution of total\nparameters, both of the same type, for a DNN block. Finally, our approach\nachieves a 26% speedup on real hardware, doubles energy efficiency, and reduces\ndensity by 2.8x compared to binary methods for ResNet 18, presenting an\nalternative solution for deploying efficient models in resource-limited\nenvironments.","terms":["cs.LG","cs.AI","cs.CV"]},{"titles":"Generating Action-conditioned Prompts for Open-vocabulary Video Action Recognition","summaries":"Exploring open-vocabulary video action recognition is a promising venture,\nwhich aims to recognize previously unseen actions within any arbitrary set of\ncategories. Existing methods typically adapt pretrained image-text models to\nthe video domain, capitalizing on their inherent strengths in generalization. A\ncommon thread among such methods is the augmentation of visual embeddings with\ntemporal information to improve the recognition of seen actions. Yet, they\ncompromise with standard less-informative action descriptions, thus faltering\nwhen confronted with novel actions. Drawing inspiration from human cognitive\nprocesses, we argue that augmenting text embeddings with human prior knowledge\nis pivotal for open-vocabulary video action recognition. To realize this, we\ninnovatively blend video models with Large Language Models (LLMs) to devise\nAction-conditioned Prompts. Specifically, we harness the knowledge in LLMs to\nproduce a set of descriptive sentences that contain distinctive features for\nidentifying given actions. Building upon this foundation, we further introduce\na multi-modal action knowledge alignment mechanism to align concepts in video\nand textual knowledge encapsulated within the prompts. Extensive experiments on\nvarious video benchmarks, including zero-shot, few-shot, and base-to-novel\ngeneralization settings, demonstrate that our method not only sets new SOTA\nperformance but also possesses excellent interpretability.","terms":["cs.CV"]},{"titles":"Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework for Online RL","summaries":"While policy optimization algorithms have played an important role in recent\nempirical success of Reinforcement Learning (RL), the existing theoretical\nunderstanding of policy optimization remains rather limited -- they are either\nrestricted to tabular MDPs or suffer from highly suboptimal sample complexity,\nespecial in online RL where exploration is necessary. This paper proposes a\nsimple efficient policy optimization framework -- Optimistic NPG for online RL.\nOptimistic NPG can be viewed as a simple combination of the classic natural\npolicy gradient (NPG) algorithm [Kakade, 2001] with optimistic policy\nevaluation subroutines to encourage exploration. For $d$-dimensional linear\nMDPs, Optimistic NPG is computationally efficient, and learns an\n$\\varepsilon$-optimal policy within $\\tilde{O}(d^2\/\\varepsilon^3)$ samples,\nwhich is the first computationally efficient algorithm whose sample complexity\nhas the optimal dimension dependence $\\tilde{\\Theta}(d^2)$. It also improves\nover state-of-the-art results of policy optimization algorithms [Zanette et\nal., 2021] by a factor of $d$. In the realm of general function approximation,\nwhich subsumes linear MDPs, Optimistic NPG, to our best knowledge, stands as\nthe first policy optimization algorithm that achieves polynomial sample\ncomplexity for learning near-optimal policies.","terms":["cs.LG","stat.ML"]},{"titles":"RJHMC-Tree for Exploration of the Bayesian Decision Tree Posterior","summaries":"Decision trees have found widespread application within the machine learning\ncommunity due to their flexibility and interpretability. This paper is directed\ntowards learning decision trees from data using a Bayesian approach, which is\nchallenging due to the potentially enormous parameter space required to span\nall tree models. Several approaches have been proposed to combat this\nchallenge, with one of the more successful being Markov chain Monte Carlo\n(MCMC) methods. The efficacy and efficiency of MCMC methods fundamentally rely\non the quality of the so-called proposals, which is the focus of this paper. In\nparticular, this paper investigates using a Hamiltonian Monte Carlo (HMC)\napproach to explore the posterior of Bayesian decision trees more efficiently\nby exploiting the geometry of the likelihood within a global update scheme. Two\nimplementations of the novel algorithm are developed and compared to existing\nmethods by testing against standard datasets in the machine learning and\nBayesian decision tree literature. HMC-based methods are shown to perform\nfavourably with respect to predictive test accuracy, acceptance rate, and tree\ncomplexity.","terms":["cs.LG","stat.CO","stat.ML"]},{"titles":"Learning Efficient Unsupervised Satellite Image-based Building Damage Detection","summaries":"Existing Building Damage Detection (BDD) methods always require\nlabour-intensive pixel-level annotations of buildings and their conditions,\nhence largely limiting their applications. In this paper, we investigate a\nchallenging yet practical scenario of BDD, Unsupervised Building Damage\nDetection (U-BDD), where only unlabelled pre- and post-disaster satellite image\npairs are provided. As a pilot study, we have first proposed an advanced U-BDD\nbaseline that leverages pre-trained vision-language foundation models (i.e.,\nGrounding DINO, SAM and CLIP) to address the U-BDD task. However, the apparent\ndomain gap between satellite and generic images causes low confidence in the\nfoundation models used to identify buildings and their damages. In response, we\nfurther present a novel self-supervised framework, U-BDD++, which improves upon\nthe U-BDD baseline by addressing domain-specific issues associated with\nsatellite imagery. Furthermore, the new Building Proposal Generation (BPG)\nmodule and the CLIP-enabled noisy Building Proposal Selection (CLIP-BPS) module\nin U-BDD++ ensure high-quality self-training. Extensive experiments on the\nwidely used building damage assessment benchmark demonstrate the effectiveness\nof the proposed method for unsupervised building damage detection. The\npresented annotation-free and foundation model-based paradigm ensures an\nefficient learning phase. This study opens a new direction for real-world BDD\nand sets a strong baseline for future research.","terms":["cs.CV","cs.MM"]},{"titles":"Universality and approximation bounds for echo state networks with random weights","summaries":"We study the uniform approximation of echo state networks with randomly\ngenerated internal weights. These models, in which only the readout weights are\noptimized during training, have made empirical success in learning dynamical\nsystems. Recent results showed that echo state networks with ReLU activation\nare universal. In this paper, we give an alternative construction and prove\nthat the universality holds for general activation functions. Specifically, our\nmain result shows that, under certain condition on the activation function,\nthere exists a sampling procedure for the internal weights so that the echo\nstate network can approximate any continuous casual time-invariant operators\nwith high probability. In particular, for ReLU activation, we give explicit\nconstruction for these sampling procedures. We also quantify the approximation\nerror of the constructed ReLU echo state networks for sufficiently regular\noperators.","terms":["cs.LG","cs.NA","cs.NE","math.NA","stat.ML"]},{"titles":"How to Configure Good In-Context Sequence for Visual Question Answering","summaries":"Inspired by the success of Large Language Models in dealing with new tasks\nvia In-Context Learning (ICL) in NLP, researchers have also developed Large\nVision-Language Models (LVLMs) with ICL capabilities. However, when\nimplementing ICL using these LVLMs, researchers usually resort to the simplest\nway like random sampling to configure the in-context sequence, thus leading to\nsub-optimal results. To enhance the ICL performance, in this study, we use\nVisual Question Answering (VQA) as case study to explore diverse in-context\nconfigurations to find the powerful ones. Additionally, through observing the\nchanges of the LVLM outputs by altering the in-context sequence, we gain\ninsights into the inner properties of LVLMs, improving our understanding of\nthem. Specifically, to explore in-context configurations, we design diverse\nretrieval methods and employ different strategies to manipulate the retrieved\ndemonstrations. Through exhaustive experiments on three VQA datasets: VQAv2,\nVizWiz, and OK-VQA, we uncover three important inner properties of the applied\nLVLM and demonstrate which strategies can consistently improve the ICL VQA\nperformance. Our code is provided in:\nhttps:\/\/github.com\/GaryJiajia\/OFv2_ICL_VQA.","terms":["cs.CV","cs.AI"]},{"titles":"Deep Unlearning: Fast and Efficient Training-free Approach to Controlled Forgetting","summaries":"Machine unlearning has emerged as a prominent and challenging area of\ninterest, driven in large part by the rising regulatory demands for industries\nto delete user data upon request and the heightened awareness of privacy.\nExisting approaches either retrain models from scratch or use several\nfinetuning steps for every deletion request, often constrained by computational\nresource limitations and restricted access to the original training data. In\nthis work, we introduce a novel class unlearning algorithm designed to\nstrategically eliminate an entire class or a group of classes from the learned\nmodel. To that end, our algorithm first estimates the Retain Space and the\nForget Space, representing the feature or activation spaces for samples from\nclasses to be retained and unlearned, respectively. To obtain these spaces, we\npropose a novel singular value decomposition-based technique that requires\nlayer wise collection of network activations from a few forward passes through\nthe network. We then compute the shared information between these spaces and\nremove it from the forget space to isolate class-discriminatory feature space\nfor unlearning. Finally, we project the model weights in the orthogonal\ndirection of the class-discriminatory space to obtain the unlearned model. We\ndemonstrate our algorithm's efficacy on ImageNet using a Vision Transformer\nwith only $\\sim$1.5% drop in retain accuracy compared to the original model\nwhile maintaining under 1% accuracy on the unlearned class samples. Further,\nour algorithm consistently performs well when subject to Membership Inference\nAttacks showing 7.8% improvement on average across a variety of image\nclassification datasets and network architectures, as compared to other\nbaselines while being $\\sim$6x more computationally efficient.","terms":["cs.LG","cs.AI","cs.CV","stat.ML"]},{"titles":"Toward Automated Quantum Variational Machine Learning","summaries":"In this work, we address the problem of automating quantum variational\nmachine learning. We develop a multi-locality parallelizable search algorithm,\ncalled MUSE, to find the initial points and the sets of parameters that achieve\nthe best performance for quantum variational circuit learning. Simulations with\nfive real-world classification datasets indicate that on average, MUSE improves\nthe detection accuracy of quantum variational classifiers 2.3 times with\nrespect to the observed lowest scores. Moreover, when applied to two real-world\nregression datasets, MUSE improves the quality of the predictions from negative\ncoefficients of determination to positive ones. Furthermore, the classification\nand regression scores of the quantum variational models trained with MUSE are\non par with the classical counterparts.","terms":["cs.LG","cs.ET","quant-ph"]},{"titles":"APoLLo: Unified Adapter and Prompt Learning for Vision Language Models","summaries":"The choice of input text prompt plays a critical role in the performance of\nVision-Language Pretrained (VLP) models such as CLIP. We present APoLLo, a\nunified multi-modal approach that combines Adapter and Prompt learning for\nVision-Language models. Our method is designed to substantially improve the\ngeneralization capabilities of VLP models when they are fine-tuned in a\nfew-shot setting. We introduce trainable cross-attention-based adapter layers\nin conjunction with vision and language encoders to strengthen the alignment\nbetween the two modalities. We enforce consistency between the respective\nencoder branches (receiving augmented inputs) to prevent overfitting in\ndownstream tasks. Our method is evaluated on three representative tasks:\ngeneralization to novel classes, cross-dataset evaluation, and unseen domain\nshifts. In practice, APoLLo achieves a relative gain up to 6.03% over MaPLe\n(SOTA) on novel classes for 10 diverse image recognition datasets.","terms":["cs.LG","cs.AI","cs.CL","cs.CV"]},{"titles":"Coneheads: Hierarchy Aware Attention","summaries":"Attention networks such as transformers have achieved state-of-the-art\nperformance in many domains. These networks rely heavily on the dot product\nattention operator, which computes the similarity between two points by taking\ntheir inner product. However, the inner product does not explicitly model the\ncomplex structural properties of real world datasets, such as hierarchies\nbetween data points. To remedy this, we introduce cone attention, a drop-in\nreplacement for dot product attention based on hyperbolic entailment cones.\nCone attention associates two points by the depth of their lowest common\nancestor in a hierarchy defined by hyperbolic cones, which intuitively measures\nthe divergence of two points and gives a hierarchy aware similarity score. We\ntest cone attention on a wide variety of models and tasks and show that it\nimproves task-level performance over dot product attention and other baselines,\nand is able to match dot-product attention with significantly fewer parameters.\nOur results suggest that cone attention is an effective way to capture\nhierarchical relationships when calculating attention.","terms":["cs.LG"]},{"titles":"On the Foundation of Distributionally Robust Reinforcement Learning","summaries":"Motivated by the need for a robust policy in the face of environment shifts\nbetween training and the deployment, we contribute to the theoretical\nfoundation of distributionally robust reinforcement learning (DRRL). This is\naccomplished through a comprehensive modeling framework centered around\ndistributionally robust Markov decision processes (DRMDPs). This framework\nobliges the decision maker to choose an optimal policy under the worst-case\ndistributional shift orchestrated by an adversary. By unifying and extending\nexisting formulations, we rigorously construct DRMDPs that embraces various\nmodeling attributes for both the decision maker and the adversary. These\nattributes include adaptability granularity, exploring history-dependent,\nMarkov, and Markov time-homogeneous decision maker and adversary dynamics.\nAdditionally, we delve into the flexibility of shifts induced by the adversary,\nexamining SA and S-rectangularity. Within this DRMDP framework, we investigate\nconditions for the existence or absence of the dynamic programming principle\n(DPP). From an algorithmic standpoint, the existence of DPP holds significant\nimplications, as the vast majority of existing data and computationally\nefficiency RL algorithms are reliant on the DPP. To study its existence, we\ncomprehensively examine combinations of controller and adversary attributes,\nproviding streamlined proofs grounded in a unified methodology. We also offer\ncounterexamples for settings in which a DPP with full generality is absent.","terms":["cs.LG","cs.SY","eess.SY","math.OC","stat.ML"]},{"titles":"Multi-View Person Matching and 3D Pose Estimation with Arbitrary Uncalibrated Camera Networks","summaries":"Cross-view person matching and 3D human pose estimation in multi-camera\nnetworks are particularly difficult when the cameras are extrinsically\nuncalibrated. Existing efforts generally require large amounts of 3D data for\ntraining neural networks or known camera poses for geometric constraints to\nsolve the problem. However, camera poses and 3D data annotation are usually\nexpensive and not always available. We present a method, PME, that solves the\ntwo tasks without requiring either information. Our idea is to address\ncross-view person matching as a clustering problem using each person as a\ncluster center, then obtain correspondences from person matches, and estimate\n3D human poses through multi-view triangulation and bundle adjustment. We solve\nthe clustering problem by introducing a \"size constraint\" using the number of\ncameras and a \"source constraint\" using the fact that two people from the same\ncamera view should not match, to narrow the solution space to a small feasible\nregion. The 2D human poses used in clustering are obtained through a\npre-trained 2D pose detector, so our method does not require expensive 3D\ntraining data for each new scene. We extensively evaluate our method on three\nopen datasets and two indoor and outdoor datasets collected using arbitrarily\nset cameras. Our method outperforms other methods by a large margin on\ncross-view person matching, reaches SOTA performance on 3D human pose\nestimation without using either camera poses or 3D training data, and shows\ngood generalization ability across five datasets of various environment\nsettings.","terms":["cs.CV"]},{"titles":"Efficient and Effective Deep Multi-view Subspace Clustering","summaries":"Recent multi-view subspace clustering achieves impressive results utilizing\ndeep networks, where the self-expressive correlation is typically modeled by a\nfully connected (FC) layer. However, they still suffer from two limitations. i)\nThe parameter scale of the FC layer is quadratic to sample numbers, resulting\nin high time and memory costs that significantly degrade their feasibility in\nlarge-scale datasets. ii) It is under-explored to extract a unified\nrepresentation that simultaneously satisfies minimal sufficiency and\ndiscriminability. To this end, we propose a novel deep framework, termed\nEfficient and Effective deep Multi-View Subspace Clustering (E$^2$MVSC).\nInstead of a parameterized FC layer, we design a Relation-Metric Net that\ndecouples network parameter scale from sample numbers for greater computational\nefficiency. Most importantly, the proposed method devises a multi-type\nauto-encoder to explicitly decouple consistent, complementary, and superfluous\ninformation from every view, which is supervised by a soft clustering\nassignment similarity constraint. Following information bottleneck theory and\nthe maximal coding rate reduction principle, a sufficient yet minimal unified\nrepresentation can be obtained, as well as pursuing intra-cluster aggregation\nand inter-cluster separability within it. Extensive experiments show that\nE$^2$MVSC yields comparable results to existing methods and achieves\nstate-of-the-art performance in various types of multi-view datasets.","terms":["cs.CV","cs.LG"]},{"titles":"Hyperspectral Image Compression Using Sampling and Implicit Neural Representations","summaries":"Hyperspectral images, which record the electromagnetic spectrum for a pixel\nin the image of a scene, often store hundreds of channels per pixel and contain\nan order of magnitude more information than a similarly-sized RBG color image.\nConsequently, concomitant with the decreasing cost of capturing these images,\nthere is a need to develop efficient techniques for storing, transmitting, and\nanalyzing hyperspectral images. This paper develops a method for hyperspectral\nimage compression using implicit neural representations where a multilayer\nperceptron network F with sinusoidal activation functions \"learns\" to map pixel\nlocations to pixel intensities for a given hyperspectral image I. F thus acts\nas a compressed encoding of this image, and the original image is reconstructed\nby evaluating F at each pixel location. We use a sampling method with two\nfactors: window size and sampling rate to reduce the compression time. We have\nevaluated our method on four benchmarks -- Indian Pines, Jasper Ridge, Pavia\nUniversity, and Cuprite using PSNR and SSIM -- and we show that the proposed\nmethod achieves better compression than JPEG, JPEG2000, and PCA-DCT at low\nbitrates. Besides, we compare our results with the learning-based methods like\nPCA+JPEG2000, FPCA+JPEG2000, 3D DCT, 3D DWT+SVR, and WSRC and show the\ncorresponding results in the \"Compression Results\" section. We also show that\nour methods with sampling achieve better speed and performance than our method\nwithout sampling.","terms":["cs.CV","eess.IV"]},{"titles":"InsPLAD: A Dataset and Benchmark for Power Line Asset Inspection in UAV Images","summaries":"Power line maintenance and inspection are essential to avoid power supply\ninterruptions, reducing its high social and financial impacts yearly.\nAutomating power line visual inspections remains a relevant open problem for\nthe industry due to the lack of public real-world datasets of power line\ncomponents and their various defects to foster new research. This paper\nintroduces InsPLAD, a Power Line Asset Inspection Dataset and Benchmark\ncontaining 10,607 high-resolution Unmanned Aerial Vehicles colour images. The\ndataset contains seventeen unique power line assets captured from real-world\noperating power lines. Additionally, five of those assets present six defects:\nfour of which are corrosion, one is a broken component, and one is a bird's\nnest presence. All assets were labelled according to their condition, whether\nnormal or the defect name found on an image level. We thoroughly evaluate\nstate-of-the-art and popular methods for three image-level computer vision\ntasks covered by InsPLAD: object detection, through the AP metric; defect\nclassification, through Balanced Accuracy; and anomaly detection, through the\nAUROC metric. InsPLAD offers various vision challenges from uncontrolled\nenvironments, such as multi-scale objects, multi-size class instances, multiple\nobjects per image, intra-class variation, cluttered background, distinct\npoint-of-views, perspective distortion, occlusion, and varied lighting\nconditions. To the best of our knowledge, InsPLAD is the first large real-world\ndataset and benchmark for power line asset inspection with multiple components\nand defects for various computer vision tasks, with a potential impact to\nimprove state-of-the-art methods in the field. It will be publicly available in\nits integrity on a repository with a thorough description. It can be found at\nhttps:\/\/github.com\/andreluizbvs\/InsPLAD.","terms":["cs.CV"]},{"titles":"ChemSpaceAL: An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation","summaries":"The incredible capabilities of generative artificial intelligence models have\ninevitably led to their application in the domain of drug discovery. Within\nthis domain, the vastness of chemical space motivates the development of more\nefficient methods for identifying regions with molecules that exhibit desired\ncharacteristics. In this work, we present a computationally efficient active\nlearning methodology that requires evaluation of only a subset of the generated\ndata in the constructed sample space to successfully align a generative model\nwith respect to a specified objective. We demonstrate the applicability of this\nmethodology to targeted molecular generation by fine-tuning a GPT-based\nmolecular generator toward a protein with FDA-approved small-molecule\ninhibitors, c-Abl kinase. Remarkably, the model learns to generate molecules\nsimilar to the inhibitors without prior knowledge of their existence, and even\nreproduces two of them exactly. We also show that the methodology is effective\nfor a protein without any commercially available small-molecule inhibitors, the\nHNH domain of the CRISPR-associated protein 9 (Cas9) enzyme. We believe that\nthe inherent generality of this method ensures that it will remain applicable\nas the exciting field of in silico molecular generation evolves. To facilitate\nimplementation and reproducibility, we have made all of our software available\nthrough the open-source ChemSpaceAL Python package.","terms":["cs.LG","q-bio.BM"]},{"titles":"KEEC: Embed to Control on An Equivariant Geometry","summaries":"This paper investigates how representation learning can enable optimal\ncontrol in unknown and complex dynamics, such as chaotic and non-linear\nsystems, without relying on prior domain knowledge of the dynamics. The core\nidea is to establish an equivariant geometry that is diffeomorphic to the\nmanifold defined by a dynamical system and to perform optimal control within\nthis corresponding geometry, which is a non-trivial task. To address this\nchallenge, Koopman Embed to Equivariant Control (KEEC) is introduced for model\nlearning and control. Inspired by Lie theory, KEEC begins by learning a\nnon-linear dynamical system defined on a manifold and embedding trajectories\ninto a Lie group. Subsequently, KEEC formulates an equivariant value function\nequation in reinforcement learning on the equivariant geometry, ensuring an\ninvariant effect as the value function on the original manifold. By deriving\nanalytical-form optimal actions on the equivariant value function, KEEC\ntheoretically achieves quadratic convergence for the optimal equivariant value\nfunction by leveraging the differential information on the equivariant\ngeometry. The effectiveness of KEEC is demonstrated in challenging dynamical\nsystems, including chaotic ones like Lorenz-63. Notably, our findings indicate\nthat isometric and isomorphic loss functions, ensuring the compactness and\nsmoothness of geometry, outperform loss functions without these properties.","terms":["cs.LG","cs.AI","cs.SY","eess.SY"]},{"titles":"Revisiting Non-separable Binary Classification and its Applications in Anomaly Detection","summaries":"The inability to linearly classify XOR has motivated much of deep learning.\nWe revisit this age-old problem and show that linear classification of XOR is\nindeed possible. Instead of separating data between halfspaces, we propose a\nslightly different paradigm, equality separation, that adapts the SVM objective\nto distinguish data within or outside the margin. Our classifier can then be\nintegrated into neural network pipelines with a smooth approximation. From its\nproperties, we intuit that equality separation is suitable for anomaly\ndetection. To formalize this notion, we introduce closing numbers, a\nquantitative measure on the capacity for classifiers to form closed decision\nregions for anomaly detection. Springboarding from this theoretical connection\nbetween binary classification and anomaly detection, we test our hypothesis on\nsupervised anomaly detection experiments, showing that equality separation can\ndetect both seen and unseen anomalies.","terms":["cs.LG","cs.AI","stat.ML","68T37 (Primary), 68T07 (Secondary)","I.2.6; I.5.1"]},{"titles":"Segment Anything Meets Point Tracking","summaries":"The Segment Anything Model (SAM) has established itself as a powerful\nzero-shot image segmentation model, enabled by efficient point-centric\nannotation and prompt-based models. While click and brush interactions are both\nwell explored in interactive image segmentation, the existing methods on videos\nfocus on mask annotation and propagation. This paper presents SAM-PT, a novel\nmethod for point-centric interactive video segmentation, empowered by SAM and\nlong-term point tracking. SAM-PT leverages robust and sparse point selection\nand propagation techniques for mask generation. Compared to traditional\nobject-centric mask propagation strategies, we uniquely use point propagation\nto exploit local structure information agnostic to object semantics. We\nhighlight the merits of point-based tracking through direct evaluation on the\nzero-shot open-world Unidentified Video Objects (UVO) benchmark. Our\nexperiments on popular video object segmentation and multi-object segmentation\ntracking benchmarks, including DAVIS, YouTube-VOS, and BDD100K, suggest that a\npoint-based segmentation tracker yields better zero-shot performance and\nefficient interactions. We release our code that integrates different point\ntrackers and video segmentation benchmarks at https:\/\/github.com\/SysCV\/sam-pt.","terms":["cs.CV"]},{"titles":"An Accurate and Fully-Automated Ensemble Model for Weekly Time Series Forecasting","summaries":"Many businesses and industries require accurate forecasts for weekly time\nseries nowadays. However, the forecasting literature does not currently provide\neasy-to-use, automatic, reproducible and accurate approaches dedicated to this\ntask. We propose a forecasting method in this domain to fill this gap,\nleveraging state-of-the-art forecasting techniques, such as forecast\ncombination, meta-learning, and global modelling. We consider different\nmeta-learning architectures, algorithms, and base model pools. Based on all\nconsidered model variants, we propose to use a stacking approach with lasso\nregression which optimally combines the forecasts of four base models: a global\nRecurrent Neural Network model (RNN), Theta, Trigonometric Box-Cox ARMA Trend\nSeasonal (TBATS) and Dynamic Harmonic Regression ARIMA (DHR-ARIMA), as it shows\nthe overall best performance across seven experimental weekly datasets on four\nevaluation metrics. Our proposed method also consistently outperforms a set of\nbenchmarks and state-of-the-art weekly forecasting models by a considerable\nmargin with statistical significance. Our method can produce the most accurate\nforecasts, in terms of mean sMAPE, for the M4 weekly dataset among all\nbenchmarks and all original competition participants.","terms":["cs.LG","cs.AI","cs.NE"]},{"titles":"Robust Computer Vision in an Ever-Changing World: A Survey of Techniques for Tackling Distribution Shifts","summaries":"AI applications are becoming increasingly visible to the general public.\nThere is a notable gap between the theoretical assumptions researchers make\nabout computer vision models and the reality those models face when deployed in\nthe real world. One of the critical reasons for this gap is a challenging\nproblem known as distribution shift. Distribution shifts tend to vary with\ncomplexity of the data, dataset size, and application type. In our paper, we\ndiscuss the identification of such a prominent gap, exploring the concept of\ndistribution shift and its critical significance. We provide an in-depth\noverview of various types of distribution shifts, elucidate their distinctions,\nand explore techniques within the realm of the data-centric domain employed to\naddress them. Distribution shifts can occur during every phase of the machine\nlearning pipeline, from the data collection stage to the stage of training a\nmachine learning model to the stage of final model deployment. As a result, it\nraises concerns about the overall robustness of the machine learning techniques\nfor computer vision applications that are deployed publicly for consumers.\nDifferent deep learning models each tailored for specific type of data and\ntasks, architectural pipelines; highlighting how variations in data\npreprocessing and feature extraction can impact robustness., data augmentation\nstrategies (e.g. geometric, synthetic and learning-based); demonstrating their\nrole in enhancing model generalization, and training mechanisms (e.g. transfer\nlearning, zero-shot) fall under the umbrella of data-centric methods. Each of\nthese components form an integral part of the neural-network we analyze\ncontributing uniquely to strengthening model robustness against distribution\nshifts. We compare and contrast numerous AI models that are built for\nmitigating shifts in hidden stratification and spurious correlations, ...","terms":["cs.CV"]},{"titles":"Recurrent Distance-Encoding Neural Networks for Graph Representation Learning","summaries":"Graph neural networks based on iterative one-hop message passing have been\nshown to struggle in harnessing information from distant nodes effectively.\nConversely, graph transformers allow each node to attend to all other nodes\ndirectly, but suffer from high computational complexity and have to rely on\nad-hoc positional encoding to bake in the graph inductive bias. In this paper,\nwe propose a new architecture to reconcile these challenges. Our approach stems\nfrom the recent breakthroughs in long-range modeling provided by deep\nstate-space models on sequential data: for a given target node, our model\naggregates other nodes by their shortest distances to the target and uses a\nparallelizable linear recurrent network over the chain of distances to provide\na natural encoding of its neighborhood structure. With no need for positional\nencoding, we empirically show that the performance of our model is highly\ncompetitive compared with that of state-of-the-art graph transformers on\nvarious benchmarks, at a drastically reduced computational complexity. In\naddition, we show that our model is theoretically more expressive than one-hop\nmessage passing neural networks.","terms":["cs.LG","cs.NE"]},{"titles":"Unlocking the Potential of Federated Learning: The Symphony of Dataset Distillation via Deep Generative Latents","summaries":"Data heterogeneity presents significant challenges for federated learning\n(FL). Recently, dataset distillation techniques have been introduced, and\nperformed at the client level, to attempt to mitigate some of these challenges.\nIn this paper, we propose a highly efficient FL dataset distillation framework\non the server side, significantly reducing both the computational and\ncommunication demands on local devices while enhancing the clients' privacy.\nUnlike previous strategies that perform dataset distillation on local devices\nand upload synthetic data to the server, our technique enables the server to\nleverage prior knowledge from pre-trained deep generative models to synthesize\nessential data representations from a heterogeneous model architecture. This\nprocess allows local devices to train smaller surrogate models while enabling\nthe training of a larger global model on the server, effectively minimizing\nresource utilization. We substantiate our claim with a theoretical analysis,\ndemonstrating the asymptotic resemblance of the process to the hypothetical\nideal of completely centralized training on a heterogeneous dataset. Empirical\nevidence from our comprehensive experiments indicates our method's superiority,\ndelivering an accuracy enhancement of up to 40% over non-dataset-distillation\ntechniques in highly heterogeneous FL contexts, and surpassing existing\ndataset-distillation methods by 18%. In addition to the high accuracy, our\nframework converges faster than the baselines because rather than the server\ntrains on several sets of heterogeneous data distributions, it trains on a\nmulti-modal distribution. Our code is available at\nhttps:\/\/github.com\/FedDG23\/FedDG-main.git","terms":["cs.LG","cs.AI"]},{"titles":"CalliPaint: Chinese Calligraphy Inpainting with Diffusion Model","summaries":"Chinese calligraphy can be viewed as a unique form of visual art. Recent\nadvancements in computer vision hold significant potential for the future\ndevelopment of generative models in the realm of Chinese calligraphy.\nNevertheless, methods of Chinese calligraphy inpainting, which can be\neffectively used in the art and education fields, remain relatively unexplored.\nIn this paper, we introduce a new model that harnesses recent advancements in\nboth Chinese calligraphy generation and image inpainting. We demonstrate that\nour proposed model CalliPaint can produce convincing Chinese calligraphy.","terms":["cs.CV"]},{"titles":"SANeRF-HQ: Segment Anything for NeRF in High Quality","summaries":"Recently, the Segment Anything Model (SAM) has showcased remarkable\ncapabilities of zero-shot segmentation, while NeRF (Neural Radiance Fields) has\ngained popularity as a method for various 3D problems beyond novel view\nsynthesis. Though there exist initial attempts to incorporate these two methods\ninto 3D segmentation, they face the challenge of accurately and consistently\nsegmenting objects in complex scenarios. In this paper, we introduce the\nSegment Anything for NeRF in High Quality (SANeRF-HQ) to achieve high quality\n3D segmentation of any object in a given scene. SANeRF-HQ utilizes SAM for\nopen-world object segmentation guided by user-supplied prompts, while\nleveraging NeRF to aggregate information from different viewpoints. To overcome\nthe aforementioned challenges, we employ density field and RGB similarity to\nenhance the accuracy of segmentation boundary during the aggregation.\nEmphasizing on segmentation accuracy, we evaluate our method quantitatively on\nmultiple NeRF datasets where high-quality ground-truths are available or\nmanually annotated. SANeRF-HQ shows a significant quality improvement over\nprevious state-of-the-art methods in NeRF object segmentation, provides higher\nflexibility for object localization, and enables more consistent object\nsegmentation across multiple views. Additional information can be found at\nhttps:\/\/lyclyc52.github.io\/SANeRF-HQ\/.","terms":["cs.CV"]},{"titles":"Achieving the Minimax Optimal Sample Complexity of Offline Reinforcement Learning: A DRO-Based Approach","summaries":"Offline reinforcement learning aims to learn from pre-collected datasets\nwithout active exploration. This problem faces significant challenges,\nincluding limited data availability and distributional shifts. Existing\napproaches adopt a pessimistic stance towards uncertainty by penalizing rewards\nof under-explored state-action pairs to estimate value functions\nconservatively. In this paper, we show that the distributionally robust\noptimization (DRO) based approach can also address these challenges and is\nminimax optimal. Specifically, we directly model the uncertainty in the\ntransition kernel and construct an uncertainty set of statistically plausible\ntransition kernels. We then find the policy that optimizes the worst-case\nperformance over this uncertainty set. We first design a metric-based\nHoeffding-style uncertainty set such that with high probability the true\ntransition kernel is in this set. We prove that to achieve a sub-optimality gap\nof $\\epsilon$, the sample complexity is\n$\\mathcal{O}(S^2C^{\\pi^*}\\epsilon^{-2}(1-\\gamma)^{-4})$, where $\\gamma$ is the\ndiscount factor, $S$ is the number of states, and $C^{\\pi^*}$ is the\nsingle-policy clipped concentrability coefficient which quantifies the\ndistribution shift. To achieve the optimal sample complexity, we further\npropose a less conservative Bernstein-style uncertainty set, which, however,\ndoes not necessarily include the true transition kernel. We show that an\nimproved sample complexity of\n$\\mathcal{O}(SC^{\\pi^*}\\epsilon^{-2}(1-\\gamma)^{-3})$ can be obtained, which\nmatches with the minimax lower bound for offline reinforcement learning, and\nthus is minimax optimal.","terms":["cs.LG"]},{"titles":"What does a platypus look like? Generating customized prompts for zero-shot image classification","summaries":"Open-vocabulary models are a promising new paradigm for image classification.\nUnlike traditional classification models, open-vocabulary models classify among\nany arbitrary set of categories specified with natural language during\ninference. This natural language, called \"prompts\", typically consists of a set\nof hand-written templates (e.g., \"a photo of a {}\") which are completed with\neach of the category names. This work introduces a simple method to generate\nhigher accuracy prompts, without relying on any explicit knowledge of the task\ndomain and with far fewer hand-constructed sentences. To achieve this, we\ncombine open-vocabulary models with large language models (LLMs) to create\nCustomized Prompts via Language models (CuPL, pronounced \"couple\"). In\nparticular, we leverage the knowledge contained in LLMs in order to generate\nmany descriptive sentences that contain important discriminating\ncharacteristics of the image categories. This allows the model to place a\ngreater importance on these regions in the image when making predictions. We\nfind that this straightforward and general approach improves accuracy on a\nrange of zero-shot image classification benchmarks, including over one\npercentage point gain on ImageNet. Finally, this simple baseline requires no\nadditional training and remains completely zero-shot. Code available at\nhttps:\/\/github.com\/sarahpratt\/CuPL.","terms":["cs.CV","cs.LG"]},{"titles":"G2D: From Global to Dense Radiography Representation Learning via Vision-Language Pre-training","summaries":"Recently, medical vision-language pre-training (VLP) has reached substantial\nprogress to learn global visual representation from medical images and their\npaired radiology reports. However, medical imaging tasks in real world usually\nrequire finer granularity in visual features. These tasks include visual\nlocalization tasks (e.g., semantic segmentation, object detection) and visual\ngrounding task. Yet, current medical VLP methods face challenges in learning\nthese fine-grained features, as they primarily focus on brute-force alignment\nbetween image patches and individual text tokens for local visual feature\nlearning, which is suboptimal for downstream dense prediction tasks. In this\nwork, we propose a new VLP framework, named \\textbf{G}lobal to \\textbf{D}ense\nlevel representation learning (G2D) that achieves significantly improved\ngranularity and more accurate grounding for the learned features, compared to\nexisting medical VLP approaches. In particular, G2D learns dense and\nsemantically-grounded image representations via a pseudo segmentation task\nparallel with the global vision-language alignment. Notably, generating pseudo\nsegmentation targets does not incur extra trainable parameters: they are\nobtained on the fly during VLP with a parameter-free processor. G2D achieves\nsuperior performance across 6 medical imaging tasks and 25 diseases,\nparticularly in semantic segmentation, which necessitates fine-grained,\nsemantically-grounded image features. In this task, G2D surpasses peer models\neven when fine-tuned with just 1\\% of the training data, compared to the 100\\%\nused by these models. The code will be released upon acceptance.","terms":["cs.CV","cs.LG"]},{"titles":"Parametric Surface Constrained Upsampler Network for Point Cloud","summaries":"Designing a point cloud upsampler, which aims to generate a clean and dense\npoint cloud given a sparse point representation, is a fundamental and\nchallenging problem in computer vision. A line of attempts achieves this goal\nby establishing a point-to-point mapping function via deep neural networks.\nHowever, these approaches are prone to produce outlier points due to the lack\nof explicit surface-level constraints. To solve this problem, we introduce a\nnovel surface regularizer into the upsampler network by forcing the neural\nnetwork to learn the underlying parametric surface represented by bicubic\nfunctions and rotation functions, where the new generated points are then\nconstrained on the underlying surface. These designs are integrated into two\ndifferent networks for two tasks that take advantages of upsampling layers -\npoint cloud upsampling and point cloud completion for evaluation. The\nstate-of-the-art experimental results on both tasks demonstrate the\neffectiveness of the proposed method. The code is available at\nhttps:\/\/github.com\/corecai163\/PSCU.","terms":["cs.CV"]},{"titles":"Tracing Hyperparameter Dependencies for Model Parsing via Learnable Graph Pooling Network","summaries":"Model Parsing defines the research task of predicting hyperparameters of the\ngenerative model (GM), given a generated image as input. Since a diverse set of\nhyperparameters is jointly employed by the generative model, and dependencies\noften exist among them, it is crucial to learn these hyperparameter\ndependencies for the improved model parsing performance. To explore such\nimportant dependencies, we propose a novel model parsing method called\nLearnable Graph Pooling Network (LGPN). Specifically, we transform model\nparsing into a graph node classification task, using graph nodes and edges to\nrepresent hyperparameters and their dependencies, respectively. Furthermore,\nLGPN incorporates a learnable pooling-unpooling mechanism tailored to model\nparsing, which adaptively learns hyperparameter dependencies of GMs used to\ngenerate the input image. We also extend our proposed method to CNN-generated\nimage detection and coordinate attacks detection. Empirically, we achieve\nstate-of-the-art results in model parsing and its extended applications,\nshowing the effectiveness of our method. Our source code are available.","terms":["cs.CV"]},{"titles":"Learning Efficient Coding of Natural Images with Maximum Manifold Capacity Representations","summaries":"The efficient coding hypothesis proposes that the response properties of\nsensory systems are adapted to the statistics of their inputs such that they\ncapture maximal information about the environment, subject to biological\nconstraints. While elegant, information theoretic properties are notoriously\ndifficult to measure in practical settings or to employ as objective functions\nin optimization. This difficulty has necessitated that computational models\ndesigned to test the hypothesis employ several different information metrics\nranging from approximations and lower bounds to proxy measures like\nreconstruction error. Recent theoretical advances have characterized a novel\nand ecologically relevant efficiency metric, the manifold capacity, which is\nthe number of object categories that may be represented in a linearly separable\nfashion. However, calculating manifold capacity is a computationally intensive\niterative procedure that until now has precluded its use as an objective. Here\nwe outline the simplifying assumptions that allow manifold capacity to be\noptimized directly, yielding Maximum Manifold Capacity Representations (MMCR).\nThe resulting method is closely related to and inspired by advances in the\nfield of self supervised learning (SSL), and we demonstrate that MMCRs are\ncompetitive with state of the art results on standard SSL benchmarks. Empirical\nanalyses reveal differences between MMCRs and representations learned by other\nSSL frameworks, and suggest a mechanism by which manifold compression gives\nrise to class separability. Finally we evaluate a set of SSL methods on a suite\nof neural predictivity benchmarks, and find MMCRs are higly competitive as\nmodels of the ventral stream.","terms":["cs.CV","q-bio.NC"]},{"titles":"CityGen: Infinite and Controllable 3D City Layout Generation","summaries":"City layout generation has recently gained significant attention. The goal of\nthis task is to automatically generate the layout of a city scene, including\nelements such as roads, buildings, vegetation, as well as other urban\ninfrastructures. Previous methods using VAEs or GANs for 3D city layout\ngeneration offer limited diversity and constrained interactivity, only allowing\nusers to selectively regenerate parts of the layout, which greatly limits\ncustomization. In this paper, we propose CityGen, a novel end-to-end framework\nfor infinite, diverse and controllable 3D city layout generation.First, we\npropose an outpainting pipeline to extend the local layout to an infinite city\nlayout. Then, we utilize a multi-scale diffusion model to generate diverse and\ncontrollable local semantic layout patches. The extensive experiments show that\nCityGen achieves state-of-the-art (SOTA) performance under FID and KID in\ngenerating an infinite and controllable 3D city layout. CityGen demonstrates\npromising applicability in fields like smart cities, urban planning, and\ndigital simulation.","terms":["cs.CV"]},{"titles":"Learn2Extend: Extending sequences by retaining their statistical properties with mixture models","summaries":"This paper addresses the challenge of extending general finite sequences of\nreal numbers within a subinterval of the real line, maintaining their inherent\nstatistical properties by employing machine learning. Our focus lies on\npreserving the gap distribution and pair correlation function of these point\nsets. Leveraging advancements in deep learning applied to point processes, this\npaper explores the use of an auto-regressive \\textit{Sequence Extension Mixture\nModel} (SEMM) for extending finite sequences, by estimating directly the\nconditional density, instead of the intensity function. We perform comparative\nexperiments on multiple types of point processes, including Poisson, locally\nattractive, and locally repelling sequences, and we perform a case study on the\nprediction of Riemann $\\zeta$ function zeroes. The results indicate that the\nproposed mixture model outperforms traditional neural network architectures in\nsequence extension with the retention of statistical properties. Given this\nmotivation, we showcase the capabilities of a mixture model to extend\nsequences, maintaining specific statistical properties, i.e. the gap\ndistribution, and pair correlation indicators.","terms":["cs.LG","stat.ML"]},{"titles":"The mechanistic basis of data dependence and abrupt learning in an in-context classification task","summaries":"Transformer models exhibit in-context learning: the ability to accurately\npredict the response to a novel query based on illustrative examples in the\ninput sequence. In-context learning contrasts with traditional in-weights\nlearning of query-output relationships. What aspects of the training data\ndistribution and architecture favor in-context vs in-weights learning? Recent\nwork has shown that specific distributional properties inherent in language,\nsuch as burstiness, large dictionaries and skewed rank-frequency distributions,\ncontrol the trade-off or simultaneous appearance of these two forms of\nlearning. We first show that these results are recapitulated in a minimal\nattention-only network trained on a simplified dataset. In-context learning\n(ICL) is driven by the abrupt emergence of an induction head, which\nsubsequently competes with in-weights learning. By identifying progress\nmeasures that precede in-context learning and targeted experiments, we\nconstruct a two-parameter model of an induction head which emulates the full\ndata distributional dependencies displayed by the attention-based network. A\nphenomenological model of induction head formation traces its abrupt emergence\nto the sequential learning of three nested logits enabled by an intrinsic\ncurriculum. We propose that the sharp transitions in attention-based networks\narise due to a specific chain of multi-layer operations necessary to achieve\nICL, which is implemented by nested nonlinearities sequentially learned during\ntraining.","terms":["cs.LG"]},{"titles":"Effectively Fine-tune to Improve Large Multimodal Models for Radiology Report Generation","summaries":"Writing radiology reports from medical images requires a high level of domain\nexpertise. It is time-consuming even for trained radiologists and can be\nerror-prone for inexperienced radiologists. It would be appealing to automate\nthis task by leveraging generative AI, which has shown drastic progress in\nvision and language understanding. In particular, Large Language Models (LLM)\nhave demonstrated impressive capabilities recently and continued to set new\nstate-of-the-art performance on almost all natural language tasks. While many\nhave proposed architectures to combine vision models with LLMs for multimodal\ntasks, few have explored practical fine-tuning strategies. In this work, we\nproposed a simple yet effective two-stage fine-tuning protocol to align visual\nfeatures to LLM's text embedding space as soft visual prompts. Our framework\nwith OpenLLaMA-7B achieved state-of-the-art level performance without\ndomain-specific pretraining. Moreover, we provide detailed analyses of soft\nvisual prompts and attention mechanisms, shedding light on future research\ndirections.","terms":["cs.CV","cs.AI","cs.CL","cs.LG"]},{"titles":"Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning","summaries":"We consider the problem of learning models for risk-sensitive reinforcement\nlearning. We theoretically demonstrate that proper value equivalence, a method\nof learning models which can be used to plan optimally in the risk-neutral\nsetting, is not sufficient to plan optimally in the risk-sensitive setting. We\nleverage distributional reinforcement learning to introduce two new notions of\nmodel equivalence, one which is general and can be used to plan for any risk\nmeasure, but is intractable; and a practical variation which allows one to\nchoose which risk measures they may plan optimally for. We demonstrate how our\nframework can be used to augment any model-free risk-sensitive algorithm, and\nprovide both tabular and large-scale experiments to demonstrate its ability.","terms":["cs.LG","cs.AI"]},{"titles":"ImagenHub: Standardizing the evaluation of conditional image generation models","summaries":"Recently, a myriad of conditional image generation and editing models have\nbeen developed to serve different downstream tasks, including text-to-image\ngeneration, text-guided image editing, subject-driven image generation,\ncontrol-guided image generation, etc. However, we observe huge inconsistencies\nin experimental conditions: datasets, inference, and evaluation metrics -\nrender fair comparisons difficult. This paper proposes ImagenHub, which is a\none-stop library to standardize the inference and evaluation of all the\nconditional image generation models. Firstly, we define seven prominent tasks\nand curate high-quality evaluation datasets for them. Secondly, we built a\nunified inference pipeline to ensure fair comparison. Thirdly, we design two\nhuman evaluation scores, i.e. Semantic Consistency and Perceptual Quality,\nalong with comprehensive guidelines to evaluate generated images. We train\nexpert raters to evaluate the model outputs based on the proposed metrics. Our\nhuman evaluation achieves a high inter-worker agreement of Krippendorff's alpha\non 76% models with a value higher than 0.4. We comprehensively evaluated a\ntotal of around 30 models and observed three key takeaways: (1) the existing\nmodels' performance is generally unsatisfying except for Text-guided Image\nGeneration and Subject-driven Image Generation, with 74% models achieving an\noverall score lower than 0.5. (2) we examined the claims from published papers\nand found 83% of them hold with a few exceptions. (3) None of the existing\nautomatic metrics has a Spearman's correlation higher than 0.2 except\nsubject-driven image generation. Moving forward, we will continue our efforts\nto evaluate newly published models and update our leaderboard to keep track of\nthe progress in conditional image generation.","terms":["cs.CV","cs.GR","cs.MM"]},{"titles":"Normed Spaces for Graph Embedding","summaries":"Theoretical results from discrete geometry suggest that normed spaces can\nabstractly embed finite metric spaces with surprisingly low theoretical bounds\non distortion in low dimensions. In this paper, inspired by this theoretical\ninsight, we highlight normed spaces as a more flexible and computationally\nefficient alternative to several popular Riemannian manifolds for learning\ngraph embeddings. Normed space embeddings significantly outperform several\npopular manifolds on a large range of synthetic and real-world graph\nreconstruction benchmark datasets while requiring significantly fewer\ncomputational resources. We also empirically verify the superiority of normed\nspace embeddings on growing families of graphs associated with negative, zero,\nand positive curvature, further reinforcing the flexibility of normed spaces in\ncapturing diverse graph structures as graph sizes increase. Lastly, we\ndemonstrate the utility of normed space embeddings on two applied graph\nembedding tasks, namely, link prediction and recommender systems. Our work\nhighlights the potential of normed spaces for geometric graph representation\nlearning, raises new research questions, and offers a valuable tool for\nexperimental mathematics in the field of finite metric space embeddings. We\nmake our code and data publically available.","terms":["cs.LG","cs.SI"]},{"titles":"DiffMesh: A Motion-aware Diffusion-like Framework for Human Mesh Recovery from Videos","summaries":"Human mesh recovery (HMR) provides rich human body information for various\nreal-world applications. While image-based HMR methods have achieved impressive\nresults, they often struggle to recover humans in dynamic scenarios, leading to\ntemporal inconsistencies and non-smooth 3D motion predictions due to the\nabsence of human motion. In contrast, video-based approaches leverage temporal\ninformation to mitigate this issue. In this paper, we present DiffMesh, an\ninnovative motion-aware Diffusion-like framework for video-based HMR. DiffMesh\nestablishes a bridge between diffusion models and human motion, efficiently\ngenerating accurate and smooth output mesh sequences by incorporating human\nmotion within the forward process and reverse process in the diffusion model.\nExtensive experiments are conducted on the widely used datasets (Human3.6M\n\\cite{h36m_pami} and 3DPW \\cite{pw3d2018}), which demonstrate the effectiveness\nand efficiency of our DiffMesh. Visual comparisons in real-world scenarios\nfurther highlight DiffMesh's suitability for practical applications.","terms":["cs.CV","cs.AI","cs.HC","cs.MM"]},{"titles":"Neural Operators for Accelerating Scientific Simulations and Design","summaries":"Scientific discovery and engineering design are currently limited by the time\nand cost of physical experiments, selected mostly through trial-and-error and\nintuition that require deep domain expertise. Numerical simulations present an\nalternative to physical experiments but are usually infeasible for complex\nreal-world domains due to the computational requirements of existing numerical\nmethods. Artificial intelligence (AI) presents a potential paradigm shift by\ndeveloping fast data-driven surrogate models. In particular, an AI framework,\nknown as neural operators, presents a principled framework for learning\nmappings between functions defined on continuous domains, e.g., spatiotemporal\nprocesses and partial differential equations (PDE). They can extrapolate and\npredict solutions at new locations unseen during training, i.e., perform\nzero-shot super-resolution. Neural operators can augment or even replace\nexisting simulators in many applications, such as computational fluid dynamics,\nweather forecasting, and material modeling, while being 4-5 orders of magnitude\nfaster. Further, neural operators can be integrated with physics and other\ndomain constraints enforced at finer resolutions to obtain high-fidelity\nsolutions and good generalization. Since neural operators are differentiable,\nthey can directly optimize parameters for inverse design and other inverse\nproblems. We believe that neural operators present a transformative approach to\nsimulation and design, enabling rapid research and development.","terms":["cs.LG","physics.comp-ph"]},{"titles":"HandyPriors: Physically Consistent Perception of Hand-Object Interactions with Differentiable Priors","summaries":"Various heuristic objectives for modeling hand-object interaction have been\nproposed in past work. However, due to the lack of a cohesive framework, these\nobjectives often possess a narrow scope of applicability and are limited by\ntheir efficiency or accuracy. In this paper, we propose HandyPriors, a unified\nand general pipeline for pose estimation in human-object interaction scenes by\nleveraging recent advances in differentiable physics and rendering. Our\napproach employs rendering priors to align with input images and segmentation\nmasks along with physics priors to mitigate penetration and relative-sliding\nacross frames. Furthermore, we present two alternatives for hand and object\npose estimation. The optimization-based pose estimation achieves higher\naccuracy, while the filtering-based tracking, which utilizes the differentiable\npriors as dynamics and observation models, executes faster. We demonstrate that\nHandyPriors attains comparable or superior results in the pose estimation task,\nand that the differentiable physics module can predict contact information for\npose refinement. We also show that our approach generalizes to perception\ntasks, including robotic hand manipulation and human-object pose estimation in\nthe wild.","terms":["cs.CV","cs.RO"]},{"titles":"Representing Data as Atoms: Unifying Intra- and Inter-Sample Relationship to Discretize Data Representation","summaries":"The quality of data representation is paramount for the performance of a\nmodel. Recent research has focused on enhancing representation learning by\nincorporating more information about the intra-sample structures of individual\ndata points, such as local and global attention. Additionally, researchers have\nexplored methods to model the inter-sample relationships, including manifold,\ncontrastive, and discrete representation learning. In this study, we introduce\na new training loss, which considers both intra-sample structure and\ninter-sample relationships, leveraging the concept of {\\it atoms} to represent\ndata points. This new approach, {\\it Atom Modeling}, offers a fresh perspective\nto discretize data representations within a continuous space. Through\nexperiments, we demonstrate that Atom Modeling enhances the performance of\nexisting models in tasks involving classification and generation, across\ndiverse domains including vision and language. These findings underscore the\npotential of Atom Modeling to enhance data representation and improve model\nlearning, suggesting a promising direction for future research.","terms":["cs.LG","cs.AI","stat.ML"]},{"titles":"Robust Evaluation of Diffusion-Based Adversarial Purification","summaries":"We question the current evaluation practice on diffusion-based purification\nmethods. Diffusion-based purification methods aim to remove adversarial effects\nfrom an input data point at test time. The approach gains increasing attention\nas an alternative to adversarial training due to the disentangling between\ntraining and testing. Well-known white-box attacks are often employed to\nmeasure the robustness of the purification. However, it is unknown whether\nthese attacks are the most effective for the diffusion-based purification since\nthe attacks are often tailored for adversarial training. We analyze the current\npractices and provide a new guideline for measuring the robustness of\npurification methods against adversarial attacks. Based on our analysis, we\nfurther propose a new purification strategy improving robustness compared to\nthe current diffusion-based purification methods.","terms":["cs.CV","cs.CR","cs.LG"]},{"titles":"GAPS: Geometry-Aware, Physics-Based, Self-Supervised Neural Garment Draping","summaries":"Recent neural, physics-based modeling of garment deformations allows faster\nand visually aesthetic results as opposed to the existing methods.\nMaterial-specific parameters are used by the formulation to control the garment\ninextensibility. This delivers unrealistic results with physically implausible\nstretching. Oftentimes, the draped garment is pushed inside the body which is\neither corrected by an expensive post-processing, thus adding to further\ninconsistent stretching; or by deploying a separate training regime for each\nbody type, restricting its scalability. Additionally, the flawed skinning\nprocess deployed by existing methods produces incorrect results on loose\ngarments.\n  In this paper, we introduce a geometrical constraint to the existing\nformulation that is collision-aware and imposes garment inextensibility\nwherever possible. Thus, we obtain realistic results where draped clothes\nstretch only while covering bigger body regions. Furthermore, we propose a\ngeometry-aware garment skinning method by defining a body-garment closeness\nmeasure which works for all garment types, especially the loose ones.","terms":["cs.CV","cs.GR","cs.LG"]},{"titles":"ADT: Agent-based Dynamic Thresholding for Anomaly Detection","summaries":"The complexity and scale of IT systems are increasing dramatically, posing\nmany challenges to real-world anomaly detection. Deep learning anomaly\ndetection has emerged, aiming at feature learning and anomaly scoring, which\nhas gained tremendous success. However, little work has been done on the\nthresholding problem despite it being a critical factor for the effectiveness\nof anomaly detection. In this paper, we model thresholding in anomaly detection\nas a Markov Decision Process and propose an agent-based dynamic thresholding\n(ADT) framework based on a deep Q-network. The proposed method can be\nintegrated into many systems that require dynamic thresholding. An auto-encoder\nis utilized in this study to obtain feature representations and produce anomaly\nscores for complex input data. ADT can adjust thresholds adaptively by\nutilizing the anomaly scores from the auto-encoder and significantly improve\nanomaly detection performance. The properties of ADT are studied through\nexperiments on three real-world datasets and compared with benchmarks, hence\ndemonstrating its thresholding capability, data-efficient learning, stability,\nand robustness. Our study validates the effectiveness of reinforcement learning\nin optimal thresholding control in anomaly detection.","terms":["cs.LG","cs.AI"]},{"titles":"InvertAvatar: Incremental GAN Inversion for Generalized Head Avatars","summaries":"While high fidelity and efficiency are central to the creation of digital\nhead avatars, recent methods relying on 2D or 3D generative models often\nexperience limitations such as shape distortion, expression inaccuracy, and\nidentity flickering. Additionally, existing one-shot inversion techniques fail\nto fully leverage multiple input images for detailed feature extraction. We\npropose a novel framework, \\textbf{Incremental 3D GAN Inversion}, that enhances\navatar reconstruction performance using an algorithm designed to increase the\nfidelity from multiple frames, resulting in improved reconstruction quality\nproportional to frame count. Our method introduces a unique animatable 3D GAN\nprior with two crucial modifications for enhanced expression controllability\nalongside an innovative neural texture encoder that categorizes texture feature\nspaces based on UV parameterization. Differentiating from traditional\ntechniques, our architecture emphasizes pixel-aligned image-to-image\ntranslation, mitigating the need to learn correspondences between observation\nand canonical spaces. Furthermore, we incorporate ConvGRU-based recurrent\nnetworks for temporal data aggregation from multiple frames, boosting geometry\nand texture detail reconstruction. The proposed paradigm demonstrates\nstate-of-the-art performance on one-shot and few-shot avatar animation tasks.","terms":["cs.CV"]},{"titles":"Slice3D: Multi-Slice, Occlusion-Revealing, Single View 3D Reconstruction","summaries":"We introduce multi-slice reasoning, a new notion for single-view 3D\nreconstruction which challenges the current and prevailing belief that\nmulti-view synthesis is the most natural conduit between single-view and 3D.\nOur key observation is that object slicing is more advantageous than altering\nviews to reveal occluded structures. Specifically, slicing is more\nocclusion-revealing since it can peel through any occluders without\nobstruction. In the limit, i.e., with infinitely many slices, it is guaranteed\nto unveil all hidden object parts. We realize our idea by developing Slice3D, a\nnovel method for single-view 3D reconstruction which first predicts multi-slice\nimages from a single RGB image and then integrates the slices into a 3D model\nusing a coordinate-based transformer network for signed distance prediction.\nThe slice images can be regressed or generated, both through a U-Net based\nnetwork. For the former, we inject a learnable slice indicator code to\ndesignate each decoded image into a spatial slice location, while the slice\ngenerator is a denoising diffusion model operating on the entirety of slice\nimages stacked on the input channels. We conduct extensive evaluation against\nstate-of-the-art alternatives to demonstrate superiority of our method,\nespecially in recovering complex and severely occluded shape structures, amid\nambiguities. All Slice3D results were produced by networks trained on a single\nNvidia A40 GPU, with an inference time less than 20 seconds.","terms":["cs.CV","cs.GR"]},{"titles":"QuantAttack: Exploiting Dynamic Quantization to Attack Vision Transformers","summaries":"In recent years, there has been a significant trend in deep neural networks\n(DNNs), particularly transformer-based models, of developing ever-larger and\nmore capable models. While they demonstrate state-of-the-art performance, their\ngrowing scale requires increased computational resources (e.g., GPUs with\ngreater memory capacity). To address this problem, quantization techniques\n(i.e., low-bit-precision representation and matrix multiplication) have been\nproposed. Most quantization techniques employ a static strategy in which the\nmodel parameters are quantized, either during training or inference, without\nconsidering the test-time sample. In contrast, dynamic quantization techniques,\nwhich have become increasingly popular, adapt during inference based on the\ninput provided, while maintaining full-precision performance. However, their\ndynamic behavior and average-case performance assumption makes them vulnerable\nto a novel threat vector -- adversarial attacks that target the model's\nefficiency and availability. In this paper, we present QuantAttack, a novel\nattack that targets the availability of quantized models, slowing down the\ninference, and increasing memory usage and energy consumption. We show that\ncarefully crafted adversarial examples, which are designed to exhaust the\nresources of the operating system, can trigger worst-case performance. In our\nexperiments, we demonstrate the effectiveness of our attack on vision\ntransformers on a wide range of tasks, both uni-modal and multi-modal. We also\nexamine the effect of different attack variants (e.g., a universal\nperturbation) and the transferability between different models.","terms":["cs.CV","cs.CR","cs.LG"]},{"titles":"Regularity as Intrinsic Reward for Free Play","summaries":"We propose regularity as a novel reward signal for intrinsically-motivated\nreinforcement learning. Taking inspiration from child development, we postulate\nthat striving for structure and order helps guide exploration towards a\nsubspace of tasks that are not favored by naive uncertainty-based intrinsic\nrewards. Our generalized formulation of Regularity as Intrinsic Reward (RaIR)\nallows us to operationalize it within model-based reinforcement learning. In a\nsynthetic environment, we showcase the plethora of structured patterns that can\nemerge from pursuing this regularity objective. We also demonstrate the\nstrength of our method in a multi-object robotic manipulation environment. We\nincorporate RaIR into free play and use it to complement the model's epistemic\nuncertainty as an intrinsic reward. Doing so, we witness the autonomous\nconstruction of towers and other regular structures during free play, which\nleads to a substantial improvement in zero-shot downstream task performance on\nassembly tasks.","terms":["cs.LG"]},{"titles":"BenchMARL: Benchmarking Multi-Agent Reinforcement Learning","summaries":"The field of Multi-Agent Reinforcement Learning (MARL) is currently facing a\nreproducibility crisis. While solutions for standardized reporting have been\nproposed to address the issue, we still lack a benchmarking tool that enables\nstandardization and reproducibility, while leveraging cutting-edge\nReinforcement Learning (RL) implementations. In this paper, we introduce\nBenchMARL, the first MARL training library created to enable standardized\nbenchmarking across different algorithms, models, and environments. BenchMARL\nuses TorchRL as its backend, granting it high performance and maintained\nstate-of-the-art implementations while addressing the broad community of MARL\nPyTorch users. Its design enables systematic configuration and reporting, thus\nallowing users to create and run complex benchmarks from simple one-line\ninputs. BenchMARL is open-sourced on GitHub:\nhttps:\/\/github.com\/facebookresearch\/BenchMARL","terms":["cs.LG","cs.AI","cs.MA"]},{"titles":"Marginal Density Ratio for Off-Policy Evaluation in Contextual Bandits","summaries":"Off-Policy Evaluation (OPE) in contextual bandits is crucial for assessing\nnew policies using existing data without costly experimentation. However,\ncurrent OPE methods, such as Inverse Probability Weighting (IPW) and Doubly\nRobust (DR) estimators, suffer from high variance, particularly in cases of low\noverlap between target and behavior policies or large action and context\nspaces. In this paper, we introduce a new OPE estimator for contextual bandits,\nthe Marginal Ratio (MR) estimator, which focuses on the shift in the marginal\ndistribution of outcomes $Y$ instead of the policies themselves. Through\nrigorous theoretical analysis, we demonstrate the benefits of the MR estimator\ncompared to conventional methods like IPW and DR in terms of variance\nreduction. Additionally, we establish a connection between the MR estimator and\nthe state-of-the-art Marginalized Inverse Propensity Score (MIPS) estimator,\nproving that MR achieves lower variance among a generalized family of MIPS\nestimators. We further illustrate the utility of the MR estimator in causal\ninference settings, where it exhibits enhanced performance in estimating\nAverage Treatment Effects (ATE). Our experiments on synthetic and real-world\ndatasets corroborate our theoretical findings and highlight the practical\nadvantages of the MR estimator in OPE for contextual bandits.","terms":["stat.ML","cs.LG","stat.ME"]},{"titles":"Compositional Policy Learning in Stochastic Control Systems with Formal Guarantees","summaries":"Reinforcement learning has shown promising results in learning neural network\npolicies for complicated control tasks. However, the lack of formal guarantees\nabout the behavior of such policies remains an impediment to their deployment.\nWe propose a novel method for learning a composition of neural network policies\nin stochastic environments, along with a formal certificate which guarantees\nthat a specification over the policy's behavior is satisfied with the desired\nprobability. Unlike prior work on verifiable RL, our approach leverages the\ncompositional nature of logical specifications provided in SpectRL, to learn\nover graphs of probabilistic reach-avoid specifications. The formal guarantees\nare provided by learning neural network policies together with reach-avoid\nsupermartingales (RASM) for the graph's sub-tasks and then composing them into\na global policy. We also derive a tighter lower bound compared to previous work\non the probability of reach-avoidance implied by a RASM, which is required to\nfind a compositional policy with an acceptable probabilistic threshold for\ncomplex tasks with multiple edge policies. We implement a prototype of our\napproach and evaluate it on a Stochastic Nine Rooms environment.","terms":["cs.LG","cs.SY","eess.SY"]},{"titles":"Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing","summaries":"There is increasing adoption of artificial intelligence in drug discovery.\nHowever, existing studies use machine learning to mainly utilize the chemical\nstructures of molecules but ignore the vast textual knowledge available in\nchemistry. Incorporating textual knowledge enables us to realize new drug\ndesign objectives, adapt to text-based instructions and predict complex\nbiological activities. Here we present a multi-modal molecule structure-text\nmodel, MoleculeSTM, by jointly learning molecules' chemical structures and\ntextual descriptions via a contrastive learning strategy. To train MoleculeSTM,\nwe construct a large multi-modal dataset, namely, PubChemSTM, with over 280,000\nchemical structure-text pairs. To demonstrate the effectiveness and utility of\nMoleculeSTM, we design two challenging zero-shot tasks based on text\ninstructions, including structure-text retrieval and molecule editing.\nMoleculeSTM has two main properties: open vocabulary and compositionality via\nnatural language. In experiments, MoleculeSTM obtains the state-of-the-art\ngeneralization ability to novel biochemical concepts across various benchmarks.","terms":["cs.LG","cs.CL","q-bio.QM","stat.ML"]},{"titles":"Foveation in the Era of Deep Learning","summaries":"In this paper, we tackle the challenge of actively attending to visual scenes\nusing a foveated sensor. We introduce an end-to-end differentiable foveated\nactive vision architecture that leverages a graph convolutional network to\nprocess foveated images, and a simple yet effective formulation for foveated\nimage sampling. Our model learns to iteratively attend to regions of the image\nrelevant for classification. We conduct detailed experiments on a variety of\nimage datasets, comparing the performance of our method with previous\napproaches to foveated vision while measuring how the impact of different\nchoices, such as the degree of foveation, and the number of fixations the\nnetwork performs, affect object recognition performance. We find that our model\noutperforms a state-of-the-art CNN and foveated vision architectures of\ncomparable parameters and a given pixel or computation budget","terms":["cs.CV","cs.AI","cs.LG","I.2.10; I.5.1; I.4.8"]},{"titles":"Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language Models","summaries":"Large Vision and Language Models have enabled significant advances in fully\nsupervised and zero-shot vision tasks. These large pre-trained architectures\nserve as the baseline to what is currently known as Instruction Tuning Large\nVision and Language models (IT-LVLMs). IT-LVLMs are general-purpose multi-modal\nassistants whose responses are modulated by natural language instructions and\narbitrary visual data. Despite this versatility, IT-LVLM effectiveness in\nfundamental computer vision problems remains unclear, primarily due to the\nabsence of a standardized evaluation benchmark. This paper introduces a\nMulti-modal Evaluation Benchmark named MERLIM, a scalable test-bed to assess\nthe performance of IT-LVLMs on fundamental computer vision tasks. MERLIM\ncontains over 279K image-question pairs, and has a strong focus on detecting\ncross-modal \"hallucination\" events in IT-LVLMs, where the language output\nrefers to visual concepts that lack any effective grounding in the image. Our\nresults show that state-of-the-art IT-LVMLs are still limited at identifying\nfine-grained visual concepts, object hallucinations are common across tasks,\nand their results are strongly biased by small variations in the input query,\neven if the queries have the very same semantics. Our findings also suggest\nthat these models have weak visual groundings but they can still make adequate\nguesses by global visual patterns or textual biases contained in the LLM\ncomponent.","terms":["cs.CV","cs.CL"]},{"titles":"MVBench: A Comprehensive Multi-modal Video Understanding Benchmark","summaries":"With the rapid development of Multi-modal Large Language Models (MLLMs), a\nnumber of diagnostic benchmarks have recently emerged to evaluate the\ncomprehension capabilities of these models. However, most benchmarks\npredominantly assess spatial understanding in the static image tasks, while\noverlooking temporal understanding in the dynamic video tasks. To alleviate\nthis issue, we introduce a comprehensive Multi-modal Video understanding\nBenchmark, namely MVBench, which covers 20 challenging video tasks that cannot\nbe effectively solved with a single frame. Specifically, we first introduce a\nnovel static-to-dynamic method to define these temporal-related tasks. By\ntransforming various static tasks into dynamic ones, we enable the systematic\ngeneration of video tasks that require a broad spectrum of temporal skills,\nranging from perception to cognition. Then, guided by the task definition, we\nautomatically convert public video annotations into multiple-choice QA to\nevaluate each task. On one hand, such a distinct paradigm allows us to build\nMVBench efficiently, without much manual intervention. On the other hand, it\nguarantees evaluation fairness with ground-truth video annotations, avoiding\nthe biased scoring of LLMs. Moreover, we further develop a robust video MLLM\nbaseline, i.e., VideoChat2, by progressive multi-modal training with diverse\ninstruction-tuning data. The extensive results on our MVBench reveal that, the\nexisting MLLMs are far from satisfactory in temporal understanding, while our\nVideoChat2 largely surpasses these leading models by over 15% on MVBench. All\nmodels and data are available at https:\/\/github.com\/OpenGVLab\/Ask-Anything.","terms":["cs.CV"]},{"titles":"Looking Inside Out: Anticipating Driver Intent From Videos","summaries":"Anticipating driver intention is an important task when vehicles of mixed and\nvarying levels of human\/machine autonomy share roadways. Driver intention can\nbe leveraged to improve road safety, such as warning surrounding vehicles in\nthe event the driver is attempting a dangerous maneuver. In this work, we\npropose a novel method of utilizing in-cabin and external camera data to\nimprove state-of-the-art (SOTA) performance in predicting future driver\nactions. Compared to existing methods, our approach explicitly extracts object\nand road-level features from external camera data, which we demonstrate are\nimportant features for predicting driver intention. Using our handcrafted\nfeatures as inputs for both a transformer and an LSTM-based architecture, we\nempirically show that jointly utilizing in-cabin and external features improves\nperformance compared to using in-cabin features alone. Furthermore, our models\npredict driver maneuvers more accurately and earlier than existing approaches,\nwith an accuracy of 87.5% and an average prediction time of 4.35 seconds before\nthe maneuver takes place. We release our model configurations and training\nscripts on https:\/\/github.com\/ykung83\/Driver-Intent-Prediction","terms":["cs.CV","cs.HC"]},{"titles":"Debiasing Conditional Stochastic Optimization","summaries":"In this paper, we study the conditional stochastic optimization (CSO) problem\nwhich covers a variety of applications including portfolio selection,\nreinforcement learning, robust learning, causal inference, etc. The\nsample-averaged gradient of the CSO objective is biased due to its nested\nstructure, and therefore requires a high sample complexity for convergence. We\nintroduce a general stochastic extrapolation technique that effectively reduces\nthe bias. We show that for nonconvex smooth objectives, combining this\nextrapolation with variance reduction techniques can achieve a significantly\nbetter sample complexity than the existing bounds. Additionally, we develop new\nalgorithms for the finite-sum variant of the CSO problem that also\nsignificantly improve upon existing results. Finally, we believe that our\ndebiasing technique has the potential to be a useful tool for addressing\nsimilar challenges in other stochastic optimization problems.","terms":["cs.LG","stat.ML"]},{"titles":"Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data","summaries":"We present novel cross-sectional and longitudinal claim count models for\nvehicle insurance built upon the Combined Actuarial Neural Network (CANN)\nframework proposed by Mario W\\\"uthrich and Michael Merz. The CANN approach\ncombines a classical actuarial model, such as a generalized linear model, with\na neural network. This blending of models results in a two-component model\ncomprising a classical regression model and a neural network part. The CANN\nmodel leverages the strengths of both components, providing a solid foundation\nand interpretability from the classical model while harnessing the flexibility\nand capacity to capture intricate relationships and interactions offered by the\nneural network. In our proposed models, we use well-known log-linear claim\ncount regression models for the classical regression part and a multilayer\nperceptron (MLP) for the neural network part. The MLP part is used to process\ntelematics car driving data given as a vector characterizing the driving\nbehavior of each insured driver. In addition to the Poisson and negative\nbinomial distributions for cross-sectional data, we propose a procedure for\ntraining our CANN model with a multivariate negative binomial (MVNB)\nspecification. By doing so, we introduce a longitudinal model that accounts for\nthe dependence between contracts from the same insured. Our results reveal that\nthe CANN models exhibit superior performance compared to log-linear models that\nrely on manually engineered telematics features.","terms":["stat.ML","cs.LG"]},{"titles":"Automatic Report Generation for Histopathology images using pre-trained Vision Transformers and BERT","summaries":"Deep learning for histopathology has been successfully used for disease\nclassification, image segmentation and more. However, combining image and text\nmodalities using current state-of-the-art methods has been a challenge due to\nthe high resolution of histopathology images. Automatic report generation for\nhistopathology images is one such challenge. In this work, we show that using\nan existing pre-trained Vision Transformer in a two-step process of first using\nit to encode 4096x4096 sized patches of the Whole Slide Image (WSI) and then\nusing it as the encoder and a pre-trained Bidirectional Encoder Representations\nfrom Transformers (BERT) model for language modeling-based decoder for report\ngeneration, we can build a fairly performant and portable report generation\nmechanism that takes into account the whole of the high resolution image,\ninstead of just the patches. Our method allows us to not only generate and\nevaluate captions that describe the image, but also helps us classify the image\ninto tissue types and the gender of the patient as well. Our best performing\nmodel achieves a 79.98% accuracy in Tissue Type classification and 66.36%\naccuracy in classifying the sex of the patient the tissue came from, with a\nBLEU-4 score of 0.5818 in our caption generation task.","terms":["cs.CV"]},{"titles":"Fast Dual Subgradient Optimization of the Integrated Transportation Distance Between Stochastic Kernels","summaries":"A generalization of the Wasserstein metric, the integrated transportation\ndistance, establishes a novel distance between probability kernels of Markov\nsystems. This metric serves as the foundation for an efficient approximation\ntechnique, enabling the replacement of the original system's kernel with a\nkernel with a discrete support of limited cardinality. To facilitate practical\nimplementation, we present a specialized dual algorithm capable of constructing\nthese approximate kernels quickly and efficiently, without requiring\ncomputationally expensive matrix operations. Finally, we demonstrate the\nefficacy of our method through several illustrative examples, showcasing its\nutility in practical scenarios. This advancement offers new possibilities for\nthe streamlined analysis and manipulation of stochastic systems represented by\nkernels.","terms":["cs.LG","math.OC"]},{"titles":"D$^2$ST-Adapter: Disentangled-and-Deformable Spatio-Temporal Adapter for Few-shot Action Recognition","summaries":"Adapting large pre-trained image models to few-shot action recognition has\nproven to be an effective and efficient strategy for learning robust feature\nextractors, which is essential for few-shot learning. Typical fine-tuning based\nadaptation paradigm is prone to overfitting in the few-shot learning scenarios\nand offers little modeling flexibility for learning temporal features in video\ndata. In this work we present the Disentangled-and-Deformable Spatio-Temporal\nAdapter (D$^2$ST-Adapter), a novel adapter tuning framework for few-shot action\nrecognition, which is designed in a dual-pathway architecture to encode spatial\nand temporal features in a disentangled manner. Furthermore, we devise the\nDeformable Spatio-Temporal Attention module as the core component of\nD$^2$ST-Adapter, which can be tailored to model both spatial and temporal\nfeatures in corresponding pathways, allowing our D$^2$ST-Adapter to encode\nfeatures in a global view in 3D spatio-temporal space while maintaining a\nlightweight design. Extensive experiments with instantiations of our method on\nboth pre-trained ResNet and ViT demonstrate the superiority of our method over\nstate-of-the-art methods for few-shot action recognition. Our method is\nparticularly well-suited to challenging scenarios where temporal dynamics are\ncritical for action recognition.","terms":["cs.CV"]},{"titles":"Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars","summaries":"Interpretability methods aim to understand the algorithm implemented by a\ntrained model (e.g., a Transofmer) by examining various aspects of the model,\nsuch as the weight matrices or the attention patterns. In this work, through a\ncombination of theoretical results and carefully controlled experiments on\nsynthetic data, we take a critical view of methods that exclusively focus on\nindividual parts of the model, rather than consider the network as a whole. We\nconsider a simple synthetic setup of learning a (bounded) Dyck language.\nTheoretically, we show that the set of models that (exactly or approximately)\nsolve this task satisfy a structural characterization derived from ideas in\nformal languages (the pumping lemma). We use this characterization to show that\nthe set of optima is qualitatively rich; in particular, the attention pattern\nof a single layer can be ``nearly randomized'', while preserving the\nfunctionality of the network. We also show via extensive experiments that these\nconstructions are not merely a theoretical artifact: even after severely\nconstraining the architecture of the model, vastly different solutions can be\nreached via standard training. Thus, interpretability claims based on\ninspecting individual heads or weight matrices in the Transformer can be\nmisleading.","terms":["cs.LG","cs.CL","stat.ML"]},{"titles":"A Text-guided Protein Design Framework","summaries":"Current AI-assisted protein design mainly utilizes protein sequential and\nstructural information. Meanwhile, there exists tremendous knowledge curated by\nhumans in the text format describing proteins' high-level functionalities. Yet,\nwhether the incorporation of such text data can help protein design tasks has\nnot been explored. To bridge this gap, we propose ProteinDT, a multi-modal\nframework that leverages textual descriptions for protein design. ProteinDT\nconsists of three subsequent steps: ProteinCLAP which aligns the representation\nof two modalities, a facilitator that generates the protein representation from\nthe text modality, and a decoder that creates the protein sequences from the\nrepresentation. To train ProteinDT, we construct a large dataset,\nSwissProtCLAP, with 441K text and protein pairs. We quantitatively verify the\neffectiveness of ProteinDT on three challenging tasks: (1) over 90\\% accuracy\nfor text-guided protein generation; (2) best hit ratio on 10 zero-shot\ntext-guided protein editing tasks; (3) superior performance on four out of six\nprotein property prediction benchmarks.","terms":["cs.LG","cs.AI","q-bio.QM","stat.ML"]},{"titles":"WavePlanes: A compact Wavelet representation for Dynamic Neural Radiance Fields","summaries":"Dynamic Neural Radiance Fields (Dynamic NeRF) enhance NeRF technology to\nmodel moving scenes. However, they are resource intensive and challenging to\ncompress. To address this issue, this paper presents WavePlanes, a fast and\nmore compact explicit model. We propose a multi-scale space and space-time\nfeature plane representation using N-level 2-D wavelet coefficients. The\ninverse discrete wavelet transform reconstructs N feature signals at varying\ndetail, which are linearly decoded to approximate the color and density of\nvolumes in a 4-D grid. Exploiting the sparsity of wavelet coefficients, we\ncompress a Hash Map containing only non-zero coefficients and their locations\non each plane. This results in a compressed model size of ~12 MB. Compared with\nstate-of-the-art plane-based models, WavePlanes is up to 15x smaller, less\ncomputationally demanding and achieves comparable results in as little as one\nhour of training - without requiring custom CUDA code or high performance\ncomputing resources. Additionally, we propose new feature fusion schemes that\nwork as well as previously proposed schemes while providing greater\ninterpretability. Our code is available at:\nhttps:\/\/github.com\/azzarelli\/waveplanes\/","terms":["cs.CV","cs.GR"]},{"titles":"Explicit Neural Surfaces: Learning Continuous Geometry With Deformation Fields","summaries":"We introduce Explicit Neural Surfaces (ENS), an efficient surface\nreconstruction method that learns an explicitly defined continuous surface from\nmultiple views. We use a series of neural deformation fields to progressively\ntransform a continuous input surface to a target shape. By sampling meshes as\ndiscrete surface proxies, we train the deformation fields through efficient\ndifferentiable rasterization, and attain a mesh-independent and smooth surface\nrepresentation. By using Laplace-Beltrami eigenfunctions as an intrinsic\npositional encoding alongside standard extrinsic Fourier features, our approach\ncan capture fine surface details. ENS trains 1 to 2 orders of magnitude faster\nand can extract meshes of higher quality compared to implicit representations,\nwhilst maintaining competitive surface reconstruction performance and real-time\ncapabilities. Finally, we apply our approach to learn a collection of objects\nin a single model, and achieve disentangled interpolations between different\nshapes, their surface details, and textures.","terms":["cs.CV","cs.GR","I.4.5; I.2.10; I.3.5"]},{"titles":"Likelihood-Free Gaussian Process for Regression","summaries":"Gaussian process regression can flexibly represent the posterior distribution\nof an interest parameter given sufficient information on the likelihood.\nHowever, in some cases, we have little knowledge regarding the probability\nmodel. For example, when investing in a financial instrument, the probability\nmodel of cash flow is generally unknown. In this paper, we propose a novel\nframework called the likelihood-free Gaussian process (LFGP), which allows\nrepresentation of the posterior distributions of interest parameters for\nscalable problems without directly setting their likelihood functions. The LFGP\nestablishes clusters in which the value of the interest parameter can be\nconsidered approximately identical, and it approximates the likelihood of the\ninterest parameter in each cluster to a Gaussian using the asymptotic normality\nof the maximum likelihood estimator. We expect that the proposed framework will\ncontribute significantly to likelihood-free modeling, particularly by reducing\nthe assumptions for the probability model and the computational costs for\nscalable problems.","terms":["cs.LG","stat.ML"]},{"titles":"Continual Learning with Strong Experience Replay","summaries":"Continual Learning (CL) aims at incrementally learning new tasks without\nforgetting the knowledge acquired from old ones. Experience Replay (ER) is a\nsimple and effective rehearsal-based strategy, which optimizes the model with\ncurrent training data and a subset of old samples stored in a memory buffer. To\nfurther reduce forgetting, recent approaches extend ER with various techniques,\nsuch as model regularization and memory sampling. However, the prediction\nconsistency between the new model and the old one on current training data has\nbeen seldom explored, resulting in less knowledge preserved when few previous\nsamples are available. To address this issue, we propose a CL method with\nStrong Experience Replay (SER), which additionally utilizes future experiences\nmimicked on the current training data, besides distilling past experience from\nthe memory buffer. In our method, the updated model will produce approximate\noutputs as its original ones, which can effectively preserve the acquired\nknowledge. Experimental results on multiple image classification datasets show\nthat our SER method surpasses the state-of-the-art methods by a noticeable\nmargin.","terms":["cs.CV"]},{"titles":"Anomaly Detection with Conditioned Denoising Diffusion Models","summaries":"Traditional reconstruction-based methods have struggled to achieve\ncompetitive performance in anomaly detection. In this paper, we introduce\nDenoising Diffusion Anomaly Detection (DDAD), a novel denoising process for\nimage reconstruction conditioned on a target image. This ensures a coherent\nrestoration that closely resembles the target image. Our anomaly detection\nframework employs the conditioning mechanism, where the target image is set as\nthe input image to guide the denoising process, leading to a defectless\nreconstruction while maintaining nominal patterns. Anomalies are then localised\nvia a pixel-wise and feature-wise comparison of the input and reconstructed\nimage. Finally, to enhance the effectiveness of the feature-wise comparison, we\nintroduce a domain adaptation method that utilises nearly identical generated\nexamples from our conditioned denoising process to fine-tune the pretrained\nfeature extractor. The veracity of DDAD is demonstrated on various datasets\nincluding MVTec and VisA benchmarks, achieving state-of-the-art results of\n\\(99.8 \\%\\) and \\(98.9 \\%\\) image-level AUROC respectively.","terms":["cs.CV"]},{"titles":"Generative Rendering: Controllable 4D-Guided Video Generation with 2D Diffusion Models","summaries":"Traditional 3D content creation tools empower users to bring their\nimagination to life by giving them direct control over a scene's geometry,\nappearance, motion, and camera path. Creating computer-generated videos,\nhowever, is a tedious manual process, which can be automated by emerging\ntext-to-video diffusion models. Despite great promise, video diffusion models\nare difficult to control, hindering a user to apply their own creativity rather\nthan amplifying it. To address this challenge, we present a novel approach that\ncombines the controllability of dynamic 3D meshes with the expressivity and\neditability of emerging diffusion models. For this purpose, our approach takes\nan animated, low-fidelity rendered mesh as input and injects the ground truth\ncorrespondence information obtained from the dynamic mesh into various stages\nof a pre-trained text-to-image generation model to output high-quality and\ntemporally consistent frames. We demonstrate our approach on various examples\nwhere motion can be obtained by animating rigged assets or changing the camera\npath.","terms":["cs.CV","cs.AI","cs.GR"]},{"titles":"Improving In-Context Learning in Diffusion Models with Visual Context-Modulated Prompts","summaries":"In light of the remarkable success of in-context learning in large language\nmodels, its potential extension to the vision domain, particularly with visual\nfoundation models like Stable Diffusion, has sparked considerable interest.\nExisting approaches in visual in-context learning frequently face hurdles such\nas expensive pretraining, limiting frameworks, inadequate visual comprehension,\nand limited adaptability to new tasks. In response to these challenges, we\nintroduce improved Prompt Diffusion (iPromptDiff) in this study. iPromptDiff\nintegrates an end-to-end trained vision encoder that converts visual context\ninto an embedding vector. This vector is subsequently used to modulate the\ntoken embeddings of text prompts. We show that a diffusion-based vision\nfoundation model, when equipped with this visual context-modulated text\nguidance and a standard ControlNet structure, exhibits versatility and\nrobustness across a variety of training tasks and excels in in-context learning\nfor novel vision tasks, such as normal-to-image or image-to-line\ntransformations. The effectiveness of these capabilities relies heavily on a\ndeep visual understanding, which is achieved through relevant visual\ndemonstrations processed by our proposed in-context learning architecture.","terms":["cs.CV"]},{"titles":"VideoRF: Rendering Dynamic Radiance Fields as 2D Feature Video Streams","summaries":"Neural Radiance Fields (NeRFs) excel in photorealistically rendering static\nscenes. However, rendering dynamic, long-duration radiance fields on ubiquitous\ndevices remains challenging, due to data storage and computational constraints.\nIn this paper, we introduce VideoRF, the first approach to enable real-time\nstreaming and rendering of dynamic radiance fields on mobile platforms. At the\ncore is a serialized 2D feature image stream representing the 4D radiance field\nall in one. We introduce a tailored training scheme directly applied to this 2D\ndomain to impose the temporal and spatial redundancy of the feature image\nstream. By leveraging the redundancy, we show that the feature image stream can\nbe efficiently compressed by 2D video codecs, which allows us to exploit video\nhardware accelerators to achieve real-time decoding. On the other hand, based\non the feature image stream, we propose a novel rendering pipeline for VideoRF,\nwhich has specialized space mappings to query radiance properties efficiently.\nPaired with a deferred shading model, VideoRF has the capability of real-time\nrendering on mobile devices thanks to its efficiency. We have developed a\nreal-time interactive player that enables online streaming and rendering of\ndynamic scenes, offering a seamless and immersive free-viewpoint experience\nacross a range of devices, from desktops to mobile phones.","terms":["cs.CV"]},{"titles":"Spatially scalable recursive estimation of Gaussian process terrain maps using local basis functions","summaries":"When an agent, person, vehicle or robot is moving through an unknown\nenvironment without GNSS signals, online mapping of nonlinear terrains can be\nused to improve position estimates when the agent returns to a previously\nmapped area. Mapping algorithms using online Gaussian process (GP) regression\nare commonly integrated in algorithms for simultaneous localisation and mapping\n(SLAM). However, GP mapping algorithms have increasing computational demands as\nthe mapped area expands relative to spatial field variations. This is due to\nthe need for estimating an increasing amount of map parameters as the area of\nthe map grows. Contrary to this, we propose a recursive GP mapping estimation\nalgorithm which uses local basis functions in an information filter to achieve\nspatial scalability. Our proposed approximation employs a global grid of finite\nsupport basis functions but restricts computations to a localized subset around\neach prediction point. As our proposed algorithm is recursive, it can naturally\nbe incorporated into existing algorithms that uses Gaussian process maps for\nSLAM. Incorporating our proposed algorithm into an extended Kalman filter (EKF)\nfor magnetic field SLAM reduces the overall computational complexity of the\nalgorithm. We show experimentally that our algorithm is faster than existing\nmethods when the mapped area is large and the map is based on many\nmeasurements, both for recursive mapping tasks and for magnetic field SLAM.","terms":["cs.LG","stat.ML","I.5.0"]},{"titles":"Visual Prompting Upgrades Neural Network Sparsification: A Data-Model Perspective","summaries":"The rapid development of large-scale deep learning models questions the\naffordability of hardware platforms, which necessitates the pruning to reduce\ntheir computational and memory footprints. Sparse neural networks as the\nproduct, have demonstrated numerous favorable benefits like low complexity,\nundamaged generalization, etc. Most of the prominent pruning strategies are\ninvented from a model-centric perspective, focusing on searching and preserving\ncrucial weights by analyzing network topologies. However, the role of data and\nits interplay with model-centric pruning has remained relatively unexplored. In\nthis research, we introduce a novel data-model co-design perspective: to\npromote superior weight sparsity by learning important model topology and\nadequate input data in a synergetic manner. Specifically, customized Visual\nPrompts are mounted to upgrade neural Network sparsification in our proposed\nVPNs framework. As a pioneering effort, this paper conducts systematic\ninvestigations about the impact of different visual prompts on model pruning\nand suggests an effective joint optimization approach. Extensive experiments\nwith 3 network architectures and 8 datasets evidence the substantial\nperformance improvements from VPNs over existing start-of-the-art pruning\nalgorithms. Furthermore, we find that subnetworks discovered by VPNs from\npre-trained models enjoy better transferability across diverse downstream\nscenarios. These insights shed light on new promising possibilities of\ndata-model co-designs for vision model sparsification.","terms":["cs.CV","cs.LG"]},{"titles":"Do highly over-parameterized neural networks generalize since bad solutions are rare?","summaries":"We study over-parameterized classifiers where Empirical Risk Minimization\n(ERM) for learning leads to zero training error. In these over-parameterized\nsettings there are many global minima with zero training error, some of which\ngeneralize better than others. We show that under certain conditions the\nfraction of \"bad\" global minima with a true error larger than {\\epsilon} decays\nto zero exponentially fast with the number of training data n. The bound\ndepends on the distribution of the true error over the set of classifier\nfunctions used for the given classification problem, and does not necessarily\ndepend on the size or complexity (e.g. the number of parameters) of the\nclassifier function set. This insight may provide a novel perspective on the\nunexpectedly good generalization even of highly over-parameterized neural\nnetworks. We substantiate our theoretical findings through experiments on\nsynthetic data and a subset of MNIST. Additionally, we assess our hypothesis\nusing VGG19 and ResNet18 on a subset of Caltech101.","terms":["cs.LG"]},{"titles":"Neural Network Characterization and Entropy Regulated Data Balancing through Principal Component Analysis","summaries":"This paper examines the relationship between the behavior of a neural network\nand the distribution formed from the projections of the data records into the\nspace spanned by the low-order principal components of the training data. For\nexample, in a benchmark calculation involving rotated and unrotated MNIST\ndigits, classes (digits) that are mapped far from the origin in a\nlow-dimensional principal component space and that overlap minimally with other\ndigits converge rapidly and exhibit high degrees of accuracy in neural network\ncalculations that employ the associated components of each data record as\ninputs. Further, if the space spanned by these low-order principal components\nis divided into bins and the input data records that are mapped into a given\nbin averaged, the resulting pattern can be distinguished by its geometric\nfeatures which interpolate between those of adjacent bins in an analogous\nmanner to variational autoencoders. Based on this observation, a simply\nrealized data balancing procedure can be realized by evaluating the entropy\nassociated with each histogram bin and subsequently repeating the original\nimage data associated with the bin by a number of times that is determined from\nthis entropy.","terms":["cs.LG"]},{"titles":"Linking convolutional kernel size to generalization bias in face analysis CNNs","summaries":"Training dataset biases are by far the most scrutinized factors when\nexplaining algorithmic biases of neural networks. In contrast, hyperparameters\nrelated to the neural network architecture have largely been ignored even\nthough different network parameterizations are known to induce different\nimplicit biases over learned features. For example, convolutional kernel size\nis known to affect the frequency content of features learned in CNNs. In this\nwork, we present a causal framework for linking an architectural hyperparameter\nto out-of-distribution algorithmic bias. Our framework is experimental, in that\nwe train several versions of a network with an intervention to a specific\nhyperparameter, and measure the resulting causal effect of this choice on\nperformance bias when a particular out-of-distribution image perturbation is\napplied. In our experiments, we focused on measuring the causal relationship\nbetween convolutional kernel size and face analysis classification bias across\ndifferent subpopulations (race\/gender), with respect to high-frequency image\ndetails. We show that modifying kernel size, even in one layer of a CNN,\nchanges the frequency content of learned features significantly across data\nsubgroups leading to biased generalization performance even in the presence of\na balanced dataset.","terms":["cs.CV","cs.LG","stat.ME"]},{"titles":"Regret Optimality of GP-UCB","summaries":"Gaussian Process Upper Confidence Bound (GP-UCB) is one of the most popular\nmethods for optimizing black-box functions with noisy observations, due to its\nsimple structure and superior performance. Its empirical successes lead to a\nnatural, yet unresolved question: Is GP-UCB regret optimal? In this paper, we\noffer the first generally affirmative answer to this important open question in\nthe Bayesian optimization literature. We establish new upper bounds on both the\nsimple and cumulative regret of GP-UCB when the objective function to optimize\nadmits certain smoothness property. These upper bounds match the known minimax\nlower bounds (up to logarithmic factors independent of the feasible region's\ndimensionality) for optimizing functions with the same smoothness.\nIntriguingly, our findings indicate that, with the same level of exploration,\nGP-UCB can simultaneously achieve optimality in both simple and cumulative\nregret. The crux of our analysis hinges on a refined uniform error bound for\nonline estimation of functions in reproducing kernel Hilbert spaces. This error\nbound, which we derive from empirical process theory, is of independent\ninterest, and its potential applications may reach beyond the scope of this\nstudy.","terms":["cs.LG","stat.ML"]},{"titles":"Language-driven All-in-one Adverse Weather Removal","summaries":"All-in-one (AiO) frameworks restore various adverse weather degradations with\na single set of networks jointly. To handle various weather conditions, an AiO\nframework is expected to adaptively learn weather-specific knowledge for\ndifferent degradations and shared knowledge for common patterns. However,\nexisting methods: 1) rely on extra supervision signals, which are usually\nunknown in real-world applications; 2) employ fixed network structures, which\nrestrict the diversity of weather-specific knowledge. In this paper, we propose\na Language-driven Restoration framework (LDR) to alleviate the aforementioned\nissues. First, we leverage the power of pre-trained vision-language (PVL)\nmodels to enrich the diversity of weather-specific knowledge by reasoning about\nthe occurrence, type, and severity of degradation, generating description-based\ndegradation priors. Then, with the guidance of degradation prior, we sparsely\nselect restoration experts from a candidate list dynamically based on a\nMixture-of-Experts (MoE) structure. This enables us to adaptively learn the\nweather-specific and shared knowledge to handle various weather conditions\n(e.g., unknown or mixed weather). Experiments on extensive restoration\nscenarios show our superior performance (see Fig. 1). The source code will be\nmade available.","terms":["cs.CV"]},{"titles":"A Conditional Denoising Diffusion Probabilistic Model for Point Cloud Upsampling","summaries":"Point cloud upsampling (PCU) enriches the representation of raw point clouds,\nsignificantly improving the performance in downstream tasks such as\nclassification and reconstruction. Most of the existing point cloud upsampling\nmethods focus on sparse point cloud feature extraction and upsampling module\ndesign. In a different way, we dive deeper into directly modelling the gradient\nof data distribution from dense point clouds. In this paper, we proposed a\nconditional denoising diffusion probability model (DDPM) for point cloud\nupsampling, called PUDM. Specifically, PUDM treats the sparse point cloud as a\ncondition, and iteratively learns the transformation relationship between the\ndense point cloud and the noise. Simultaneously, PUDM aligns with a dual\nmapping paradigm to further improve the discernment of point features. In this\ncontext, PUDM enables learning complex geometry details in the ground truth\nthrough the dominant features, while avoiding an additional upsampling module\ndesign. Furthermore, to generate high-quality arbitrary-scale point clouds\nduring inference, PUDM exploits the prior knowledge of the scale between sparse\npoint clouds and dense point clouds during training by parameterizing a rate\nfactor. Moreover, PUDM exhibits strong noise robustness in experimental\nresults. In the quantitative and qualitative evaluations on PU1K and PUGAN,\nPUDM significantly outperformed existing methods in terms of Chamfer Distance\n(CD) and Hausdorff Distance (HD), achieving state of the art (SOTA)\nperformance.","terms":["cs.CV"]},{"titles":"Space-Time Diffusion Features for Zero-Shot Text-Driven Motion Transfer","summaries":"We present a new method for text-driven motion transfer - synthesizing a\nvideo that complies with an input text prompt describing the target objects and\nscene while maintaining an input video's motion and scene layout. Prior methods\nare confined to transferring motion across two subjects within the same or\nclosely related object categories and are applicable for limited domains (e.g.,\nhumans). In this work, we consider a significantly more challenging setting in\nwhich the target and source objects differ drastically in shape and\nfine-grained motion characteristics (e.g., translating a jumping dog into a\ndolphin). To this end, we leverage a pre-trained and fixed text-to-video\ndiffusion model, which provides us with generative and motion priors. The\npillar of our method is a new space-time feature loss derived directly from the\nmodel. This loss guides the generation process to preserve the overall motion\nof the input video while complying with the target object in terms of shape and\nfine-grained motion traits.","terms":["cs.CV"]},{"titles":"DiFace: Cross-Modal Face Recognition through Controlled Diffusion","summaries":"Diffusion probabilistic models (DPMs) have exhibited exceptional proficiency\nin generating visual media of outstanding quality and realism. Nonetheless,\ntheir potential in non-generative domains, such as face recognition, has yet to\nbe thoroughly investigated. Meanwhile, despite the extensive development of\nmulti-modal face recognition methods, their emphasis has predominantly centered\non visual modalities. In this context, face recognition through textual\ndescription presents a unique and promising solution that not only transcends\nthe limitations from application scenarios but also expands the potential for\nresearch in the field of cross-modal face recognition. It is regrettable that\nthis avenue remains unexplored and underutilized, a consequence from the\nchallenges mainly associated with three aspects: 1) the intrinsic imprecision\nof verbal descriptions; 2) the significant gaps between texts and images; and\n3) the immense hurdle posed by insufficient databases.To tackle this problem,\nwe present DiFace, a solution that effectively achieves face recognition via\ntext through a controllable diffusion process, by establishing its theoretical\nconnection with probability transport. Our approach not only unleashes the\npotential of DPMs across a broader spectrum of tasks but also achieves, to the\nbest of our knowledge, a significant accuracy in text-to-image face recognition\nfor the first time, as demonstrated by our experiments on verification and\nidentification.","terms":["cs.CV","cs.AI"]},{"titles":"HallE-Switch: Controlling Object Hallucination in Large Vision Language Models","summaries":"Current large vision-language models (LVLMs) achieve remarkable progress, yet\nthere remains significant uncertainty regarding their ability to accurately\napprehend visual details, that is, in performing detailed captioning. To\naddress this, we introduce $\\textit{CCEval}$, a GPT-4 assisted evaluation\nmethod tailored for detailed captioning. Interestingly, while LVLMs demonstrate\nminimal object existence hallucination in existing VQA benchmarks, our proposed\nevaluation reveals continued susceptibility to such hallucinations. In this\npaper, we make the first attempt to investigate such hallucination from\ndifferent aspects, including image resolution, the language decoder size, and\ninstruction data amount, quality, granularity. Our findings underscore the\nunwarranted inference when the language description includes details at a finer\nobject granularity than what the vision module can ground or verify, thus\ninducing hallucination. To control such hallucinations, we further attribute\nthe reliability of captioning to contextual knowledge (involving only\ncontextually grounded objects) and parametric knowledge (containing inferred\nobjects by the model). Thus, we introduce $\\textit{HallE-Switch}$, a\ncontrollable LVLM in terms of $\\textbf{Hall}$ucination in object\n$\\textbf{E}$xistence. HallE-Switch can condition the captioning to shift\nbetween (i) exclusively depicting contextual knowledge for grounded objects and\n(ii) blending it with parametric knowledge to imagine inferred objects. Our\nmethod reduces hallucination by 44% compared to LLaVA$_{7B}$ and maintains the\nsame object coverage.","terms":["cs.CV"]},{"titles":"MoEC: Mixture of Experts Implicit Neural Compression","summaries":"Emerging Implicit Neural Representation (INR) is a promising data compression\ntechnique, which represents the data using the parameters of a Deep Neural\nNetwork (DNN). Existing methods manually partition a complex scene into local\nregions and overfit the INRs into those regions. However, manually designing\nthe partition scheme for a complex scene is very challenging and fails to\njointly learn the partition and INRs. To solve the problem, we propose MoEC, a\nnovel implicit neural compression method based on the theory of mixture of\nexperts. Specifically, we use a gating network to automatically assign a\nspecific INR to a 3D point in the scene. The gating network is trained jointly\nwith the INRs of different local regions. Compared with block-wise and\ntree-structured partitions, our learnable partition can adaptively find the\noptimal partition in an end-to-end manner. We conduct detailed experiments on\nmassive and diverse biomedical data to demonstrate the advantages of MoEC\nagainst existing approaches. In most of experiment settings, we have achieved\nstate-of-the-art results. Especially in cases of extreme compression ratios,\nsuch as 6000x, we are able to uphold the PSNR of 48.16.","terms":["cs.CV","cs.LG","eess.IV"]},{"titles":"Analyze the robustness of three NMF algorithms (Robust NMF with L1 norm, L2-1 norm NMF, L2 NMF)","summaries":"Non-negative matrix factorization (NMF) and its variants have been widely\nemployed in clustering and classification tasks (Long, & Jian , 2021). However,\nnoises can seriously affect the results of our experiments. Our research is\ndedicated to investigating the noise robustness of non-negative matrix\nfactorization (NMF) in the face of different types of noise. Specifically, we\nadopt three different NMF algorithms, namely L1 NMF, L2 NMF, and L21 NMF, and\nuse the ORL and YaleB data sets to simulate a series of experiments with\nsalt-and-pepper noise and Block-occlusion noise separately. In the experiment,\nwe use a variety of evaluation indicators, including root mean square error\n(RMSE), accuracy (ACC), and normalized mutual information (NMI), to evaluate\nthe performance of different NMF algorithms in noisy environments. Through\nthese indicators, we quantify the resistance of NMF algorithms to noise and\ngain insights into their feasibility in practical applications.","terms":["cs.LG","cs.AI"]},{"titles":"tsMorph: generation of semi-synthetic time series to understand algorithm performance","summaries":"Time series forecasting is a subject of significant scientific and industrial\nimportance. Despite the widespread utilization of forecasting methods, there is\na dearth of research aimed at comprehending the conditions under which these\nmethods yield favorable or unfavorable performances. Empirical studies,\nalthough common, encounter challenges due to the limited availability of\ndatasets, impeding the extraction of reliable insights. To address this, we\npresent tsMorph, a straightforward approach for generating semi-synthetic time\nseries through dataset morphing. tsMorph operates by creating a sequence of\ndatasets derived from two original datasets. These newly generated datasets\nexhibit a progressive departure from the characteristics of one dataset and a\nconvergence toward the attributes of the other. This method provides a valuable\nalternative for obtaining substantial datasets. In this paper, we demonstrate\nthe utility of tsMorph by assessing the performance of the Long Short-Term\nMemory Network forecasting algorithm. The time series under examination are\nsourced from the NN5 Competition. The findings reveal compelling insights.\nNotably, the performance of the Long Short-Term Memory Network improves\nproportionally with the frequency of the time series. These experiments affirm\nthat tsMorph serves as an effective tool for gaining an understanding of\nforecasting algorithm behaviors, offering a pathway to overcome the limitations\nposed by empirical studies and enabling more extensive and reliable\nexperimentation.","terms":["cs.LG","cs.AI"]},{"titles":"Graph Coordinates and Conventional Neural Networks -- An Alternative for Graph Neural Networks","summaries":"Graph-based data present unique challenges and opportunities for machine\nlearning. Graph Neural Networks (GNNs), and especially those algorithms that\ncapture graph topology through message passing for neighborhood aggregation,\nhave been a leading solution. However, these networks often require substantial\ncomputational resources and may not optimally leverage the information\ncontained in the graph's topology, particularly for large-scale or complex\ngraphs. We propose Topology Coordinate Neural Network (TCNN) and Directional\nVirtual Coordinate Neural Network (DVCNN) as novel and efficient alternatives\nto message passing GNNs, that directly leverage the graph's topology,\nsidestepping the computational challenges presented by competing algorithms.\nOur proposed methods can be viewed as a reprise of classic techniques for graph\nembedding for neural network feature engineering, but they are novel in that\nour embedding techniques leverage ideas in Graph Coordinates (GC) that are\nlacking in current practice. Experimental results, benchmarked against the Open\nGraph Benchmark Leaderboard, demonstrate that TCNN and DVCNN achieve\ncompetitive or superior performance to message passing GNNs. For similar levels\nof accuracy and ROC-AUC, TCNN and DVCNN need far fewer trainable parameters\nthan contenders of the OGBN Leaderboard. The proposed TCNN architecture\nrequires fewer parameters than any neural network method currently listed in\nthe OGBN Leaderboard for both OGBN-Proteins and OGBN-Products datasets.\nConversely, our methods achieve higher performance for a similar number of\ntrainable parameters. By providing an efficient and effective alternative to\nmessage passing GNNs, our work expands the toolbox of techniques for\ngraph-based machine learning.","terms":["cs.LG"]},{"titles":"CARLA: Self-supervised Contrastive Representation Learning for Time Series Anomaly Detection","summaries":"One main challenge in time series anomaly detection (TAD) is the lack of\nlabelled data in many real-life scenarios. Most of the existing anomaly\ndetection methods focus on learning the normal behaviour of unlabelled time\nseries in an unsupervised manner. The normal boundary is often defined tightly,\nresulting in slight deviations being classified as anomalies, consequently\nleading to a high false positive rate and a limited ability to generalise\nnormal patterns. To address this, we introduce a novel end-to-end\nself-supervised ContrAstive Representation Learning approach for time series\nAnomaly detection (CARLA). While existing contrastive learning methods assume\nthat augmented time series windows are positive samples and temporally distant\nwindows are negative samples, we argue that these assumptions are limited as\naugmentation of time series can transform them to negative samples, and a\ntemporally distant window can represent a positive sample. Our contrastive\napproach leverages existing generic knowledge about time series anomalies and\ninjects various types of anomalies as negative samples. Therefore, CARLA not\nonly learns normal behaviour but also learns deviations indicating anomalies.\nIt creates similar representations for temporally closed windows and distinct\nones for anomalies. Additionally, it leverages the information about\nrepresentations' neighbours through a self-supervised approach to classify\nwindows based on their nearest\/furthest neighbours to further enhance the\nperformance of anomaly detection. In extensive tests on seven major real-world\ntime series anomaly detection datasets, CARLA shows superior performance over\nstate-of-the-art self-supervised and unsupervised TAD methods. Our research\nshows the potential of contrastive representation learning to advance time\nseries anomaly detection.","terms":["cs.LG","cs.NE"]},{"titles":"Facial Emotion Recognition Under Mask Coverage Using a Data Augmentation Technique","summaries":"Identifying human emotions using AI-based computer vision systems, when\nindividuals wear face masks, presents a new challenge in the current Covid-19\npandemic. In this study, we propose a facial emotion recognition system capable\nof recognizing emotions from individuals wearing different face masks. A novel\ndata augmentation technique was utilized to improve the performance of our\nmodel using four mask types for each face image. We evaluated the effectiveness\nof four convolutional neural networks, Alexnet, Squeezenet, Resnet50 and\nVGGFace2 that were trained using transfer learning. The experimental findings\nrevealed that our model works effectively in multi-mask mode compared to\nsingle-mask mode. The VGGFace2 network achieved the highest accuracy rate, with\n97.82% for the person-dependent mode and 74.21% for the person-independent mode\nusing the JAFFE dataset. However, we evaluated our proposed model using the\nUIBVFED dataset. The Resnet50 has demonstrated superior performance, with\naccuracies of 73.68% for the person-dependent mode and 59.57% for the\nperson-independent mode. Moreover, we employed metrics such as precision,\nsensitivity, specificity, AUC, F1 score, and confusion matrix to measure our\nsystem's efficiency in detail. Additionally, the LIME algorithm was used to\nvisualize CNN's decision-making strategy.","terms":["cs.CV","cs.AI","cs.HC"]},{"titles":"Efficient IoT Inference via Context-Awareness","summaries":"While existing strategies to execute deep learning-based classification on\nlow-power platforms assume the models are trained on all classes of interest,\nthis paper posits that adopting context-awareness i.e. narrowing down a\nclassification task to the current deployment context consisting of only recent\ninference queries can substantially enhance performance in resource-constrained\nenvironments. We propose a new paradigm, CACTUS, for scalable and efficient\ncontext-aware classification where a micro-classifier recognizes a small set of\nclasses relevant to the current context and, when context change happens (e.g.,\na new class comes into the scene), rapidly switches to another suitable\nmicro-classifier. CACTUS features several innovations, including optimizing the\ntraining cost of context-aware classifiers, enabling on-the-fly context-aware\nswitching between classifiers, and balancing context switching costs and\nperformance gains via simple yet effective switching policies. We show that\nCACTUS achieves significant benefits in accuracy, latency, and compute budget\nacross a range of datasets and IoT platforms.","terms":["cs.CV","cs.AI","cs.LG"]},{"titles":"MABViT -- Modified Attention Block Enhances Vision Transformers","summaries":"Recent studies have demonstrated the effectiveness of Gated Linear Units\n(GLU) in enhancing transformer models, particularly in Large Language Models\n(LLMs). Additionally, utilizing a parallel configuration within each\nTransformer block rather than the conventional serialized method has been\nrevealed to accelerate the training of LLMs without significantly impacting\nperformance. However, when the MLP and attention block were run in parallel for\nthe image classification task, we observed a noticeable decline in performance.\nWe propose a novel transformer variant that integrates non-linearity within the\nattention block to tackle this problem. We implemented the GLU-based activation\nfunction on the Value tensor, and this new technique surpasses the current\nstate-of-the-art S\/16 variant of Vision Transformers by 0.6% on the ImageNet-1K\ndataset while utilizing fewer parameters. It also supersedes the B\/16 variant\nwhile using only half the parameters. Furthermore, we provide results with the\nGELU activation function variant to confirm our assertions. Lastly, we showcase\nthat the MABViT variants exhibit greater potential when utilized in deep\ntransformers compared to the standard architecture.","terms":["cs.CV","cs.LG"]},{"titles":"A Theoretical Perspective of Machine Learning with Computational Resource Concerns","summaries":"Conventional theoretical machine learning studies generally assume explicitly\nor implicitly that there are enough or even infinitely supplied computational\nresources. In real practice, however, computational resources are usually\nlimited, and the performance of machine learning depends not only on how many\ndata have been received, but also on how many data can be handled with the\ncomputational resources available. Note that most current ``intelligent\nsupercomputing'' facilities work like exclusive operating systems, where a\nfixed amount of resources are allocated to a machine learning task without\nadaptive scheduling strategies considering important factors such as learning\nperformance demands and learning process status. In this article, we introduce\nthe notion of machine learning throughput, define Computational Resource\nEfficient Learning (CoRE-Learning) and present a theoretical framework that\ntakes into account the influence of computational resources in learning theory.\nThis framework can be naturally applied to stream learning where the incoming\ndata streams can be potentially endless with overwhelming size and it is\nimpractical to assume that all received data can be handled in time. It may\nalso provide a theoretical perspective for the design of intelligent\nsupercomputing operating systems.","terms":["cs.LG"]},{"titles":"ALSTER: A Local Spatio-Temporal Expert for Online 3D Semantic Reconstruction","summaries":"We propose an online 3D semantic segmentation method that incrementally\nreconstructs a 3D semantic map from a stream of RGB-D frames. Unlike offline\nmethods, ours is directly applicable to scenarios with real-time constraints,\nsuch as robotics or mixed reality. To overcome the inherent challenges of\nonline methods, we make two main contributions. First, to effectively extract\ninformation from the input RGB-D video stream, we jointly estimate geometry and\nsemantic labels per frame in 3D. A key focus of our approach is to reason about\nsemantic entities both in the 2D input and the local 3D domain to leverage\ndifferences in spatial context and network architectures. Our method predicts\n2D features using an off-the-shelf segmentation network. The extracted 2D\nfeatures are refined by a lightweight 3D network to enable reasoning about the\nlocal 3D structure. Second, to efficiently deal with an infinite stream of\ninput RGB-D frames, a subsequent network serves as a temporal expert predicting\nthe incremental scene updates by leveraging 2D, 3D, and past information in a\nlearned manner. These updates are then integrated into a global scene\nrepresentation. Using these main contributions, our method can enable scenarios\nwith real-time constraints and can scale to arbitrary scene sizes by processing\nand updating the scene only in a local region defined by the new measurement.\nOur experiments demonstrate improved results compared to existing online\nmethods that purely operate in local regions and show that complementary\nsources of information can boost the performance. We provide a thorough\nablation study on the benefits of different architectural as well as\nalgorithmic design decisions. Our method yields competitive results on the\npopular ScanNet benchmark and SceneNN dataset.","terms":["cs.CV"]},{"titles":"Time-Series Forecasting: Unleashing Long-Term Dependencies with Fractionally Differenced Data","summaries":"This study introduces a novel forecasting strategy that leverages the power\nof fractional differencing (FD) to capture both short- and long-term\ndependencies in time series data. Unlike traditional integer differencing\nmethods, FD preserves memory in series while stabilizing it for modeling\npurposes. By applying FD to financial data from the SPY index and incorporating\nsentiment analysis from news reports, this empirical analysis explores the\neffectiveness of FD in conjunction with binary classification of target\nvariables. Supervised classification algorithms were employed to validate the\nperformance of FD series. The results demonstrate the superiority of FD over\ninteger differencing, as confirmed by Receiver Operating Characteristic\/Area\nUnder the Curve (ROCAUC) and Mathews Correlation Coefficient (MCC) evaluations.","terms":["cs.LG","cs.AI","cs.NA","math.NA","math.ST","stat.TH"]},{"titles":"Few-shot Shape Recognition by Learning Deep Shape-aware Features","summaries":"Traditional shape descriptors have been gradually replaced by convolutional\nneural networks due to their superior performance in feature extraction and\nclassification. The state-of-the-art methods recognize object shapes via image\nreconstruction or pixel classification. However , these methods are biased\ntoward texture information and overlook the essential shape descriptions, thus,\nthey fail to generalize to unseen shapes. We are the first to propose a fewshot\nshape descriptor (FSSD) to recognize object shapes given only one or a few\nsamples. We employ an embedding module for FSSD to extract\ntransformation-invariant shape features. Secondly, we develop a dual attention\nmechanism to decompose and reconstruct the shape features via learnable shape\nprimitives. In this way, any shape can be formed through a finite set basis,\nand the learned representation model is highly interpretable and extendable to\nunseen shapes. Thirdly, we propose a decoding module to include the supervision\nof shape masks and edges and align the original and reconstructed shape\nfeatures, enforcing the learned features to be more shape-aware. Lastly, all\nthe proposed modules are assembled into a few-shot shape recognition scheme.\nExperiments on five datasets show that our FSSD significantly improves the\nshape classification compared to the state-of-the-art under the few-shot\nsetting.","terms":["cs.CV"]},{"titles":"AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection","summaries":"Zero-shot anomaly detection (ZSAD) requires detection models trained using\nauxiliary data to detect anomalies without any training sample in a target\ndataset. It is a crucial task when training data is not accessible due to\nvarious concerns, \\eg, data privacy, yet it is challenging since the models\nneed to generalize to anomalies across different domains where the appearance\nof foreground objects, abnormal regions, and background features, such as\ndefects\/tumors on different products\/organs, can vary significantly. Recently\nlarge pre-trained vision-language models (VLMs), such as CLIP, have\ndemonstrated strong zero-shot recognition ability in various vision tasks,\nincluding anomaly detection. However, their ZSAD performance is weak since the\nVLMs focus more on modeling the class semantics of the foreground objects\nrather than the abnormality\/normality in the images. In this paper we introduce\na novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across\ndifferent domains. The key insight of AnomalyCLIP is to learn object-agnostic\ntext prompts that capture generic normality and abnormality in an image\nregardless of its foreground objects. This allows our model to focus on the\nabnormal image regions rather than the object semantics, enabling generalized\nnormality and abnormality recognition on diverse types of objects. Large-scale\nexperiments on 17 real-world anomaly detection datasets show that AnomalyCLIP\nachieves superior zero-shot performance of detecting and segmenting anomalies\nin datasets of highly diverse class semantics from various defect inspection\nand medical imaging domains. Code will be made available at\nhttps:\/\/github.com\/zqhang\/AnomalyCLIP.","terms":["cs.CV"]},{"titles":"FlashAvatar: High-Fidelity Digital Avatar Rendering at 300FPS","summaries":"We propose FlashAvatar, a novel and lightweight 3D animatable avatar\nrepresentation that could reconstruct a digital avatar from a short monocular\nvideo sequence in minutes and render high-fidelity photo-realistic images at\n300FPS on a consumer-grade GPU. To achieve this, we maintain a uniform 3D\nGaussian field embedded in the surface of a parametric face model and learn\nextra spatial offset to model non-surface regions and subtle facial details.\nWhile full use of geometric priors can capture high-frequency facial details\nand preserve exaggerated expressions, proper initialization can help reduce the\nnumber of Gaussians, thus enabling super-fast rendering speed. Extensive\nexperimental results demonstrate that FlashAvatar outperforms existing works\nregarding visual quality and personalized details and is almost an order of\nmagnitude faster in rendering speed. Project page:\nhttps:\/\/ustc3dv.github.io\/FlashAvatar\/","terms":["cs.CV","cs.GR"]},{"titles":"Biphasic Face Photo-Sketch Synthesis via Semantic-Driven Generative Adversarial Network with Graph Representation Learning","summaries":"Biphasic face photo-sketch synthesis has significant practical value in\nwide-ranging fields such as digital entertainment and law enforcement. Previous\napproaches directly generate the photo-sketch in a global view, they always\nsuffer from the low quality of sketches and complex photo variations, leading\nto unnatural and low-fidelity results. In this paper, we propose a novel\nSemantic-Driven Generative Adversarial Network to address the above issues,\ncooperating with Graph Representation Learning. Considering that human faces\nhave distinct spatial structures, we first inject class-wise semantic layouts\ninto the generator to provide style-based spatial information for synthesized\nface photos and sketches. Additionally, to enhance the authenticity of details\nin generated faces, we construct two types of representational graphs via\nsemantic parsing maps upon input faces, dubbed the IntrA-class Semantic Graph\n(IASG) and the InteR-class Structure Graph (IRSG). Specifically, the IASG\neffectively models the intra-class semantic correlations of each facial\nsemantic component, thus producing realistic facial details. To preserve the\ngenerated faces being more structure-coordinated, the IRSG models inter-class\nstructural relations among every facial component by graph representation\nlearning. To further enhance the perceptual quality of synthesized images, we\npresent a biphasic interactive cycle training strategy by fully taking\nadvantage of the multi-level feature consistency between the photo and sketch.\nExtensive experiments demonstrate that our method outperforms the\nstate-of-the-art competitors on the CUFS and CUFSF datasets.","terms":["cs.CV"]},{"titles":"JarviX: A LLM No code Platform for Tabular Data Analysis and Optimization","summaries":"In this study, we introduce JarviX, a sophisticated data analytics framework.\nJarviX is designed to employ Large Language Models (LLMs) to facilitate an\nautomated guide and execute high-precision data analyzes on tabular datasets.\nThis framework emphasizes the significance of varying column types,\ncapitalizing on state-of-the-art LLMs to generate concise data insight\nsummaries, propose relevant analysis inquiries, visualize data effectively, and\nprovide comprehensive explanations for results drawn from an extensive data\nanalysis pipeline. Moreover, JarviX incorporates an automated machine learning\n(AutoML) pipeline for predictive modeling. This integration forms a\ncomprehensive and automated optimization cycle, which proves particularly\nadvantageous for optimizing machine configuration. The efficacy and\nadaptability of JarviX are substantiated through a series of practical use case\nstudies.","terms":["cs.LG","cs.AI","cs.DB","stat.AP"]},{"titles":"ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models","summaries":"Generating novel views of an object from a single image is a challenging\ntask. It requires an understanding of the underlying 3D structure of the object\nfrom an image and rendering high-quality, spatially consistent new views. While\nrecent methods for view synthesis based on diffusion have shown great progress,\nachieving consistency among various view estimates and at the same time abiding\nby the desired camera pose remains a critical problem yet to be solved. In this\nwork, we demonstrate a strikingly simple method, where we utilize a pre-trained\nvideo diffusion model to solve this problem. Our key idea is that synthesizing\na novel view could be reformulated as synthesizing a video of a camera going\naround the object of interest -- a scanning video -- which then allows us to\nleverage the powerful priors that a video diffusion model would have learned.\nThus, to perform novel-view synthesis, we create a smooth camera trajectory to\nthe target view that we wish to render, and denoise using both a\nview-conditioned diffusion model and a video diffusion model. By doing so, we\nobtain a highly consistent novel view synthesis, outperforming the state of the\nart.","terms":["cs.CV","cs.AI","cs.GR"]},{"titles":"Portrait Diffusion: Training-free Face Stylization with Chain-of-Painting","summaries":"Face stylization refers to the transformation of a face into a specific\nportrait style. However, current methods require the use of example-based\nadaptation approaches to fine-tune pre-trained generative models so that they\ndemand lots of time and storage space and fail to achieve detailed style\ntransformation. This paper proposes a training-free face stylization framework,\nnamed Portrait Diffusion. This framework leverages off-the-shelf text-to-image\ndiffusion models, eliminating the need for fine-tuning specific examples.\nSpecifically, the content and style images are first inverted into latent\ncodes. Then, during image reconstruction using the corresponding latent code,\nthe content and style features in the attention space are delicately blended\nthrough a modified self-attention operation called Style Attention Control.\nAdditionally, a Chain-of-Painting method is proposed for the gradual redrawing\nof unsatisfactory areas from rough adjustments to fine-tuning. Extensive\nexperiments validate the effectiveness of our Portrait Diffusion method and\ndemonstrate the superiority of Chain-of-Painting in achieving precise face\nstylization. Code will be released at\n\\url{https:\/\/github.com\/liujin112\/PortraitDiffusion}.","terms":["cs.CV"]},{"titles":"On the Temperature of Bayesian Graph Neural Networks for Conformal Prediction","summaries":"Accurate uncertainty quantification in graph neural networks (GNNs) is\nessential, especially in high-stakes domains where GNNs are frequently\nemployed. Conformal prediction (CP) offers a promising framework for\nquantifying uncertainty by providing $\\textit{valid}$ prediction sets for any\nblack-box model. CP ensures formal probabilistic guarantees that a prediction\nset contains a true label with a desired probability. However, the size of\nprediction sets, known as $\\textit{inefficiency}$, is influenced by the\nunderlying model and data generating process. On the other hand, Bayesian\nlearning also provides a credible region based on the estimated posterior\ndistribution, but this region is $\\textit{well-calibrated}$ only when the model\nis correctly specified. Building on a recent work that introduced a scaling\nparameter for constructing valid credible regions from posterior estimate, our\nstudy explores the advantages of incorporating a temperature parameter into\nBayesian GNNs within CP framework. We empirically demonstrate the existence of\ntemperatures that result in more efficient prediction sets. Furthermore, we\nconduct an analysis to identify the factors contributing to inefficiency and\noffer valuable insights into the relationship between CP performance and model\ncalibration.","terms":["cs.LG","stat.ML"]},{"titles":"Churn Prediction via Multimodal Fusion Learning:Integrating Customer Financial Literacy, Voice, and Behavioral Data","summaries":"In todays competitive landscape, businesses grapple with customer retention.\nChurn prediction models, although beneficial, often lack accuracy due to the\nreliance on a single data source. The intricate nature of human behavior and\nhigh dimensional customer data further complicate these efforts. To address\nthese concerns, this paper proposes a multimodal fusion learning model for\nidentifying customer churn risk levels in financial service providers. Our\nmultimodal approach integrates customer sentiments financial literacy (FL)\nlevel, and financial behavioral data, enabling more accurate and bias-free\nchurn prediction models. The proposed FL model utilizes a SMOGN COREG\nsupervised model to gauge customer FL levels from their financial data. The\nbaseline churn model applies an ensemble artificial neural network and\noversampling techniques to predict churn propensity in high-dimensional\nfinancial data. We also incorporate a speech emotion recognition model\nemploying a pre-trained CNN-VGG16 to recognize customer emotions based on\npitch, energy, and tone. To integrate these diverse features while retaining\nunique insights, we introduced late and hybrid fusion techniques that\ncomplementary boost coordinated multimodal co learning. Robust metrics were\nutilized to evaluate the proposed multimodal fusion model and hence the\napproach validity, including mean average precision and macro-averaged F1\nscore. Our novel approach demonstrates a marked improvement in churn\nprediction, achieving a test accuracy of 91.2%, a Mean Average Precision (MAP)\nscore of 66, and a Macro-Averaged F1 score of 54 through the proposed hybrid\nfusion learning technique compared with late fusion and baseline models.\nFurthermore, the analysis demonstrates a positive correlation between negative\nemotions, low FL scores, and high-risk customers.","terms":["cs.LG","cs.AI","cs.CE","cs.CV","cs.HC"]},{"titles":"Robust Non-parametric Knowledge-based Diffusion Least Mean Squares over Adaptive Networks","summaries":"The present study proposes incorporating non-parametric knowledge into the\ndiffusion least-mean-squares algorithm in the framework of a maximum a\nposteriori (MAP) estimation. The proposed algorithm leads to a robust\nestimation of an unknown parameter vector in a group of cooperative estimators.\nUtilizing kernel density estimation and buffering some intermediate\nestimations, the prior distribution and conditional likelihood of the\nparameters vector in each node are calculated. Pseudo Huber loss function is\nused for designing the likelihood function. Also, an error thresholding\nfunction is defined to reduce the computational overhead as well as more\nrelaxation against noise, which stops the update every time an error is less\nthan a predefined threshold. The performance of the proposed algorithm is\nexamined in the stationary and non-stationary scenarios in the presence of\nGaussian and non-Gaussian noise. Results show the robustness of the proposed\nalgorithm in the presence of different noise types.","terms":["cs.LG"]},{"titles":"Deep Ensembles Meets Quantile Regression: Uncertainty-aware Imputation for Time Series","summaries":"Multivariate time series are everywhere. Nevertheless, real-world time series\ndata often exhibit numerous missing values, which is the time series imputation\ntask. Although previous deep learning methods have been shown to be effective\nfor time series imputation, they are shown to produce overconfident\nimputations, which might be a potentially overlooked threat to the reliability\nof the intelligence system. Score-based diffusion method(i.e., CSDI) is\neffective for the time series imputation task but computationally expensive due\nto the nature of the generative diffusion model framework. In this paper, we\npropose a non-generative time series imputation method that produces accurate\nimputations with inherent uncertainty and meanwhile is computationally\nefficient. Specifically, we incorporate deep ensembles into quantile regression\nwith a shared model backbone and a series of quantile discrimination\nfunctions.This framework combines the merits of accurate uncertainty estimation\nof deep ensembles and quantile regression and above all, the shared model\nbackbone tremendously reduces most of the computation overhead of the multiple\nensembles. We examine the performance of the proposed method on two real-world\ndatasets: air quality and health-care datasets and conduct extensive\nexperiments to show that our method excels at making deterministic and\nprobabilistic predictions. Compared with the score-based diffusion method:\nCSDI, we can obtain comparable forecasting results and is better when more data\nis missing. Furthermore, as a non-generative model compared with CSDI, the\nproposed method consumes a much smaller computation overhead, yielding much\nfaster training speed and fewer model parameters.","terms":["cs.LG","stat.ML"]},{"titles":"Ensemble Machine Learning Model Trained on a New Synthesized Dataset Generalizes Well for Stress Prediction Using Wearable Devices","summaries":"Introduction. We investigate the generalization ability of models built on\ndatasets containing a small number of subjects, recorded in single study\nprotocols. Next, we propose and evaluate methods combining these datasets into\na single, large dataset. Finally, we propose and evaluate the use of ensemble\ntechniques by combining gradient boosting with an artificial neural network to\nmeasure predictive power on new, unseen data.\n  Methods. Sensor biomarker data from six public datasets were utilized in this\nstudy. To test model generalization, we developed a gradient boosting model\ntrained on one dataset (SWELL), and tested its predictive power on two datasets\npreviously used in other studies (WESAD, NEURO). Next, we merged four small\ndatasets, i.e. (SWELL, NEURO, WESAD, UBFC-Phys), to provide a combined total of\n99 subjects,. In addition, we utilized random sampling combined with another\ndataset (EXAM) to build a larger training dataset consisting of 200 synthesized\nsubjects,. Finally, we developed an ensemble model that combines our gradient\nboosting model with an artificial neural network, and tested it on two\nadditional, unseen publicly available stress datasets (WESAD and Toadstool).\n  Results. Our method delivers a robust stress measurement system capable of\nachieving 85% predictive accuracy on new, unseen validation data, achieving a\n25% performance improvement over single models trained on small datasets.\n  Conclusion. Models trained on small, single study protocol datasets do not\ngeneralize well for use on new, unseen data and lack statistical power.\nMa-chine learning models trained on a dataset containing a larger number of\nvaried study subjects capture physiological variance better, resulting in more\nrobust stress detection.","terms":["cs.LG","cs.AI","eess.SP","I.2.0"]},{"titles":"Continuous Convolutional Neural Networks for Disruption Prediction in Nuclear Fusion Plasmas","summaries":"Grid decarbonization for climate change requires dispatchable carbon-free\nenergy like nuclear fusion. The tokamak concept offers a promising path for\nfusion, but one of the foremost challenges in implementation is the occurrence\nof energetic plasma disruptions. In this study, we delve into Machine Learning\napproaches to predict plasma state outcomes. Our contributions are twofold: (1)\nWe present a novel application of Continuous Convolutional Neural Networks for\ndisruption prediction and (2) We examine the advantages and disadvantages of\ncontinuous models over discrete models for disruption prediction by comparing\nour model with the previous, discrete state of the art, and show that\ncontinuous models offer significantly better performance (Area Under the\nReceiver Operating Characteristic Curve = 0.974 v.s. 0.799) with fewer\nparameters","terms":["cs.LG","physics.plasm-ph"]},{"titles":"Stable Messenger: Steganography for Message-Concealed Image Generation","summaries":"In the ever-expanding digital landscape, safeguarding sensitive information\nremains paramount. This paper delves deep into digital protection, specifically\nfocusing on steganography. While prior research predominantly fixated on\nindividual bit decoding, we address this limitation by introducing ``message\naccuracy'', a novel metric evaluating the entirety of decoded messages for a\nmore holistic evaluation. In addition, we propose an adaptive universal loss\ntailored to enhance message accuracy, named Log-Sum-Exponential (LSE) loss,\nthereby significantly improving the message accuracy of recent approaches.\nFurthermore, we also introduce a new latent-aware encoding technique in our\nframework named \\Approach, harnessing pretrained Stable Diffusion for advanced\nsteganographic image generation, giving rise to a better trade-off between\nimage quality and message recovery. Throughout experimental results, we have\ndemonstrated the superior performance of the new LSE loss and latent-aware\nencoding technique. This comprehensive approach marks a significant step in\nevolving evaluation metrics, refining loss functions, and innovating image\nconcealment techniques, aiming for more robust and dependable information\nprotection.","terms":["cs.CV"]},{"titles":"Deeper into Self-Supervised Monocular Indoor Depth Estimation","summaries":"Monocular depth estimation using Convolutional Neural Networks (CNNs) has\nshown impressive performance in outdoor driving scenes. However,\nself-supervised learning of indoor depth from monocular sequences is quite\nchallenging for researchers because of the following two main reasons. One is\nthe large areas of low-texture regions and the other is the complex ego-motion\non indoor training datasets. In this work, our proposed method, named\nIndoorDepth, consists of two innovations. In particular, we first propose a\nnovel photometric loss with improved structural similarity (SSIM) function to\ntackle the challenge from low-texture regions. Moreover, in order to further\nmitigate the issue of inaccurate ego-motion prediction, multiple photometric\nlosses at different stages are used to train a deeper pose network with two\nresidual pose blocks. Subsequent ablation study can validate the effectiveness\nof each new idea. Experiments on the NYUv2 benchmark demonstrate that our\nIndoorDepth outperforms the previous state-of-the-art methods by a large\nmargin. In addition, we also validate the generalization ability of our method\non ScanNet dataset. Code is availabe at https:\/\/github.com\/fcntes\/IndoorDepth.","terms":["cs.CV"]},{"titles":"Edge-aware Hard Clustering Graph Pooling for Brain Imaging","summaries":"Graph Convolutional Networks (GCNs) can capture non-Euclidean spatial\ndependence between different brain regions. The graph pooling operator, a\ncrucial element of GCNs, enhances the representation learning capability and\nfacilitates the acquisition of abnormal brain maps. However, most existing\nresearch designs graph pooling operators solely from the perspective of nodes\nwhile disregarding the original edge features. This confines graph pooling\napplication scenarios and diminishes its ability to capture critical\nsubstructures. In this paper, we propose a novel edge-aware hard clustering\ngraph pool (EHCPool), which is tailored to dominant edge features and redefines\nthe clustering process. EHCPool initially introduced the 'Edge-to-Node' score\ncriterion which utilized edge information to evaluate the significance of\nnodes. An innovative Iteration n-top strategy was then developed, guided by\nedge scores, to adaptively learn sparse hard clustering assignments for graphs.\nAdditionally, a N-E Aggregation strategy is designed to aggregate node and edge\nfeatures in each independent subgraph. Extensive experiments on the multi-site\npublic datasets demonstrate the superiority and robustness of the proposed\nmodel. More notably, EHCPool has the potential to probe different types of\ndysfunctional brain networks from a data-driven perspective. Method code:\nhttps:\/\/github.com\/swfen\/EHCPool","terms":["cs.CV","cs.GR"]},{"titles":"Brain Decodes Deep Nets","summaries":"We developed a tool for visualizing and analyzing large pre-trained vision\nmodels by mapping them onto the brain, thus exposing their hidden inside. Our\ninnovation arises from a surprising usage of brain encoding: predicting brain\nfMRI measurements in response to images. We report two findings. First,\nexplicit mapping between the brain and deep-network features across dimensions\nof space, layers, scales, and channels is crucial. This mapping method,\nFactorTopy, is plug-and-play for any deep-network; with it, one can paint a\npicture of the network onto the brain (literally!). Second, our visualization\nshows how different training methods matter: they lead to remarkable\ndifferences in hierarchical organization and scaling behavior, growing with\nmore data or network capacity. It also provides insight into finetuning: how\npre-trained models change when adapting to small datasets. Our method is\npractical: only 3K images are enough to learn a network-to-brain mapping.","terms":["cs.CV"]},{"titles":"Low-Precision Mixed-Computation Models for Inference on Edge","summaries":"This paper presents a mixed-computation neural network processing approach\nfor edge applications that incorporates low-precision (low-width) Posit and\nlow-precision fixed point (FixP) number systems. This mixed-computation\napproach employs 4-bit Posit (Posit4), which has higher precision around zero,\nfor representing weights with high sensitivity, while it uses 4-bit FixP\n(FixP4) for representing other weights. A heuristic for analyzing the\nimportance and the quantization error of the weights is presented to assign the\nproper number system to different weights. Additionally, a gradient\napproximation for Posit representation is introduced to improve the quality of\nweight updates in the backpropagation process. Due to the high energy\nconsumption of the fully Posit-based computations, neural network operations\nare carried out in FixP or Posit\/FixP. An efficient hardware implementation of\na MAC operation with a first Posit operand and FixP for a second operand and\naccumulator is presented. The efficacy of the proposed low-precision\nmixed-computation approach is extensively assessed on vision and language\nmodels. The results show that, on average, the accuracy of the\nmixed-computation is about 1.5% higher than that of FixP with a cost of 0.19%\nenergy overhead.","terms":["cs.LG","cs.AI"]},{"titles":"Learning to Compose SuperWeights for Neural Parameter Allocation Search","summaries":"Neural parameter allocation search (NPAS) automates parameter sharing by\nobtaining weights for a network given an arbitrary, fixed parameter budget.\nPrior work has two major drawbacks we aim to address. First, there is a\ndisconnect in the sharing pattern between the search and training steps, where\nweights are warped for layers of different sizes during the search to measure\nsimilarity, but not during training, resulting in reduced performance. To\naddress this, we generate layer weights by learning to compose sets of\nSuperWeights, which represent a group of trainable parameters. These\nSuperWeights are created to be large enough so they can be used to represent\nany layer in the network, but small enough that they are computationally\nefficient. The second drawback we address is the method of measuring similarity\nbetween shared parameters. Whereas prior work compared the weights themselves,\nwe argue this does not take into account the amount of conflict between the\nshared weights. Instead, we use gradient information to identify layers with\nshared weights that wish to diverge from each other. We demonstrate that our\nSuperWeight Networks consistently boost performance over the state-of-the-art\non the ImageNet and CIFAR datasets in the NPAS setting. We further show that\nour approach can generate parameters for many network architectures using the\nsame set of weights. This enables us to support tasks like efficient ensembling\nand anytime prediction, outperforming fully-parameterized ensembles with 17%\nfewer parameters.","terms":["cs.CV"]},{"titles":"Distributed Reinforcement Learning for Molecular Design: Antioxidant case","summaries":"Deep reinforcement learning has successfully been applied for molecular\ndiscovery as shown by the Molecule Deep Q-network (MolDQN) algorithm. This\nalgorithm has challenges when applied to optimizing new molecules: training\nsuch a model is limited in terms of scalability to larger datasets and the\ntrained model cannot be generalized to different molecules in the same dataset.\nIn this paper, a distributed reinforcement learning algorithm for antioxidants,\ncalled DA-MolDQN is proposed to address these problems. State-of-the-art bond\ndissociation energy (BDE) and ionization potential (IP) predictors are\nintegrated into DA-MolDQN, which are critical chemical properties while\noptimizing antioxidants. Training time is reduced by algorithmic improvements\nfor molecular modifications. The algorithm is distributed, scalable for up to\n512 molecules, and generalizes the model to a diverse set of molecules. The\nproposed models are trained with a proprietary antioxidant dataset. The results\nhave been reproduced with both proprietary and public datasets. The proposed\nmolecules have been validated with DFT simulations and a subset of them\nconfirmed in public \"unseen\" datasets. In summary, DA-MolDQN is up to 100x\nfaster than previous algorithms and can discover new optimized molecules from\nproprietary and public antioxidants.","terms":["cs.LG","cs.DC","q-bio.BM"]},{"titles":"Microscale 3-D Capacitance Tomography with a CMOS Sensor Array","summaries":"Electrical capacitance tomography (ECT) is a nonoptical imaging technique in\nwhich a map of the interior permittivity of a volume is estimated by making\ncapacitance measurements at its boundary and solving an inverse problem. While\nprevious ECT demonstrations have often been at centimeter scales, ECT is not\nlimited to macroscopic systems. In this paper, we demonstrate ECT imaging of\npolymer microspheres and bacterial biofilms using a CMOS microelectrode array,\nachieving spatial resolution of 10 microns. Additionally, we propose a deep\nlearning architecture and an improved multi-objective training scheme for\nreconstructing out-of-plane permittivity maps from the sensor measurements.\nExperimental results show that the proposed approach is able to resolve\nmicroscopic 3-D structures, achieving 91.5% prediction accuracy on the\nmicrosphere dataset and 82.7% on the biofilm dataset, including an average of\n4.6% improvement over baseline computational methods.","terms":["cs.CV"]},{"titles":"A Review and A Robust Framework of Data-Efficient 3D Scene Parsing with Traditional\/Learned 3D Descriptors","summaries":"Existing state-of-the-art 3D point cloud understanding methods merely perform\nwell in a fully supervised manner. To the best of our knowledge, there exists\nno unified framework that simultaneously solves the downstream high-level\nunderstanding tasks including both segmentation and detection, especially when\nlabels are extremely limited. This work presents a general and simple framework\nto tackle point cloud understanding when labels are limited. The first\ncontribution is that we have done extensive methodology comparisons of\ntraditional and learned 3D descriptors for the task of weakly supervised 3D\nscene understanding, and validated that our adapted traditional PFH-based 3D\ndescriptors show excellent generalization ability across different domains. The\nsecond contribution is that we proposed a learning-based region merging\nstrategy based on the affinity provided by both the traditional\/learned 3D\ndescriptors and learned semantics. The merging process takes both low-level\ngeometric and high-level semantic feature correlations into consideration.\nExperimental results demonstrate that our framework has the best performance\namong the three most important weakly supervised point clouds understanding\ntasks including semantic segmentation, instance segmentation, and object\ndetection even when very limited number of points are labeled. Our method,\ntermed Region Merging 3D (RM3D), has superior performance on ScanNet\ndata-efficient learning online benchmarks and other four large-scale 3D\nunderstanding benchmarks under various experimental settings, outperforming\ncurrent arts by a margin for various 3D understanding tasks without complicated\nlearning strategies such as active learning.","terms":["cs.CV","cs.RO"]},{"titles":"A Data-efficient Framework for Robotics Large-scale LiDAR Scene Parsing","summaries":"Existing state-of-the-art 3D point clouds understanding methods only perform\nwell in a fully supervised manner. To the best of our knowledge, there exists\nno unified framework which simultaneously solves the downstream high-level\nunderstanding tasks, especially when labels are extremely limited. This work\npresents a general and simple framework to tackle point clouds understanding\nwhen labels are limited. We propose a novel unsupervised region expansion based\nclustering method for generating clusters. More importantly, we innovatively\npropose to learn to merge the over-divided clusters based on the local\nlow-level geometric property similarities and the learned high-level feature\nsimilarities supervised by weak labels. Hence, the true weak labels guide\npseudo labels merging taking both geometric and semantic feature correlations\ninto consideration. Finally, the self-supervised reconstruction and data\naugmentation optimization modules are proposed to guide the propagation of\nlabels among semantically similar points within a scene. Experimental Results\ndemonstrate that our framework has the best performance among the three most\nimportant weakly supervised point clouds understanding tasks including semantic\nsegmentation, instance segmentation, and object detection even when limited\npoints are labeled, under the data-efficient settings for the large-scale 3D\nsemantic scene parsing. The developed techniques have postentials to be applied\nto downstream tasks for better representations in robotic manipulation and\nrobotic autonomous navigation. Codes and models are publicly available at:\nhttps:\/\/github.com\/KangchengLiu.","terms":["cs.CV","cs.RO"]},{"titles":"TIBET: Identifying and Evaluating Biases in Text-to-Image Generative Models","summaries":"Text-to-Image (TTI) generative models have shown great progress in the past\nfew years in terms of their ability to generate complex and high-quality\nimagery. At the same time, these models have been shown to suffer from harmful\nbiases, including exaggerated societal biases (e.g., gender, ethnicity), as\nwell as incidental correlations that limit such model's ability to generate\nmore diverse imagery. In this paper, we propose a general approach to study and\nquantify a broad spectrum of biases, for any TTI model and for any prompt,\nusing counterfactual reasoning. Unlike other works that evaluate generated\nimages on a predefined set of bias axes, our approach automatically identifies\npotential biases that might be relevant to the given prompt, and measures those\nbiases. In addition, our paper extends quantitative scores with post-hoc\nexplanations in terms of semantic concepts in the images generated. We show\nthat our method is uniquely capable of explaining complex multi-dimensional\nbiases through semantic concepts, as well as the intersectionality between\ndifferent biases for any given prompt. We perform extensive user studies to\nillustrate that the results of our method and analysis are consistent with\nhuman judgements.","terms":["cs.CV","cs.CY"]},{"titles":"Rethinking PGD Attack: Is Sign Function Necessary?","summaries":"Neural networks have demonstrated success in various domains, yet their\nperformance can be significantly degraded by even a small input perturbation.\nConsequently, the construction of such perturbations, known as adversarial\nattacks, has gained significant attention, many of which fall within\n\"white-box\" scenarios where we have full access to the neural network. Existing\nattack algorithms, such as the projected gradient descent (PGD), commonly take\nthe sign function on the raw gradient before updating adversarial inputs,\nthereby neglecting gradient magnitude information. In this paper, we present a\ntheoretical analysis of how such sign-based update algorithm influences\nstep-wise attack performance, as well as its caveat. We also interpret why\nprevious attempts of directly using raw gradients failed. Based on that, we\nfurther propose a new raw gradient descent (RGD) algorithm that eliminates the\nuse of sign. Specifically, we convert the constrained optimization problem into\nan unconstrained one, by introducing a new hidden variable of non-clipped\nperturbation that can move beyond the constraint. The effectiveness of the\nproposed RGD algorithm has been demonstrated extensively in experiments,\noutperforming PGD and other competitors in various settings, without incurring\nany additional computational overhead. The codes is available in\nhttps:\/\/github.com\/JunjieYang97\/RGD.","terms":["cs.LG","cs.CR","stat.ML"]},{"titles":"Meta ControlNet: Enhancing Task Adaptation via Meta Learning","summaries":"Diffusion-based image synthesis has attracted extensive attention recently.\nIn particular, ControlNet that uses image-based prompts exhibits powerful\ncapability in image tasks such as canny edge detection and generates images\nwell aligned with these prompts. However, vanilla ControlNet generally requires\nextensive training of around 5000 steps to achieve a desirable control for a\nsingle task. Recent context-learning approaches have improved its adaptability,\nbut mainly for edge-based tasks, and rely on paired examples. Thus, two\nimportant open issues are yet to be addressed to reach the full potential of\nControlNet: (i) zero-shot control for certain tasks and (ii) faster adaptation\nfor non-edge-based tasks. In this paper, we introduce a novel Meta ControlNet\nmethod, which adopts the task-agnostic meta learning technique and features a\nnew layer freezing design. Meta ControlNet significantly reduces learning steps\nto attain control ability from 5000 to 1000. Further, Meta ControlNet exhibits\ndirect zero-shot adaptability in edge-based tasks without any finetuning, and\nachieves control within only 100 finetuning steps in more complex non-edge\ntasks such as Human Pose, outperforming all existing methods. The codes is\navailable in https:\/\/github.com\/JunjieYang97\/Meta-ControlNet.","terms":["cs.CV","cs.LG"]},{"titles":"MovieChat: From Dense Token to Sparse Memory for Long Video Understanding","summaries":"Recently, integrating video foundation models and large language models to\nbuild a video understanding system can overcome the limitations of specific\npre-defined vision tasks. Yet, existing systems can only handle videos with\nvery few frames. For long videos, the computation complexity, memory cost, and\nlong-term temporal connection impose additional challenges. Taking advantage of\nthe Atkinson-Shiffrin memory model, with tokens in Transformers being employed\nas the carriers of memory in combination with our specially designed memory\nmechanism, we propose the MovieChat to overcome these challenges. MovieChat\nachieves state-of-the-art performance in long video understanding, along with\nthe released MovieChat-1K benchmark with 1K long video and 14K manual\nannotations for validation of the effectiveness of our method.","terms":["cs.CV"]},{"titles":"TranSegPGD: Improving Transferability of Adversarial Examples on Semantic Segmentation","summaries":"Transferability of adversarial examples on image classification has been\nsystematically explored, which generates adversarial examples in black-box\nmode. However, the transferability of adversarial examples on semantic\nsegmentation has been largely overlooked. In this paper, we propose an\neffective two-stage adversarial attack strategy to improve the transferability\nof adversarial examples on semantic segmentation, dubbed TranSegPGD.\nSpecifically, at the first stage, every pixel in an input image is divided into\ndifferent branches based on its adversarial property. Different branches are\nassigned different weights for optimization to improve the adversarial\nperformance of all pixels.We assign high weights to the loss of the\nhard-to-attack pixels to misclassify all pixels. At the second stage, the\npixels are divided into different branches based on their transferable property\nwhich is dependent on Kullback-Leibler divergence. Different branches are\nassigned different weights for optimization to improve the transferability of\nthe adversarial examples. We assign high weights to the loss of the\nhigh-transferability pixels to improve the transferability of adversarial\nexamples. Extensive experiments with various segmentation models are conducted\non PASCAL VOC 2012 and Cityscapes datasets to demonstrate the effectiveness of\nthe proposed method. The proposed adversarial attack method can achieve\nstate-of-the-art performance.","terms":["cs.CV"]},{"titles":"Feature Likelihood Score: Evaluating the Generalization of Generative Models Using Samples","summaries":"The past few years have seen impressive progress in the development of deep\ngenerative models capable of producing high-dimensional, complex, and\nphoto-realistic data. However, current methods for evaluating such models\nremain incomplete: standard likelihood-based metrics do not always apply and\nrarely correlate with perceptual fidelity, while sample-based metrics, such as\nFID, are insensitive to overfitting, i.e., inability to generalize beyond the\ntraining set. To address these limitations, we propose a new metric called the\nFeature Likelihood Score (FLS), a parametric sample-based score that uses\ndensity estimation to provide a comprehensive trichotomic evaluation accounting\nfor novelty (i.e., different from the training samples), fidelity, and\ndiversity of generated samples. We empirically demonstrate the ability of FLS\nto identify specific overfitting problem cases, where previously proposed\nmetrics fail. We also extensively evaluate FLS on various image datasets and\nmodel classes, demonstrating its ability to match intuitions of previous\nmetrics like FID while offering a more comprehensive evaluation of generative\nmodels. Code is available at https:\/\/github.com\/marcojira\/fls.","terms":["cs.LG","cs.CV"]},{"titles":"TransNAS-TSAD: Harnessing Transformers for Multi-Objective Neural Architecture Search in Time Series Anomaly Detection","summaries":"The surge in real-time data collection across various industries has\nunderscored the need for advanced anomaly detection in both univariate and\nmultivariate time series data. Traditional methods, while comprehensive, often\nstruggle to capture the complex interdependencies in such data. This paper\nintroduces TransNAS-TSAD, a novel framework that synergizes transformer\narchitecture with neural architecture search (NAS), enhanced through NSGA-II\nalgorithm optimization. This innovative approach effectively tackles the\ncomplexities of both univariate and multivariate time series, balancing\ncomputational efficiency with detection accuracy. Our evaluation reveals that\nTransNAS-TSAD surpasses conventional anomaly detection models, demonstrating\nmarked improvements in diverse data scenarios. We also propose the\nEfficiency-Accuracy-Complexity Score (EACS) as a new metric for assessing model\nperformance, emphasizing the crucial balance between accuracy and computational\nresources. TransNAS-TSAD sets a new benchmark in time series anomaly detection,\noffering a versatile, efficient solution for complex real-world applications.\nThis research paves the way for future developments in the field, highlighting\nits potential in a wide range of industry applications.","terms":["cs.LG","cs.NE"]},{"titles":"Parkinson's Disease Detection through Vocal Biomarkers and Advanced Machine Learning Algorithms","summaries":"Parkinson's disease (PD) is a prevalent neurodegenerative disorder known for\nits impact on motor neurons, causing symptoms like tremors, stiffness, and gait\ndifficulties. This study explores the potential of vocal feature alterations in\nPD patients as a means of early disease prediction. This research aims to\npredict the onset of Parkinson's disease. Utilizing a variety of advanced\nmachine-learning algorithms, including XGBoost, LightGBM, Bagging, AdaBoost,\nand Support Vector Machine, among others, the study evaluates the predictive\nperformance of these models using metrics such as accuracy, area under the\ncurve (AUC), sensitivity, and specificity. The findings of this comprehensive\nanalysis highlight LightGBM as the most effective model, achieving an\nimpressive accuracy rate of 96% alongside a matching AUC of 96%. LightGBM\nexhibited a remarkable sensitivity of 100% and specificity of 94.43%,\nsurpassing other machine learning algorithms in accuracy and AUC scores. Given\nthe complexities of Parkinson's disease and its challenges in early diagnosis,\nthis study underscores the significance of leveraging vocal biomarkers coupled\nwith advanced machine-learning techniques for precise and timely PD detection.","terms":["cs.LG","cs.SD","eess.AS"]},{"titles":"Large Scale Masked Autoencoding for Reducing Label Requirements on SAR Data","summaries":"Satellite-based remote sensing is instrumental in the monitoring and\nmitigation of the effects of anthropogenic climate change. Large scale, high\nresolution data derived from these sensors can be used to inform intervention\nand policy decision making, but the timeliness and accuracy of these\ninterventions is limited by use of optical data, which cannot operate at night\nand is affected by adverse weather conditions. Synthetic Aperture Radar (SAR)\noffers a robust alternative to optical data, but its associated complexities\nlimit the scope of labelled data generation for traditional deep learning. In\nthis work, we apply a self-supervised pretraining scheme, masked autoencoding,\nto SAR amplitude data covering 8.7\\% of the Earth's land surface area, and tune\nthe pretrained weights on two downstream tasks crucial to monitoring climate\nchange - vegetation cover prediction and land cover classification. We show\nthat the use of this pretraining scheme reduces labelling requirements for the\ndownstream tasks by more than an order of magnitude, and that this pretraining\ngeneralises geographically, with the performance gain increasing when tuned\ndownstream on regions outside the pretraining set. Our findings significantly\nadvance climate change mitigation by facilitating the development of task and\nregion-specific SAR models, allowing local communities and organizations to\ndeploy tailored solutions for rapid, accurate monitoring of climate change\neffects.","terms":["cs.CV","eess.IV","I.4.8; I.5"]},{"titles":"Fluid Viscosity Prediction Leveraging Computer Vision and Robot Interaction","summaries":"Accurately determining fluid viscosity is crucial for various industrial and\nscientific applications. Traditional methods of viscosity measurement, though\nreliable, often require manual intervention and cannot easily adapt to\nreal-time monitoring. With advancements in machine learning and computer\nvision, this work explores the feasibility of predicting fluid viscosity by\nanalyzing fluid oscillations captured in video data. The pipeline employs a 3D\nconvolutional autoencoder pretrained in a self-supervised manner to extract and\nlearn features from semantic segmentation masks of oscillating fluids. Then,\nthe latent representations of the input data, produced from the pretrained\nautoencoder, is processed with a distinct inference head to infer either the\nfluid category (classification) or the fluid viscosity (regression) in a\ntime-resolved manner. When the latent representations generated by the\npretrained autoencoder are used for classification, the system achieves a 97.1%\naccuracy across a total of 4,140 test datapoints. Similarly, for regression\ntasks, employing an additional fully-connected network as a regression head\nallows the pipeline to achieve a mean absolute error of 0.258 over 4,416 test\ndatapoints. This study represents an innovative contribution to both fluid\ncharacterization and the evolving landscape of Artificial Intelligence,\ndemonstrating the potential of deep learning in achieving near real-time\nviscosity estimation and addressing practical challenges in fluid dynamics\nthrough the analysis of video data capturing oscillating fluid dynamics.","terms":["cs.LG"]},{"titles":"Fewshot learning on global multimodal embeddings for earth observation tasks","summaries":"In this work we pretrain a CLIP\/ViT based model using three different\nmodalities of satellite imagery across five AOIs covering over ~10\\% of Earth's\ntotal landmass, namely Sentinel 2 RGB optical imagery, Sentinel 1 SAR radar\namplitude and interferometric coherence. This model uses $\\sim 250$ M\nparameters. Then, we use the embeddings produced for each modality with a\nclassical machine learning method to attempt different downstream tasks for\nearth observation related to vegetation, built up surface, croplands and\npermanent water. We consistently show how we reduce the need for labeled data\nby 99\\%, so that with ~200-500 randomly selected labeled examples (around\n4K-10K km$^2$) we reach performance levels analogous to those achieved with the\nfull labeled datasets (about 150K image chips or 3M km$^2$ in each area of\ninterest - AOI) on all modalities, AOIs and downstream tasks. This leads us to\nthink that the model has captured significant earth features useful in a wide\nvariety of scenarios. To enhance our model's usability in practice, its\narchitecture allows inference in contexts with missing modalities and even\nmissing channels within each modality. Additionally, we visually show that this\nembedding space, obtained with no labels, is sensible to the different earth\nfeatures represented by the labelled datasets we selected.","terms":["cs.CV","I.4.8; I.5"]},{"titles":"Optimal Scalarizations for Sublinear Hypervolume Regret","summaries":"Scalarization is a general technique that can be deployed in any\nmultiobjective setting to reduce multiple objectives into one, such as recently\nin RLHF for training reward models that align human preferences. Yet some have\ndismissed this classical approach because linear scalarizations are known to\nmiss concave regions of the Pareto frontier. To that end, we aim to find simple\nnon-linear scalarizations that can explore a diverse set of $k$ objectives on\nthe Pareto frontier, as measured by the dominated hypervolume. We show that\nhypervolume scalarizations with uniformly random weights are surprisingly\noptimal for provably minimizing the hypervolume regret, achieving an optimal\nsublinear regret bound of $O(T^{-1\/k})$, with matching lower bounds that\npreclude any algorithm from doing better asymptotically. As a theoretical case\nstudy, we consider the multiobjective stochastic linear bandits problem and\ndemonstrate that by exploiting the sublinear regret bounds of the hypervolume\nscalarizations, we can derive a novel non-Euclidean analysis that produces\nimproved hypervolume regret bounds of $\\tilde{O}( d T^{-1\/2} + T^{-1\/k})$. We\nsupport our theory with strong empirical performance of using simple\nhypervolume scalarizations that consistently outperforms both the linear and\nChebyshev scalarizations, as well as standard multiobjective algorithms in\nbayesian optimization, such as EHVI.","terms":["cs.LG","cs.DS","math.OC"]},{"titles":"Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery","summaries":"Self-supervised learning (SSL) models have recently demonstrated remarkable\nperformance across various tasks, including image segmentation. This study\ndelves into the emergent characteristics of the Self-Distillation with No\nLabels (DINO) algorithm and its application to Synthetic Aperture Radar (SAR)\nimagery. We pre-train a vision transformer (ViT)-based DINO model using\nunlabeled SAR data, and later fine-tune the model to predict high-resolution\nland cover maps. We rigorously evaluate the utility of attention maps generated\nby the ViT backbone and compare them with the model's token embedding space. We\nobserve a small improvement in model performance with pre-training compared to\ntraining from scratch and discuss the limitations and opportunities of SSL for\nremote sensing and land cover segmentation. Beyond small performance increases,\nwe show that ViT attention maps hold great intrinsic value for remote sensing,\nand could provide useful inputs to other algorithms. With this, our work lays\nthe groundwork for bigger and better SSL models for Earth Observation.","terms":["cs.CV","I.4.8; I.5"]},{"titles":"Exploring Generalisability of Self-Distillation with No Labels for SAR-Based Vegetation Prediction","summaries":"In this work we pre-train a DINO-ViT based model using two Synthetic Aperture\nRadar datasets (S1GRD or GSSIC) across three regions (China, Conus, Europe). We\nfine-tune the models on smaller labeled datasets to predict vegetation\npercentage, and empirically study the connection between the embedding space of\nthe models and their ability to generalize across diverse geographic regions\nand to unseen data. For S1GRD, embedding spaces of different regions are\nclearly separated, while GSSIC's overlaps. Positional patterns remain during\nfine-tuning, and greater distances in embeddings often result in higher errors\nfor unfamiliar regions. With this, our work increases our understanding of\ngeneralizability for self-supervised models applied to remote sensing.","terms":["cs.CV","I.4.8; I.5"]},{"titles":"DDxT: Deep Generative Transformer Models for Differential Diagnosis","summaries":"Differential Diagnosis (DDx) is the process of identifying the most likely\nmedical condition among the possible pathologies through the process of\nelimination based on evidence. An automated process that narrows a large set of\npathologies down to the most likely pathologies will be of great importance.\nThe primary prior works have relied on the Reinforcement Learning (RL) paradigm\nunder the intuition that it aligns better with how physicians perform DDx. In\nthis paper, we show that a generative approach trained with simpler supervised\nand self-supervised learning signals can achieve superior results on the\ncurrent benchmark. The proposed Transformer-based generative network, named\nDDxT, autoregressively produces a set of possible pathologies, i.e., DDx, and\npredicts the actual pathology using a neural network. Experiments are performed\nusing the DDXPlus dataset. In the case of DDx, the proposed network has\nachieved a mean accuracy of 99.82% and a mean F1 score of 0.9472. Additionally,\nmean accuracy reaches 99.98% with a mean F1 score of 0.9949 while predicting\nground truth pathology. The proposed DDxT outperformed the previous RL-based\napproaches by a big margin. Overall, the automated Transformer-based DDx\ngenerative model has the potential to become a useful tool for a physician in\ntimes of urgency.","terms":["cs.LG","cs.AI"]},{"titles":"Disentangling the Effects of Data Augmentation and Format Transform in Self-Supervised Learning of Image Representations","summaries":"Self-Supervised Learning (SSL) enables training performant models using\nlimited labeled data. One of the pillars underlying vision SSL is the use of\ndata augmentations\/perturbations of the input which do not significantly alter\nits semantic content. For audio and other temporal signals, augmentations are\ncommonly used alongside format transforms such as Fourier transforms or wavelet\ntransforms. Unlike augmentations, format transforms do not change the\ninformation contained in the data; rather, they express the same information in\ndifferent coordinates. In this paper, we study the effects of format transforms\nand augmentations both separately and together on vision SSL. We define\naugmentations in frequency space called Fourier Domain Augmentations (FDA) and\nshow that training SSL models on a combination of these and image augmentations\ncan improve the downstream classification accuracy by up to 1.3% on\nImageNet-1K. We also show improvements against SSL baselines in few-shot and\ntransfer learning setups using FDA. Surprisingly, we also observe that format\ntransforms can improve the quality of learned representations even without\naugmentations; however, the combination of the two techniques yields better\nquality.","terms":["cs.CV","cs.LG"]},{"titles":"A deep learning pipeline for cross-sectional and longitudinal multiview data integration","summaries":"Biomedical research now commonly integrates diverse data types or views from\nthe same individuals to better understand the pathobiology of complex diseases,\nbut the challenge lies in meaningfully integrating these diverse views.\nExisting methods often require the same type of data from all views\n(cross-sectional data only or longitudinal data only) or do not consider any\nclass outcome in the integration method, presenting limitations. To overcome\nthese limitations, we have developed a pipeline that harnesses the power of\nstatistical and deep learning methods to integrate cross-sectional and\nlongitudinal data from multiple sources. Additionally, it identifies key\nvariables contributing to the association between views and the separation\namong classes, providing deeper biological insights. This pipeline includes\nvariable selection\/ranking using linear and nonlinear methods, feature\nextraction using functional principal component analysis and Euler\ncharacteristics, and joint integration and classification using dense\nfeed-forward networks and recurrent neural networks. We applied this pipeline\nto cross-sectional and longitudinal multi-omics data (metagenomics,\ntranscriptomics, and metabolomics) from an inflammatory bowel disease (IBD)\nstudy and we identified microbial pathways, metabolites, and genes that\ndiscriminate by IBD status, providing information on the etiology of IBD. We\nconducted simulations to compare the two feature extraction methods. The\nproposed pipeline is available from the following GitHub repository:\nhttps:\/\/github.com\/lasandrall\/DeepIDA-GRU.","terms":["cs.LG","stat.AP","stat.CO","stat.ME","stat.ML"]}]